<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShiYaya</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-06-06T10:08:51.619Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ShiYaya</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据集收集过程--各论文汇总</title>
    <link href="http://yoursite.com/2020/06/06/%E6%95%B0%E6%8D%AE%E9%9B%86%E6%94%B6%E9%9B%86%E8%BF%87%E7%A8%8B-%E5%90%84%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/"/>
    <id>http://yoursite.com/2020/06/06/数据集收集过程-各论文汇总/</id>
    <published>2020-06-06T09:58:30.000Z</published>
    <updated>2020-06-06T10:08:51.619Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Stanford-Natural-Language-Inference"><a href="#Stanford-Natural-Language-Inference" class="headerlink" title=" Stanford Natural Language Inference "></a><font color="red"> Stanford Natural Language Inference </font></h3><h4 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a><strong>数据收集</strong></h4><p>从Flickr 上的human written captions 作为 <strong>premises</strong> 。</p><p>（1）AMT workers 手工写 与 premises 相对应label下的句子：（不提供图像的前提下）Asked AMT workers to supply hypotheses for each of our three labels entailment, neutral, and contradiction. 即得到<strong>hypotheses</strong> 。 (这种label, 称为 author label)</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfhjx642jrj30k90pk441.jpg" style="zoom:50%"><p>（2）基于得到的premises-hypotheses pair，再由4个 AMT workers进行评估，从三种label中，选择一个label，基于此，则对于每个premises-hypotheses pair，有5个label，基于共识，为该pair选择一个最终的label–称为 gold label.</p><h4 id="Data-validation"><a href="#Data-validation" class="headerlink" title="*Data validation  *"></a>*<em>Data validation  *</em></h4><p><strong>为了评估，基于上述的数据标注，得到的数据是否可靠</strong>。又从整个标注的数据中取了5%，再次由AMT workers做如（2）中的标注工作。看看前后两次标注的结果是否相关。这次AMT workers 选择的label 称为 Individual label 。得到如下的统计结果。可以看出Individual label 与之前的 gold label/ author’s label 有较高的一致性，即，之前的标注工作是可靠的。</p><ul><li><font color="red"><strong>yaya</strong> </font>个人觉得，这是一种马后炮的行为，没什么用，因为之前的标注，已经完成了，钱也花了。这种验证，即便是验证结果不好，那也没有什么修正措施。</li></ul><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfhl3kdkdtj30gj0na42d.jpg" style="zoom:50%"><h3 id="Visual-Entailment-Dataset"><a href="#Visual-Entailment-Dataset" class="headerlink" title=" Visual Entailment Dataset  "></a><font color="red"> Visual Entailment Dataset  </font></h3><h4 id="数据收集-1"><a href="#数据收集-1" class="headerlink" title="数据收集"></a>数据收集</h4><p>该数据集是在 Flickr 和 SNLI dataset 的一个简单集成。</p><p>任务是，给出一个image-text pair，希望model 预测该pair 的匹配程度[Entailment, Contradiction, Neutral]。</p><p>数据的收集：在 SNLI dataset 就是基于 Flickr30k image captions 构建的，</p><ul><li><strong>Entailment</strong> holds if there is enough evidence in image to conclude that text is true.</li><li><strong>Contradiction</strong> holds if there is enough evidence in image to conclude that text is false.</li><li><strong>Neutral</strong>, implying the evidence in image is insufficient to draw a conclusion about text.  </li></ul><h4 id="该文提出了几个构建数据集的准则"><a href="#该文提出了几个构建数据集的准则" class="headerlink" title="该文提出了几个构建数据集的准则"></a><strong>该文提出了几个构建数据集的准则</strong></h4><p>基于在SNLI, VQA-v1.0, VQA-v2.0, and CLEVR, 这个几个数据集上的经验， 这里提出了四个准则来开发一个新的数据集:</p><ul><li><strong>Structured set of real-world images.</strong> The dataset should be based on real-world images and the same image can be paired with different hypotheses to form different labels. </li><li><strong>Fine-grained.</strong> The dataset should enforce fine-grained reasoning about subtle changes in hypotheses that could lead to distinct labels. </li><li><strong>Sanitization.</strong> No instance overlapping across different dataset partitions. One image can only exist in a single partition.  </li><li><strong>Account for any bias.</strong> Measure the dataset bias and  provide baselines to serve as the performance lower bound for potential future evaluations.  该文中提出了一些单纯仅仅使用</li></ul><h3 id="WMT-Shared-Task"><a href="#WMT-Shared-Task" class="headerlink" title="WMT Shared Task  "></a><font color="red">WMT Shared Task  </font></h3><h4 id="用户界面"><a href="#用户界面" class="headerlink" title="用户界面"></a>用户界面</h4><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfijyrn8qsj30zw0gvwl7.jpg" style="zoom:50%"><h4 id="Human-judgement-quality-control"><a href="#Human-judgement-quality-control" class="headerlink" title="Human judgement quality control"></a>Human judgement quality control</h4><ul><li><p>每个标注者，每次HIT任务：给定100个 （reference+ candidate）pair, 针对给定的reference, 评估生成的candidate的好坏。</p></li><li><p>100个pair中有60个用于quality control，40个由participating systems 生成的翻译组成。</p><p>（1）这60个pair，是官方设计出来的，包括三类，repeat pairs (expecting a similar judgment), damage MT outputs/ bad reference (expecting significantly worse scores) and use references instead of MT outputs (expecting high scores). 因此仅仅会有20%的资源消耗：bad reference; good reference</p><p>Specifically，先从正常的MT system 中 得到30个 （reference, MT output）pair，如 table 5 中的 original system output， 然后1)对1-10对，进行重复，得到10对。2）对11-20对，将MT output搞破坏。或者是对reference caption搞破坏，得到10对。3）对21-30对，取corresponding reference–&gt; (reference_1, reference_2)，得到10对。</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gepxhtqcpgj311h052abz.jpg"><p>（2）within each 100-translation HIT， 每个articipating system<strong>等比例的贡献</strong>a（within each 100-translation HIT, the same proportion of translations are included from each participating system for that language pair.  ）这是为了确保每个参与的 系统含有近似的相同数量的评估。同时，这也从三个方面得到了公平性的评估：1）每有一个workers做一个HIT, 则就会为所有参与的系统增加human judgement。2）不会轻易受到worker个性差异的影响，因为每个worker都会给所有参与的系统进行评估。3）尽管DA判断是绝对的，但众所周知，判断者会根据观察到的总体翻译质量来“校准”他们使用量表的方式。 对于每个HIT（包括所有参与的系统），这种影响都是平均的。</p></li></ul><h4 id="Annotator-Agreement"><a href="#Annotator-Agreement" class="headerlink" title="Annotator Agreement"></a>Annotator Agreement</h4><p>（1）由于 bad reference pairs 的质量应该是显著偏低的，通过查看人类在这类pairs 上的评分是否也是显著偏低。来过滤掉可信赖度低的human assessors。</p><p>set（A, bad reference） 与  set（A, translatin_B）这两个集合上的人类评估，计算一个p-value， 若p-value&gt;0.05 则说明该human assessor的可信度低。</p><p>（2）对于 repeat pairs, 查看得到 repeat assessments的程度。</p><h4 id="Producing-the-Human-Ranking"><a href="#Producing-the-Human-Ranking" class="headerlink" title="Producing the Human Ranking"></a>Producing the Human Ranking</h4><ul><li><p>Standardized </p><p>为了消除不同的人类评估者的评分策略的差异，首先根据每个人类评估者的总体平均得分和标准差得分对翻译的人类评估得分进行<strong>标准化</strong>。</p></li></ul><h3 id="VIOLIN-Video-and-Language-Inference"><a href="#VIOLIN-Video-and-Language-Inference" class="headerlink" title="[VIOLIN] Video-and-Language Inference "></a><font color="red">[VIOLIN] Video-and-Language Inference </font></h3><p>yaya blog: <a href="https://shiyaya.github.io/2020/03/28/VIOLIN-A-Large-Scale-Dataset-for-Video-and-Language-Inference/" target="_blank" rel="noopener">https://shiyaya.github.io/2020/03/28/VIOLIN-A-Large-Scale-Dataset-for-Video-and-Language-Inference/</a></p><h4 id="数据集收集简介"><a href="#数据集收集简介" class="headerlink" title="数据集收集简介"></a>数据集收集简介</h4><p> 该任务是在给定 Subtitles  和 video 的情况下，推断一个statement 是否与 video 相符合[entailed (label 1) ，contradicts (label 0) ]。</p><p><strong>positive statements 的收集</strong>：给出 subtitles + video，然后annotators 写出与其想对应的 statements。</p><p><strong>negative statements 的收集</strong>：（1）要求annotators通过只更改positive statements 的几个单词或短语来编写negative statements。（2）进行<strong>对抗匹配</strong>：对于每个视频，从其他视频的陈述库中选择具有挑战性和令人困惑的陈述作为否定陈述。具体地，对于video_i/j 已经分别有其相对应的 positive statement H_i/j , 则通过查找与H_i 最相近的H_j 作为 negative statements。 对抗匹配的方式可以消除 human bias的影响。</p><h4 id="数据收集-Instruction"><a href="#数据收集-Instruction" class="headerlink" title="数据收集 Instruction"></a>数据收集 Instruction</h4><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfil6timz7j31c60van7q.jpg"><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfil6tix9ij313e150na4.jpg"><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfil6tj9shj30ts14mtl2.jpg"><h4 id="用户界面-1"><a href="#用户界面-1" class="headerlink" title="用户界面"></a>用户界面</h4><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfilt0g35qj316o128n4z.jpg" style="zoom:50%"><h3 id="VCR"><a href="#VCR" class="headerlink" title="VCR"></a><font color="red">VCR</font></h3><p>yaya blog: shiyaya.github.io/2020/05/16/From-Recognition-to-Cognition-Visual-Commonsense-Reasoning/</p><h4 id="用户界面-2"><a href="#用户界面-2" class="headerlink" title="用户界面"></a>用户界面</h4><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfioadgc3qj30r20pbn3e.jpg"><h4 id="Crowdsourcing-quality-data"><a href="#Crowdsourcing-quality-data" class="headerlink" title="Crowdsourcing quality data"></a>Crowdsourcing quality data</h4><p><strong>Automated quality checks</strong></p><p>在众包UI中加入了一些<strong>自动</strong>的检测，比如，workers 在写 question、answer、rationale 时，有单词数量的限制且必须要指定一个detection</p><p>*<em>Instructions  *</em></p><p>鼓励workers，编写的question，是比较high-level的（需要一定的推理步骤），同时不要编写一些general questions, 即不针对image 本身的那些问题。</p><p>同时为workers 提供了一些例子展示。</p><p>*<em>Qualification exam  *</em></p><p>再进行正式的标注之前，先做一个测试，验证该worker 具有标注VCR数据的能努力。</p><p>The qualification test included a mix of multiple-choice graded answers as well as a short written section, which was to provide a single question, answer, and rationale for an image.  </p><p>在完成这个质量测试之后，由发布该任务的requester(即VCR论文作者)来手工看，这个worker是否具有资格。</p><p><strong>Work verification</strong>  </p><p>查看 workers 编写的 question、answer、rationales是否符合要求。由于这项检查工作，工作量也很大，因此，将这种检查工作也当做另外一种 HIT 当做任务进行发布，由那些outstanding workers in previous annotation work 来完成这项检查工作, 每为 一位another worker完成了检查工作，将得到 $0.4</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Stanford-Natural-Language-Inference&quot;&gt;&lt;a href=&quot;#Stanford-Natural-Language-Inference&quot; class=&quot;headerlink&quot; title=&quot; Stanford Natural Lang
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>图像描述-评价指标-中用到的数据集汇总</title>
    <link href="http://yoursite.com/2020/06/04/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-%E4%B8%AD%E7%94%A8%E5%88%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B1%87%E6%80%BB/"/>
    <id>http://yoursite.com/2020/06/04/图像描述-评价指标-中用到的数据集汇总/</id>
    <published>2020-06-04T03:40:10.000Z</published>
    <updated>2020-06-04T10:27:32.777Z</updated>
    
    <content type="html"><![CDATA[<p>为了评估 提出的metric 与 human judgement的相关性，提出了一些数据集。这些数据集，包含image-text-human_score, 通过利用统计学分析 metric_evaluation 与 human_score 的相关性，来验证提出评价指标的合理性。</p><h1 id="Caption-level-Correlation"><a href="#Caption-level-Correlation" class="headerlink" title="Caption-level Correlation"></a>Caption-level Correlation</h1><h2 id="Flickr-8k-Dataset"><a href="#Flickr-8k-Dataset" class="headerlink" title="Flickr 8k Dataset"></a>Flickr 8k Dataset</h2><ul><li>website: <a href="http://academictorrents.com/details/9dea07ba660a722ae1008c4c8afdd303b6f6e53b" target="_blank" rel="noopener">http://academictorrents.com/details/9dea07ba660a722ae1008c4c8afdd303b6f6e53b</a></li><li>reference: </li></ul><p><font size="2"> <strong>数据集介绍</strong> </font></p><ul><li><p>Cite: Framing image description as a ranking task: Data, models and evaluation metrics.  </p></li><li><p>对于图像描述任务：该数据集包含8092张image。训练集6000张，验证集1000张，测试集1000张，，很奇怪。。数据加起来对不上。。每张image 都对应有人类标注的5个句子。</p></li><li><p>对于图像描述评价指标任务： </p><p><strong>测试集1000张</strong>，<font color="blue"><strong>通过image-text retrieval 算法</strong></font> 检索candidate caption，为每张图片从整个测试集的语料库上进行检索（检索的数量没有固定，像是根据检索结果阈值截取的）。由于是在整个测试集的预料库上进行检索，则，也有可能检索到自身image对应的groundtruth。</p><p>得到这些新的image-text pair，对于每个pair，再由<strong>三个人工</strong>去标注image与text的匹配程度( give a score from 1 (not related to the image content) to 4 (very related))。</p><p>则，构建了一个可以衡量metric 与 human judgement 相关性的一个数据集。</p></li></ul><p><font size="2"> <strong>使用 Note</strong> </font></p><ul><li>在TIGER [1] : <strong>Because 158 candidates are actual references of target images, we excluded these for further analysis。</strong> <font color="red">在TIGER 的实验设置中：若Flickr 8k数据集中检索到了本image对应的reference，则去掉该条检索。</font></li></ul><p><font size="2"> <strong>评估方式</strong></font></p><p><code>Kendall</code> and <code>Spearman</code> rank correlations reflect the similarity of the pairwise rankings whereas <code>Pearson’s</code> p captures the linear association between data points.</p><h2 id="Composite-Dataset"><a href="#Composite-Dataset" class="headerlink" title="Composite Dataset"></a>Composite Dataset</h2><p><font size="2"> <strong>数据集介绍</strong> </font></p><p>这个数据集是由三个数据集组成的。包括：testing captions for 2007 MS-COCO images, 997 Flickr 8k pictures, and 991 Flickr 30k images.  </p><p>每张图片对应3个candidate captions，包括1个human written reference和 2个machine generated。</p><p>这里总计有11,985 candidates, 标注与image 之间的相关性，from 1 (not relevant) to 5 (very relevant)。</p><p><font size="2"> <strong>评估方式</strong></font></p><p><code>Kendall</code> and <code>Spearman</code> rank correlations reflect the similarity of the pairwise rankings whereas <code>Pearson’s</code> p captures the linear association between data points.</p><h2 id="Pascal-50s-Dataset"><a href="#Pascal-50s-Dataset" class="headerlink" title="Pascal 50s Dataset"></a>Pascal 50s Dataset</h2><ul><li>website: <a href="http://vrama91.github.io/cider/" target="_blank" rel="noopener">http://vrama91.github.io/cider/</a></li></ul><p><font size="2"> <strong>数据集介绍</strong> </font></p><p>Cite: <code>CIDEr: Consensus-based Image Description Evaluation</code></p><p>从 UIUC PASCAL Sentence Dataset中提取1000张image，原数据集中，每个image配有5个human written sentence。</p><p>对于图像描述评价指标任务： </p><p>在以上基础上每个image 又由 AMT workers标注了50个captions。以此构成了pascal 50s 数据集。</p><p>不同于以上的两个数据集评估image-text 之间的匹配，该数据集考量candidate 与 reference之间的匹配。具体地：对于一个image，（1）使用48 of 50 human written caption as <strong>reference</strong>。（2）剩下的两个human written caption as <strong>candidate</strong>，同时也使用 machine generated caption as <strong>candidate</strong>，另外other image 的 human written caption通过检索的方式也可以当 这样candidate可以当做<strong>candidate</strong>。</p><p>基于此，构建三元组：（A, (B, C)）–(reference, (candidate_1, candidate_2)) 根据(B, C) 组合方式的不同，分为四类：HC，HI，HM，MM。（1）human–human correct pairs (HC), where we pick two human sentences describing the same image. （2） human–human incorrect pairs (HI), where one of the sentences is a human description for the image and the other is also a human sentence but describing some other image from the dataset picked at random. （3）human–machine (HM) pairs formed by pairing a human sentence describing an image with a machine generated sentence describing the same image. （4）machine–machine (MM) pairs, where we compare two machine generated sentences describing the same image</p><p>则，可以得到 1000image × 48reference(A) × 4(B,C) = 192000个三元组</p><p><strong>human judgement 的标注</strong> 对于任意给出的一个三元组(A, B, C)。A 是一个reference sentence, (B, C) 是两个candidate captions pair. 标注者被要求从B和C中选择一个与A最相似的句子。这样就可以收集到一个human judgements for each triplet. 如果B的投票对于C则认为B is winner.</p><p><font size="2"> <strong>使用 Note</strong> </font></p><ul><li>解读如下的表格的acuracy是如何计算的吧！首先在PASCAL-50s数据集里含有4种模式：HC、HI、HM、MM， 即对于（A,(B, C)）中的pair（B, C）含有四种模式。当前AMT workers对pair(B, C)已经有了排序，当proposed metric也对这些B，C sentences进行评分的时候，自然也会有一个对B，C的排序，即score高的sentence, 排序就在前。基于人类已经给了人工的标注排序，即获得了GT，那么就可以去评判 proposed metric 对该pair的评分是否正确。进而可以得到对该类HC/HI/HM/MM的准确率。其实也可以在整个数据集上进行测试得到一个准确率。</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gcdc0x4s91j30ex04uaas.jpg" alt="搜狗截图20200229160306.png"></p><p><font size="2"> <strong>评估方式</strong></font></p><p>Pairwise Classification Accuracy</p><h1 id="System-Level-Correlation"><a href="#System-Level-Correlation" class="headerlink" title="System-Level Correlation"></a>System-Level Correlation</h1><h2 id="the-2015-COCO-Captioning-Challenge-for-12-teams"><a href="#the-2015-COCO-Captioning-Challenge-for-12-teams" class="headerlink" title="the 2015 COCO Captioning Challenge for 12 teams"></a>the 2015 COCO Captioning Challenge for 12 teams</h2><p><font size="2"> <strong>数据集介绍</strong> </font></p><ul><li><p>Cite: The coco 2015 captioning challenge. <a href="http://mscoco.org/dataset/#captions-challenge2015" target="_blank" rel="noopener">http://mscoco.org/dataset/#captions-challenge2015</a>.  </p></li><li><p>use human judgements collected in the 2015 COCO Captioning Challenge for 12 teams who participated in this captioning challenge.</p></li><li><p>We report</p><ul><li><p>M1: Percentage of captions that are evaluated as better or equal to human caption,</p></li><li><p>M2: Percentage of captions that pass the Turing Test,</p></li><li><p>M3: Average correctness of the captions on a scale of 1-5 (incorrect - correct),</p></li><li><p>M4: Average amount of detail of the captions on a scale of 1-5 (lack of details - very detailed) and</p></li><li><p>M5: Percentage of captions that are similar to human description.</p></li><li><p>While M1 and M2 were used to rank the captioning models in the COCO challenge.   </p><p>M3, M4 and M5  are not used to rank image captioning models , but are intended for an ablation study to understand the various aspects of caption quality.  </p></li></ul></li></ul><p><font size="2"><strong>使用 Note</strong> </font></p><p>计算system-level correlation, （1）需要为每个 caption model 来计算一个metric score, 这个分数聚合了由该model 生成的所有的caption 的 metric socre。（2） 然后，该captio model 的aggregate metric score 与 system-level human assessments之间的相关性被计算。</p><p><font size="2"> <strong>评估方式</strong></font></p><ul><li>Compare proposed metric with others on the <strong>Pearson’s ρ correlation</strong> between all common metrics and human judgments collected in the 2015 COCO Captioning Challenge. </li></ul><h1 id="论文引用情况"><a href="#论文引用情况" class="headerlink" title="论文引用情况"></a>论文引用情况</h1><table><thead><tr><th></th><th>Caption-level Correlation</th><th></th><th></th><th>System-Level Correlation</th></tr></thead><tbody><tr><td></td><td>Flickr 8k</td><td>Composite</td><td>pascal-50s</td><td>2015 COCO Captioning Challenge</td></tr><tr><td>CIDEr</td><td></td><td></td><td>√</td><td></td></tr><tr><td>SPICE</td><td>√</td><td>√</td><td>√</td><td>√</td></tr><tr><td>(CVPR 2018) Learning to Evaluate Image Captioning</td><td>√</td><td></td><td></td><td>√</td></tr><tr><td>(EMNLP-IJCNLP 2019) REO-Relevance, Extraness, Omission A Fine-grained Evaluation for Image Captioning</td><td></td><td>√</td><td>√</td><td></td></tr><tr><td>(ACL 2019) VIFIDEL Evaluating the visual fidelity of image descriptions</td><td></td><td>√</td><td>√</td><td></td></tr><tr><td>(EMNLP2019) TIGEr Text-to-Image Grounding for Image Caption Evaluation</td><td>√</td><td>√</td><td>√</td><td></td></tr><tr><td>(IJCV)Learning-based Composite Metrics for Improved Caption Evaluation</td><td>√</td><td>√</td><td>√</td><td>√</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;为了评估 提出的metric 与 human judgement的相关性，提出了一些数据集。这些数据集，包含image-text-human_score, 通过利用统计学分析 metric_evaluation 与 human_score 的相关性，来验证提出评价指标的合理
      
    
    </summary>
    
      <category term="评价指标" scheme="http://yoursite.com/categories/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
    
      <category term="评价指标" scheme="http://yoursite.com/tags/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
  </entry>
  
  <entry>
    <title>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
    <link href="http://yoursite.com/2020/05/31/Sentence-BERT-Sentence-Embeddings-using-Siamese-BERT-Networks/"/>
    <id>http://yoursite.com/2020/05/31/Sentence-BERT-Sentence-Embeddings-using-Siamese-BERT-Networks/</id>
    <published>2020-05-31T09:22:03.000Z</published>
    <updated>2020-06-01T02:06:59.743Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>BERT 和 RoBERTa 在 sentence-pair regression tasks (eg: semantic textual similarity ) 上取得了非常不错的成绩，但是由于需要将两个句子都送入到网络中，这将造成计算量过载。举个例子，若在10000个句子中要找到最相似的对，则需要50 million的推理计算（需要计算一个上三角阵：（10000+1）*10000/2）, 使用BERT，则需要 65 hours。</p><p><strong>这可以看到the construction of BERT，不适于semantic similarity search、 像聚类这种无监督任务、information retrieval via semantic search等等。</strong></p><p><strong>因此本文：</strong> 本文提出了 sentence-BERT, 是建立在 预训练的BERT上的一种修改，使用siamese 和 triplet 网络结构来得到语义上有意义的sentence embeddings, 从而方便的计算cosine similarity.</p><h4 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h4><p>三种训练策略，现在只说到了一种，分类损失，，另外两种，是在哪些数据集上使用的？</p><h4 id="yaya-启发"><a href="#yaya-启发" class="headerlink" title="yaya 启发"></a>yaya 启发</h4><p>一、本文中提出的当前 BERT存在的缺陷，也正是 video-text retrieval ，这种多模态任务存在的缺陷。</p><p>二、We showed in (Reimers et al., 2016)[1] that Pearson correlation is badly suited for  STS. Instead, we compute the Spearman’s rank correlation between the cosine-similarity of the sentence embeddings and the gold labels  </p><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p>[1] Nils Reimers, Philip Beyer, and Iryna Gurevych. 2016.<br><strong>Task-Oriented Intrinsic Evaluation of Semantic Textual Similarity.</strong><br>In Proceedings of the 26th International Conference on Computational Linguistics (COLING), pages 87–96.      </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h4&gt;&lt;p&gt;BERT 和 RoBERTa 在 sentence-pair regress
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>阅读论文 tips</title>
    <link href="http://yoursite.com/2020/05/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87-tips/"/>
    <id>http://yoursite.com/2020/05/29/阅读论文-tips/</id>
    <published>2020-05-29T10:40:00.000Z</published>
    <updated>2020-05-29T10:54:35.243Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h4 id="快速阅读：划分结构层次"><a href="#快速阅读：划分结构层次" class="headerlink" title="快速阅读：划分结构层次"></a>快速阅读：划分结构层次</h4><p>对于快速阅读，一个小的技巧是图文浏览。因为一些好的论文必然是图文并茂，所以只要弄清楚论文中表格和图片的标题和注释，就能够获得这篇论文八、九成的信息。</p><p>读者在读论文的时候也应该要有逻辑，首先要清楚论文中的表达是否是我想要学习到的；其次，我能从论文中学到多少呢；最后，这篇论文的背景是什么——是什么样的背景让这篇论文变得重要和有趣。</p><h4 id="仔细阅读：批判思维"><a href="#仔细阅读：批判思维" class="headerlink" title="仔细阅读：批判思维"></a>仔细阅读：批判思维</h4><p>以评判性阅读开始，带着质疑的心态问问题。如果作者论文中声称解决了一个问题，那么你就要在心里问自己：<strong>论文是否正确、真正地解决了问题？</strong> <strong>作者论文中所用方法是否有局限性</strong>？如果<strong>所读的论文没有解决问题，那么我能解决么</strong>？我能采用<strong>比论文中更简单的方法解决么</strong>？所以，一旦进入仔细阅读的状态，要在读论文之前对自己说：这篇论文可能有问题，我要找出来。</p><h4 id="创造性阅读：积极思考"><a href="#创造性阅读：积极思考" class="headerlink" title="创造性阅读：积极思考"></a>创造性阅读：积极思考</h4><p>问自己：在我所读的论文中，作者有<strong>哪些点还没有想到</strong>？如果我现在做这项研究，我<strong>能做的新事情是什么</strong>？创造性的阅读需要<strong>把你所读的论文和其他相关的论文建立联系，从而产生一些新的想法</strong>，这些想法可以支撑你进行三个月到五个月的研究。</p><p>如果你真正想理解你所读的论文，那么就写一个摘要吧，最好做一个口头展示，这样你会发现，只有把东西写下来或者说出来才能真正深刻理解。如果你能做一个报告，那就更好了，因为做报告的时候，别人可以问你问题，这会强迫你理解所读的论文。</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gf9in4p85tj30f8088t92.jpg"><p>在做这个演讲之前，我曾经向我的同事、学生询问了关于论文阅读有哪些问题可以“问自己”，上面这张图片是一个总结，图片的上半部分是比较客观的问题，包括论文的核心观点是什么？主要的局限性是什么？代码和数据是不是可得的？论文的贡献是否有意义？论文中的实验是否足够好？</p><p>图片的下半部分是比较主观的问题，包括我错过了什么相关论文么？这对我的工作有何帮助么？这是一篇值得关注的论文么？这个研究领域的领头人是谁呢？其他的人对这篇论文有何看法呢？如果有机会见到作者，我应该问作者什么问题？</p><p>当你在阅读论文的时候如果能回答出上面列出的问题，我相信你会对你所读论文有非常深刻的理解。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h4 id=&quot;快速阅读：划分结构层次&quot;&gt;&lt;a href=&quot;#快速阅读：划分结构层次&quot; class=&quot;headerlink&quot; title=&quot;快速阅读：划分结构层次&quot;&gt;&lt;/a&gt;快速阅读：划分结构层次&lt;/h4&gt;&lt;p&gt;对于快速阅读，一个小的技巧是图文浏览。因为
      
    
    </summary>
    
      <category term="杂类" scheme="http://yoursite.com/categories/%E6%9D%82%E7%B1%BB/"/>
    
    
      <category term="杂类" scheme="http://yoursite.com/tags/%E6%9D%82%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>From Recognition to Cognition: Visual Commonsense Reasoning</title>
    <link href="http://yoursite.com/2020/05/16/From-Recognition-to-Cognition-Visual-Commonsense-Reasoning/"/>
    <id>http://yoursite.com/2020/05/16/From-Recognition-to-Cognition-Visual-Commonsense-Reasoning/</id>
    <published>2020-05-16T10:34:25.000Z</published>
    <updated>2020-05-19T06:49:00.292Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Visual-Commonsense-Reasoning-VCR"><a href="#Visual-Commonsense-Reasoning-VCR" class="headerlink" title="Visual Commonsense Reasoning (VCR)"></a>Visual Commonsense Reasoning (VCR)</h4><p>VCR: Given an image, a list of regions, and a question, a model must answer the question and provide a rationale explaining why its answer is right. </p><p>标注数据：为了避免生成式问题中评价指标的缺陷，本任务设计成 <strong>选择题</strong> 任务，即，提供一个image，一个question，多个answer，该任务要求从多个答案中选择一个正确的答案。对于correct answer：给定一张图片，要求AMT workers 写一个question，一个answer。对于wrong answer：使用adversarial matching 来获得其余的negative answer。</p><h4 id="The-Motivation-of-Adversarial-Matching"><a href="#The-Motivation-of-Adversarial-Matching" class="headerlink" title="The Motivation of Adversarial Matching"></a>The Motivation of Adversarial Matching</h4><p>在构建数据集时，常常存在两种挑战：</p><ul><li><p><strong>A crucial challenge</strong> in constructing a dataset of this complexity at this scale is how to avoid <strong>annotation artifacts</strong>. </p></li><li><p><strong>A recurring challenge</strong> in most recent QA datasets has been that human-written answers contain unexpected but distinct <strong>biases</strong> that models can easily exploit. 现实世界中的偏置</p></li></ul><p>通常，这些<strong>偏见</strong>非常明显，以至于模型无需看问题就可以选择正确的答案。</p><h4 id="Adversarial-Matching"><a href="#Adversarial-Matching" class="headerlink" title="Adversarial Matching"></a>Adversarial Matching</h4><p>negative answer的生成可以在correct answer上进行改造，但是这个过程非常耗钱，更甚，可能会引入annotation artifacts，subtle patterns that are by themselves highly predictive of the ‘correct’ or ‘incorrect’ label. 【1，2，3】</p><p>The key idea of Adversarial Matching is to <strong>recycle</strong> each correct answer for a question exactly three times — as a <strong>negative answer</strong> for three other questions.  这样每个answer 将会有1/4的机会是正例。这可以<strong>解决掉 answer-only bais 的问题</strong>，从而避免了模型总是选择 most generic answer. </p><p>在为每个image 选择negative answer时，希望<strong>negative answer: relevant as possible to the context/question (so that they appeal to machines), while they cannot be overly similar to the correct response (so that they don’t become correct answers incidentally).</strong> </p><p>因此计算一个weight，能够同时考虑到与query中的questeion相关度大，但是与query 的correct answer的相似性小。在本文中使用bert来计算相关度，用ESIM+ELM来计算相似性。</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1geuprb6zmvj310305qwfg.jpg"><p>为了获得多个negative answer，需要执行多次的双向匹配。为了确保nagetive pairs是多样性的，在依次获得negative answer的过程中，在下一次从其他的image中查找negative answer时，需要遍历当前所有的negative answer，然后取最大值。（replace the similarity term with the maximum similarity between a candidate response rj and all responses currently assigned to qi.）</p><h4 id="Language-Priors-and-Annotation-Artifacts-Discussion"><a href="#Language-Priors-and-Annotation-Artifacts-Discussion" class="headerlink" title="Language Priors and Annotation Artifacts Discussion"></a>Language Priors and Annotation Artifacts Discussion</h4><p><strong>Answer Priors</strong>: A model can select a correct answer without even looking at the question. </p><p><strong>Non-Visual Priors ：</strong>A model can select a correct answer using only non-visual elements of the question. </p><p>这些priors可能是来自于现实世界中的偏置，比如，当问消火栓是什么颜色的，模型常常预测出，红色。这是由于现实世界中消火栓是红色的。 </p><p>又可能来自于annotation artifacts ， 人们在编写class-conditioned answers 时出现的模式。比如：标注者经常使用否定之类的方式写与句子相矛盾的句子。</p><ul><li><strong>实验证明，对抗匹配的方式，可以帮助消除 artificial bias。</strong> </li></ul><h4 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h4><ol><li>The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task.  </li><li>Annotation artifacts in natural language inference data. </li><li>Hypothesis Only Baselines in Natural Language Inference. </li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Visual-Commonsense-Reasoning-VCR&quot;&gt;&lt;a href=&quot;#Visual-Commonsense-Reasoning-VCR&quot; class=&quot;headerlink&quot; title=&quot;Visual Commonsense Reasoning
      
    
    </summary>
    
      <category term="视觉推理" scheme="http://yoursite.com/categories/%E8%A7%86%E8%A7%89%E6%8E%A8%E7%90%86/"/>
    
    
      <category term="视觉推理" scheme="http://yoursite.com/tags/%E8%A7%86%E8%A7%89%E6%8E%A8%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>WMT Shared Tasks -- Human Evaluation </title>
    <link href="http://yoursite.com/2020/05/14/WMT-Shared-Tasks-Human-Evaluation/"/>
    <id>http://yoursite.com/2020/05/14/WMT-Shared-Tasks-Human-Evaluation/</id>
    <published>2020-05-14T01:12:37.000Z</published>
    <updated>2020-05-14T01:15:33.933Z</updated>
    
    <content type="html"><![CDATA[<h3 id="WMT-Shared-Tasks-–-Human-Evaluation"><a href="#WMT-Shared-Tasks-–-Human-Evaluation" class="headerlink" title="WMT Shared Tasks – Human Evaluation"></a>WMT Shared Tasks – Human Evaluation</h3><h4 id="Human-Evaluation"><a href="#Human-Evaluation" class="headerlink" title="Human Evaluation"></a>Human Evaluation</h4><p>两种评估的方式：direct assessments (DA); language pairs evaluated with relative ranking (RR)</p><p>但是DA相比于RR更具有优势，namely，对翻译质量的评估采取 absolute score 的方式。可以<strong>实施quality control</strong> 。</p><h4 id="Human-judgement-quality-control"><a href="#Human-judgement-quality-control" class="headerlink" title="Human judgement quality control"></a>Human judgement quality control</h4><ul><li><p>每个标注者，每次任务：给定100个 （reference+ candidate）pair, 针对给定的reference, 评估生成的candidate的好坏。</p></li><li><p>100个pair中有60个用于quality control，40个由participating systems 生成的翻译组成。</p><p>（1）这60个pair，是官方设计出来的，包括三类，repeat pairs (expecting a similar judgment), damage MT outputs/ bad reference (expecting significantly worse scores) and use references instead of MT outputs (expecting high scores). 因此仅仅会有20%的资源消耗：bad reference; good reference</p><p>Specifically，先从正常的MT system 中 得到30个 （reference, MT output）pair，如 table 5 中的 original system output， 然后1)对1-10对，进行重复，得到10对。2）对11-20对，将MT output搞破坏。得到10对。3）对21-30对，取corresponding reference–&gt; (reference_1, reference_2)，得到10对。</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gepxhtqcpgj311h052abz.jpg"><p>（2）within each 100-translation HIT， 每个articipating system<strong>等比例的贡献</strong>a（within each 100-translation HIT, the same proportion of translations are included from each participating system for that language pair.  ）这是为了确保每个参与的 系统含有近似的相同数量的评估。同时，这也从三个方面得到了公平性的评估：1）每有一个workers做一个HIT, 则就会为所有参与的系统增加human judgement。2）不会轻易受到worker个性差异的影响，因为每个worker都会给所有参与的系统进行评估。3）尽管DA判断是绝对的，但众所周知，判断者会根据观察到的总体翻译质量来“校准”他们使用量表的方式。 对于每个HIT（包括所有参与的系统），这种影响都是平均的。</p></li></ul><h4 id="Annotator-Agreement"><a href="#Annotator-Agreement" class="headerlink" title="Annotator Agreement"></a>Annotator Agreement</h4><ul><li><p><strong>【bad reference pairs】</strong> 由于 bad reference pairs 的质量应该是显著偏低的，通过查看人类在这类pairs 上的评分是否也是显著偏低。来过滤掉可信赖度低的human assessors。</p><p>set（A, bad reference） 与  set（A, translatin_B）这两个集合上的人类评估，计算一个p-value， 若p-value&gt;0.05 则说明该human assessor的可信度低。</p></li><li><p><strong>【repeat pairs】</strong> 对于 repeat pairs, 查看得到 repeat assessments的程度。</p></li></ul><h4 id="Producing-the-Human-Ranking"><a href="#Producing-the-Human-Ranking" class="headerlink" title="Producing the Human Ranking"></a>Producing the Human Ranking</h4><ul><li><p>Standardized </p><p>为了消除不同的人类评估者的评分策略的差异，首先根据每个人类评估者的总体平均得分和标准差得分对翻译的人类评估得分进行<strong>标准化</strong>。</p></li><li><p>system  score ……</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;WMT-Shared-Tasks-–-Human-Evaluation&quot;&gt;&lt;a href=&quot;#WMT-Shared-Tasks-–-Human-Evaluation&quot; class=&quot;headerlink&quot; title=&quot;WMT Shared Tasks – Hum
      
    
    </summary>
    
      <category term="机器翻译" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
    
    
      <category term="机器翻译" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning</title>
    <link href="http://yoursite.com/2020/05/10/Attacking-Visual-Language-Grounding-with-Adversarial-Examples-A-Case-Study-on-Neural-Image-Captioning/"/>
    <id>http://yoursite.com/2020/05/10/Attacking-Visual-Language-Grounding-with-Adversarial-Examples-A-Case-Study-on-Neural-Image-Captioning/</id>
    <published>2020-05-10T09:07:54.000Z</published>
    <updated>2020-05-11T05:50:38.137Z</updated>
    
    <content type="html"><![CDATA[<h4 id="对抗样本的影响"><a href="#对抗样本的影响" class="headerlink" title="对抗样本的影响"></a>对抗样本的影响</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1genh1kz7svj30i40dpag7.jpg" alt="Fig_stopsign_2_small.png"></p><p>图1，在image RGB 上添加了一些扰动，结果使得captioning model的输出也发生了很大的变化。基于此，发现了两个问题。（1）我们的结果指出了在tested image captioning systems中的致命问题。（2）captioning model 中的对抗性例子突出了 人与机器之间visual language grounding 的不一致，表明当前的机器视觉和感知机制可能存在缺陷。</p><h4 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h4><ul><li>本文提出了一种<strong>设计对抗样本</strong>的方法</li><li>本文的这种对抗样本可以拿去用来分析captioning model 的鲁棒性-</li><li>本文，还利用 对抗样本，来做什么了，有没有做一些对抗性的训练？？</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;对抗样本的影响&quot;&gt;&lt;a href=&quot;#对抗样本的影响&quot; class=&quot;headerlink&quot; title=&quot;对抗样本的影响&quot;&gt;&lt;/a&gt;对抗样本的影响&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/006uWRWVly1g
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Learning to Evaluate Image Captioning</title>
    <link href="http://yoursite.com/2020/05/09/Learning-to-Evaluate-Image-Captioning/"/>
    <id>http://yoursite.com/2020/05/09/Learning-to-Evaluate-Image-Captioning/</id>
    <published>2020-05-09T06:33:59.000Z</published>
    <updated>2020-05-09T09:58:38.767Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>本文提出一个可学习的图像描述评价指标。</p><p><strong>Motivation</strong>: 由于当前的评价指标不是很完美，不能处理句子中存在的所有的病理行为，或者是说，当遇到某些病理行为时，则不能正常工作，比如，SPICE对字幕的语义很敏感，但往往会忽略其句法质量，SPICE倾向于对带有重复子句的长句子给予高分。每个评估指标都有其众所周知的盲点，基于规则的指标通常不灵活，无法应对新的病理病例。</p><p>因此本文提出，使用几种数据增强的方式，来扩展出很多的存在特征几种病理问题的对抗样本，并纳入训练过程中，使得训练出来的评价指标对于这些对抗样本更加的鲁棒。（即，可以识别出这些对抗样本的能力）</p><h4 id="How-to-Use-the-Proposed-Metric-in-Practice"><a href="#How-to-Use-the-Proposed-Metric-in-Practice" class="headerlink" title="How to Use the Proposed Metric in Practice"></a>How to Use the Proposed Metric in Practice</h4><p>由于涉及到需要学习 ，则评价指标的训练的数据分布 与 被测试的captioning dataset 之间存在差异。</p><p>本文解决: 假设要评估 coco  <strong>test</strong> captioning, 则将该份submission 分成两半，一半用于scratch 训练该评价指标，另外一半则使用该训练好的评价指标得到得分；然后交替，则得到了所有的得分！</p><h4 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h4><ul><li>(1) One direction of future work could aim to capture the heterogeneous nature of human annotated captions and incorporate such information into captioning evaluation.  <strong>Human annotated captions 带有人的个性</strong></li><li>(2) Another direction for future work could be training a caption generator together with the proposed evaluation metric (discriminator) in a generative adversarial setting. <strong>captioning model 与提出的评价指标，一起生成对抗的训练</strong></li><li>(3) Finally, gameability is definitely a concern, not only for our learning based metric, but also for other rule-based metrics. Learning to be more robust to adversarial examples is also a future direction of learning based evaluation metrics.  <strong>对 对抗样本更加的鲁棒，是基于学习的评价指标的一个未来的方向</strong></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h4&gt;&lt;p&gt;本文提出一个可学习的图像描述评价指标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivation&lt;
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Cross-modal Coherence Modeling for Caption Generation</title>
    <link href="http://yoursite.com/2020/04/22/Cross-modal-Coherence-Modeling-for-Caption-Generation/"/>
    <id>http://yoursite.com/2020/04/22/Cross-modal-Coherence-Modeling-for-Caption-Generation/</id>
    <published>2020-04-22T02:44:15.000Z</published>
    <updated>2020-04-23T11:27:08.374Z</updated>
    
    <content type="html"><![CDATA[<h4 id="现在图像描述中存在的问题"><a href="#现在图像描述中存在的问题" class="headerlink" title="现在图像描述中存在的问题"></a>现在图像描述中存在的问题</h4><ul><li>标注方式上：让工作人员标注出image 对应的text。</li><li>这导致的问题：（1）Unfortunately, such dedicated annotation efforts cannot yield enough data for training robust generation models; the resulting generated captions are plagued by content<br>hallucinations (Rohrbach et al., 2018; Sharma et al., 2018) that effectively preclude them for being used in real-world applications. </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;现在图像描述中存在的问题&quot;&gt;&lt;a href=&quot;#现在图像描述中存在的问题&quot; class=&quot;headerlink&quot; title=&quot;现在图像描述中存在的问题&quot;&gt;&lt;/a&gt;现在图像描述中存在的问题&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;标注方式上：让工作人员标注出image 对应的t
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</title>
    <link href="http://yoursite.com/2020/04/16/Oscar-Object-Semantics-Aligned-Pre-training-for-Vision-Language-Tasks/"/>
    <id>http://yoursite.com/2020/04/16/Oscar-Object-Semantics-Aligned-Pre-training-for-Vision-Language-Tasks/</id>
    <published>2020-04-16T08:14:45.000Z</published>
    <updated>2020-04-17T01:26:40.517Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><ul><li>现在基于bert 来处理的vision-language task 存在的问题：现在的方将image region features 和 text features 拼起来，然后利用自我注意机制以暴力方式学习图像区域和文本之间的语义对齐。（1）<strong>由于没有显示的region 与 text poses之间的对齐监督，因此是一种弱监督的任务。</strong> （2）另外，vision region常常过采样(region之间有重叠)，从而带来噪声和歧义（由于重叠，导致region之间的特征区分性不大），这将会使得vision-language task任务更加具有挑战性。</li><li>本文通过引入从images中检测出的object tags 作为anchor points来减轻images 和 text 之间语义对齐的学习。</li><li>本文提出了一个新的vision-language pre-training method <strong>OSCAR</strong> ，设计训练样本是一个三元组：（word sequence, a set of object tags, and a set of image region features. ）</li><li>Motivated by: the salient objects in an image can be accurately detected by modern object detectors, and that these objects are often mentioned in the paired text.</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gdvtabmqe2j30qh0f247k.jpg" alt="搜狗截图20200416190213.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;现在基于bert 来处理的vision-lang
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>A negative case analysis of visual grounding methods for VQA</title>
    <link href="http://yoursite.com/2020/04/15/A-negative-case-analysis-of-visual-grounding-methods-for-VQA/"/>
    <id>http://yoursite.com/2020/04/15/A-negative-case-analysis-of-visual-grounding-methods-for-VQA/</id>
    <published>2020-04-15T10:46:54.000Z</published>
    <updated>2020-04-16T00:26:21.276Z</updated>
    
    <content type="html"><![CDATA[<h4 id="yaya简述"><a href="#yaya简述" class="headerlink" title="yaya简述"></a>yaya简述</h4><p>在VQA任务中，现在的方法尝试希望模型在回答问题时，同时能够关注到相对应的正确的物体（出发点：当模型关注到正确的物体时，能够更好的帮助模型选择出正确的答案）。于是，基于这样的方式，提出了一些方法 [1] [2]. 但是本文发现即便在模型中给了vision grounding 的监督，但是模型的grounding 能力却未必很好。那么提升VQA性能的真正原因其实是这个监督，仅仅是一种正则化效果。</p><p>作者使用了Grounding using irrelevant cues；Grounding using fixed random cues；Grounding using variable random cues 来说明，即使是错误的监督信息，相比于正确的监督也不会使得性能下降很多。</p><p>作者使用Regularization by zeroing out answers  来说明，给损失函数中加一个正则化项，使得training accuracy下降，就会达到正则化的效果，其VQA的性能与用grounding 监督的效果差距也不大。这就证明了使用grounding来监督，其实仅仅是起到了正则化的效果。</p><h4 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h4><ul><li><p>未来的方法必须设法通过使用与本文中介绍的类似的实验设置来验证性能增益不是源于spurious source.</p></li><li><p>创建一个数据集，使得能够评估  if methods are able to focus on relevant information.</p></li><li><p>Use tasks  that explicitly test grounding, e.g., in visual query detection an agent must output boxes around any regions of a scene that match the natural language query .</p></li></ul><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p>[1] Ramprasaath R Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi Parikh. <strong>Taking a hint: Leveraging explanations to make vision and language models more grounded.</strong>  In ICCV 2019. </p><p>[2] Jialin Wu and Raymond Mooney. <strong>Self-critical reasoning for robust visual question answering.</strong> In NeurIPS 2019</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;yaya简述&quot;&gt;&lt;a href=&quot;#yaya简述&quot; class=&quot;headerlink&quot; title=&quot;yaya简述&quot;&gt;&lt;/a&gt;yaya简述&lt;/h4&gt;&lt;p&gt;在VQA任务中，现在的方法尝试希望模型在回答问题时，同时能够关注到相对应的正确的物体（出发点：当模型关注到正
      
    
    </summary>
    
      <category term="Grounding 相关" scheme="http://yoursite.com/categories/Grounding-%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="Grounding 相关" scheme="http://yoursite.com/tags/Grounding-%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models</title>
    <link href="http://yoursite.com/2020/04/01/Egoshots-an-ego-vision-life-logging-dataset-and-semantic-fidelity-metric-to-evaluate-diversity-in-image-captioning-models/"/>
    <id>http://yoursite.com/2020/04/01/Egoshots-an-ego-vision-life-logging-dataset-and-semantic-fidelity-metric-to-evaluate-diversity-in-image-captioning-models/</id>
    <published>2020-04-01T12:55:53.000Z</published>
    <updated>2020-04-02T00:39:08.259Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul><li><p>当前的image caption dataset 存在的问题</p><p>图像字幕模型已经能够生成语法正确且易于理解的句子。但是，大多数字幕传达的信息有限，因为所使用的模型是在数据集上训练的，而该数据集并未为日常生活中存在的所有可能的对象提供字幕。由于缺少先验信息，因此大多数字幕仅偏向场景中出现的少数几个对象，因此限制了它们在日常生活中的使用。在本文中，我们试图证明当前现有图像字幕模型的偏向性，并提出一个新的图像字幕数据集<em>Egoshots</em>，由978张不带字幕的现实生活图像组成。我们进一步利用最先进的预训练图像字幕和对象识别网络来注释我们的图像并显示现有作品的局限性。</p></li><li><p>当前的standard metric存在的问题</p><p>此外，为了评估所生成字幕的质量，我们提出了一种新的图像字幕度量标准，即基于对象的<em>语义保真度</em>（SF）。现有的图像字幕度量标准只能在存在其相应注释（reference captions）的情况下评估字幕。但是，SF允许评估为图像生成的字幕而没有注释，这对于现实生活中生成的字幕非常有用。</p></li></ul><h4 id="SF"><a href="#SF" class="headerlink" title="SF"></a>SF</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gdel85qhz8j30se0g97eb.jpg" alt="搜狗截图20200401212845.png"></p><h4 id="Annotation-Pipline-and-Sementic-Fidelity-Metric"><a href="#Annotation-Pipline-and-Sementic-Fidelity-Metric" class="headerlink" title="Annotation Pipline and Sementic Fidelity Metric"></a>Annotation Pipline and Sementic Fidelity Metric</h4><ul><li><p>annotation pipline</p><p>使用三个预训练好的caption model: Show Attend And Tell (SAT), nocaps: novel object captioning at scale (NOC), and Decoupled Novel Object Captioner (DNOC) 在新的数据集Egoshots上进行captioning 任务。</p></li><li><p>sementic fidelity metric</p><p>我们提出了一种称为<em>语义保真</em>度的新图像字幕指标。SF考虑了两个元素：1）生成的字幕与图像中检测到的对象的语义接近度； 2）相对于检测到的对象实例数量的对象多样性。假设有一个最新的准完美对象检测器，通过考虑这两组（带字幕和检测到的）实体（即对象）之间的语义亲密性，当一个模型输出的caption中包含了并没有出现在image scene中的objects时，将进行惩罚。</p><p>公式：<img src="http://ww1.sinaimg.cn/large/006uWRWVly1gdelh0afpej3055021a9x.jpg" alt="搜狗截图20200401213725.png"></p><p>对于图像i，si是其预测字幕c i中的名词词与OD检测到的对象名词之间的语义相似性，＃O是O O D的基数，＃N是名词的数量（表示对象in N i）存在于caption i中。SF的范围为[0，1]：SF接近1的字幕传达更多信息，并且在语义上更接近于要字幕的场景（就字幕所涉及的对象而言）。</p><p>关于si的计算：Recent works (Mikolov et al., 2013; Conneau et al., 2017) show the ability of word embeddings that is transforming a word into its vectored form efficiently capture the semantic closeness of two given words. The SF metric uses this approach to calculate such semantic similarity between the noun words and objects in an image.</p><p>上述公式存在一个假设：that #O ≥ #N (Assumption 1) for all images. This approach to compute SF will work only assuming robust object detectors satisfying enough scene annotation granularity.  </p><p>同时为保证分母不为0，还需要一个假设：Assumption 2: #O ！= 0 (i.e., the object detector can at least detect one object in the image). </p><h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4></li><li><p>在上述新提出的metric中，如果使用SF执行字幕评估，则良好通用化且鲁棒的对象检测模型将扮演最重要的角色。（a well generalized and robust object detection model plays the most important role if<br>the evaluation of captions is performed using SF. ）</p></li><li><p>在物体检测器发生故障的情况下，度量是不可靠的。由于SF将无法惩罚字幕模型，因为它不能依赖忠实（即足够鲁棒）的对象检测器（＃O = 0，假设2损坏），因此无法应用SF。</p></li></ul><h4 id="Appendix-指标限制"><a href="#Appendix-指标限制" class="headerlink" title="Appendix: 指标限制"></a>Appendix: 指标限制</h4><ul><li><p>我们必须注意到度量标准的一些局限性，应加以补充/扩展为（1）解释字幕的动词和其他句法元素（当前只考虑了名词）；（2）根据解释的质量对字幕进行评分，并考虑图像中相同类型的对象相对于字幕中存在的对象的数量。诸如（Cohen17）之类的特定计数模型是有关如何增强此处提出的无标签数据集注释管道的特定示例。</p><p>应该在更有针对性的应用程序使用案例中评估指标，例如，在诸如导航，对目标用户（如盲人）的有用性。</p></li></ul><h4 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h4><ul><li>其实本文尝试使用一个open-domain dataset 来测试在 in-domain 上训练的captioning model的泛化性能。但是这本身就存在问题！因为，model本身就会受限于训练数据，因此这里却希望它有很强的泛化性能，这本身就太难为model了。<code>eg: 不能要求一个学了小学课程的人来做高中生的题目</code></li></ul><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ul><li><p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. </p><p>Distributed Representations of Words and Phrases and their  Compositionality. In NIPS, 2013. </p></li><li><p>Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herve J ´ egou. ´<br>Word Translation Without Parallel Data. ArXiv, abs/1710.04087, 2017 </p></li><li><p>Joseph Paul Cohen, Genevieve Boucher, Craig A. Glastonbury, Henry Z. Lo, and Yoshua Bengio.<br>Count-ception: Counting by Fully Convolutional Redundant Counting. In The IEEE International<br>Conference on Computer Vision (ICCV) Workshops, Oct 2017. </p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;当前的image caption dataset 存在的问题&lt;/p&gt;
&lt;p
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>(ACL 2019)Putting Evaluation in Context: Contextual Embeddings improve Machine Translation Evaluation</title>
    <link href="http://yoursite.com/2020/04/01/ACL-2019-Putting-Evaluation-in-Context-Contextual-Embeddings-improve-Machine-Translation-Evaluation/"/>
    <id>http://yoursite.com/2020/04/01/ACL-2019-Putting-Evaluation-in-Context-Contextual-Embeddings-improve-Machine-Translation-Evaluation/</id>
    <published>2020-04-01T09:35:03.000Z</published>
    <updated>2020-04-01T09:43:09.981Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>没有认真阅读本篇文章，但是其中提到了尝试去拟合Human judgements，这一训练方案。</p><p> （1）We treat the human reference translation and the MT output as the premise and hypothesis, respectively 。</p><p>（2）Using squared error as part of regression loss – being better suited to Pearson’s r — and might be resolved through a different loss. Using hinge loss over pairwise preferences which would better reflect Kendall’s Tau</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;没有认真阅读本篇文章，但是其中提到了尝试去拟合Human judgements，这一训练方案。&lt;/p&gt;
&lt;p&gt; （1）We treat the human reference translation and the MT output as the pre
      
    
    </summary>
    
      <category term="自然语言理解" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3/"/>
    
    
      <category term="自然语言理解" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3/"/>
    
  </entry>
  
  <entry>
    <title>BERTScore: Evaluating Text Generation with BERT</title>
    <link href="http://yoursite.com/2020/04/01/BERTScore-Evaluating-Text-Generation-with-BERT/"/>
    <id>http://yoursite.com/2020/04/01/BERTScore-Evaluating-Text-Generation-with-BERT/</id>
    <published>2020-04-01T03:22:30.000Z</published>
    <updated>2020-04-01T09:10:45.905Z</updated>
    
    <content type="html"><![CDATA[<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>我们提出BERTScore，这是一种用于文本生成的自动评估指标。</p><p>类似于通用指标，BERTScore计算候选句子中每个token与参考中每个token的相似性得分。但是，我们不是使用精确匹配，而是使用上下文化的BERT embedding 来计算相似度。</p><p>我们对几种机器翻译和图像字幕基准进行了评估，结果表明BERTScore与人为判断的关联性比现有指标更好，通常甚至大大超过特定于任务的监督指标。</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>在本文中，我们将重点放在句子级别的生成评估上，并提出了：BERTScore，这是一种基于预训练的BERT上下文嵌入 （bert）的评估指标。 BERTScore将两个句子之间的相似度计算为它们的标记之间的余弦相似度的加权汇总。</p><p>基于n-gram matching metric 的常见缺陷：</p><ul><li><p>semantically-correct phrases are penalized because they differ from the surface form of the reference.</p><p>解决： In contrast to string matching (e.g., in BLEU) or matching heuristics (e.g., in METEOR), we compute similarity using contextualized token embeddings, which have been shown to be effective for paraphrase detection  </p></li><li><p>n-gram models fail to capture distant dependencies and penalize semantically-critical ordering changes.</p><p>解决： contextualized embeddings are trained to effectively capture distant dependencies and ordering  </p></li></ul><p>实验结果：（1）In machine translation, BERTSCORE shows stronger system-level and segment-level correlations<br>with human judgments than existing metrics on multiple common benchmarks.（2）BERTSCORE is well-correlated with human annotators for image captioning, surpassing SPICE.</p><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><ul><li>见论文，比较好理解</li></ul><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ul><li>Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance  <ul><li>同样尝试使用contextual word embeddings  来构建一个metric.</li></ul></li><li>Putting evaluation in context: Contextual embeddings improve machine translation evaluation. In ACL, 2019.  </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h4&gt;&lt;p&gt;我们提出BERTScore，这是一种用于文本生成的自动评估指标。&lt;/p&gt;
&lt;p&gt;类似于通用指标，BERTScore计算候选句子中每个toke
      
    
    </summary>
    
      <category term="自然语言理解" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3/"/>
    
    
      <category term="自然语言理解" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3/"/>
    
  </entry>
  
  <entry>
    <title>Grounded Situation Recognition</title>
    <link href="http://yoursite.com/2020/03/31/Grounded-Situation-Recognition/"/>
    <id>http://yoursite.com/2020/03/31/Grounded-Situation-Recognition/</id>
    <published>2020-03-31T02:19:26.000Z</published>
    <updated>2020-04-01T03:21:19.045Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Grounded-Situation-Recognition-Task"><a href="#Grounded-Situation-Recognition-Task" class="headerlink" title="Grounded Situation Recognition Task"></a>Grounded Situation Recognition <strong>Task</strong></h4><h5 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h5><ul><li><p>以前的situation recognition task: </p><p><strong>Situation Recognition</strong> is the task of recognizing the activity happening in an image, the actors and objects involved in this activity, and the roles they play. Semantic roles describe how objects in the image participate in the activity described by the verb. </p><p>While situation recognition addresses <strong><em>what</em></strong> is happening in an image, <strong><em>who</em></strong> is playing a part in this and <strong><em>what</em></strong> their roles are, it does not address a critical aspect of visual understanding: <strong>where</strong> the involved entities lie in the image. </p></li><li><p>本文：We address this shortcoming and present <strong>Grounded Situation Recognition (GSR)</strong>, a task that builds upon situation recognition and requires one to not just identify the situation observed in the image but also visually ground the identified roles within the corresponding image.</p></li></ul><h4 id="Challenge-of-Grounded-Situation-Recognition-GSR"><a href="#Challenge-of-Grounded-Situation-Recognition-GSR" class="headerlink" title="Challenge of Grounded Situation Recognition (GSR)"></a>Challenge of Grounded Situation Recognition (GSR)</h4><ul><li><em>语义显著性</em>：与识别图像中的所有实体不同，它需要在呈现的<strong>主要活动的背景下</strong>识别关键对象和参与者。</li><li><em>语义稀疏性</em>：GSR存在语义稀疏性问题，  在训练中很少见到role and groundings 的许多组合。这一挑战要求模型从有限的数据中学习。</li><li><em>Ambiguity</em>：将角色定位到图像中通常需要消除在同一类别下的多个观察到的实体之间的歧义。</li><li><em>Scale</em>：grounded entities 的比例尺变化很大，图像中也缺少某些实体（在这种情况下，模型负责检测这种缺失）。</li><li><em>Hallucination</em>：标记语义角色并grounding 通常需要弄清物体的存在，因为它们可能被完全遮挡或不在屏幕上。</li></ul><h4 id="Situations-With-Groundings-SWiG-dataset"><a href="#Situations-With-Groundings-SWiG-dataset" class="headerlink" title="Situations With Groundings (SWiG) dataset"></a>Situations With Groundings (SWiG) dataset</h4><p><a href="https://prior.allenai.org/assets/project-content/gsr/gsr_banner.png" target="_blank" rel="noopener"><img src="https://prior.allenai.org/assets/project-content/gsr/gsr_banner.png" alt="SWiG examples">A sample of images from the SWiG dataset</a></p><p>We present the Situations With Groundings (SWiG) Dataset for training and evalutation on the GSR task. This dataset builds upon the <a href="https://homes.cs.washington.edu/~ali/papers/SituationRecognition.pdf" target="_blank" rel="noopener">Situation Recognition dataset</a> presented by Yatskar et al. The SWiG dataset contains approximately 125,000 images. Each image is associated with one verb. Three different annotators then label each <strong>entity</strong> in the frame associated with that <strong>verb</strong> and mark the <strong>location</strong> of the entity in the image. All three labels for each role are given in the SWiG dataset as well as an average of the three localizations.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Grounded-Situation-Recognition-Task&quot;&gt;&lt;a href=&quot;#Grounded-Situation-Recognition-Task&quot; class=&quot;headerlink&quot; title=&quot;Grounded Situation Rec
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Multi-task Collaborative Network for Joint Referring Expression Comprehension and Segmentation</title>
    <link href="http://yoursite.com/2020/03/30/Multi-task-Collaborative-Network-for-Joint-Referring-Expression-Comprehension-and-Segmentation/"/>
    <id>http://yoursite.com/2020/03/30/Multi-task-Collaborative-Network-for-Joint-Referring-Expression-Comprehension-and-Segmentation/</id>
    <published>2020-03-30T09:26:57.000Z</published>
    <updated>2020-03-31T01:40:39.416Z</updated>
    
    <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>VIOLIN: A Large-Scale Dataset for Video-and-Language Inference</title>
    <link href="http://yoursite.com/2020/03/28/VIOLIN-A-Large-Scale-Dataset-for-Video-and-Language-Inference/"/>
    <id>http://yoursite.com/2020/03/28/VIOLIN-A-Large-Scale-Dataset-for-Video-and-Language-Inference/</id>
    <published>2020-03-28T02:43:22.000Z</published>
    <updated>2020-06-06T06:47:44.830Z</updated>
    
    <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">U2FsdGVkX1+bHMXiX+u2uv6c4L2t/fR3aHg0jvzH03+9B7duASFjTxJ4RpdPGXUak8644AP3Kwh5NkqjN+0Rj5eOrcPiJ/LQQMGBWKXmcyfcYKcaUvvc+nmlJGyo41VnEW0xMEUsahN/dq+27L1I4NC1NAlr3sG1UHWgtN5bXSn11eROILl8VSUjt75Fj/5zc9PM78pm0JXnDrEXdQ1jPkAohbo0OqkDvWWRVPdbCE12Wzb3MwZEjWDgaFmHdGGgyn7AfktPrFaVtquUNidSmrEdUdYaYKd9hgIakWOF5F5gzSAJtMV4ebMGvoBlhOEKfUXLcZvYAJ/Gg7w4vW5vfDC5FyKTURxR5hSWslEtFVzQtjejeZ+jiQq3oOpPGJhhuTDqGz8cBqbj3uDgPc4tuD/0PXZ5ows6smFjimtcd7CgJ5MT6pLe/iAqde1m+5zMBvUvT2QHcKiXMifgdMkXbC70S9MmnyOd8iPJIyGaB0i/MQe2zg0B/nktFbSCHTOL/8ueercdjQpa1pnX3olwdXuQgMxHG9xCLZjEJS1oIxpm1d26ojYVglpjSnhQKhge6oAs8Yce7RWYmW2H6lWHZ00cXUjUeKA2uOdOYWedme1SkWDo0AYyA/UByBSXvvXHNPZqSNqWPy03lKIGRwAiDHgB6WdtSkWJxbm0/jn1tTCfzsiyuE66hf+2cLw8mvbPraK4ZhSAwB0VLvV2bmLwsyy1HdtuEsksrGSL17YmIfecgYDkxDgbK81h2YrEdO35eplVLWkSv4eChlOgoqzl1W7mWixLQuj2G7GuTjO91nnsItuI8i69+UgteWGaPzrCpbiU2gOa7jegj9nGGunYrgTidcQiopWAEwC8i4IFN9Pr/UmGQW909iJnZkVoNH7A7lZwOVT7esWfiNb4Xn4ykoI2LrMjBZUmktAed30Q6yaqc5Pe8EhJC4titfcGRaEOt/GXKe1G0FznrYJnJhfJwT0dIPAAiwW5tNSamVDny59GlGgr0D8uC7BMp+oa6J4qT6qIU5AsBivP4XvGZaZEmz5yCgmXgBaqo/RA4F6u//Ijf31ai4acPrlTvXb5HAu/5C81ZbtTz2cbYXF1UOyID6o7mltXTlEH052GN5UMaFgwk2Nm3ZLrTT31GYkrAQXiXJpjYf0+Oa0ys8hU9umgdFP3V+/gKH1KLUxFi2uiWnvOWAbp6GhCUkMHvFumR6mHEHR98LWl4zv4pdO23Uuc6mmxP/OTrpn0zSuvTL59E49tW0+ozrUOz0B7ZTULgjsmhuKvjoR2SXmolz6l84c/va9qOlHVMY3kCIKDTO9iQ9iKuOXmunv6Q6EGsB+2VjQZ44Mt8+pWGttWg59VZHOSJd12h9p4cEgg9iU0WbL8u9WDQYLTY5M0F9w5ykcBuD9ouM2VxeDj4XjmYMYiBKOr1lpegOjujo4v+CvCJs0hSz/neO2kOBayqfCtPtp1GKyZaGs4u+WJkmBGW92tym80JcvMrS4yh+hXRTX3EUuhrWMOohXdZkoF5n7/bgiPx+/mTn8G03FZLhQRd3qQctkcMbMR7BhhfbcpKmFF4OS3tZ8NJU7hKaygY23S6Qv2jLOb9r+L+cRxKqnjEXfeKeZBgdij2EM3fpUKhpLPCzOtQYIdbvNhw07EEAnSljMegr+/imhAfnZHk9vM/mqQh9Oo2y0JEJ8+cIzP86KoAWi56m64zXRtXlT/tPINy+UxdH19vpRovk3l3FF4fcl99qtXsiJ7rZTUoZ+C8o8Tk73W+mYA+Z0e6JzfQqXCKYmCX9F0oNgrA4urRqe+WVwuf2qoYsWfipVWbqbs1ewigXuBt5pcZ67X2iauvC6ZZIpBQDGvIsxqkHDwqhHbfLPubC8tWz4e67KqoDxijHCFlXgUIfDISznR86flS1qjMEhTjNFhfdcD3+1jvrWM6UVI/ukvw/dzvgEt7z35huKBHuYA/qraFDfk2M431wrCQoVmJw1DfNobDcUn/dZBEVyFCN+I5MAjNAvuprfcUEv/72pBnFc3QbCnqYd44Lkuncgr0U10AKg1iOOhEgC+Waunhgco4xW7tQIYQMrKMQ0e9KSYsBcvCurlFRicxhBfkHXaTiI6Yk0lVH26N3mbw3uS2Z1kY15NXN/BvhNnQC6Sp5gljZD8r/fRRmh8Hbl9OYSlYwd9eKVMiGuXS7KILOFNVM/mc10q1pgdeSjD+QUdzfiBiMnvdqJzz9WppMHROvYMhckOyfb+DGgm8/NU84nIv32S78rTCEZlT2MPWOpL5H8oVcKkt5QONaC61z0IurtPa+GjfYYuMGSok38WS+UR2p/kD8rJhZQ8HOryhfb8Z7GKE31gFjv0nC+gPJlQpPQ5X8yHkShA5S5/LuMMieELi8Bxxm8YSLvEvVi9Uf23hm+y1zDR39tKUOuAnGz8kwgPMR40Ox/gHbFypvB2zbnhdCmhfeV0bBjczwalGV0jquh9mDn7eyxA7jwxnoCMlztYdPfyy/QS4CerULLe8dIo4M/Tkcy+wwdgPz1mBHvQKUio7jTDBQbplQnM7DH+tOQ+4usRypD926E3YEge3yH46g4K7ra6R00TE36JYDzZSI4cHbBqq2Gm8h3ijQ1flWgXjhtbW5ofTQHD24Vk/u63Ih2xapCKT43jTfBWOOHJL05xAm2N+441qlWNtiE51xiB2Qgnk0zRKbKKjrDbD1inLNAQd94906i7FMnJZO0sfIJ2kZRBsTD1f9PqMAD+OtvH7qpxTe0szOLv/T0qTV5ak9pWKxl3stUfJTsxhQ1spnaPF2VucocbFfujuI0eAoUfuOoJUqpjaFCpFHigF93zJbfsleNIcldtv6U70UrirglTSSM9YVisJZ64Wm4jPSgz/PPJszXTKz6vkbATVEfDjYn1Ng3aSodINa79erOboTTWB5tVSXvSvLPRa9ZwPe49PXUdK8CUcOE7EuxcdrxaFOHHpxHl1kSG/yJsqGKj+q4WPmYgmurk6EwSEEcdwvAsO5k53YmkFzrP7VjwyRpNOR0n+oSeiyMivvUl5rNBuutPZJg6MtNgfO0PoYvO4OigDce+i7EDKnVcBLx/UA9MgOYJDKt2qSHOxyR8dYxDuS34yrCCgCHIpVHRUCHU+bLtgvrPzrX04FI+MXwDmcUTKlwlrKY0gnAhG8EFsFCi5d7aJghSJBIYLFKEWAFlkwPdlzh9eHYG/y6H4WpHZw5FIo4gD+mUCM2b8/5yZOW5zAIHPT+m82zp2gX62VZqqdUApfbZzFSbfUknfqVMyW0T5LAqXM54CPu/uCTwk3ZI2d4sX1/alqOcbQKilKdT9rOGL+PMA1KZliLgu6thpfQgoV97GXa5hd6RKjQRdO3zR7WUy6GS+IjZEtWTiLbJM4Z+g57XcFAJ7Yd8utPkgH6m4XfxPAM9oHlnt4Yvx7nnp+5VybuzdUF26bJmZwNP3N4ZZF7W9DjyeA5VdXzMhwz2UNlIWyxSthcEnU+vaeiyS4oZv8GIP6wSKukWVIbFKjhmsmOYYAq0eIwvZIjDfj2nB3PHdsq1v3+eKsuvdUpLeFfInpLeEcdP10ndjNq3CRy+vGWR8vYc+Zb62J8nVKoEtw7GmYJ1Ei9OURdJ2t8L+qpxtMnpnMzJWJdQt3Y29hyWIAWJlNyt8Te6tF+iOgU9MMc5Hj3/rFd8cyuJVRnZnBEwvg8DMHvKI3L4UbhPG4bzU+of9eG9DtxUA22H1vDM4UO/HSxGr+vdU13OkBnSj4JyHrXxcThovHNb8e6n2CvAcFgHX0nGHwd3ekAAfZm4c+CksA5IJLtAr7VASv+7i9Pd0nWefpE522aWFF0gadZ1A5Q9n9H+quRrzPSzNqYyyKeM6qPGmGBnAFH6zzaTdGurcaR68akjDbam9sbFWLT/e/eM78WpwgGT2AsIWXZffrM/P1AUUeNuntr58IyIrnJUDHREoppVWJp1euxzOx13QVKfdyRfGoKlgNNfMkdWvX6hHg1WfRWXuFXuQz42fkUS3cvlc60T4Q+LzugT9N/eUlvsAWsvWqPDfWyMH2c22DSBXGVglMGhzK/h1ZW0NgZmqv7338blksggfir8+uta6IY2geUre1QL5lluvnxxUhBD22yK1p8Zk/kuHFKeiOYF9M3gLKjQa2ZHigFCxA5WCqXUp1jyJBiwsmRFd3LnLpvTtNVDjy8DBL4nk8CQasIJHzzQgtsmm5VGudSr0yw7xPDCyoGWKGzFm0cbe7m4P1lS4adxjQ6HVpyCGwZtA5b1cB7VHplaf6Rkhl8Rf5zZy8hp/UQMy2Nbd8WFDcSu9Qlp3eXcZIpXkc4htg2R7cOdHzokzlYrGIuHmr5bXXIflG/ME8+0RKivT0Yfc0L/sAxW/M/WmxSZFroYp9NhOQ4pc6mEtfBULkbYmBmISQAhPBIAYztecotFqNbkWErgpLvLuoepQCMjFi1153T8iD217rR4Cm7L7YT58USIgQTC/HmELp6O4gO4qFj1vGXEa16G0YevmfLa7PuuNIBIrpCeJWSpjUb/rqFq4x6Y1n2xpn2bmC0kSyNZH7L5gAixQoaV8AuAsuNmrCcwcuS0UeY4kxfQYikVq4POTbmUI6FBEGkb8H7+nOJ87MdlFRtjpDUAYPR0YHjNzx8qive5cnIZITqhnP1F39cTFUodNmxsH7NI7oQEm83ZTc7hJNhXKgoQS48OQwu/CclLRwE2eZtkdmkjb1W91lKExiU1UAPvcKySDlAl5Tt3fJsaq7LX5tCP0oeEkaAdW8CgM+wxPr7R7XWzfv3gh/DKgFcG8749GX5tUFAmOi8jhJTrSV8+NYsH/Uk2QxNMV6mIPQP5fSMF9XkAF800HUC2IPSGJh3n36pKlAUkoOsHXpEYvT4bLhdsWPPBvRL5hI8q7upyabAl2P+xRfA+0/0JDgQQpkAEZiophROZXfmegVxizdFr7lC3ozlM9FsjAAJPqBsJBBKyGOVMzC4abgVt0xOxY/BHCCdb3YfU0VRwG5l/VevhrOPjjLC30rN8GIZkw1BFW3Bm+oSyk6gh4SO/ZCyg003/dJCdV03fff0XxuUwOL49oGgCLilHvEpwQ9048HOdYRYgIs2yWChFfyxGLAnN1i6Rz6hnp+pxBnNvijD6Q19ypKUV9yumjrJG4Eih394ZD3dGvlPZlhK3l0+I9j1V7REJkln0VKuPxiCF2F+jHGkkiQxWGQbh02VrU/xsvVNUjF768klbrW+wPky+AL2l1QqC0AASIaTe3Nu7+iUSYQvdBmIVVkYbQNf91gJIhmYJYzXBHIr7ET3DXTJbl3iv9U1A4wUfGaukWUPHjMDV8iO/A3y3nJczBHtpMc04D1TOrMNgAZs/QrCvLTQdGFv8UZ5ikWeYiKom0Hu5XBc87rxm5NsWppjDIlbeHVyItM8NLjJfc8UBtOtwYaP9b8aew9Z/26EA7RbhXuCgP9E0xbr8+avPg7oivJ8Z1qktG0NODAgbCg0/Q2Ct9m8B/N1Jix8vX0GRIkhMCgWZC2Tafjx5ZpKH8TXH0RMLYanV6uebvh05AxxisEFFw76z2CcumfpigTZatM078PWKMsQ/SyL7TlrFnRC3nTj7tgiOa1U4QK4sVV1XqfJGONInpTpn7aSKKUKhlKcrmVxyVoaFxPHQO3wCcEsSw8KcqMFdx1h8IaBC3vKg/EDd9eKDh+bounBpIrxi7V0puM+e8rs75gBXyhgPjrMxzhAumMYMSKvII8WbAybCUaOnAS1koX9pdq7mvRomUrlNZbIcO8L8g3YExvKRNU0zH4J3EUQm0NoRUmN8vyS/00yy2gIniw+OnCxoj2AyDMKxnuj/1mqDDCk1eXrVwwI90VoKaoz3qAFk8V2xMnC+G6pcqwiUVhsa1XJR7XT1uUMojJh1frsZAjatQpSK8IC7nU2XtXlN/lkxdaD6fIZQf7EiDxqNlPV4YyOrzkX+/oZd6H5ITB0/052ixgL37ZYjmLbuYiwTYqj/PjQ8/UjVI7/FV84Qd6kxd+a5tWHTQx2iOEd+ZECHWzQHwaUbI+Os6E6WuJ3Me28tVBAdQ/pVLCOWaEYy/zOylSsqiTRi45tuF8hHHIJC2KnYVMxt6M74yaWKMKyynllxRBgv6luTGuju+HcKDJIFqbifuRMRpHwz8sY8S94Te+KmEqcYdcP7tgo3HQtN9Pc87lDvp+CR54jg0MDp7ElEzBTBVtQwJGOsa3yfHtpGIfBSY0rbTwHxuh8gkRArgyCNX6puN6UKO8U76BOaDhLiTqLs7NEupXqK+H6NgDM+qFJtHfWx6vzJ5xyQSbfjme80t+nUh2upgTZXNyVbi7K/5JSuVgY8iVmotm1s9ZWTesLf5oEsdyurfxyfbNvb04l730juqYgVNr2pWQU7Nf7MjqOxcJgVgp1LfMzmfZvCeaCB36FrQi0Ldb8pS/ZNdLdvTlI+g4KKY8bKg+UCFzV79kjGBBoXwqJfyE4zHVJbTNAvLWywwST0Kz0l0rCIrcfNvOhdT0MiOZ247sOgLydmkjcTcQvtackqzrb4GNL1V8x+md8nf2uRvCbmxooEjK6mvQEAis3hiPhHxHTPLVCBWclrKAhzxSUL9CtTD+A/IjpDBKw6OOxC/eskJQ+vn8Vbt9HcuKvt4lBPKHzVS0RKgBOuN4Y3iz7HpKu/QLa51BnVnPf9wtqiLL/4pmoo44gLpOsoiGd8ehn9hn1/tCd+osH6u9l3+MasD2G7XnZTz/gxsHmSdgylYvFf3ImwRYFTU62f2AwOsOTkBS9qm4lo8gqRU72RUsdRARaTnVALYZ0yppgHXBJMlDlkxUJfcSg/FrQiOxx4noLAbJ4AiXFsyrCSbQZS4v0/xaNp4nLmkc4fBhl1u6Mf3pgbWOJCQQ7aXHdMiLLgSEapotQakgb/ieS0FtoTGdhQAOfx65Ts66cTUMR3sm5RUPkQd8LxPCv44h65jkGCPVTQJfNatk2P9TnOFzuvFiUrEa4JX4CWucGUTovYOe7eo4M2r7MAw/ZyYUznG0WpCsNrqy8hchrM7+V174IPpDJSVFXSsAOafcuftAlgSgdRFwK5PdbSzDoj4mw7Ru7N6niMXgifK2TqFrqBW221ObOy2Ydncm7m2yFKW1mHB1GueFcjsnW2O1dezFXJl+om9vaL4A2q7xjtKaiJfv918yX5g8QtasMh+RMXqoSwoaiK1272tD0poroJQu26TjhwiaiP30gnmzruE+D10dgmHEYnwzaeVYqaFCLPKBb3No+en7g2DXUUKBRoiDugEanoFINLhWdymmbIbXz/nJ2+eZ650OMd4cqEPk8Z8DnmUXHYgm0zdVOMx+mgYejKusfJdMbH3urFBFhWmiJrRz0FWUa/rwi5kC+adx5B1ZZlCweUe3B1wfTEh9rSmbbpR7w9Ol2p0x+Pejy0A11uRMf4LVSFgWKnDHNZkjs/5Gsk/8OGS1bY41mx9qnVRCYL4ju1qNZyh7dPr8xoaNIiwaa+bUtKn8vAkSL7gFbkFnMvIJmYJ3CkW7ZnIQ2gjTUMZDMIiF+lOZYecs2LeYOtiF6nZ76UXvFJlHbI1epp7geWf0Ko5ad3yInXUUzIPDIQCHUx8GiaREYxjFe6/4a9pLoX/jBiEwIjr7pWvC+WuKWK8gHB6JUx0Gxq9SIgn8YEv2pg+nTg91QqwofmUyhmhM+2XqWofoRY48LFwFyQLS/FZhu+nPc/NqEPIhK5f+ZexLJQVow8BJvNfFBfIrI4kOFkyfVQMJA6fp8WergrndWFBHEye60eWYy+tGooF491hDT52Hcl6+baH7XxKyVwKREqyi565jsBBssDngoI9iQRpt7lRyoaAzDPUgnaPcp2AyeT7V20RTCgBvMYqjqAtypny8U9gHwEqx8hyIBNaGG20TypfPcLMTndX8sYevePFKaO5N+zp5G4xM5zW4kVVT8r+EckpTpxG5sAxDWLueMRhbrhUDl/TTfjcGINVpDqfOP/ohZuaaVhYON00VKOu+3LH0HNq5r9jxh3SRUl2rSM5lIdtinwxfls+K5p6Rz3QYlXmriBcEKL+5U0WcnH/Jb7nYEWTX53LULzs6+4nThqCZUQNG12R3qaoJNcUvQ4Pp/Nbory55wKXjFLje1hr/hKj7aHPwgwhhHzpmQU6zLBp9D7p/x2o6RJwS3GqF5fmL+Q/i2+eYtHMykc7V6E7Uybh6cJ2L1ahKc0MQWrge5ia6YjM2hRfKvZ29V3Lp0o5ctlZ34Dy+pCNQf9k8UksI6Pxcb2yhOhKuogGvZeDDux26EnOejl4N/nrDEzsJMw7ao3TqG+k7ZrjuhjOxoockrxzaULS5x4la/Yk+U4fM3YetQGztJdyweat2WtWvVSg4d2ikcj0U7EV5ZYSDRy9TksKVBPfwgyErMdftYXWmELJzHGqKrw5y+bGY8R1imP2niZJ6MNlJ134Y5t55CrIawXZY9+x+ZaV+dOCKEPDVcm0W20J+hJT1WQYGT65nNzhodlEN2VDmY+BSMFFTmJrMnvXueDm9pRChQUuxJYOogjN0hAoUOzcwoHEug/RNEt78CmaZn5X+TAGfy1s+SVGzfmSSsxT0kC3x3hdZE0VOSATmx+rTerXfPBHMshd3OERzBoXn+ugU4DeGAMnKXtB0+xpFq/9oC1BhkdTbCwzZ2oyQphEVZeAuFR4L66Vt/LKI/ewsEAe0EmP9+m8/5XKVqd1xvpYdvA0dQPQ899EmswPsAOaycceLfS/5W4jOcvwkmZZUhjZ13u8IBvVa390BopZT+M2y5kByUNyhTxFDO6vtmLr/fMT0EzStx28ikyoYvV97duc4uA5tjAtg/RS0CZeTUXWpR14aqvdubCDSYJzbvlLCvWOfcDK5V09n1f8R2PaR4T0nvNuokK433ixlw1/Pj9O2w71xCJjGVn1GXtCm3Tgmhq4AtfpTgeCbgrz2g4PU6Sng0HGbC3H+aiJ5FcY0Qpnb+upB5vW7qiy9Xu6qhIYxMD2AM/VacEERYNz5SbMtcRx/B0KIHPHSLSzylw5/xLLUC+TlYWSsjEFDr1v1AaFGfvU5OBBtbDF29CshQeYavqW5DAPBCMNKMLJDgN55eCBSEfluHTg+H7FSKqfMGI02/x/tH0AuJ9FdMUqqjiivgIV2HivVgHGPHl7J/BP9RdDUuBHEwYrgm3Kw1zEu9DEtzcl1pSbH/MGks44HMHr8kWQl5M2jXYxNTC0GksPN0X5IdKDD+m20/Rtu56qUn8khaw+vSUkUagjJQ2Murj2/PsW5byvjjqMK5tsftla3OBmH82Gc4izSnt9X4mk9UUhBXKq9I1fGK8u9N48SPfshBjZWv7oopIFyWr0Genv4oJQITBcEVfUFdc6zZ9AiOIllgQpz+ZJyVAN6BPd5miQtZNWd4d043whIlUQQED2G2nqLPPjnBgnlkeCuNgnOE+0RhiPc6uotS/u3rprPlr7oP+RWpocgbxn/wHm01JUf94WPlemzTemlqBtpxPqTQh2PJsBFB9st6SOQycakMznALeJ/9S2KnhM+JPW5YusAnIhK8/DkxaKWy3ZfPi57URg8dEKJ5rMaimHgMG3jOBTNbf2/sGFssDqRXhuyP2UTSBrl60OKInGQ9Ath8RPDYMZ9e3vfkCJMxe7PuZHrUQK5TKoPizLo9TF54/9ykWQANymOIPw2ZmvgqUCDMZM2YKktPFvxuEwwd3+J4GP9BYK51+WQwH1Td8PyH1iAL+3s4UQRVuzyuBE+WdCefuh8U9JeUuQgkye823AE2ODyqKb7N3rXRBGgUStc6IeG3ODwWILqx+L5n+qUUPOELXdj9PVPP0CiMCoEYq2Sb+bIdKabhICGT7ag5BLLFSpPLm5GqzRs2tzj5mJIPom79+RnXdeGXbjVXhyz55ziPNcgV0Fzz0w0kQ2WY5yzHFwwAVdb3ycOmNi83UXop3FRgeFJrFhfLj1MmKmL5dircqEsT3oCkUf9TUITZGHhPfdxtyrikRkUqWuherkDuWWieePNLH0s2u9AtmIZI1l8gvEgau+W7IbsuUU625xHqPmg3EqDnSeVyk2EbOYuUbqXntZe2Mu4+RcxLF3HK7GfMKqS5/h7B6oT1daKRH4IgxG9Cp6uQXNv8Q/rmNrO+fpgKNO7X73p502lQaIjNKL0jz5A30pP5WPAVMneRaV740zFcB6axTYxmBm4FVFoSdKtj8Da/1OIJ2nf3Bzg6V+Nu537zxySH24MxoQgIylIYSTMeDxZxoZnNoNHcHkT/rBqrEQQZKxQgiBK0YX/hD720PXyEBZBpJh7TLMJwmlb48UH/qBdO+HXK/Gw7Wxuanq46BqNNyrEgkrgAL1397j+akzsn+tDGJxl7Af09MiG8LwQA3baJzGYIJJOkAtBKKCueMnKTYvkBXteJknIH8gSj8PciLwDAx5NknY3pPFj8b5oMmPXBTlVb+GGDuN39IYwMsm5e1s3hSdbMRejCpRO6I27GOoKCwwIXV2zH1xXDjE06eZMpLNoHChQPjGzLrY6FSczZ0bdC6AfHZxabq7OU4C16eWAYZKvVzQwVvos76e8ppwut4Nw4wnvTQTwDIfICMJ0WleBGzsSbMuo58C7NckSFb+ZplzE4T5PsDRxMMlBzidOSOTV5oFc1zEXlPApDWZy0fGudvlwwSCqM+tncXIsbfgdXUAEs+AbO5JFREgahBgoAAWnp5p5113NFv0ELSHtk04WEw9XObzD9Mb23Ub0Zh74vVIz4sCstxQIem+OmQocQpMOUJFs6Vi5yvzx/SePW/yNtaNK7Akhf8zpSxzA5N4bDur6ZEa2zlf4SjSos6oiRu+mz43Ul+Q5YJD5eGk9T0xQa33Dt8yJ886P5PQOpXJLnGxnXZTA8+KvH8nFMLmlv6emEtiZGLsWeoECoeCVXELzF7zJFBBpGFmW49gnMrYjCqek3HSif+kcJvj9qAHKOxK8O+533u+jCxqJd81cJvgPB4SYNFjiBj452ftgCZe0rEgbfy5Qhm9JhdpsEcYlrEoxbJWPtTBWYnUU5IUqGQHeAKZ1F1F1YbUkkgjPI31YxWKgINsiFSo23X9wAjV3iMZRcqNJwHIlY+3EfXxc2hpz6VofUUps1caHVKFPYyAa1zwTJ/gMZzEsSrgJQJpN8zQDVyoAx3cqiZ9jQIxfqPWc40kCPBY7xiamojvclU50e+wZJdJJ/0px3nL+vt6uCOXbbXSCms+z1ZWijPmOCOf1Kt/XXkWojjrhPihDybAR6D7zGSmhr9R3rXWdBjywoxQlJJT+1M7tcvEVaqCei5WqoE5L8If+GRZJYTlNCnzj7FAf6vElcnWsQXrTFoFDYLyoiMnzrNvW6PN8vFQXuoebMC4IujX0YT+Klq5pj7mV9xog3MvRX2jbr+zKzQXetWUWCRFDlaWsy88gcmxVkpd2YPzp+/PZ+y8upymOIUQmrlVQZXjriRJA+jymiprUS1tuJ7rOXZi5CrhEzg+B1B1OVU455qXf0CZ9wdrwZQdWWHfhCMO7OUIB9XttseASUZT8xAVb2K63ZkFil8F9bxnw2s6nGa3PNvUU6UutNc6g21x2Ym8yph4KmvXZne4iNUtd0xxKc0B3CSTX0wD3vNagcW7AvQkhnPR8onrRq3NA1F9S+0fD7XPhSc0/aknqcF1Y/mbl6WO6/2eDyJqheDQdZ30qp1D931hdIKYysGYVBNq1Y0WVqi/03HyTDpCeUTaPPZ/E5qcji8uSeDO4ojuEq7lN1mXdK/hmpbqlblbX6r622rycEFFDewHyi8YE1a1AndoM3zYEr+UBLYspKQoWbv/fryaTWSusX7w2ngEJCiVoEsFl033e9HJ/qOBmq3G7EMIz9pFHF7ViFyIzrZT/moePeFCUiIJY6njXmXhvkVJymMCGfhJiaCU5toEZZHUxsCtYjWG1bEZXzq8pJ8WwKkLN+gUBGkmgTqPzRyRAtW+cWbO8ABnzkuOSY52x6SvIEc9rYWoc2Bpde9tL8bjWrnOFnIVj/qiGxC87+Pm5pFvPCl+AX1d3a4EFpXjMTwz2V0iPUsu7Txt97tHSIkDi+4IofUXiYoBD5O8uz/3BLRg/ONh4dEKJfP9M0/xqb94ic6pFcbIvjLPpN52yyB2hYwa26LaT1v+1FnBuh64DVg823O6m356ZKMxkpP4BCXYcptxDlKg1OwiW/cb2DVVvfpFoTxKWWC2zqfqRV8bxnU+52YXlFiVgE5OdXfza2XohtDC78dcVCZp9/SLbtezySo07IfEDqQyj5uMlFN4a+gech35fvNOyEfexdF7+hQNexm/heWRTUtg+54ASCdiAbaSD0W9H0ci0APo971DE3EAIeugH6VGmc5Qu7jaOtSfLI8i/mH1BQ03vy1+JGUUi5qPuEMkU8D4Lpq31l8geWovtSCf0vewG46ABSHmp7qI0JJiFGjz60V3PmOzVD84HRCYsY5Xru8a+9oe9k7/wE69X63G0GxW2WAh1nVw1ooe2/tvBGOQMUEwR9plNo2VBbLoFCrY4TvTKJfZLRcMd5UE7b+TlpCNI5CA0neVo0Ehgno+HVKDdnpWWPYTZ/DNQf1Euo6XrGHlNWsRfgqAriZU+uxN8yc9AWbOGtTyIKj5ZwgddHr8+CwRe7cc7yrRqlhujWVgfUb7ICdOdhnKHTIiKoAXsAkX4gPfhpxbLYY/RIJgg8DXKC/zmDxGvSw2Xrs34+0RILsQ3QiKg2edqq4ZL3FkJwIMk90DFkNw+RCNBQFc/IB4nG6zNm/qBwEWEODMBabiSYnyJpgdlTSpu143Jp2zxE9q/2mjQvHK8AoJDnwreI3MQ/13FUFuiqh9+meuumzGTVA1MX+8GR2NNB08ZCIdxTOL0N6lIJKGFmcKo7qsQVgk7uq77darr4VLn5yth4sSDPqzOEhSff+9pulhhqphYrANZM1ZR30T4FmeBjCsCJUXYpFJQp4ZfDQQMZkxAyiFPmiZD5D6zTSpXgLv5n2RRMM8o+wHjzpvVfr6uKD4vQdwGTxxiI6Ho1fJSSTHrpxv+sETaT1QB+olThOsgz8x7P3ZYF4UTnx0tuUhJxl5WxmeRjzifa1TPj8m9Nd81wjYgs08TG2oc3nGWygQsIc6Pcse4F+sYRC7iJdfQPv59PToFu4siDDsMEDuzwRUi9QHPSSEM6oU7ceRctNOEWrbtg6N1ek6+a5bo83pxuJwoeRtOTxOBMbzlvfAEEgdjVfwxZhQJSaWh08mgz6Fsk1E1JgtPvyJmeS3ZNTaRmjonXMoByCoEf7lW7mgXZhmJ3qRVvkWWsO/ehAyOD4/TWGr4CJ0YcIUOk0PYqiYEHcd4yno70pJBTT0Eyntv0mebc1eSm+hK3bo8OyOCP0VNoLnkpI02icraSY7SBmmIRsrZOzF2PaThuI8NjrfHTx6tIBG9rsTOYhsEolIrOsUsXAu2VX2Pwo3R+mK0fPXDuQ/79N65v0oXXIosRExJnN2d5tGIuxqClCaT6EDiUALQCqUhPxpOizx1HoysTlSAUYDYJW7kKnvu5L9TZxCylJ+4bTgePGVkogvYzLKdFx0niVxMxu5SD+WHOYplzB5+5E74SQ6Z617lqwlmw7oQuyFUPzvoNta8OpFETN6r/bS+DpndniunfF/D8Z/GsY0IBUBrPoD9laX0BtHDApcahhEv78FPoHMCs1eD4V80qMGRtSbG8VEUxmhbZmIiR/fodaepO2hkcJik4PiGM/Z+o8x5W3HGSCjIfo4A6dIaP5+ZW4CzvNErAcE8Lc/SCi0gqlq9wxwzAPvHOTMUFqlXHGE6XPRssH8Ti0n+HzZAtnihtiXnZKlqrYlyHq/pdMYJfQR3ZtUaQq6XhESn+L4LOiKVLNzHCKJJUT2gMq57L5Fl/ocRTIL1eLQiPRy01e8vREVWbuCrQbNYHe3tkY6PB+b2yn1/+8RG7Oe2bjUUb39C+uzp80bKBfkDavrWyd7b184p2CEnVqzPGjhhY/fjS6j13IBbPak0lUi4BsJIM4Y+YPxWADx/7m4ptMld7Ls5ACFJF4K4eFf71tmEikH1KPG/IdGdBVxMwVdFZBwHiXJkfEVBVaUdTiP6LP2PYXeWiPki16bMo2GD8cqsccSxDMLAS+yl34kB27Q0fHnNAEpsC7Ul/nQiDttFnHo29BYwmMi2XBm+pTnIMnCtJy04BwhyXPDa8tW3MegrejWCaUfyxhd79DKmoTVm+LwDRVb90nZoJbhB9BYM9w5DC9sJ6lK2dhxTCemKdsPDflnOkbcucsXQ7MvYWUWkXlwlIa9RVK00rKjsXEgyvNUoGZTfZkO6WVsWP4ejeU8MSKrFK/cNyAHujYXHHbp31uGJSz2XRXzcst4LtNTyH6q1DF1swwE4sSz0m/VvMbWoE4AgRCJHPz1xB36V2NAitijPa0H9X1VsDAUmU/bRXhg50Yx2Eg9feIgKrLULZSfbqp6CEZqiC9X9EMKGxkTzPYGbBfW1LkoGZ9ClHOmjI+nntLa3H+M9s0lw5hB6AtNkLZIW3lE5TP+dXXc59MNe/EVNDXaaX4ghEWpeoEcvronx3qTGPU4pnjkJ2nTyQVktPBO2ICgI7i5HCo0VdPeQqQsFKVSs4wRUd96YrEcWA/uZZFwo8t+eatkYBQQZO0VSK7cW8Ld0/km/po6bXdDXdJ/SY6dXwRk8e8Ypkt0Js4GguTQK7JiJqLe/Gyq9Hs35GrVwXw6x/VJdGswjD071R1FHlx4+w39sFyx/7JiJbUjrl+mcqq6NJ0ZVQuu49bxMGe8meHF7hIrIbmu0Qs0CBgMl+7JieTQ9Dit232zpM7h/eWSMpUbIN2Abe7pjExOd8lW2qUDFEYTxzHKUKRyCaI7p45UoDkUhjJHQw6NaLAyPocpusiX/ft8W1bpAf3GA/3W3OcGCvJwtkxN0jgwC5b5hS7meROFcVxTcHwFJrZTM0C5+ZCV3FQVB5WZTCKra6d+9fnOGBtpKyt18IWf7ndxFoawjG/RKyKv41rTFCoAclY4dOHAn4RPsiSZpoOtr5M37ZFgzFfBCzkK94kopOSaCRle2F4ZiTvzz87y6o/mj9Tdc7rQ8Iv05jT3sW8z7MBKLr43ceJdCFUzlmYa1VsENh3adj1ir0XlFqhZTj0bPygHylSbuVVBDHjinLR3CHr6UI8Twic306YDj+kOlFqnPApfNir7pEC4JZ8Bvrp3YkIbunODFDKkg3D3sq9QmJ1upKUk9BQmC2FEM1BEdfwdpKymorZMtV8eJlD0yJ3wL1rb1Pr4IdwYOIOD/RLxqht3WjUy+loOk+H8UVpor/eTCW78ekn3WiHXFUObKnYXCaomeDSN/1DG6z72q2RbaQQpBLi1bF/Vj7o+Q/mo7ceWFhlLv2AmbDwLxoMadvvPsYssOZ14GNj/padU5OO9mWMgXiejiOxmO4kULhwPAs8gmrjXYHrIvffJ1+/C82mch1CcBLNwjaZ0WJG+XUfaHDlMRbujl5QczBhgGWlDBiMQI3BfvYfxt5fYZXS79pUC27A2qZjk6kYTzcXtloBatkibC+rNd3z+8FOLHKfUZoQVt+wNvBhxwJQbsf5GeJuxv9qnLNLEP+RXqDxE2dLZ4iqdZFSII4SfbKR66tG9DL8rp7rJvWUUWM7Q0T2gmsedqRHUpieEKQTry3mS7P5k+IhvAeW1F99AXYcGJwCZGgEKhF0qTB0/vGVfLmkj1kC86keCqenxzCul02L7LqiHZjJfeGSwijvaTj88XeyWBTBfRKMSgmrAUEPTA9niLBxNJsKnjuFTq3WzC+vOdcpqQRKAJ+8ZPAM9DtNR0MkbG4wYlT8jhPx4EAlIbAftAUFYSYmXGW1G7MgmOiIwhKLU1saOv2Nui7EeTViduCLjVCvKCyhmyaXpDkQe4J0XPRnKmnXae8SrAp8QDJ/yY/JbCNGX7gj/gmIhauw982U9cBLOsvbvQIFCrYWGycvMWIUTnothZNinrZNg0kNXAcMiRKafbiuHoLFogVL3nTkQVYpqmiRJhUHHH4KxU1n7yux6XIIPB0wSyDChK/556HFOq5jljX982E7eKlau18BpsD+cOF6YaCydjxJFiu1ObR/zX9wlq+VwC/tNVCPhpDVMjRyynWRdNKLWuBGnVVWYJy2VXHLQQLUxe5uMz4kqw/C05WFl89yMwxFa2BCuZFzo8KCRN4bT7O4KSrz/Z6Wa+zJ4fw8cznwa+H8JMNY9tOFR3sjEOwxjcgbEf/uvyYET986bhphy1tTTNxeJxN9rZGp/ZW6Ag67NN/EebaCRPZB+QT14gQB6u+qAx9hmU2sr5M7dyrzCP7BMevJ1eGKvHOzGKpxkimpj00SZZYVhTUpf6A8RgCyOScDsJka4rRZbr/GFMZ6bkc4m9UVIK4S+kw1Da6xwFc4MnbUrH2et8ogYBmkyF6xUT67e/AEym5NOwrGjBbVsrh4sGkfXIv1LBsvJvD/PFgZi94EmVFSaxe3ZFmAs0N2pYoevUvrDIwDiCSohsUgmY2V9inh+upY5LcYuomokKUbWHzSJwqCYIi56lOQlPs6ZXXgo9yvCR36XTWk2i6TcBmTygaEe/mvT04TYr6CyrnXb6v+NILTAbrVgCShV1mHLpZK5tkJ+Zduqvnw6SBMg9nVKGdKnYmOsg6k9gy0esWt8TRPYyaHhv23CSV6hd9n7elMvz4lS1gqTtk/OrlKBvNs0XxcauFE7RWGOqpcI71bDpQT03Qrw40lum/kG68pyNDHlg2xcND9CDO1Tx7zw3OdRG5YuRUiysc/kw28pswpPVh900qZVNJPkKjzQJgiqxwiD046QDfRdJNfBRQdu+7asgbX4KGRP0dC4fKj+Y/5dz9TX+a7T37zaB/JhtvsJ4vzl82AX3wqBgaHvp1FH4ljyI73LxYeaGAFQNjw3mR308yvDkeoTGX1pQxSxGIop4PcgCAlodby+su608DTh5o2ESo9/KWBL/zTxbFaGGKd4aOPaxKU5QfGrX9Vfpovu6FVGHpnwVK3gm2qd7q5q2Y4Uu88Rbif7IQ+wnrSrf1Rbl6oftbn129nasNq/42VArRMcv0BCDzuaGoXuTD/bvRc9dXXYOgMF4VCEVUBheg2cRLA1BclY5Wa9mUNUwerq3XZK7NXOXbogTyqExFKK9bdPI2Puv86gDFPu286eYsgjCUJUKCIBj5IaZYRsh8kvTpvNkVaTr9IRx9I/1ElSCpIhylttU42iRM6+F+IZC/9ZsenASE2TU34e4cv1g4vDWK7kIk/2goRTRCV9DD39sW+CjFHcf4xWxrzSppiAVzChf5PCRCf6u5VUv15EzaCZdwfwBYHllTgFp8QFmZM680paLihnnhhe1wl+Qt3L7H5plElSR8MgC3mp+5TpsR75yaiifOi0ZBAfxwmk6MRgQ1KNqHNXfcB+ViCSLHe8R0TvsRy3LJJr6n74ZJ7qYS2UPffsZJl42Aai8PBo6E58nnlyMuhFJIcsGwlnV+zZDCMxFFCHRZyASXeHlj+upy6pkwgX3BOg4B8qFbg+YG0j2zMSaRelPVCk0yy6xtkLdQHWVBZQwG+oHPatELcA4+EZ/Gq96tv8K+I87fFg7Q07WYvjPR4/mPAlh6WtMpKyOf8RKFyJ1YqUS/vvUwEdQMzv42Wq3uD8wNTzE2ReL6EScMByK5nFGqkMNAYKccJdielYLNcBtb+dC7a6qqKgrOHeU8SJ6G18X/mmF+j2R/ifi2/h6e4tyU5j0hBsuWxHxWUO9qbc8nPgcJCYLcU9os4gPJadMyDJQgcwruRhT4vHnYNEGkXmDKEzl+mV01mMBudkCuXz1g0y+tCWC00VQi2X5FPvJccrrDzmyZwWkPDyzR7eovw5qGntSC+iMb2MOHG+M1HbPLAfP5nTy1NKyLtSY28cmncKYlW9stYtuspOCSagy/DtBwWngz2Gh1/NjIGhuX4c++ZGfDQDy/c5stFltBwJIs+BMY1Xax3YUh282vW+edtRFQOzqD9IE+ic+6YGhUTrggrUjBFC7M30y2ev+r+Vq47bpL+llqPjE12tuo4ioT0l3RqnaWS1PhyGOvpyF2nmVwg0imbotc16Xynz9cW/5DAm6JPrhHvh2O7+Rp8bfzNhYsw2gGhIQ4cdS1qao5pdvP8Hp94CaAmzELi2GN1uZePUV9Nkz3u9Zr3jlwezPwvrPzVsWJYDd9CP0uRWy08RLllBPO4QqoZwdcZuHmG38asVx8qMqjgK7ebOHEMGSRqjcnqYStcowvyIAAQ8dkeI+WFka7ayODV49SKElfEShioqK4/p4rRoyq0M4/YytehLgu/o0pZ2zTkBbgw9MP0tqWoFxh0fgwLMRDup8AzXc6J0ljmDc1m7eQMJeCYOqom0wUiHh1lAU5u2S7f1DnNQR0eixaU2uE0O54LjLQIcM9rt7oVVaw6tkW/TKVPTPUGgWQbmU8nBBt5yF75nLj6uOwrZrQ2kvJKfYKUW7QPHeqNsdck446sEoeRKB96hyrvmikvmPdscIA1tvSyXJeOJSbsayAFebljYJMhRKeqjz/NJXjoAxfHsosmGOgjFPV3611DNfGFt1k6YBxygpcO6LUKb8M0j2tddYSpbRR3P/wphjC7//9upWG0la80vY01Xd6i/MtY2Xvp6Es31vBN8TYeyel0CmE/K/x1260QoqZRmAvaTULFm1i+LtE1KqXPBUkFCFrqD1RE9avSUP5U2QjYfZxQW+L6OBN2Xm0uV4ciXfuRjCFAJzy4wntmwNUalSEKgBTZcy3s0OTdewnRrq2yXNyJg4JVCQ0Ic+eSzNbKObSq3znw/Kn6rGhTiVfeVDF6JJhHjaNyyBm9iAHtTRGUuya9Chpi7KzJIg8XIgAGY7HgeIENdhFuIexSa/+6hSj+8Ix6BJh/MtxRor9Bom4zHN9m/XneC4gSWkPx3gVwAtOuHBQ1rLo5SjBt2coIbZTtIhGFeeZ6KO2GZfcCflOEA0f4n40aqKGoTc/Rsu53iztRCvto9EBLapLihkV6lDwFCk+AFGDBie/kY6IgcX+HHsMmUGtRBPk4M+2E3cga5uDFNqDxXKZjHDffskuvXzU8vGQqSuukNlJttAMW4FNmJmfkLB2RS2suDb5rK2E3llRcynmr6gjRn2IFuC63L1zaBdOzZdzcL0NFIENV9Rh0JIhe/Bqnyur04EfoEQSy3p4EoKQ4H1S6N4FDGpLb7VF5/wv9AOQoBqh34tQgf/7JD3u6bINVt9z8COpBhoFsT/8xpMEl5491yjpGmmYtVjLq2RFXgCm2lVXpJ50DBZAtwphm7yqbog1bnQddl8mvQdWQtbf6BT+uJsQ4exi2XYxaOSeB6DUrGKzutndgaGLwTmKEDeMLaaU/DEpCCChcNTU0HnjQ9HvrvLv0/R2QwjfrqWAQHgXe63qah1VA7nSylPozy6GUTpthj3jlPYlzt4ITiM+KqC8jSVeCHQToPnIYjmMyVlD6t9yRtx4mk0VP3OiP/qyskRLdonr1zRWHm0EdkLsLbEl60qovXPKX+cqe0TdU3K+5Gh1uvlGzcHP9uUkjSr59txRqOxgMmuaEw9M9srx1lzDc0bLgm83REScwcQNaLhuwWWG4ai14vE1boc8BkRxr0b4YGEve3SAvXh633IBCUI+zwbvfQ0x7KWvplUBmfJYPO9Axc4sft5FnFE0JqMn/iigBXp9PBB0cCkGfVwQCE2eurIZwntRsy38M6kMHVz5vUZspUSgkSNGVZE54uquhlPVXL0IMqBusOqQvcITnOpumQaDZY6lJIRzSlcW/rjC37RU76/dH7QYWGpfzb0DO7QIZeevotByZcnhOGLUWy45TWJJnP28RNo/MAENl/YqNwx2PO4tcgh0IDt8nPHd6+d1J2nr7MJV5sTdLmD+TeWhYZuJ2Wg5r3B7x79huQ/NrqwCh7SoZHSonD38Y9dmg7CXrOivECfe2FpjgctoBW5fl7S1hALKf9dzx8i3bwgudI8DzWsMCqKaRYqeBBogKzuNHnnFeJ3vZLzD06fkVaBD+Meihox/fjJljCwgrR1PJ17tEdaapXWzsZUYHglJTSZhvTXg5ogfmmURx++6GdTSXw4HPo6wX+sHKt4bz7tQ5eoJ4RuLGpi6bLd67CKUyXEWiH6+0iqevkhgEC4Q+WgvAPT/b1ZhZ66cw2Bldj7xqXzoS+Kxr6/yprJ0K4v1WN6dP0VYudl5dIjT1uW38s4XpIM3AHbDvd5sa/KCE7FRMiiukPGp77sOG/FIqXJ4ayfTUGFkHXqA0C3kKAH9gDVjc1xHTFWvesoaEFjc4QGq7lqrTSdGUq56YCTqW6q655bHU6kipZB7eS49coMAU6pMDSyDZtXPhtiqZXuzTjiklwHBixV5zN1jxeKKUWkvEPcaBvARG/ov2g05B8SleoHb6o2GWSdzSfCtgRABYr22iV/BoTik8e/DrqXyawzPIZwsB66Bukw95DYOuxjUe+rwIquwakUfQNA+JvDd2yZACbcVsJnI/0fNXdB4QzNCA18yQovVWTcBJ4wE+jPysBlx67FvXsGUHIqzgFcIk1PEchu7pbxQHLGdp64V7vWSqa4yBtYoEMkqRhQT7RE+0JPPzFKibgoQ8RpRJnG9s0AEu8bwyAZ5fQdsvjFaH0Ga27peVc9hKTWrl/CXlJqOJpA9hOYGpeLZHP7RUZcZHhwIKuXuijP4u2cG9vEoEGNtCmKu1ulT04BhVNXDDj05BicewrdwfZstMWHm1tep1iQpH1+L9ubbpkfvEPaDIKz4MmFXUgQmORo8VN45HIJ9xbIb2SaRhAQzDBBSfkuUrtlTZCXhK2/v3S800MEjmCNYqkgWDz0tptqztJyyBM5UJZ30eVeU86W18CDeRqvQOJZC9AuzbIChGDqAFbp5wZPIJiQG6SqIwRGV5YUVmwp83cUwkwLOxD/AY/RViykLCv9oop9Pe+lAJED1J7tuAYMdhrXb6oN52BbRj2/2zGWMRcYi4t37FixqFNC1+TuVEeKgXBM0AhfxwXawohJTAuVCNCKHWPLUfdSw8kL2EH7KLTAA0RiqE4przpdeVXqITVhOdgIHZkk5jY8YfWq8EK3FXebppoR2p/PS46eZ5cYc/9eypENQiIVFCaNjmVwxgkApGueGX5SOHrDEIfA/hdBX2cLgKdX4F3ka46HmRbu8fKfU79Q5/mLZufFjHyEkO3Sv1HKid6YR8aZrc1IBOQZSBbKFNVlVc6dmevL1tQ2ATcZ3WufToHhN1sRXtYAy+F3ZbuPnwdcJLTxery6kh2aDN9aysAkoO1lTZ6Oqh4gH8GXWfapfvpZ7L5nyJOJAM/bpm7GKKoPiM8lnJyo44kQirECKvk6nQRuTkb9vlxEW0mTR4maWCgHuLu0bXqcCvJegupL6C3mreWIvi+3Zo0ORH8iz/DB//UHBoMYNR7vOuucLbQFh/8DAV8lhAixeiNl98O4kei8I/gWLJ0BEwZ+oS0Zs5tk1M8UIZofBMNyuxOjDuPuHg3h7shwKUTGCRnPsDeQnHQCRmFcy9cimwHOMJmf7hxEoyCeeq9SVGaJSuxCTNVTbUgpNXKC5J17Hs7G4caT3iH8PRnfBNg1QiGYtuzKnMyD3AnJYRNWeKHgLCMe9oL499gWxRAwbRsIsT0+d3r3lx/qqftd3gIbeQdDvXAeBPPLkeGrMMHyZs0DpE69RR8JveljesPoExJTiHREE3nxRL7/sCtDvuVEu6xLt0sqvPkQ/CnD5LRHNAy9I3EHvGebE1j3moA9wiqiSF+hmGGlWyDeloLkjG5Fb5J/yUislQIKJPpDV+vVZchFrIBJTzUqYdIg9BEBtO29Auydh2xJKNzYG+QP+TJ0dkaYCaGMK2QrfMZWhuOW13c5YSLk65sRmxsno1zE8Se/It8oBxsp5v1dAtxZhoGbH/OGt0ep3RYRFOL6w5IDsQ0bCN4+BsbFv4OIVdaSaBEEVw2H3Q3g0HhcgpMjF1ULbuP9EVjcadKRAOt4PsZTX+qtcGlFT3IlsHkd3/mcozln/xFRobVwuGQs/dMHCUeobPx0YmaUV41n9opBT8jnUgA1DH5Xp6D5yW+iMyrEGGmDySX3pq6vJEYUe09MeRsZMN8e1v+8i9adUePjsPDLaX4jprf/AKttlQLyO2cnoFFbMtee2xQSeSPnoW8vpM8/XYJshSlKLGzKVtOqZ1FI3OP91wdXBCTyihsjawybLe3Lv+EtzSiN7B5FXPeTx0VCxjj8RPk3dt7QpGwYShVy4BeP1FeN+zgFf8Wv7g7mV5w5AQyY0TP9J0Oft7jVYLRmHAIf2/aGL56SkEfRQAiaWxbSNmHzbDl27tadbsrEu3upM8NoJ1Rp49SxHFXMIU29UKxmikIyimZlJDEgL0bQrTT0dYNXYhbBFWHBOl/eJ2kJqPDKuEVnahvFz7p82t4/aAPN2QtN0R4ZevAup/Ehd42KHi1RP2hbdi7237A7up/r9RM3WopxMySe6o6VbOG0QBevaYVzBTBxzoDPIyaaldPzAaO/mATf977OtoGdBmUNwllQUuhOCyiTGeQhGyM2MTpVqpckUVF+dHiXt2IJsrM5/mSmwegzqvRJ+fwdvVxM5dG8DvGd2ACv0Y6s+c5ySkguK3BDRC1rMt+U/7rWzbR82mYShiq0XGxlBLmauMedlBWakksDaKsFu3+iMktJrt14cn4rZ2SSJg+iivXfsx+7EgTYdUdCTlIFonX70FgN41gL2U2c3+Ny4i2P0AAn0XoV+qTaDjR8G1JuVl4UTWgXw0ELoibTS+pwe9bMUL8MF57Pb8WoggZwjjZT4UCcf17d0rEnaJUomhEvASE6JL9S0yLxGFXaPB56gjWeQv2DIiQyrfTNUNXs6WOIm0Cjuww7MKpXyeC+GpGv7ny44Cgdb8QxslcpxjP/n5SYIjXB52pwDneii8WcOKy/1XPDrwVeOZHutoe570E8C4Te6HPkjBUdAs49dJeYleHdxgL7q3iQbcsG8hYSXnp+BB9hB+0BlCcUsI/UPBcY7MYn7ooFQje318LW99FQVdD6FNdFwmc1ZfzE404B5gXX700UMIk9rGN9PLdCB+xN9lm/1j2W/jpBeHWSCVxlz7TEQm2qWAxq8l7AfX+zWitiR8ZqcqBVBLTus2Ceo7zp/QJUiv9xMK4O7EGOfJF2TyqZC6at2BsqGTajdlI8MZ4tuZ+P65WYzBxwlzfnqpr9gaFYTn6zlBqsrmnjgmM+cPN3XgRhvd9SabdmQb7gOTdZBIyC1fQwk8VEPVd1pW9biUfds2Kt8cpSCG7irSfmiJY3LId+MykjgAi7r5PHBC+GH6o9ZakmamXpf3ub4bDh1FoPkKwQz5M8Q/IHQY0u5EaKpsHp8sd7PK+9ICFKAYv+TSbjfGrxL0y82SlIrjcPUaZfym4cZLTKus7n4j+pJMLo8KAySzMCOJrjbevcA8o4o4rZ1ymg6chH61Hf00yml5jxyS4P77GiZeM/lTAM163dxHvqT1nIU7kyRNeFG58wcnQtIcUm/4bDkg8/G/UKl6yEIm1mcg5RLXiv7cASvy3l3e38N0gp/PiFh+MVERsxw8THlnyLrHpPbiRyhY3/JfBDdS0qySnDtLKsssBqacGtyGz7anVLZsk/h5IEm08K7NJQrr2CMQTdG/Qdu3fQKjeFRh+PfsUwRVRkx4DpsDiuFgdKaOKQMCCwdJgiwE1+23Mm+H0If+Qj79KDNTCurGZr+5EouAeAIssCfVQvat7fYHMAbRYUv8uWs6Lb0oaoUfLQR8NxfxjpkGFSwzgZtV/mKdmtZu5GbKX6//HH/cTRe0bd3g/Y2L6j2K7NcnojwU/dS/eIAsXQsbfTThLdkqOqN5Rm5Wv32ch9Cv2Xmk5uNAEE2/IukJ375ApqJ2m+iLExgE4BP5b0VzXENnP/DlPlUyEEDfEUI8CyP4LgAc29Qg3GHZLifjYXNS1jpvU1ZK2u06kT2FqoE5xTbC25jXcK1BnpN0sZQGdCOJ7iWits93V3SkTR82FpqH+okWKmHKak7wOhMkpk/8+u5N3tL7AzM3Zo82h59WlSejr6Aq9yxMitG6jJKaxkmC2VhHmzpTC78PsoRFEkhOmyVA4vEbK4SB0Hwj0YCtdkzWUfuutY9l2vJjUwGMIHIBT7G5wy5VlTtL2HKwrt4IyE9lahn8I4GkCqHViijvUw8uyPZBcdXJ2rJDXH7rCAXlX9bRUcR0J/K2Wi7j2HY97bcvWpQRVaEVXqM4CB86irVlZxOIda9NqGB6xcM/eA+zk6EUQdHLO5cKhkK8IpV9liMjv0FRdriLWNZWx43R20P9mWaPDEERocsGAMhCNbdHcDFDm5BXh6dW9Fdh8t7y7CY9IpIHqch6TNKx9Oquzea4VfGQPisFlfmwUdV5zQrozmlOxVHAa3JCf9fd5kpkVpLk3nkDwJUMiHA165W9PN72I5Ireht54FNbaPbN6fmXnMCn9QXDdYmPblEUfWt64JNizCEHwDZHNocElKoNu17Z6i4/1Nm89pq6/jb1HfyNOAeoZ4K89U6BM/SkeWqw+JPso1e7LRYDd2CZDgIkNdCKTd9s2AsvF3iKfONQqB6SgyBJGg7Ox8Au18LCdKt51yuk8tZdDg2PjmB9hnNzTD5wU9POBF5JaXWtjU4YDv62VDeqnzXUsG92w8pMbPH7j5D7Ofrf6P+k55ZNs0nfYfC5CAEJ7NhuTOjlvGmOa/MQkxMiN0U2UpafFzU2OI5nXsZrc2UFpVoR4+rwOIJg2xXqvjFh5qnzXAtQHOFHIqXtkqP6c6+4d6bUdZhiYjxXiB5wHWL1WkwKF4mTTJSYszLTASZrhRh47AmTlsGIw4FVE83mUlu939gJBQipYUfksSrZ2FSQZ0KiQnZXlMseNgNvJsICHRlwUi4xocen4WcQHNYYPEJNgRpL0KP8lwYWKhplYMeaNmV492cubv4QH/xHtslRwHRkfjOOPEeXo/lgHIBs+VC9ZDlnlj0W9KLtfd5m+Q8TNAEaeWt3A/hYuMBZ5kK3myHli0a2N6KTPJlGr/pq1j/6VfKgizbv7oekRRToKpX2nV0yooeEYyMcO+OcFYMULV1Z7abmfWeL9nFkif1ahwn2o07749e9Qihkuy9U0XbEJnHUUIZcYo8aJ0F5wZj1xzNf8a4mPl9rO15TvRUih7EZvRFduv19qo2NH2nYXOH2cQyFw/vy1laBHxBk8JSGpup524NaUclHlD6W4cpEPSDfa+EEwrAdDtdQjxGJvSa2ufodcnHXpFP/1cQ6pyguPyceF1yYaAAdA6hhp0VmfSfusarZacCAEVKVq6EXvahXKVQ7G8x6l2mfqUN9uxvvMyIuezIntn0MMc36SHnOiYxcx7eanAom3gEkUkdpZRxjm64PVvJNxapBzipeCgApO8PzpMZJtfELuCZpo0AFHasTO9r0/QyLs0DmQ70iSiiJshJFGDZlih6pJX0JVglUO7x8/ERTj+p0O8ZTKDyvB9YB06Z9+fn3lTfXEOpZQlObLv8mfzVOqFghwut0XRGIfu5jspD122ifn6bSj0DhZUS/vpo8I3xubILWjKUjYlicdpjy/wDRr/vRVHPtM0BnEgY64XZRM0N1OrZzMZPwJpKKAL/PQWk9g+EGbMgha1re7YdEgwQqz7AZvhjmbi17ZndUUiVwBh1QbFWRInO4dLIZ1PGjHKZXpNv68ZlwcMCazFM/eny/NdyqFWlXYde3clU/go1D5ASGgifuMxpiWdvTpjg8S9Xyh0J3EWgVtz+OHRKhEi+FQHFtH6jIeFgxMUHmwZou4KSuAhz7g0v5OrFdewO5X7xF7cJkuUKvFypZoWn3JdXDcwp0JzSqbYZYHQoav+IjN18HgNmYlKPYaVYkSMQWkwCMLyI0p1ebhc6R4TTj+Hfgz5LGADyG4MTA1LCXSyhLcMzeepxd1D/ZsA3tqKvh108z4M9YsS6JRzm7PQiwMSdRbot/kO2BbIT7eU3kmGkCb2Gs7zAePyh5gIUchsWAxoX2O3odJVHVd2yq8SVjaW5X3RXZezqB9Iv+KU+2tNgpXCNPx9HAIHvR9sKFPA+l3UcmnHERiW2i39paWl6DGelO7fpSpF3qfpVcHO0I1PnVBB3LtMBGtyb3jwLEOPS9G0w5yaElF00weG0E+3RvwDpRa75hQTza+Li2rhdkpackW387cvFFfT/d6hG/i9NkRvwAJWFt9CQ92+2qFDYEipl/clLtwKEtyw9uEuZwuJJ8B5Hx6Ml8CbuOwEkDFyIiHhMusTejRACtnWEZ19axfyPM5ziFjRBqef4S9qypF+4JjS9F7H4xi0r0u1EjMFSp6fdp2UJSkHM0ZGroEK6nnWcs2ctuCvZeryh/LhvLLS3I3pVKqwXx6si9NCmJNKh26y0EWgeAXks/+yFQQgMORJaPvayWKB2psxHq5HJIj/kchlBI2eb7WsVnH3eVnt/R4s6bRyQuf4H2hcHSzNtYK3wH79CeFmOgFaoD3VfqO4Gmu8tbGb/7jcseHSA2hbvA/Uq0SrHhu0oOClCaIUoUeZPgM9NMmYTBGCr6eoioDgnMmtyKgLAsumbHBIKsLGHo50IzwLkfW2Qzg1hgdhlpKNJFCmIKbPaeMCSHqjjQDEK4m9m7a34m98mPQac087ccaFIG+sOjssahTfvypmfwricXwt5PuXRPTQgHCtzNd6LaZm5IfrzaByKGihr8m/UFnSSxntaY3K23k+DUQQePZWRKPc4d4bKyCz4KaVWoEUAtWOQJI4lJSiiygl66wtSWfR1/vDU5IyNFAs4AS2TWCnC/+OI0oZ8H94zPiC4g3PVcob9rvFth7Zob/HvM1F6jnW6BU/0CneavdklNxSYuM+DMTFSUGL+bTYHllf3Vr6+/gSf+JuuzuH6jn3xSGqA/Xm8ne0omRY9Lh9LIzLXVWLg/3kScTwL7d40Jf4xC9bukFW0kR+1VVyNAHFzUWUAzW70MsAHf81T5laaL+17NTUbLo2Pkoy3SgFxHinmWAhityiQHJLH9IZBTklY6vI7WRMPta1kl3xFabia0SnkP1nnZiY08BQ0+9ZfF2Aizf2zUPpZ//VuQFy+MYsR5Ie1id0s3BiYinie5UdCYeabAr5gWcD1OOu6hDHH2ZQYFqzkzBChdtzYSIukjAai2wDEA6D4ZP5PvLabyRLcMFqWtbmLGZSAeFzVZpmbNuZEsOgWZeVk8S7HwXfnFFY8LP/FiubiTWg0nYy3lwTvUxujsx53Ou+MnS5uwIH63FAHZPhiyA3/sBVAw5Y6OBq++43wSfet+EdW1JserFN+/+u906VsVUNdC3uxxlneNZHEYAI4MMxmp0A1L+znUAluIIb6+U6ap55YbSf/63b9SmNpVrXNP3gFOGEFivMiN7613CCXNuHVLfL2+dZPi2BRhdjNqUdRVXElFqcVBisYZQ/zAKkwVtCWEB9vUH2rg4GArOQkaDXAdT5htEkFIkqhHO+vP4oYLUnQeYe83DHi2XuC4/cwsvNLxWBkaEn9LyK9r6+OElOR7xjEU57ncQ+q0/7bSTbDSdBeIMVuMU1BjuYvVBhXqEWb9pRkInBdhuAv7X7MZQS5vKjDFq9CBM4k+/SkSKlKGA+22HrbYtBqKMFfLlhMpbsbInceEKQrPBcT3+pTQ5cT7sAsmFAoVNsJXejBqSVRxEP8jVrBFN92Ea+8R9R/Jcf9db98n49j1jc8DhyAIf++Zp3q7o3HG39OIvD/Avj9OspLchQadAPossp3wfJ2GpetHhzrOC9Zu6jsDVkhswa0gX9kguqYpjZJ4RSc8+m/sw5aq/WKXmXMfsET0s99nCKQsipE02c3866z3v1RL0EDcEBgZbqhQvKdtsZFR5MPKa+HYKdhIaeuKHsX5VJqfbUtGxS4OY37pdyHJ38Ks/fDkUyOfrAof7Fwma3EaqEyRod1PM1XbehAxuj8VdYih+2HeYpC9ajP+kA508aPGh4NnYv8708zyYVJYYdBHP4SuiYwEOlblsy4l4DSZ9/EUtLNzuordYKoxk4Rub7WXIrwcAeXdowoZu380gD9ySQyfhrB0iE8QBnJwGsl2ANMlGPWEkBY2PoCfM4w5plVeSfHA5l6OqCoazHzCBiyXbqUefKdOg6HUUN4jnhMGoPWZfK2DbBv8BEYFEdTVvunwUaujd/esxb/Iu4pBj98PJgheES7mj5F2qQayTv7N1rmAWVltm877uxBg4wPTlw6QsEfwLEJ8qUbRBWp6Mq1txS0YZemEYwNvbaNbSEADi/15p/Vz/1CQxc7ldhBaGkJfiv296EV7cOoNzDNTDyaJQ0jpKBO2dUTRLFwpD8ICMV2nr+sZFuOP3BCcZNnapaV8Z7Ekmf1qokImy4BszOjVcmc9T4eipNs5kq2UdLUxvpWaH+W9nzmfr5B1L8PVMkG5TIAG0EeME5HBUa/vMfoR14s24XbWSMwQ7xF6zDHCU0dBx+QTmGkRNPK4XCUCz1ENZCXyB0QvvSBI7xpqKnogvRimGGF1qSMLky7DC1qlDa5mw6BGQT/PaNYv7RN3UfT52KVTjnqB7HtkrT6/cEaubP9c3V8InsP1j4Br4kwYqEdBOMp82tmDFROGVmv2zVkw9rTNBLoSPGL5Qstbclj6KoYMgY1V8FjVMTFKVNa9qt+KN/MdSc7+2z7a1o17VARCgqlpakRwpGF80CqeZR659LSiOEMEw/Sp0otOtpicUHmvDt2WAShVtmw/XlM52lrJM+zfRBVpR23BIT3ht1wd2K4ykuUKM9JX0q0ENI3o+Wx9n1NplOy+74jHRVn/n73rcQ6lqOSx4qnRNuWTyLj13PuPFOrQC8CVMd1zZyYosu/athmu4KTcSNPXU0mDFJUZKiyTvGHluXux8C5c8VSIAl2tbP/3BHJfh/D2Y3N+hWE0D+t+BBvApWKeuh41TYmO6i94a9tN8cvUsWKQ/Dg263/PMM0cD2D5xtwJD0zURmspwMw7+/EMPhW5/AJNub7zjx0oFzLtuo1YuD/BAp6G9iDciGWXtnyu0HYOYozZNKdv8XtusT/NPqiXkePjhrc8ANsMl6LS75HR0VQExuGWj3ozSuNWfG6/udi6OIyo8gvxRv5Xs6waL84fjfKvlqxfHIPVTAxzv9sa2Ogdv2/+dqIfpamcyqJf3FGUYBKzrOij2H+gPbYWNiNSykynOU153kJ659xLOVYr9gNbvRNYs3JyzCnEithJddtceZXJk1b8R3B+mdFCWP6da6tTzArvtwnqynfbYQP+GganSFqtkFDkwyCja2oML8cLUUK5FcGyx5LaPbQ8gzGtUFnau599XDLYVyWSGuJT5K8Jcrc01M8V6D9Yg0RdCZAbSql331Xm7YbF8QLFRptGBNiqH2gfvT6p3CxrzvyKoMc9vJqvFG8IwVYvrNZDmxb9gFV3JSb06mCGAFK/F/byDRljsmyyQuHjYwBpS185fy2hOdH30tuj7QU0P2JDCL8zzndeJZljuGC165c5Jx0wLYpQ4TPdGedsoS1CllDrKGyJOxQyx23ILpFwSzLeyjdnr8IuYf/xACLAk5uTQ0Fuvushm/N31TR+yOQimvMDoQGPxEHEWuI3+1ytfQFuJtlr8ZYXqSFGiygfqL6bGAuJrUnsdTuhvmZk39CDyrIdPQnzWDCU8idbVycePMw/nX1ik6lugKeb/QeT+Z3Jptk4yuCDdKRPg0qye2hfNzzM/sOTodp7GK6LQIlZp1xVu4AuHGgqQLtOAg3iV381zUJ/0aBbcomQd8e3CTQyIlFqvOE7ScqbugXkfIbSHqUHLXQjaebL3gcxQXLGuj4KXL+IicBR/OlKfKfb96Y//5QTEKQyH7lcwyYlaNDQoAIcaIL/CT9/ez1VHx7udQ8nn1go03x2FP9GjtNWVTi6/wkx9dMbr36q4El5fxvibAlumlcHauKIiZRVnEnu7d0q746beKTNWQAFiDG36q9UyTx2EN3UIDtBAYX0jvlg43LGTaDXZe+xmlNNsX4FqtL0OYVxsXHJx4ggbTkprKadIpq6MMfY97mlmrRANJ/eFf30JSpGF923kDQ3LryFeiaaYVTqg5ISr4AbAv/z+Hd34q1vCh2W1WYRfcikFfpABK9bpBSmtvBnq6cwT7qjwUheaDXIVjhwU9WADekR9UKiUqnZmBvKh90pLRFo3WfaiuocXsuhJyI8gQOEZAANaCOAt7wHil8zCExwtEPWlkC1qJGeUgBH6eRil/IukHEm/J7BhDeFhbdA2SE6230FNRE02xOf0zvtVGv93IC9eq3zACNtwVEamTDXxu3RNyLURF4X5mlGWbGFfTqQsaEBz4To+dMFBXC8znHtYacAMyY3OJzSy1100AY2UNjRi+Zamqbb2WEvllSkn9USSWWb6olkURNvxd/cIpQMHUeo1vJ3kQ+OEaBFLhnuJZg3FgelRP3h9tkdZnwlPG04kor04Fwt1k0LutADpsKUj3ZcQ1nh5LDkeNry42m/Q6Qq6ZH+gJgBn+FTfLqbD48Pz3qTd2zT1MY7bL7nlM11W27SCrAXgkTGxJGntDK9etLJHE7uLX5i7Cti1+HA5ag/0d/bOM83/FkWX+Ww7MZbd7YQ858EHQ7YqTMPCiqerya5uUsHlidHbqUN7VwYuAHCeIPd5+7msKJgxA/32H8gRZ117pfKP4BDVstcdleQYN/gY0j1PVih7s49zLcgQh2LnCmE5gvs1Z6TY/dM9ShKfeyPEXU3JaOwyffNy6kJNiRItEHdS17gU2+7allVeanacdjxC6nXX73pyMKuyJPJ/kh5xnWvxvWYtnIVbjVYwH06bHlIYdikv4S8l9QvgPG+yjtxe6SRRYrtC8+msohYDRpU2M/ay3rH/VpErKEofOhNjZrtB9I+LbRqi0f0WThUKgkv2RXQnNfmyALE/AhNr4FNLIO3bgFvs/WAY56DM0baPL7xVno7I4pXBOTnMjyyXaSW0pBh+42BFF6OM2Ta1Vhp/Js6W9YszY4Df6mPv1PQYMVtjvnC5xIU2PaEk7kp0OS+K//+SJ2h9c/2uvx6RfqbOC0/zDozKIHHFHtBiVEdb+bNl49Yt4xuOg07y3/ufZirLvlsH/v/yfSC62B8m4J9cLfyBC2svtKT5aXpjlXf/MqXc6SF59w8WzsiGjOfCriOZFW8BIPU+wWb9mqJm8bIc86bS6URU+CUmojlAZxnUMRsiU/nZC9LpPZTcOLjFraEr7bSPK4Blufd010712sbOtL1oOfZp1HMbcpRGKcTejUeOmTHtVRXyLfpOchmWBmoKp9lKMsgeWEFFJWTez5r5ztmYzpy0vrhegsGhXAXlNbyTbPOZ5gKFOrDl3XzgcJPD/q29Qy4BD89TcP+pbD9j8talCfBU9Y9/+WahCJSGnbdos9vM5bjZpEVDRc5n4HNf7UKL0w6gZw+ps3XOwylAWmdXw76Vrs/jIQt+Y5Q4znMt1SofokJN7c0m0EROceIq95w52qEFxvebZIpOxG9RVB2+4YU1r84ejBg0xtmBD4XJRjYltc8e/d92wLA8plMh2er4ATER5rrsvNFFu9CjI7Ho39XWrvMIIEtftd+xGgiKOcFdtH3jWCe8Gqo6mSx4N6s6MFNyVn5KWAAISMUcPJpr8mWU1bQRJRQcM+Htg6nsxPBe5GB1vkOrJQPnRSbmjJK+DhTW3o7rKM4qlVaY1iw4pOFfj5UnnumiYuWly72p/jqtgF0zEEllLJVKArXVLVVCqQNWZ4WWrFqZ6+c4uKIfhXCEhQ0SHQiDPF6St0/ItMr0yqUtDJLFIAXqi8bH9LqAMWGQjhvBp+Nr3oMMUedmXV+XQjdVvdm774zPwIyv1OlWkZeFGgoXWx/nXIcn8Lk+mUWeR4cTw+KKg1nYu5bFLwPHQ7Zc3w9F3h1CucRQA80D8rOECUJn6tTrD2+hOrCOMVoxFmrw2sTaj7gFDk7noYPMWmpCSg5HlK4sGfUWRVnzot+Gadbivimkqb1rE8njyi8M31fGJcXkhRKs4EWdEkakbN10yL8aUW1NV2UGvMCoTViROAWfpuHrQXdk58eRDTxc2Ee0rtSEkn5GUn5r2hjyjBbh66b+QHQsPV5VpQUFQflkIBBMu1l03S6mJdAK0eZ1BfoV9TFACuaq+WMY0Rq73+pXUipuJfWai9LxPoj6g8KNgf8frMLa62wTB9Xea2dR9wxjmyxQjKHjbWDpipP7u1tI0n/8odk6n/2x3ka07Y/es6Qd7MgCD4PXv8mTqVffxO3UX45y/vEklHWDXXSZ/D/s/Pz6cX/6HWGQzyee97H4b97Bjq1XJZzVWCbJjpb8P7KiIdtOTNiexOVVyYuNHXjuowBq5j3X5DJtFxAaMbAdCwunkf2BrDkB3fdnt+35QsXaoygb9JzYLvL+gustGhXzYaJogvtu6Hk0JAYWafss9De+ALxu9vWGT9+/TBRiCnREiO16VZdAFCeSi2k6xkFcrggAPZ5ltukXL+IVkw5Rioy0x6CaviiIn/BF5txtNfoPA69/SC6hwiPVwXFDIkX4fCBYrq6nNZHUxaG6y90ePv37G6rgRq18FjfUEclbusxLOXyY3vvm9JGvaaeUrLuLqacCh9HTpr7bsbMqvTznPwIYdKVb81ajsg1q2LD4fh2AxurYpTqCKbhxsq2aRCECLNA767VhROabGnZ5NqG6H57tQCEwrunQE9iRhs77rb5R/PEECF9pxSzl6rRY08+lrAm0UwXGoqoeIW7LK8YFi2xS8kRzBn5EGhwokI1Lmztak0siR1QtOzbeILay5Fu+bw0yQw4iZMmyek7lRP5GwxjQ9B6dMoN+TAVvinTzF6TNFGEjCZjWmsNArxfSTiQgjZ2pR0njgjKRxdwtCBGru/i/ZSTn/a0IZH93eA6IgCovZ89gNKK8fqCqZ/yu6LFfhmojUT4/WXa594Nj2rdxdWpJ3JV/tK1RGZgsKEsvo/yjEhArFg6G5Cu801b2/BOil5KRvn5odLHlbD6fzlEUCbA1rHaYuIkr6nBzghkv9dITeRVDrwhqOOIHfubEac4Vb7/eHxZEAfdEq7Mu3RDY9ae8kTwhgVBbXGZ0TmYyV9ZEW2Mpwzq4IbFfp7HLMyOuhCtYkmD24yYt5AjaH1h4AkC3mWMZxjTJgXQrdHw9SO5/GNBMXPXli4qbjyHYek3jnqcWe4szNCNAHzqIcnASnRVIXoHoRqYSRE1IAFu4U1seqUH0d6ow4JeT5q9p0gCcaQCnUwJtMtN4t2FZn74PNIHOUHpytK82O4lue7oyJtLHaIkcpPJcmg3Fmc6pgZgfGRmx69p/QpB8QXQFYphgSCjNbXHD4EHh01QEkZCLvXk0axz+gy+65qKj4VsIy4g8kOQlFVN2ZFzX3d/0KFVZVpsiqRl5zVkehRyLqPDxK6MktYTeOrgoNsJQCflqKGBv6mqLRCtu2I8agbrQVmFZjygr8RqQcWxnv6frrqWnzowmeCZzlNuhAtNIm2V9orr98ziTiZGavGKpykhk/ZpolWShzcrqxwNODaBqmaHhwdOfZN2P3/zp6JDZyhSZpvGm76/LVEsPXTRvBQhsh0vb7ManeBJIAYOxPd2xvlLmHISLKZJpYNjpkbHXksWvesGyQYGplxlqKVnkCEI7F0ooPql+YJ5ZKJboBVftLXClOrakkF8YU35+NI9KRfIsWVMvprf5YKacz8PFyI7B2WyUjSpSLo8+L5YwLH7OuDFzFC+loo31y+7pBcAaWepK8/seK0NEBRaLfe2NvX2WmZFZXoJcg+fZmdY0dMVl5mXTtsy2iC99by1EmcBSbw0S7q58E0EYWp+kPeT3c+OeelJQkqcrOGAGrPueQcDRxzHnBBaOO7MrHxGfhvOuRQW3bBigUGopfj2W5+cjZiW7YqITVSp5mEExE+Y1+OFRrwDHE3pXa1QeNioxlhreP1buN3gRcLF3vM76PbbZ6ldJdYPLgYfZPHQ6kPcOPwZ/B+bIONCLSk3FT6Dxn0wIG7Lut9fLLAgr3BLLY6dkWAZ1c9x8XANOx/r6xTY9Oe2ycPXcbep37ipXAqPMTbt7svIb08XMnHN04S6JVVrzmO7Ab2XyrAhCmdqY+oTB7rBW9SMZuSDarA9WXTMZg8M+O3dg746mz5z2Ud8eIfLkCKuYV//PtcDmNyvgHwml4rTDlVLBbwUSRNospbX7I8711ZNqFwFJftWjOOqMhiQR6RsJX3oVdtUov4UEDEIfP+1ZCC7BKnzCiTEJ867WkqpTp1EEQZbfdLQM2OjYkq+rsWmRf2b25tj6bY4CAW3tpyN6ha+U/n6bspDAPD4P81uGfKqli+gOEfeXRDed692I0YcO3Hu+Xviz+4pCtRDVGfIVFFUheDfyHoxXnJtLRBiPtCZ49MDoXfu+o0GCFEfTWvckKHOqX++vBxviwgZi7o9xJboVqPGrtP9cmJnyC3KCU9KMmgFRykeTVMyY/ts1AFr8rlXxUsaRDvSAgUCBi9QA56PU/OOrtD3POXYxzCQ7x4ti3kY0y/5SrYdmZDB7dsUmX6KpSd6aKQ4rI2zFVJLR2Ti+BHOoykLtgYxWI1zmbKzfvyLxca87eYwEVOXrWvZHWJLZzPUCTJDtu20+eud4JJ6USZudSr3KWrBVe2DAsNxj8yVqP72Rb+RcsCOsnmPoCNVXZ+DCVM26FSsptomoXvpw6QxWJojWv5Rs/x7fqQA5vzSVX8x4KcUaDpVZs3EldA+50GDSy2soWlBCjxn1G9ES1kxxrNFQmuE87pi15ML52DCvICDiU6J1CdJwOY/YNK/Zybq8QfI9SpKFZWrlHcHF+QMYLlejkUBvK3NFJGqiXIdqv92NxBqFP6L+CdZdivUQfCGNgE+lDu2t4X6JWYner+/qUb8D1QTLThFAGu+T854deT+9vABYr08zIOd4GQP7cV1KJOMMSrm5lbIextLZaqtjrZuXEyJJHIHq9ucAc3L4RzQjTKUCDSpHFWEDIOHeJXX3s+Coo9BzBSEWf0hts0kf2WLcE8+qlG9+7yCzg5xBnX03TdKPuKc6MupX3N6k198SDQaW0tmcDQxvabkjYJdw9qQhka4lGH37flz6W61w7BBAF4Vp//21aOmwk4h+D7JaqImqVVqYsjEWWf7+dfyDCU+Mk6Iq6IjwE/HMI+mYUeAI/aiDHwYzt/YA4pnpFHjRWxKZaxZJf524dAetUX4ZD8u9bxWWlfeudPfHfYTy19G86dG3mR9Al+Hbp5laOQGvpaqBxIB1KyoXmhfLWDHjQTgozNqclen1sy/izmz8OZIBiPvzNyVB6GNGa5YD0WJ7JLiBop/LSM/ef7HU4Ur9w2X97nIgSfRUpvqSHUzIdvwB2k1H27+5OcclwyCXuzT1uHxkv5wjOw/6GK4UQfjK8+kAgJxEZ97bymr0p7Lt/uaEBDJeujIEcP7xctTLPktRnah++nrjlF9R2mzBYr0Ag+jPfmNAv0FgbTaBaGqVV86XNNieHyDokCwIifuHdx5yg1S0IRHMpN2+fSo59z+BpFunY/FX2g1NhkmQe7XjvPyTL9OWDGdSPGOQ1w2RG05wm5PczRXmV6gt50VbEsIJnA9HTfB2Szbg+OStcTFALQMhkPfHJeNz2/qMKccjJ35MGb2fLisY3hqEoHl1guLAtfVOobnu43NuWJdcIig5Jp1VNLnGh2TrSQVor3/9FrIxG1V71ZjnY2RDOBbQWzbopPELRLSGpPMoBAhLbhnGlmyuZujbHiS2ruw66SOMmAsWBZIOpIvXyPyKN43ebzNX/FSfBnfnXzSL0NmtkFZubOoXACsfmSNX8jwt6bQula+/IPRn9rkFNimU6oZ099zQSP7u8hfk4UtXflkVsR34HEN6+tI7NTnkAXf0rPoDcZBK5Cdf70nencMYiiq7u412Nbk1TkPXCx3oKOcscYAxrY84YjjYq5rI7QGG9S+HD+H33HyY462YDX9ZlEcf4zpqnHClUWBnMn9IQhFIVWs2drs/LyRFhpwD/cqTOaIY7xpiWqDWHSPN/7dFxECm1cnF7sxRJTXnOVUePjoAJfKtGzDPaFh89D5WmdHaaO0QM/Jv05IBk2vg83/iwG6E34k1YpGGwuUEkhAytaPKfhGrTboa3TmbX7QC8T6mVJ456x4QVKmCqGZcYPxapBpRwELn1ONj6wi2QkLrcDVEowopgf3CPW0ViY9u+8hY0BYSGsqu98xyRCAPrwk3giJemhk5f6cPxLcyE1dKCY6yfGxJPzlyA1hnViOgBPRllnUk8ImGQ7BYSy4eFdJ3Kt/VX1NZHENMIFQZSpIaikxkaggYTujdeNyk2RNtjtI1Qbn8InS/HvlVxYqhlW9OqyE9P4y3v+5R2Ufr5Pd8lj9GHlSjBcJ2sJaCXB6S/g9QvH+xDjWOVM09jBJGwS+VL2x1u0lFogUoIaDIO2MYvdLzD71ZdTbxUcZyNCAFKa1YyyRRAktdbC40U93y7OF+Od9kG6jBOECLRvn0JGpwC76mqu3eM10NbyRvdVq6vQjllfMKNVl6BhsME1KDAWIKl/0+qFpheB9UQc1Zp99aM8RRZ9rufdJrqsH+AO/l0Y8/sgPgYk8AsAdkvjop7cF55U/eAFX7acTf4Kip3hbgzecmTKe1CLJvdhbYrn5I9xC8Vq8nDNrN/1rETJEy/yu9dYDScbpOAQWAIC7f5uYd4Edkbj8yC0R7NV2466qxF8Tq9dYjuR8HXVaXq2WIJWV1TxKwYhDOqkUdaR5IiNAZeNBUh8UknTt1SqDCzbU4umRxIwV/pNtclITHFDUfrSBq3wLkPf4V7iM716oqk6kPBDJC1ZxMl5UxQ==</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Object Hallucination in Image Captioning </title>
    <link href="http://yoursite.com/2020/03/25/Object-Hallucination-in-Image-Captioning/"/>
    <id>http://yoursite.com/2020/03/25/Object-Hallucination-in-Image-Captioning/</id>
    <published>2020-03-25T12:19:50.000Z</published>
    <updated>2020-03-27T03:03:36.395Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul><li>当前captioning task 存在的问题<ul><li>当前的caption model 目前存在的问题是，生成的句子中出现的object 常常是在corresponding vision scene 中没有出现到的。</li><li>当前使用的评价指标只能评估 candidate caption 与 gt captions 之间的一个相似性，不能捕捉到candidate caption 与 image information之间的relevance. </li></ul></li></ul><ul><li>因此本文进行的一个工作：<ul><li>We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination.  </li></ul></li></ul><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><ul><li><p>（一）关于 object hallucination 的四点讨论</p><ul><li><p>从人类的角度</p><ul><li>丢失对显著物体的描述是一个 failure mode，但是captions is summaries，<strong>因此一般不期待其描述出场景中的所有的objects.</strong> 另外在人类的标注数据中，也不偏向于标注出所有出现在scene 中的objects</li><li>研究报告表明，human judgements 比较不待见那些caption中包含了image content中未出现的 object，*<em>Correctness is more important to human judges than specificity.   *</em></li><li>Many visually impaired who *<em>value correctness over coverage, *</em>hallucination is an obvious concern.  </li></ul></li><li><p>从模型的角度</p><ul><li>object hallucination  揭示了caption model 存在的一个问题，可能caption model并没有对视觉场景学习到一个很好的视觉表征，而是对损失函数过拟合。</li></ul></li></ul></li><li><p>（二）本文要研究的问题</p><ul><li>本文研究当前captioning models中存在的object hallucination现象</li><li>考虑了几个关键问题：<ul><li>(1) <strong>Which models are more prone to hallucination?</strong>  spanning different architectures and learning objectives.   <ul><li>一个新的评价指标来评估object hallucination：CHAIR (Caption Hallucination Assessment with Image Relevance)  </li></ul></li><li>(2) *<em>What are the likely causes of hallucination?   *</em><ul><li>造成object hallucination这一现象的原因主要有两点：visual misclassification and over-reliance on language priors  <ul><li>提出：image and language model consistency scores  </li></ul></li></ul></li><li>(3) <strong>How well do the standard metrics capture hallucination?</strong>  <ul><li>当前的评价指标并不能很好的捕捉到object hallucination 这一现象。</li></ul></li></ul></li></ul></li></ul><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><h5 id="CHAIR-Metric"><a href="#CHAIR-Metric" class="headerlink" title="CHAIR Metric"></a>CHAIR Metric</h5><ul><li>同时使用GT sentence 和 coco image segmentation这两个信息 to measure object hallucination。</li></ul><p>  <img src="http://ww1.sinaimg.cn/large/006uWRWVly1gd7d25xmjcj30h605k0te.jpg" alt="搜狗截图20200326152719.png"></p><h5 id="Image-Consistency"><a href="#Image-Consistency" class="headerlink" title="Image Consistency"></a>Image Consistency</h5><ul><li>对比 <strong>caption model 与 image (alone) model</strong> 两个模型对于预测objects 之间的一致性误差。</li></ul><h5 id="Language-Consistency"><a href="#Language-Consistency" class="headerlink" title="Language Consistency"></a>Language Consistency</h5><ul><li>对比 <strong>caption model 与 sentence (alone) model</strong> 两个模型对于预测下一个word 之间的一致性误差。</li></ul><h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><h5 id="Which-Models-Are-More-Prone-To-Hallucination"><a href="#Which-Models-Are-More-Prone-To-Hallucination" class="headerlink" title="Which Models Are More Prone To Hallucination?"></a>Which Models Are More Prone To Hallucination?</h5><ul><li>一般情况下 ，在标准的evaluation metrics 上表现性能好的模型，在CHAIR metric 上也能表现的比较好，即object hallucination现象相对较弱。但是当模型基于 cider 进行强化学习的训练之后，则不是这种一致的现象。</li><li>（1）使用attention 的模型更加偏向于有较低的object hallucination；NBT模型在标准的evaluation metrics 上表现性能没有topdown-BB 好，但是CHAIR性能却更好，原因在于其使用的pre-trained  detector 与 captioning dataset is in a same domain 。</li><li>（2）当模型基于 cider 进行强化学习的训练之后，将会增加hallucination 的数量。</li><li>（3）LRCN Model 比 FC Model有更多的object hallucination，说明仅仅在第一个 time step 输入视觉特征，将会产生更少的object hallucination </li><li>（4）the GAN loss actually helps decrease hallucination.  the GAN loss encourages sentences to be human-like。</li><li>（5）CE loss: beam size 5, object hallucination 会比 lower beam size 小很多；self-critical loss: beam size sometimes leads to worse performance on CHAIR.   即object hallucination 数量会更多。</li></ul><h5 id="What-Are-The-Likely-Causes-Of-Hallucination"><a href="#What-Are-The-Likely-Causes-Of-Hallucination" class="headerlink" title="What Are The Likely Causes Of Hallucination?"></a>What Are The Likely Causes Of Hallucination?</h5><ul><li><p>We rely on the deconstructed TopDown models to analyze the impact of model components on hallucination  </p><ul><li>通过设计的 几个 deconstructed TopDown models 的分析可以看出，使得object hallucination 数量减少的原因是：due to access to feature maps with spatial locality, not the actual attention mechanism.  </li><li>LRCN Model 比 FC Model 有更多的object hallucination，说明仅仅在第一个 time step 输入视觉特征，将会产生更少的object hallucination 。作者在文中对这一现象给出的解释是，在每一步都输入视觉特征 fc_feature, 而不是 spatial feature, 这将导致对视觉特征的过拟合。</li></ul></li><li><p>Investigate what causes hallucination using the deconstructed TopDown models and the image consistency and language consistency scores. </p><ul><li>We note that models with less hallucination tend to make errors consistent with the image model, whereas models with more hallucination tend to make errors consistent with the  language model.  这说明有更少object hallutition 的models 有更强的能力从Image 中提取知识到句子生成过程中。</li><li>在Robust split 上进行实验发现，所有models之间的language consistency 差异度不大；相比于 Karpathy split，相对应下的models image consistency 有所下降。这是由于 Robust split 在测试集上，会出现 novel compositions of objects at test time. 使得所有的模型有很强的language prior.</li></ul></li><li><p>在训练过程中，分析FC model 的image/language consistency，结果发现在训练开始，与language model 的一致性更好，随着训练的结束，与 image model 的一致性更好。这说明，模型首先学习生成流畅的语言，而后再去学习结合视觉信息。</p><h5 id="How-Well-Do-The-Standard-Metrics-Capture-Hallucination"><a href="#How-Well-Do-The-Standard-Metrics-Capture-Hallucination" class="headerlink" title="How Well Do The Standard Metrics Capture Hallucination?"></a>How Well Do The Standard Metrics Capture Hallucination?</h5></li><li><p>作者分析了 standard metric 与 CHAIRs  之间的 pearsom correlation coefficient ，结果发现 SPICE 的一致性更好。</p></li><li><p>object hallucination can not be always predicted based on the traditional sentence metrics.  </p></li><li><p><strong>与当前使用的standard metrci 互为补充，可以提升与人类评分的一致程度。</strong></p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gd8997jm24j30gi08375g.jpg" alt="搜狗截图20200327100117.png"></p><p>这个意思是说，第一列，单独分析各个automatic metric 与 human judgement 的一致性。第二/三列，各个评价指标分别加上1-CHs/ 1-CHi 之后再与human judgement 计算一致性。可以发现，一致性得到提升。即 <strong>CHAIR is complementary to standard metrics</strong></p></li></ul><h5 id="Does-hallucination-impact-generation-of-other-words"><a href="#Does-hallucination-impact-generation-of-other-words" class="headerlink" title="Does hallucination impact generation of other words?"></a>Does hallucination impact generation of other words?</h5><ul><li>Hallucinating objects 影响句子生成的质量，不仅是由于 object 没有被正确的预测，也是由于hallucinated word 影响到了生成的其他的words.</li><li>通过比较TopDown 和 TD-Restrict 生成的句子可以分析这个现象。We find that after the hallucinated word is generated, the following words in the sentence are different 47.3% of  the time.  </li><li>一旦一个hallucination words 被生成，则其又会由于language prior(hallucinating a “cat” leading to hallucinating<br>a “chair”  )，产生更多的hallucination words 。</li></ul><h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4><ul><li><p>the popular self critical loss increases CIDEr score, but also the amount of hallucination. </p></li><li><p>CHAIR complements the standard sentence metrics in capturing human preference( judgements ).  </p></li><li><p>Models with stronger image consistency frequently hallucinate fewer objects, suggesting that strong visual processing is important for  avoiding hallucination.  </p></li><li><p><strong>Advises for captioning task:</strong> 仅使用CE-loss/ standard sentence metrics来优化，不太能解决object hallucination 这个问题，若同时以 image relevance 来优化，会更好。</p></li></ul><h4 id="yaya-总结"><a href="#yaya-总结" class="headerlink" title="yaya 总结"></a>yaya 总结</h4><ul><li><p>在设计评价指标上给我的几点启发</p><ul><li>(1) 不需要要求machine generated caption 可以概括所有的objects which have been occurred in the vision scene</li><li>(2) machine generated caption 进行评价时，正确性比全面性更加重要</li></ul></li><li><p>本文的一个巧妙的点</p><ul><li>本文为了查看object hallucination，使用COCO的80个类。对于candidata caption 首先将其token， 然后调整成单数形式，然后使用同义词的思想，去跟COCO 的80个类别进行匹配。</li><li>另外对于GT sentences，也提出一个list，这个地方不太知道它说的什么意思？？？？？  </li></ul></li><li><p>GVD 好像也类似的提到过类似的思想</p></li><li><p>关于the image consistency and language consistency scores.</p><ul><li>在这个得分的计算方式上，是否还有什么可以改进的地方？</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;当前captioning task 存在的问题&lt;ul&gt;
&lt;li&gt;当前的capti
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>NLP: 自编码 and 自回归</title>
    <link href="http://yoursite.com/2020/03/24/NLP-%E8%87%AA%E7%BC%96%E7%A0%81-and-%E8%87%AA%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2020/03/24/NLP-自编码-and-自回归/</id>
    <published>2020-03-24T05:44:04.000Z</published>
    <updated>2020-03-31T02:21:34.096Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p><a href="https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff" target="_blank" rel="noopener">https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff</a>      </p></li><li><p>这篇博文写的不错<br><a href="https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff" target="_blank" rel="noopener">https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.infoq.cn/article/4SRM7UMV
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions</title>
    <link href="http://yoursite.com/2020/01/16/VIFIDEL-Evaluating-the-Visual-Fidelity-of-Image-Descriptions/"/>
    <id>http://yoursite.com/2020/01/16/VIFIDEL-Evaluating-the-Visual-Fidelity-of-Image-Descriptions/</id>
    <published>2020-01-16T01:41:25.000Z</published>
    <updated>2020-01-16T14:31:11.909Z</updated>
    
    <content type="html"><![CDATA[<h3 id="当前指标存在的问题"><a href="#当前指标存在的问题" class="headerlink" title="当前指标存在的问题"></a>当前指标存在的问题</h3><ul><li>BLEU, ROUGE, Meteor, CIDEr 这些指标， 他们依靠精确的字符串匹配来测量 condidate 文本和reference文献之间的surface-level 、 n-gram 重叠。当 references 有限的情况下，这会导致样本稀疏问题 （reference数量对 metric 得分有很大影响，因为reference 数量越多，多样性更好）。Meteor 通过匹配字典和释义表中的同义词来部分解决此问题，但受限于此类字典的可用性，也不能很好地适用于其他的 language。SPICE and BAST 通过计算语义级别的相似性来解决 exact string matching。但是这个方法严重的依赖于语言资源，例如 parsers, semantic role labellers, tailored rules, 使其很难适应到不同的语言和领域。</li></ul><h3 id="仅仅使用-reference-description-来-评估-image-description-的缺点"><a href="#仅仅使用-reference-description-来-评估-image-description-的缺点" class="headerlink" title="仅仅使用 reference description 来 评估 image description 的缺点"></a>仅仅使用 reference description 来 评估 image description 的缺点</h3><ul><li>受限于 reference 的数量，可能会造成<strong>样本稀缺</strong>的问题。</li><li>reference description 是<strong>主观的，有歧义的</strong>，可能不能涵盖 image 中所有的关键信息，可能只包含 image content 的一个子集。<strong>使用 object labels 可以解决这个问题</strong> </li><li>references 可能含有错误的信息。</li></ul><h3 id="基于-object-information-来-作为评价指标的优点"><a href="#基于-object-information-来-作为评价指标的优点" class="headerlink" title="基于 object information 来 作为评价指标的优点"></a>基于 object information 来 作为评价指标的优点</h3><ul><li><strong>少的标注时间消耗</strong>： 若仅使用 multiple descriptions 来作为参考，则必然需要人类为 每个 image 来标注 多个 descriptions，在标注数据上需要花费很多时间。且为每个 image 标注的description 数量越多，评估的越准确，则也需要更多的标注时间。</li><li>但是若使用基于 object imformation ， 则可以使用 predicted objects 或者 object annotations</li></ul><h3 id="Modelling-object-importance-with-reference-descriptions"><a href="#Modelling-object-importance-with-reference-descriptions" class="headerlink" title="Modelling object importance with reference descriptions"></a>Modelling object importance with reference descriptions</h3><ul><li><p>human reference 可以作为一种guidance 提供信息 – 人类对该张图片关注的重点在哪里。</p></li><li><p>与 CIDEr 很相似，都是考虑与 human reference 之间的consensus 信息，但是有以下两点不同：  </p><p>（1）使用reference 来建模object的重要性，而不是直接将 候选与 参考进行比较。<br>（2）在语义空间使用word embedding来执行 word matching，而不是直接在计算表面的匹配度（eg: n-gram）   </p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;当前指标存在的问题&quot;&gt;&lt;a href=&quot;#当前指标存在的问题&quot; class=&quot;headerlink&quot; title=&quot;当前指标存在的问题&quot;&gt;&lt;/a&gt;当前指标存在的问题&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;BLEU, ROUGE, Meteor, CIDEr 这些指标， 他们依
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
</feed>
