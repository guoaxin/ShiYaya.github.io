<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>图像描述-评价指标-中用到的数据集汇总</title>
      <link href="/2020/06/04/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-%E4%B8%AD%E7%94%A8%E5%88%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B1%87%E6%80%BB/"/>
      <url>/2020/06/04/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-%E4%B8%AD%E7%94%A8%E5%88%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<p>为了评估 提出的metric 与 human judgement的相关性，提出了一些数据集。这些数据集，包含image-text-human_score, 通过利用统计学分析 metric_evaluation 与 human_score 的相关性，来验证提出评价指标的合理性。</p><h1 id="Caption-level-Correlation"><a href="#Caption-level-Correlation" class="headerlink" title="Caption-level Correlation"></a>Caption-level Correlation</h1><h2 id="Flickr-8k-Dataset"><a href="#Flickr-8k-Dataset" class="headerlink" title="Flickr 8k Dataset"></a>Flickr 8k Dataset</h2><ul><li>website: <a href="http://academictorrents.com/details/9dea07ba660a722ae1008c4c8afdd303b6f6e53b" target="_blank" rel="noopener">http://academictorrents.com/details/9dea07ba660a722ae1008c4c8afdd303b6f6e53b</a></li><li>reference: </li></ul><p><font size="3"> <strong>数据集介绍</strong> </font></p><ul><li><p>Cite: Framing image description as a ranking task: Data, models and evaluation metrics.  </p></li><li><p>对于图像描述任务：该数据集包含8092张image。训练集6000张，验证集1000张，测试集1000张，，很奇怪。。数据加起来对不上。。每张image 都对应有人类标注的5个句子。</p></li><li><p>对于图像描述评价指标任务： </p><p><strong>测试集1000张</strong>，<font color="blue"><strong>通过image-text retrieval 算法</strong></font> 检索candidate caption，为每张图片从整个测试集的语料库上进行检索（检索的数量没有固定，像是根据检索结果阈值截取的）。由于是在整个测试集的预料库上进行检索，则，也有可能检索到自身image对应的groundtruth。</p><p>得到这些新的image-text pair，对于每个pair，再由<strong>三个人工</strong>去标注image与text的匹配程度( give a score from 1 (not related to the image content) to 4 (very related))。</p><p>则，构建了一个可以衡量metric 与 human judgement 相关性的一个数据集。</p></li></ul><p><font size="3"> <strong>使用 Note</strong> </font></p><ul><li>在TIGER [1] : <strong>Because 158 candidates are actual references of target images, we excluded these for further analysis。</strong> <font color="red">在TIGER 的实验设置中：若Flickr 8k数据集中检索到了本image对应的reference，则去掉该条检索。</font></li></ul><p><font size="3"> <strong>评估方式</strong></font></p><p><code>Kendall</code> and <code>Spearman</code> rank correlations reflect the similarity of the pairwise rankings whereas <code>Pearson’s</code> p captures the linear association between data points.</p><h2 id="Composite-Dataset"><a href="#Composite-Dataset" class="headerlink" title="Composite Dataset"></a>Composite Dataset</h2><p><font size="3"> <strong>数据集介绍</strong> </font></p><p>这个数据集是由三个数据集组成的。包括：testing captions for 2007 MS-COCO images, 997 Flickr 8k pictures, and 991 Flickr 30k images.  </p><p>每张图片对应3个candidate captions，包括1个human written reference和 2个machine generated。</p><p>这里总计有11,985 candidates, 标注与image 之间的相关性，from 1 (not relevant) to 5 (very relevant)。</p><p><font size="3"> <strong>评估方式</strong></font></p><p><code>Kendall</code> and <code>Spearman</code> rank correlations reflect the similarity of the pairwise rankings whereas <code>Pearson’s</code> p captures the linear association between data points.</p><h2 id="Pascal-50s-Dataset"><a href="#Pascal-50s-Dataset" class="headerlink" title="Pascal 50s Dataset"></a>Pascal 50s Dataset</h2><ul><li>website: <a href="http://vrama91.github.io/cider/" target="_blank" rel="noopener">http://vrama91.github.io/cider/</a></li></ul><p><font size="3"> <strong>数据集介绍</strong> </font></p><p>Cite: <code>CIDEr: Consensus-based Image Description Evaluation</code></p><p>从 UIUC PASCAL Sentence Dataset中提取1000张image，原数据集中，每个image配有5个human written sentence。</p><p>对于图像描述评价指标任务： </p><p>在以上基础上每个image 又由 AMT workers标注了50个captions。以此构成了pascal 50s 数据集。</p><p>不同于以上的两个数据集评估image-text 之间的匹配，该数据集考量candidate 与 reference之间的匹配。具体地：对于一个image，（1）使用48 of 50 human written caption as <strong>reference</strong>。（2）剩下的两个human written caption as <strong>candidate</strong>，同时也使用 machine generated caption as <strong>candidate</strong>，另外other image 的 human written caption通过检索的方式也可以当 这样candidate可以当做<strong>candidate</strong>。</p><p>基于此，构建三元组：（A, (B, C)）–(reference, (candidate_1, candidate_2)) 根据(B, C) 组合方式的不同，分为四类：HC，HI，HM，MM。（1）human–human correct pairs (HC), where we pick two human sentences describing the same image. （2） human–human incorrect pairs (HI), where one of the sentences is a human description for the image and the other is also a human sentence but describing some other image from the dataset picked at random. （3）human–machine (HM) pairs formed by pairing a human sentence describing an image with a machine generated sentence describing the same image. （4）machine–machine (MM) pairs, where we compare two machine generated sentences describing the same image</p><p>则，可以得到 1000image × 48reference(A) × 4(B,C) = 192000个三元组</p><p><strong>human judgement 的标注</strong> 对于任意给出的一个三元组(A, B, C)。A 是一个reference sentence, (B, C) 是两个candidate captions pair. 标注者被要求从B和C中选择一个与A最相似的句子。这样就可以收集到一个human judgements for each triplet. 如果B的投票对于C则认为B is winner.</p><p><font size="3"> <strong>使用 Note</strong> </font></p><ul><li>解读如下的表格的acuracy是如何计算的吧！首先在PASCAL-50s数据集里含有4种模式：HC、HI、HM、MM， 即对于（A,(B, C)）中的pair（B, C）含有四种模式。当前AMT workers对pair(B, C)已经有了排序，当proposed metric也对这些B，C sentences进行评分的时候，自然也会有一个对B，C的排序，即score高的sentence, 排序就在前。基于人类已经给了人工的标注排序，即获得了GT，那么就可以去评判 proposed metric 对该pair的评分是否正确。进而可以得到对该类HC/HI/HM/MM的准确率。其实也可以在整个数据集上进行测试得到一个准确率。</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gcdc0x4s91j30ex04uaas.jpg" alt="搜狗截图20200229160306.png"></p><p><font size="3"> <strong>评估方式</strong></font></p><p>Pairwise Classification Accuracy</p><h1 id="System-Level-Correlation"><a href="#System-Level-Correlation" class="headerlink" title="System-Level Correlation"></a>System-Level Correlation</h1><h2 id="the-2015-COCO-Captioning-Challenge-for-12-teams"><a href="#the-2015-COCO-Captioning-Challenge-for-12-teams" class="headerlink" title="the 2015 COCO Captioning Challenge for 12 teams"></a>the 2015 COCO Captioning Challenge for 12 teams</h2><p><font size="3"> <strong>数据集介绍</strong> </font></p><ul><li><p>Cite: The coco 2015 captioning challenge. <a href="http://mscoco.org/dataset/#captions-challenge2015" target="_blank" rel="noopener">http://mscoco.org/dataset/#captions-challenge2015</a>.  </p></li><li><p>use human judgements collected in the 2015 COCO Captioning Challenge for 12 teams who participated in this captioning challenge.</p></li><li><p>We report</p><ul><li><p>M1: Percentage of captions that are evaluated as better or equal to human caption,</p></li><li><p>M2: Percentage of captions that pass the Turing Test,</p></li><li><p>M3: Average correctness of the captions on a scale of 1-5 (incorrect - correct),</p></li><li><p>M4: Average amount of detail of the captions on a scale of 1-5 (lack of details - very detailed) and</p></li><li><p>M5: Percentage of captions that are similar to human description.</p></li><li><p>While M1 and M2 were used to rank the captioning models in the COCO challenge.   </p><p>M3, M4 and M5  are not used to rank image captioning models , but are intended for an ablation study to understand the various aspects of caption quality.  </p></li></ul></li></ul><p><font size="3"><strong>使用 Note</strong> </font></p><p>计算system-level correlation, （1）需要为每个 caption model 来计算一个metric score, 这个分数聚合了由该model 生成的所有的caption 的 metric socre。（2） 然后，该captio model 的aggregate metric score 与 system-level human assessments之间的相关性被计算。</p><p><font size="3"> <strong>评估方式</strong></font></p><ul><li>Compare proposed metric with others on the <strong>Pearson’s ρ correlation</strong> between all common metrics and human judgments collected in the 2015 COCO Captioning Challenge. </li></ul><h1 id="论文引用情况"><a href="#论文引用情况" class="headerlink" title="论文引用情况"></a>论文引用情况</h1><table><thead><tr><th></th><th>Caption-level Correlation</th><th></th><th></th><th>System-Level Correlation</th></tr></thead><tbody><tr><td></td><td>Flickr 8k</td><td>Composite</td><td>pascal-50s</td><td>2015 COCO Captioning Challenge</td></tr><tr><td>CIDEr</td><td></td><td></td><td>√</td><td></td></tr><tr><td>SPICE</td><td>√</td><td>√</td><td>√</td><td>√</td></tr><tr><td>(CVPR 2018) Learning to Evaluate Image Captioning</td><td>√</td><td></td><td></td><td>√</td></tr><tr><td>(EMNLP-IJCNLP 2019) REO-Relevance, Extraness, Omission A Fine-grained Evaluation for Image Captioning</td><td></td><td>√</td><td>√</td><td></td></tr><tr><td>(ACL 2019) VIFIDEL Evaluating the visual fidelity of image descriptions</td><td></td><td>√</td><td>√</td><td></td></tr><tr><td>(EMNLP2019) TIGEr Text-to-Image Grounding for Image Caption Evaluation</td><td>√</td><td>√</td><td>√</td><td></td></tr><tr><td>(IJCV)Learning-based Composite Metrics for Improved Caption Evaluation</td><td>√</td><td>√</td><td>√</td><td>√</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 评价指标 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 评价指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
      <link href="/2020/05/31/Sentence-BERT-Sentence-Embeddings-using-Siamese-BERT-Networks/"/>
      <url>/2020/05/31/Sentence-BERT-Sentence-Embeddings-using-Siamese-BERT-Networks/</url>
      
        <content type="html"><![CDATA[<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>BERT 和 RoBERTa 在 sentence-pair regression tasks (eg: semantic textual similarity ) 上取得了非常不错的成绩，但是由于需要将两个句子都送入到网络中，这将造成计算量过载。举个例子，若在10000个句子中要找到最相似的对，则需要50 million的推理计算（需要计算一个上三角阵：（10000+1）*10000/2）, 使用BERT，则需要 65 hours。</p><p><strong>这可以看到the construction of BERT，不适于semantic similarity search、 像聚类这种无监督任务、information retrieval via semantic search等等。</strong></p><p><strong>因此本文：</strong> 本文提出了 sentence-BERT, 是建立在 预训练的BERT上的一种修改，使用siamese 和 triplet 网络结构来得到语义上有意义的sentence embeddings, 从而方便的计算cosine similarity.</p><h4 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h4><p>三种训练策略，现在只说到了一种，分类损失，，另外两种，是在哪些数据集上使用的？</p><h4 id="yaya-启发"><a href="#yaya-启发" class="headerlink" title="yaya 启发"></a>yaya 启发</h4><p>一、本文中提出的当前 BERT存在的缺陷，也正是 video-text retrieval ，这种多模态任务存在的缺陷。</p><p>二、We showed in (Reimers et al., 2016)[1] that Pearson correlation is badly suited for  STS. Instead, we compute the Spearman’s rank correlation between the cosine-similarity of the sentence embeddings and the gold labels  </p><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p>[1] Nils Reimers, Philip Beyer, and Iryna Gurevych. 2016.<br><strong>Task-Oriented Intrinsic Evaluation of Semantic Textual Similarity.</strong><br>In Proceedings of the 26th International Conference on Computational Linguistics (COLING), pages 87–96.      </p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>阅读论文 tips</title>
      <link href="/2020/05/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87-tips/"/>
      <url>/2020/05/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87-tips/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h4 id="快速阅读：划分结构层次"><a href="#快速阅读：划分结构层次" class="headerlink" title="快速阅读：划分结构层次"></a>快速阅读：划分结构层次</h4><p>对于快速阅读，一个小的技巧是图文浏览。因为一些好的论文必然是图文并茂，所以只要弄清楚论文中表格和图片的标题和注释，就能够获得这篇论文八、九成的信息。</p><p>读者在读论文的时候也应该要有逻辑，首先要清楚论文中的表达是否是我想要学习到的；其次，我能从论文中学到多少呢；最后，这篇论文的背景是什么——是什么样的背景让这篇论文变得重要和有趣。</p><h4 id="仔细阅读：批判思维"><a href="#仔细阅读：批判思维" class="headerlink" title="仔细阅读：批判思维"></a>仔细阅读：批判思维</h4><p>以评判性阅读开始，带着质疑的心态问问题。如果作者论文中声称解决了一个问题，那么你就要在心里问自己：<strong>论文是否正确、真正地解决了问题？</strong> <strong>作者论文中所用方法是否有局限性</strong>？如果<strong>所读的论文没有解决问题，那么我能解决么</strong>？我能采用<strong>比论文中更简单的方法解决么</strong>？所以，一旦进入仔细阅读的状态，要在读论文之前对自己说：这篇论文可能有问题，我要找出来。</p><h4 id="创造性阅读：积极思考"><a href="#创造性阅读：积极思考" class="headerlink" title="创造性阅读：积极思考"></a>创造性阅读：积极思考</h4><p>问自己：在我所读的论文中，作者有<strong>哪些点还没有想到</strong>？如果我现在做这项研究，我<strong>能做的新事情是什么</strong>？创造性的阅读需要<strong>把你所读的论文和其他相关的论文建立联系，从而产生一些新的想法</strong>，这些想法可以支撑你进行三个月到五个月的研究。</p><p>如果你真正想理解你所读的论文，那么就写一个摘要吧，最好做一个口头展示，这样你会发现，只有把东西写下来或者说出来才能真正深刻理解。如果你能做一个报告，那就更好了，因为做报告的时候，别人可以问你问题，这会强迫你理解所读的论文。</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gf9in4p85tj30f8088t92.jpg"><p>在做这个演讲之前，我曾经向我的同事、学生询问了关于论文阅读有哪些问题可以“问自己”，上面这张图片是一个总结，图片的上半部分是比较客观的问题，包括论文的核心观点是什么？主要的局限性是什么？代码和数据是不是可得的？论文的贡献是否有意义？论文中的实验是否足够好？</p><p>图片的下半部分是比较主观的问题，包括我错过了什么相关论文么？这对我的工作有何帮助么？这是一篇值得关注的论文么？这个研究领域的领头人是谁呢？其他的人对这篇论文有何看法呢？如果有机会见到作者，我应该问作者什么问题？</p><p>当你在阅读论文的时候如果能回答出上面列出的问题，我相信你会对你所读论文有非常深刻的理解。</p>]]></content>
      
      
      <categories>
          
          <category> 杂类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>From Recognition to Cognition: Visual Commonsense Reasoning</title>
      <link href="/2020/05/16/From-Recognition-to-Cognition-Visual-Commonsense-Reasoning/"/>
      <url>/2020/05/16/From-Recognition-to-Cognition-Visual-Commonsense-Reasoning/</url>
      
        <content type="html"><![CDATA[<h4 id="Visual-Commonsense-Reasoning-VCR"><a href="#Visual-Commonsense-Reasoning-VCR" class="headerlink" title="Visual Commonsense Reasoning (VCR)"></a>Visual Commonsense Reasoning (VCR)</h4><p>VCR: Given an image, a list of regions, and a question, a model must answer the question and provide a rationale explaining why its answer is right. </p><p>标注数据：为了避免生成式问题中评价指标的缺陷，本任务设计成 <strong>选择题</strong> 任务，即，提供一个image，一个question，多个answer，该任务要求从多个答案中选择一个正确的答案。对于correct answer：给定一张图片，要求AMT workers 写一个question，一个answer。对于wrong answer：使用adversarial matching 来获得其余的negative answer。</p><h4 id="The-Motivation-of-Adversarial-Matching"><a href="#The-Motivation-of-Adversarial-Matching" class="headerlink" title="The Motivation of Adversarial Matching"></a>The Motivation of Adversarial Matching</h4><p>在构建数据集时，常常存在两种挑战：</p><ul><li><p><strong>A crucial challenge</strong> in constructing a dataset of this complexity at this scale is how to avoid <strong>annotation artifacts</strong>. </p></li><li><p><strong>A recurring challenge</strong> in most recent QA datasets has been that human-written answers contain unexpected but distinct <strong>biases</strong> that models can easily exploit. 现实世界中的偏置</p></li></ul><p>通常，这些<strong>偏见</strong>非常明显，以至于模型无需看问题就可以选择正确的答案。</p><h4 id="Adversarial-Matching"><a href="#Adversarial-Matching" class="headerlink" title="Adversarial Matching"></a>Adversarial Matching</h4><p>negative answer的生成可以在correct answer上进行改造，但是这个过程非常耗钱，更甚，可能会引入annotation artifacts，subtle patterns that are by themselves highly predictive of the ‘correct’ or ‘incorrect’ label. 【1，2，3】</p><p>The key idea of Adversarial Matching is to <strong>recycle</strong> each correct answer for a question exactly three times — as a <strong>negative answer</strong> for three other questions.  这样每个answer 将会有1/4的机会是正例。这可以<strong>解决掉 answer-only bais 的问题</strong>，从而避免了模型总是选择 most generic answer. </p><p>在为每个image 选择negative answer时，希望<strong>negative answer: relevant as possible to the context/question (so that they appeal to machines), while they cannot be overly similar to the correct response (so that they don’t become correct answers incidentally).</strong> </p><p>因此计算一个weight，能够同时考虑到与query中的questeion相关度大，但是与query 的correct answer的相似性小。在本文中使用bert来计算相关度，用ESIM+ELM来计算相似性。</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1geuprb6zmvj310305qwfg.jpg"><p>为了获得多个negative answer，需要执行多次的双向匹配。为了确保nagetive pairs是多样性的，在依次获得negative answer的过程中，在下一次从其他的image中查找negative answer时，需要遍历当前所有的negative answer，然后取最大值。（replace the similarity term with the maximum similarity between a candidate response rj and all responses currently assigned to qi.）</p><h4 id="Language-Priors-and-Annotation-Artifacts-Discussion"><a href="#Language-Priors-and-Annotation-Artifacts-Discussion" class="headerlink" title="Language Priors and Annotation Artifacts Discussion"></a>Language Priors and Annotation Artifacts Discussion</h4><p><strong>Answer Priors</strong>: A model can select a correct answer without even looking at the question. </p><p><strong>Non-Visual Priors ：</strong>A model can select a correct answer using only non-visual elements of the question. </p><p>这些priors可能是来自于现实世界中的偏置，比如，当问消火栓是什么颜色的，模型常常预测出，红色。这是由于现实世界中消火栓是红色的。 </p><p>又可能来自于annotation artifacts ， 人们在编写class-conditioned answers 时出现的模式。比如：标注者经常使用否定之类的方式写与句子相矛盾的句子。</p><ul><li><strong>实验证明，对抗匹配的方式，可以帮助消除 artificial bias。</strong> </li></ul><h4 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h4><ol><li>The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task.  </li><li>Annotation artifacts in natural language inference data. </li><li>Hypothesis Only Baselines in Natural Language Inference. </li></ol>]]></content>
      
      
      <categories>
          
          <category> 视觉推理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视觉推理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WMT Shared Tasks -- Human Evaluation </title>
      <link href="/2020/05/14/WMT-Shared-Tasks-Human-Evaluation/"/>
      <url>/2020/05/14/WMT-Shared-Tasks-Human-Evaluation/</url>
      
        <content type="html"><![CDATA[<h3 id="WMT-Shared-Tasks-–-Human-Evaluation"><a href="#WMT-Shared-Tasks-–-Human-Evaluation" class="headerlink" title="WMT Shared Tasks – Human Evaluation"></a>WMT Shared Tasks – Human Evaluation</h3><h4 id="Human-Evaluation"><a href="#Human-Evaluation" class="headerlink" title="Human Evaluation"></a>Human Evaluation</h4><p>两种评估的方式：direct assessments (DA); language pairs evaluated with relative ranking (RR)</p><p>但是DA相比于RR更具有优势，namely，对翻译质量的评估采取 absolute score 的方式。可以<strong>实施quality control</strong> 。</p><h4 id="Human-judgement-quality-control"><a href="#Human-judgement-quality-control" class="headerlink" title="Human judgement quality control"></a>Human judgement quality control</h4><ul><li><p>每个标注者，每次任务：给定100个 （reference+ candidate）pair, 针对给定的reference, 评估生成的candidate的好坏。</p></li><li><p>100个pair中有60个用于quality control，40个由participating systems 生成的翻译组成。</p><p>（1）这60个pair，是官方设计出来的，包括三类，repeat pairs (expecting a similar judgment), damage MT outputs/ bad reference (expecting significantly worse scores) and use references instead of MT outputs (expecting high scores). 因此仅仅会有20%的资源消耗：bad reference; good reference</p><p>Specifically，先从正常的MT system 中 得到30个 （reference, MT output）pair，如 table 5 中的 original system output， 然后1)对1-10对，进行重复，得到10对。2）对11-20对，将MT output搞破坏。得到10对。3）对21-30对，取corresponding reference–&gt; (reference_1, reference_2)，得到10对。</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gepxhtqcpgj311h052abz.jpg"><p>（2）within each 100-translation HIT， 每个articipating system<strong>等比例的贡献</strong>a（within each 100-translation HIT, the same proportion of translations are included from each participating system for that language pair.  ）这是为了确保每个参与的 系统含有近似的相同数量的评估。同时，这也从三个方面得到了公平性的评估：1）每有一个workers做一个HIT, 则就会为所有参与的系统增加human judgement。2）不会轻易受到worker个性差异的影响，因为每个worker都会给所有参与的系统进行评估。3）尽管DA判断是绝对的，但众所周知，判断者会根据观察到的总体翻译质量来“校准”他们使用量表的方式。 对于每个HIT（包括所有参与的系统），这种影响都是平均的。</p></li></ul><h4 id="Annotator-Agreement"><a href="#Annotator-Agreement" class="headerlink" title="Annotator Agreement"></a>Annotator Agreement</h4><ul><li><p><strong>【bad reference pairs】</strong> 由于 bad reference pairs 的质量应该是显著偏低的，通过查看人类在这类pairs 上的评分是否也是显著偏低。来过滤掉可信赖度低的human assessors。</p><p>set（A, bad reference） 与  set（A, translatin_B）这两个集合上的人类评估，计算一个p-value， 若p-value&gt;0.05 则说明该human assessor的可信度低。</p></li><li><p><strong>【repeat pairs】</strong> 对于 repeat pairs, 查看得到 repeat assessments的程度。</p></li></ul><h4 id="Producing-the-Human-Ranking"><a href="#Producing-the-Human-Ranking" class="headerlink" title="Producing the Human Ranking"></a>Producing the Human Ranking</h4><ul><li><p>Standardized </p><p>为了消除不同的人类评估者的评分策略的差异，首先根据每个人类评估者的总体平均得分和标准差得分对翻译的人类评估得分进行<strong>标准化</strong>。</p></li><li><p>system  score ……</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器翻译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning</title>
      <link href="/2020/05/10/Attacking-Visual-Language-Grounding-with-Adversarial-Examples-A-Case-Study-on-Neural-Image-Captioning/"/>
      <url>/2020/05/10/Attacking-Visual-Language-Grounding-with-Adversarial-Examples-A-Case-Study-on-Neural-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<h4 id="对抗样本的影响"><a href="#对抗样本的影响" class="headerlink" title="对抗样本的影响"></a>对抗样本的影响</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1genh1kz7svj30i40dpag7.jpg" alt="Fig_stopsign_2_small.png"></p><p>图1，在image RGB 上添加了一些扰动，结果使得captioning model的输出也发生了很大的变化。基于此，发现了两个问题。（1）我们的结果指出了在tested image captioning systems中的致命问题。（2）captioning model 中的对抗性例子突出了 人与机器之间visual language grounding 的不一致，表明当前的机器视觉和感知机制可能存在缺陷。</p><h4 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h4><ul><li>本文提出了一种<strong>设计对抗样本</strong>的方法</li><li>本文的这种对抗样本可以拿去用来分析captioning model 的鲁棒性-</li><li>本文，还利用 对抗样本，来做什么了，有没有做一些对抗性的训练？？</li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning to Evaluate Image Captioning</title>
      <link href="/2020/05/09/Learning-to-Evaluate-Image-Captioning/"/>
      <url>/2020/05/09/Learning-to-Evaluate-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>本文提出一个可学习的图像描述评价指标。</p><p><strong>Motivation</strong>: 由于当前的评价指标不是很完美，不能处理句子中存在的所有的病理行为，或者是说，当遇到某些病理行为时，则不能正常工作，比如，SPICE对字幕的语义很敏感，但往往会忽略其句法质量，SPICE倾向于对带有重复子句的长句子给予高分。每个评估指标都有其众所周知的盲点，基于规则的指标通常不灵活，无法应对新的病理病例。</p><p>因此本文提出，使用几种数据增强的方式，来扩展出很多的存在特征几种病理问题的对抗样本，并纳入训练过程中，使得训练出来的评价指标对于这些对抗样本更加的鲁棒。（即，可以识别出这些对抗样本的能力）</p><h4 id="How-to-Use-the-Proposed-Metric-in-Practice"><a href="#How-to-Use-the-Proposed-Metric-in-Practice" class="headerlink" title="How to Use the Proposed Metric in Practice"></a>How to Use the Proposed Metric in Practice</h4><p>由于涉及到需要学习 ，则评价指标的训练的数据分布 与 被测试的captioning dataset 之间存在差异。</p><p>本文解决: 假设要评估 coco  <strong>test</strong> captioning, 则将该份submission 分成两半，一半用于scratch 训练该评价指标，另外一半则使用该训练好的评价指标得到得分；然后交替，则得到了所有的得分！</p><h4 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h4><ul><li>(1) One direction of future work could aim to capture the heterogeneous nature of human annotated captions and incorporate such information into captioning evaluation.  <strong>Human annotated captions 带有人的个性</strong></li><li>(2) Another direction for future work could be training a caption generator together with the proposed evaluation metric (discriminator) in a generative adversarial setting. <strong>captioning model 与提出的评价指标，一起生成对抗的训练</strong></li><li>(3) Finally, gameability is definitely a concern, not only for our learning based metric, but also for other rule-based metrics. Learning to be more robust to adversarial examples is also a future direction of learning based evaluation metrics.  <strong>对 对抗样本更加的鲁棒，是基于学习的评价指标的一个未来的方向</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cross-modal Coherence Modeling for Caption Generation</title>
      <link href="/2020/04/22/Cross-modal-Coherence-Modeling-for-Caption-Generation/"/>
      <url>/2020/04/22/Cross-modal-Coherence-Modeling-for-Caption-Generation/</url>
      
        <content type="html"><![CDATA[<h4 id="现在图像描述中存在的问题"><a href="#现在图像描述中存在的问题" class="headerlink" title="现在图像描述中存在的问题"></a>现在图像描述中存在的问题</h4><ul><li>标注方式上：让工作人员标注出image 对应的text。</li><li>这导致的问题：（1）Unfortunately, such dedicated annotation efforts cannot yield enough data for training robust generation models; the resulting generated captions are plagued by content<br>hallucinations (Rohrbach et al., 2018; Sharma et al., 2018) that effectively preclude them for being used in real-world applications. </li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</title>
      <link href="/2020/04/16/Oscar-Object-Semantics-Aligned-Pre-training-for-Vision-Language-Tasks/"/>
      <url>/2020/04/16/Oscar-Object-Semantics-Aligned-Pre-training-for-Vision-Language-Tasks/</url>
      
        <content type="html"><![CDATA[<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><ul><li>现在基于bert 来处理的vision-language task 存在的问题：现在的方将image region features 和 text features 拼起来，然后利用自我注意机制以暴力方式学习图像区域和文本之间的语义对齐。（1）<strong>由于没有显示的region 与 text poses之间的对齐监督，因此是一种弱监督的任务。</strong> （2）另外，vision region常常过采样(region之间有重叠)，从而带来噪声和歧义（由于重叠，导致region之间的特征区分性不大），这将会使得vision-language task任务更加具有挑战性。</li><li>本文通过引入从images中检测出的object tags 作为anchor points来减轻images 和 text 之间语义对齐的学习。</li><li>本文提出了一个新的vision-language pre-training method <strong>OSCAR</strong> ，设计训练样本是一个三元组：（word sequence, a set of object tags, and a set of image region features. ）</li><li>Motivated by: the salient objects in an image can be accurately detected by modern object detectors, and that these objects are often mentioned in the paired text.</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gdvtabmqe2j30qh0f247k.jpg" alt="搜狗截图20200416190213.png"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>A negative case analysis of visual grounding methods for VQA</title>
      <link href="/2020/04/15/A-negative-case-analysis-of-visual-grounding-methods-for-VQA/"/>
      <url>/2020/04/15/A-negative-case-analysis-of-visual-grounding-methods-for-VQA/</url>
      
        <content type="html"><![CDATA[<h4 id="yaya简述"><a href="#yaya简述" class="headerlink" title="yaya简述"></a>yaya简述</h4><p>在VQA任务中，现在的方法尝试希望模型在回答问题时，同时能够关注到相对应的正确的物体（出发点：当模型关注到正确的物体时，能够更好的帮助模型选择出正确的答案）。于是，基于这样的方式，提出了一些方法 [1] [2]. 但是本文发现即便在模型中给了vision grounding 的监督，但是模型的grounding 能力却未必很好。那么提升VQA性能的真正原因其实是这个监督，仅仅是一种正则化效果。</p><p>作者使用了Grounding using irrelevant cues；Grounding using fixed random cues；Grounding using variable random cues 来说明，即使是错误的监督信息，相比于正确的监督也不会使得性能下降很多。</p><p>作者使用Regularization by zeroing out answers  来说明，给损失函数中加一个正则化项，使得training accuracy下降，就会达到正则化的效果，其VQA的性能与用grounding 监督的效果差距也不大。这就证明了使用grounding来监督，其实仅仅是起到了正则化的效果。</p><h4 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h4><ul><li><p>未来的方法必须设法通过使用与本文中介绍的类似的实验设置来验证性能增益不是源于spurious source.</p></li><li><p>创建一个数据集，使得能够评估  if methods are able to focus on relevant information.</p></li><li><p>Use tasks  that explicitly test grounding, e.g., in visual query detection an agent must output boxes around any regions of a scene that match the natural language query .</p></li></ul><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p>[1] Ramprasaath R Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi Parikh. <strong>Taking a hint: Leveraging explanations to make vision and language models more grounded.</strong>  In ICCV 2019. </p><p>[2] Jialin Wu and Raymond Mooney. <strong>Self-critical reasoning for robust visual question answering.</strong> In NeurIPS 2019</p>]]></content>
      
      
      <categories>
          
          <category> Grounding 相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Grounding 相关 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models</title>
      <link href="/2020/04/01/Egoshots-an-ego-vision-life-logging-dataset-and-semantic-fidelity-metric-to-evaluate-diversity-in-image-captioning-models/"/>
      <url>/2020/04/01/Egoshots-an-ego-vision-life-logging-dataset-and-semantic-fidelity-metric-to-evaluate-diversity-in-image-captioning-models/</url>
      
        <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul><li><p>当前的image caption dataset 存在的问题</p><p>图像字幕模型已经能够生成语法正确且易于理解的句子。但是，大多数字幕传达的信息有限，因为所使用的模型是在数据集上训练的，而该数据集并未为日常生活中存在的所有可能的对象提供字幕。由于缺少先验信息，因此大多数字幕仅偏向场景中出现的少数几个对象，因此限制了它们在日常生活中的使用。在本文中，我们试图证明当前现有图像字幕模型的偏向性，并提出一个新的图像字幕数据集<em>Egoshots</em>，由978张不带字幕的现实生活图像组成。我们进一步利用最先进的预训练图像字幕和对象识别网络来注释我们的图像并显示现有作品的局限性。</p></li><li><p>当前的standard metric存在的问题</p><p>此外，为了评估所生成字幕的质量，我们提出了一种新的图像字幕度量标准，即基于对象的<em>语义保真度</em>（SF）。现有的图像字幕度量标准只能在存在其相应注释（reference captions）的情况下评估字幕。但是，SF允许评估为图像生成的字幕而没有注释，这对于现实生活中生成的字幕非常有用。</p></li></ul><h4 id="SF"><a href="#SF" class="headerlink" title="SF"></a>SF</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gdel85qhz8j30se0g97eb.jpg" alt="搜狗截图20200401212845.png"></p><h4 id="Annotation-Pipline-and-Sementic-Fidelity-Metric"><a href="#Annotation-Pipline-and-Sementic-Fidelity-Metric" class="headerlink" title="Annotation Pipline and Sementic Fidelity Metric"></a>Annotation Pipline and Sementic Fidelity Metric</h4><ul><li><p>annotation pipline</p><p>使用三个预训练好的caption model: Show Attend And Tell (SAT), nocaps: novel object captioning at scale (NOC), and Decoupled Novel Object Captioner (DNOC) 在新的数据集Egoshots上进行captioning 任务。</p></li><li><p>sementic fidelity metric</p><p>我们提出了一种称为<em>语义保真</em>度的新图像字幕指标。SF考虑了两个元素：1）生成的字幕与图像中检测到的对象的语义接近度； 2）相对于检测到的对象实例数量的对象多样性。假设有一个最新的准完美对象检测器，通过考虑这两组（带字幕和检测到的）实体（即对象）之间的语义亲密性，当一个模型输出的caption中包含了并没有出现在image scene中的objects时，将进行惩罚。</p><p>公式：<img src="http://ww1.sinaimg.cn/large/006uWRWVly1gdelh0afpej3055021a9x.jpg" alt="搜狗截图20200401213725.png"></p><p>对于图像i，si是其预测字幕c i中的名词词与OD检测到的对象名词之间的语义相似性，＃O是O O D的基数，＃N是名词的数量（表示对象in N i）存在于caption i中。SF的范围为[0，1]：SF接近1的字幕传达更多信息，并且在语义上更接近于要字幕的场景（就字幕所涉及的对象而言）。</p><p>关于si的计算：Recent works (Mikolov et al., 2013; Conneau et al., 2017) show the ability of word embeddings that is transforming a word into its vectored form efficiently capture the semantic closeness of two given words. The SF metric uses this approach to calculate such semantic similarity between the noun words and objects in an image.</p><p>上述公式存在一个假设：that #O ≥ #N (Assumption 1) for all images. This approach to compute SF will work only assuming robust object detectors satisfying enough scene annotation granularity.  </p><p>同时为保证分母不为0，还需要一个假设：Assumption 2: #O ！= 0 (i.e., the object detector can at least detect one object in the image). </p><h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4></li><li><p>在上述新提出的metric中，如果使用SF执行字幕评估，则良好通用化且鲁棒的对象检测模型将扮演最重要的角色。（a well generalized and robust object detection model plays the most important role if<br>the evaluation of captions is performed using SF. ）</p></li><li><p>在物体检测器发生故障的情况下，度量是不可靠的。由于SF将无法惩罚字幕模型，因为它不能依赖忠实（即足够鲁棒）的对象检测器（＃O = 0，假设2损坏），因此无法应用SF。</p></li></ul><h4 id="Appendix-指标限制"><a href="#Appendix-指标限制" class="headerlink" title="Appendix: 指标限制"></a>Appendix: 指标限制</h4><ul><li><p>我们必须注意到度量标准的一些局限性，应加以补充/扩展为（1）解释字幕的动词和其他句法元素（当前只考虑了名词）；（2）根据解释的质量对字幕进行评分，并考虑图像中相同类型的对象相对于字幕中存在的对象的数量。诸如（Cohen17）之类的特定计数模型是有关如何增强此处提出的无标签数据集注释管道的特定示例。</p><p>应该在更有针对性的应用程序使用案例中评估指标，例如，在诸如导航，对目标用户（如盲人）的有用性。</p></li></ul><h4 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h4><ul><li>其实本文尝试使用一个open-domain dataset 来测试在 in-domain 上训练的captioning model的泛化性能。但是这本身就存在问题！因为，model本身就会受限于训练数据，因此这里却希望它有很强的泛化性能，这本身就太难为model了。<code>eg: 不能要求一个学了小学课程的人来做高中生的题目</code></li></ul><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ul><li><p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. </p><p>Distributed Representations of Words and Phrases and their  Compositionality. In NIPS, 2013. </p></li><li><p>Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herve J ´ egou. ´<br>Word Translation Without Parallel Data. ArXiv, abs/1710.04087, 2017 </p></li><li><p>Joseph Paul Cohen, Genevieve Boucher, Craig A. Glastonbury, Henry Z. Lo, and Yoshua Bengio.<br>Count-ception: Counting by Fully Convolutional Redundant Counting. In The IEEE International<br>Conference on Computer Vision (ICCV) Workshops, Oct 2017. </p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(ACL 2019)Putting Evaluation in Context: Contextual Embeddings improve Machine Translation Evaluation</title>
      <link href="/2020/04/01/ACL-2019-Putting-Evaluation-in-Context-Contextual-Embeddings-improve-Machine-Translation-Evaluation/"/>
      <url>/2020/04/01/ACL-2019-Putting-Evaluation-in-Context-Contextual-Embeddings-improve-Machine-Translation-Evaluation/</url>
      
        <content type="html"><![CDATA[<ul><li><p>没有认真阅读本篇文章，但是其中提到了尝试去拟合Human judgements，这一训练方案。</p><p> （1）We treat the human reference translation and the MT output as the premise and hypothesis, respectively 。</p><p>（2）Using squared error as part of regression loss – being better suited to Pearson’s r — and might be resolved through a different loss. Using hinge loss over pairwise preferences which would better reflect Kendall’s Tau</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自然语言理解 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BERTScore: Evaluating Text Generation with BERT</title>
      <link href="/2020/04/01/BERTScore-Evaluating-Text-Generation-with-BERT/"/>
      <url>/2020/04/01/BERTScore-Evaluating-Text-Generation-with-BERT/</url>
      
        <content type="html"><![CDATA[<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>我们提出BERTScore，这是一种用于文本生成的自动评估指标。</p><p>类似于通用指标，BERTScore计算候选句子中每个token与参考中每个token的相似性得分。但是，我们不是使用精确匹配，而是使用上下文化的BERT embedding 来计算相似度。</p><p>我们对几种机器翻译和图像字幕基准进行了评估，结果表明BERTScore与人为判断的关联性比现有指标更好，通常甚至大大超过特定于任务的监督指标。</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>在本文中，我们将重点放在句子级别的生成评估上，并提出了：BERTScore，这是一种基于预训练的BERT上下文嵌入 （bert）的评估指标。 BERTScore将两个句子之间的相似度计算为它们的标记之间的余弦相似度的加权汇总。</p><p>基于n-gram matching metric 的常见缺陷：</p><ul><li><p>semantically-correct phrases are penalized because they differ from the surface form of the reference.</p><p>解决： In contrast to string matching (e.g., in BLEU) or matching heuristics (e.g., in METEOR), we compute similarity using contextualized token embeddings, which have been shown to be effective for paraphrase detection  </p></li><li><p>n-gram models fail to capture distant dependencies and penalize semantically-critical ordering changes.</p><p>解决： contextualized embeddings are trained to effectively capture distant dependencies and ordering  </p></li></ul><p>实验结果：（1）In machine translation, BERTSCORE shows stronger system-level and segment-level correlations<br>with human judgments than existing metrics on multiple common benchmarks.（2）BERTSCORE is well-correlated with human annotators for image captioning, surpassing SPICE.</p><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><ul><li>见论文，比较好理解</li></ul><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ul><li>Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance  <ul><li>同样尝试使用contextual word embeddings  来构建一个metric.</li></ul></li><li>Putting evaluation in context: Contextual embeddings improve machine translation evaluation. In ACL, 2019.  </li></ul>]]></content>
      
      
      <categories>
          
          <category> 自然语言理解 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Grounded Situation Recognition</title>
      <link href="/2020/03/31/Grounded-Situation-Recognition/"/>
      <url>/2020/03/31/Grounded-Situation-Recognition/</url>
      
        <content type="html"><![CDATA[<h4 id="Grounded-Situation-Recognition-Task"><a href="#Grounded-Situation-Recognition-Task" class="headerlink" title="Grounded Situation Recognition Task"></a>Grounded Situation Recognition <strong>Task</strong></h4><h5 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h5><ul><li><p>以前的situation recognition task: </p><p><strong>Situation Recognition</strong> is the task of recognizing the activity happening in an image, the actors and objects involved in this activity, and the roles they play. Semantic roles describe how objects in the image participate in the activity described by the verb. </p><p>While situation recognition addresses <strong><em>what</em></strong> is happening in an image, <strong><em>who</em></strong> is playing a part in this and <strong><em>what</em></strong> their roles are, it does not address a critical aspect of visual understanding: <strong>where</strong> the involved entities lie in the image. </p></li><li><p>本文：We address this shortcoming and present <strong>Grounded Situation Recognition (GSR)</strong>, a task that builds upon situation recognition and requires one to not just identify the situation observed in the image but also visually ground the identified roles within the corresponding image.</p></li></ul><h4 id="Challenge-of-Grounded-Situation-Recognition-GSR"><a href="#Challenge-of-Grounded-Situation-Recognition-GSR" class="headerlink" title="Challenge of Grounded Situation Recognition (GSR)"></a>Challenge of Grounded Situation Recognition (GSR)</h4><ul><li><em>语义显著性</em>：与识别图像中的所有实体不同，它需要在呈现的<strong>主要活动的背景下</strong>识别关键对象和参与者。</li><li><em>语义稀疏性</em>：GSR存在语义稀疏性问题，  在训练中很少见到role and groundings 的许多组合。这一挑战要求模型从有限的数据中学习。</li><li><em>Ambiguity</em>：将角色定位到图像中通常需要消除在同一类别下的多个观察到的实体之间的歧义。</li><li><em>Scale</em>：grounded entities 的比例尺变化很大，图像中也缺少某些实体（在这种情况下，模型负责检测这种缺失）。</li><li><em>Hallucination</em>：标记语义角色并grounding 通常需要弄清物体的存在，因为它们可能被完全遮挡或不在屏幕上。</li></ul><h4 id="Situations-With-Groundings-SWiG-dataset"><a href="#Situations-With-Groundings-SWiG-dataset" class="headerlink" title="Situations With Groundings (SWiG) dataset"></a>Situations With Groundings (SWiG) dataset</h4><p><a href="https://prior.allenai.org/assets/project-content/gsr/gsr_banner.png" target="_blank" rel="noopener"><img src="https://prior.allenai.org/assets/project-content/gsr/gsr_banner.png" alt="SWiG examples">A sample of images from the SWiG dataset</a></p><p>We present the Situations With Groundings (SWiG) Dataset for training and evalutation on the GSR task. This dataset builds upon the <a href="https://homes.cs.washington.edu/~ali/papers/SituationRecognition.pdf" target="_blank" rel="noopener">Situation Recognition dataset</a> presented by Yatskar et al. The SWiG dataset contains approximately 125,000 images. Each image is associated with one verb. Three different annotators then label each <strong>entity</strong> in the frame associated with that <strong>verb</strong> and mark the <strong>location</strong> of the entity in the image. All three labels for each role are given in the SWiG dataset as well as an average of the three localizations.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Multi-task Collaborative Network for Joint Referring Expression Comprehension and Segmentation</title>
      <link href="/2020/03/30/Multi-task-Collaborative-Network-for-Joint-Referring-Expression-Comprehension-and-Segmentation/"/>
      <url>/2020/03/30/Multi-task-Collaborative-Network-for-Joint-Referring-Expression-Comprehension-and-Segmentation/</url>
      
        <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>VIOLIN: A Large-Scale Dataset for Video-and-Language Inference</title>
      <link href="/2020/03/28/VIOLIN-A-Large-Scale-Dataset-for-Video-and-Language-Inference/"/>
      <url>/2020/03/28/VIOLIN-A-Large-Scale-Dataset-for-Video-and-Language-Inference/</url>
      
        <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Object Hallucination in Image Captioning </title>
      <link href="/2020/03/25/Object-Hallucination-in-Image-Captioning/"/>
      <url>/2020/03/25/Object-Hallucination-in-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul><li>当前captioning task 存在的问题<ul><li>当前的caption model 目前存在的问题是，生成的句子中出现的object 常常是在corresponding vision scene 中没有出现到的。</li><li>当前使用的评价指标只能评估 candidate caption 与 gt captions 之间的一个相似性，不能捕捉到candidate caption 与 image information之间的relevance. </li></ul></li></ul><ul><li>因此本文进行的一个工作：<ul><li>We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination.  </li></ul></li></ul><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><ul><li><p>（一）关于 object hallucination 的四点讨论</p><ul><li><p>从人类的角度</p><ul><li>丢失对显著物体的描述是一个 failure mode，但是captions is summaries，<strong>因此一般不期待其描述出场景中的所有的objects.</strong> 另外在人类的标注数据中，也不偏向于标注出所有出现在scene 中的objects</li><li>研究报告表明，human judgements 比较不待见那些caption中包含了image content中未出现的 object，*<em>Correctness is more important to human judges than specificity.   *</em></li><li>Many visually impaired who *<em>value correctness over coverage, *</em>hallucination is an obvious concern.  </li></ul></li><li><p>从模型的角度</p><ul><li>object hallucination  揭示了caption model 存在的一个问题，可能caption model并没有对视觉场景学习到一个很好的视觉表征，而是对损失函数过拟合。</li></ul></li></ul></li><li><p>（二）本文要研究的问题</p><ul><li>本文研究当前captioning models中存在的object hallucination现象</li><li>考虑了几个关键问题：<ul><li>(1) <strong>Which models are more prone to hallucination?</strong>  spanning different architectures and learning objectives.   <ul><li>一个新的评价指标来评估object hallucination：CHAIR (Caption Hallucination Assessment with Image Relevance)  </li></ul></li><li>(2) *<em>What are the likely causes of hallucination?   *</em><ul><li>造成object hallucination这一现象的原因主要有两点：visual misclassification and over-reliance on language priors  <ul><li>提出：image and language model consistency scores  </li></ul></li></ul></li><li>(3) <strong>How well do the standard metrics capture hallucination?</strong>  <ul><li>当前的评价指标并不能很好的捕捉到object hallucination 这一现象。</li></ul></li></ul></li></ul></li></ul><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><h5 id="CHAIR-Metric"><a href="#CHAIR-Metric" class="headerlink" title="CHAIR Metric"></a>CHAIR Metric</h5><ul><li>同时使用GT sentence 和 coco image segmentation这两个信息 to measure object hallucination。</li></ul><p>  <img src="http://ww1.sinaimg.cn/large/006uWRWVly1gd7d25xmjcj30h605k0te.jpg" alt="搜狗截图20200326152719.png"></p><h5 id="Image-Consistency"><a href="#Image-Consistency" class="headerlink" title="Image Consistency"></a>Image Consistency</h5><ul><li>对比 <strong>caption model 与 image (alone) model</strong> 两个模型对于预测objects 之间的一致性误差。</li></ul><h5 id="Language-Consistency"><a href="#Language-Consistency" class="headerlink" title="Language Consistency"></a>Language Consistency</h5><ul><li>对比 <strong>caption model 与 sentence (alone) model</strong> 两个模型对于预测下一个word 之间的一致性误差。</li></ul><h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><h5 id="Which-Models-Are-More-Prone-To-Hallucination"><a href="#Which-Models-Are-More-Prone-To-Hallucination" class="headerlink" title="Which Models Are More Prone To Hallucination?"></a>Which Models Are More Prone To Hallucination?</h5><ul><li>一般情况下 ，在标准的evaluation metrics 上表现性能好的模型，在CHAIR metric 上也能表现的比较好，即object hallucination现象相对较弱。但是当模型基于 cider 进行强化学习的训练之后，则不是这种一致的现象。</li><li>（1）使用attention 的模型更加偏向于有较低的object hallucination；NBT模型在标准的evaluation metrics 上表现性能没有topdown-BB 好，但是CHAIR性能却更好，原因在于其使用的pre-trained  detector 与 captioning dataset is in a same domain 。</li><li>（2）当模型基于 cider 进行强化学习的训练之后，将会增加hallucination 的数量。</li><li>（3）LRCN Model 比 FC Model有更多的object hallucination，说明仅仅在第一个 time step 输入视觉特征，将会产生更少的object hallucination </li><li>（4）the GAN loss actually helps decrease hallucination.  the GAN loss encourages sentences to be human-like。</li><li>（5）CE loss: beam size 5, object hallucination 会比 lower beam size 小很多；self-critical loss: beam size sometimes leads to worse performance on CHAIR.   即object hallucination 数量会更多。</li></ul><h5 id="What-Are-The-Likely-Causes-Of-Hallucination"><a href="#What-Are-The-Likely-Causes-Of-Hallucination" class="headerlink" title="What Are The Likely Causes Of Hallucination?"></a>What Are The Likely Causes Of Hallucination?</h5><ul><li><p>We rely on the deconstructed TopDown models to analyze the impact of model components on hallucination  </p><ul><li>通过设计的 几个 deconstructed TopDown models 的分析可以看出，使得object hallucination 数量减少的原因是：due to access to feature maps with spatial locality, not the actual attention mechanism.  </li><li>LRCN Model 比 FC Model 有更多的object hallucination，说明仅仅在第一个 time step 输入视觉特征，将会产生更少的object hallucination 。作者在文中对这一现象给出的解释是，在每一步都输入视觉特征 fc_feature, 而不是 spatial feature, 这将导致对视觉特征的过拟合。</li></ul></li><li><p>Investigate what causes hallucination using the deconstructed TopDown models and the image consistency and language consistency scores. </p><ul><li>We note that models with less hallucination tend to make errors consistent with the image model, whereas models with more hallucination tend to make errors consistent with the  language model.  这说明有更少object hallutition 的models 有更强的能力从Image 中提取知识到句子生成过程中。</li><li>在Robust split 上进行实验发现，所有models之间的language consistency 差异度不大；相比于 Karpathy split，相对应下的models image consistency 有所下降。这是由于 Robust split 在测试集上，会出现 novel compositions of objects at test time. 使得所有的模型有很强的language prior.</li></ul></li><li><p>在训练过程中，分析FC model 的image/language consistency，结果发现在训练开始，与language model 的一致性更好，随着训练的结束，与 image model 的一致性更好。这说明，模型首先学习生成流畅的语言，而后再去学习结合视觉信息。</p><h5 id="How-Well-Do-The-Standard-Metrics-Capture-Hallucination"><a href="#How-Well-Do-The-Standard-Metrics-Capture-Hallucination" class="headerlink" title="How Well Do The Standard Metrics Capture Hallucination?"></a>How Well Do The Standard Metrics Capture Hallucination?</h5></li><li><p>作者分析了 standard metric 与 CHAIRs  之间的 pearsom correlation coefficient ，结果发现 SPICE 的一致性更好。</p></li><li><p>object hallucination can not be always predicted based on the traditional sentence metrics.  </p></li><li><p><strong>与当前使用的standard metrci 互为补充，可以提升与人类评分的一致程度。</strong></p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gd8997jm24j30gi08375g.jpg" alt="搜狗截图20200327100117.png"></p><p>这个意思是说，第一列，单独分析各个automatic metric 与 human judgement 的一致性。第二/三列，各个评价指标分别加上1-CHs/ 1-CHi 之后再与human judgement 计算一致性。可以发现，一致性得到提升。即 <strong>CHAIR is complementary to standard metrics</strong></p></li></ul><h5 id="Does-hallucination-impact-generation-of-other-words"><a href="#Does-hallucination-impact-generation-of-other-words" class="headerlink" title="Does hallucination impact generation of other words?"></a>Does hallucination impact generation of other words?</h5><ul><li>Hallucinating objects 影响句子生成的质量，不仅是由于 object 没有被正确的预测，也是由于hallucinated word 影响到了生成的其他的words.</li><li>通过比较TopDown 和 TD-Restrict 生成的句子可以分析这个现象。We find that after the hallucinated word is generated, the following words in the sentence are different 47.3% of  the time.  </li><li>一旦一个hallucination words 被生成，则其又会由于language prior(hallucinating a “cat” leading to hallucinating<br>a “chair”  )，产生更多的hallucination words 。</li></ul><h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4><ul><li><p>the popular self critical loss increases CIDEr score, but also the amount of hallucination. </p></li><li><p>CHAIR complements the standard sentence metrics in capturing human preference( judgements ).  </p></li><li><p>Models with stronger image consistency frequently hallucinate fewer objects, suggesting that strong visual processing is important for  avoiding hallucination.  </p></li><li><p><strong>Advises for captioning task:</strong> 仅使用CE-loss/ standard sentence metrics来优化，不太能解决object hallucination 这个问题，若同时以 image relevance 来优化，会更好。</p></li></ul><h4 id="yaya-总结"><a href="#yaya-总结" class="headerlink" title="yaya 总结"></a>yaya 总结</h4><ul><li><p>在设计评价指标上给我的几点启发</p><ul><li>(1) 不需要要求machine generated caption 可以概括所有的objects which have been occurred in the vision scene</li><li>(2) machine generated caption 进行评价时，正确性比全面性更加重要</li></ul></li><li><p>本文的一个巧妙的点</p><ul><li>本文为了查看object hallucination，使用COCO的80个类。对于candidata caption 首先将其token， 然后调整成单数形式，然后使用同义词的思想，去跟COCO 的80个类别进行匹配。</li><li>另外对于GT sentences，也提出一个list，这个地方不太知道它说的什么意思？？？？？  </li></ul></li><li><p>GVD 好像也类似的提到过类似的思想</p></li><li><p>关于the image consistency and language consistency scores.</p><ul><li>在这个得分的计算方式上，是否还有什么可以改进的地方？</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP: 自编码 and 自回归</title>
      <link href="/2020/03/24/NLP-%E8%87%AA%E7%BC%96%E7%A0%81-and-%E8%87%AA%E5%9B%9E%E5%BD%92/"/>
      <url>/2020/03/24/NLP-%E8%87%AA%E7%BC%96%E7%A0%81-and-%E8%87%AA%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<ul><li><p><a href="https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff" target="_blank" rel="noopener">https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff</a>      </p></li><li><p>这篇博文写的不错<br><a href="https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff" target="_blank" rel="noopener">https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions</title>
      <link href="/2020/01/16/VIFIDEL-Evaluating-the-Visual-Fidelity-of-Image-Descriptions/"/>
      <url>/2020/01/16/VIFIDEL-Evaluating-the-Visual-Fidelity-of-Image-Descriptions/</url>
      
        <content type="html"><![CDATA[<h3 id="当前指标存在的问题"><a href="#当前指标存在的问题" class="headerlink" title="当前指标存在的问题"></a>当前指标存在的问题</h3><ul><li>BLEU, ROUGE, Meteor, CIDEr 这些指标， 他们依靠精确的字符串匹配来测量 condidate 文本和reference文献之间的surface-level 、 n-gram 重叠。当 references 有限的情况下，这会导致样本稀疏问题 （reference数量对 metric 得分有很大影响，因为reference 数量越多，多样性更好）。Meteor 通过匹配字典和释义表中的同义词来部分解决此问题，但受限于此类字典的可用性，也不能很好地适用于其他的 language。SPICE and BAST 通过计算语义级别的相似性来解决 exact string matching。但是这个方法严重的依赖于语言资源，例如 parsers, semantic role labellers, tailored rules, 使其很难适应到不同的语言和领域。</li></ul><h3 id="仅仅使用-reference-description-来-评估-image-description-的缺点"><a href="#仅仅使用-reference-description-来-评估-image-description-的缺点" class="headerlink" title="仅仅使用 reference description 来 评估 image description 的缺点"></a>仅仅使用 reference description 来 评估 image description 的缺点</h3><ul><li>受限于 reference 的数量，可能会造成<strong>样本稀缺</strong>的问题。</li><li>reference description 是<strong>主观的，有歧义的</strong>，可能不能涵盖 image 中所有的关键信息，可能只包含 image content 的一个子集。<strong>使用 object labels 可以解决这个问题</strong> </li><li>references 可能含有错误的信息。</li></ul><h3 id="基于-object-information-来-作为评价指标的优点"><a href="#基于-object-information-来-作为评价指标的优点" class="headerlink" title="基于 object information 来 作为评价指标的优点"></a>基于 object information 来 作为评价指标的优点</h3><ul><li><strong>少的标注时间消耗</strong>： 若仅使用 multiple descriptions 来作为参考，则必然需要人类为 每个 image 来标注 多个 descriptions，在标注数据上需要花费很多时间。且为每个 image 标注的description 数量越多，评估的越准确，则也需要更多的标注时间。</li><li>但是若使用基于 object imformation ， 则可以使用 predicted objects 或者 object annotations</li></ul><h3 id="Modelling-object-importance-with-reference-descriptions"><a href="#Modelling-object-importance-with-reference-descriptions" class="headerlink" title="Modelling object importance with reference descriptions"></a>Modelling object importance with reference descriptions</h3><ul><li><p>human reference 可以作为一种guidance 提供信息 – 人类对该张图片关注的重点在哪里。</p></li><li><p>与 CIDEr 很相似，都是考虑与 human reference 之间的consensus 信息，但是有以下两点不同：  </p><p>（1）使用reference 来建模object的重要性，而不是直接将 候选与 参考进行比较。<br>（2）在语义空间使用word embedding来执行 word matching，而不是直接在计算表面的匹配度（eg: n-gram）   </p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TIGEr: Text-to-Image Grounding for Image Caption Evaluation</title>
      <link href="/2020/01/15/TIGEr-Text-to-Image-Grounding-for-Image-Caption-Evaluation/"/>
      <url>/2020/01/15/TIGEr-Text-to-Image-Grounding-for-Image-Caption-Evaluation/</url>
      
        <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li><p>当前在图像描述领域使用的 automatic metric 仅仅考虑了 gt 与 pred sentence 之间的匹配度。这就存在问题：（1）给出的 references 可能不能  fully cover the image content。（2）自然语言本质上就是有歧义的（模棱两可的）</p></li><li><p>因此提出了 TIGEr，该指标，（1）不仅可以评估 pred caption 与  image content 之间的匹配度，（2）也能评估 pred caption 与 gt caption 之间的匹配度</p></li><li><p>（1） 对于  pred caption 与 gt caption， 使用预训练的 image-text grounding model 来 grounds the content of texts。然后分别比较 relevance ranking 和 distribution of grounding weights。</p></li><li><p>（2）计算 pred 与 gt caption 之间的匹配度时，不采用 n-gram matching，而是将他们映射到一个共同的语义空间，再对得到的映射向量进行比较。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Graph Matching Networks for Learning the Similarity of Graph Structured Objects</title>
      <link href="/2019/12/20/Graph-Matching-Networks-for-Learning-the-Similarity-of-Graph-Structured-Objects/"/>
      <url>/2019/12/20/Graph-Matching-Networks-for-Learning-the-Similarity-of-Graph-Structured-Objects/</url>
      
        <content type="html"><![CDATA[<ul><li>ICML 2019</li></ul><h3 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h3><ul><li>本文主要是提出了一种 新的方法来计算图的相似度问题</li><li>普通的方法分别单独计算 graph vector，而后再计算graph 之间的相似性</li><li>新提出的方法在计算  graph vector 时考虑了 cross graph matching vector来得到 更具有判别性的 graph vector，从而更好的用于 计算 graph similarity.</li></ul><h3 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h3><ul><li>Graph Edit Distance（GED）：文中对问题进行了简化，两个graph（G1, G2），相同数量的节点数和边数, 如何变动一个图中的edge(i, j) 到 edge(i’, j’) 才能使两个graph 完全一样。以下是 GED=1 的例子。</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ga3ats0xyzj308p07jaaw.jpg" alt="搜狗截图20191220170651.png"></p><ul><li>该文主要是想解决graph 的相似性问题，而不是真正的要求解出来需要几步的 graph edit distance。因此对问题做了如下的设定：<code>positive pair:（原图G，对G变动一条边：G1）</code>，<code>negative pair: （原图G，对G变动两条边：G1）</code></li><li>positive pair 认为这两个 graph 是相似的，label=1; 而negative pair认为两个graph 是不相似的, label=-1。</li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ga3b3udt8bj30pe0bsgnq.jpg" alt="搜狗截图20191220171642.png"></p><ul><li><p>一般的计算两个graph之间的相似性问题采用上图中的左图的方法，分别单独计算出 graph vector，而后再计算 vector space similarity</p></li><li><p>而本文：计算两个graph之间的匹配，然后互相作为补充特征（cross-graph matching vector），得到更加 <strong>discriminative</strong>  graph representation， 从而更加有效的graph 之间的相似度问题。</p></li><li><p>yaya: 文中使用的是 <code>difference between node_i and its closest neighbor in the other graph</code>  来计算  <code>cross-graph matching vector</code> 。我认为还可以有其他的方法或许会更加有效。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图卷积网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图卷积网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Quality Estimation for Image Captions Based on Large-scale Human Evaluations</title>
      <link href="/2019/12/12/Quality-Estimation-for-Image-Captions-Based-on-Large-scale-Human-Evaluations/"/>
      <url>/2019/12/12/Quality-Estimation-for-Image-Captions-Based-on-Large-scale-Human-Evaluations/</url>
      
        <content type="html"><![CDATA[<h3 id="Quality-Estimation-QE-of-image-captions"><a href="#Quality-Estimation-QE-of-image-captions" class="headerlink" title="Quality Estimation (QE) of image-captions"></a>Quality Estimation (QE) of image-captions</h3><ul><li>本文提出了在图像描述领域一个新的问题，Quality Estimation。由于当前的 automatic metric 非常依赖 ground-truth references，因此当一个模型训练好后，若是对一个 unseen images which don’t have gt sentence 进行描述，则无法对该描述进行评价。    </li></ul><h3 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h3><h4 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h4><ul><li>（1）首先在 Conceptual Captions dataset  上训练多个 image-captioning model （这个数据集会在比 coco上训练的captioning model 更好），captioning model的差异可以体现在 image feature extraction model，object detection提取的object 数量，caption decoder。    </li><li>（2）以上的 image-captioning model 可以为一个image 提供多个 sentence，作者对image 进行了脱敏处理    </li></ul><h4 id="数据打分及处理"><a href="#数据打分及处理" class="headerlink" title="数据打分及处理"></a>数据打分及处理</h4><ul><li><p>（1）这些 image-caption pairs 放到 crowdsource.google上让大家对这些 captioning，进行评价：好、坏或者跳过。每个image-captioning pair 被分配给10个人进行打分     </p></li><li><p>（2）得到收集的 rating image-captioning pairs 之后，对 unique image，将10个评分进行处理， using the equation y = round(mean(ri) ∗ 8)/8.     </p></li></ul><h4 id="QE-Model"><a href="#QE-Model" class="headerlink" title="QE Model"></a>QE Model</h4><ul><li>本文作者设计了两个模型（并把这两个模型进行融合）来处理 QE task。一个是使用到 image-captioning model，两一个是不使用   </li><li>（1）使用image-captioning model：<strong>Confidence-based Features QE Model</strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVgy1g9txhzr2swj30yc0i642k.jpg" alt="搜狗截图20191212143453.png"></li><li>（2） 不使用 image-captioning model：<strong>Generation-independent Bilinear QE model</strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVgy1g9txhzmt4rj30xs0apq5u.jpg" alt="搜狗截图20191212143515.png"></li></ul><h4 id="Spearman’s-ρ-Analysis"><a href="#Spearman’s-ρ-Analysis" class="headerlink" title="Spearman’s ρ Analysis"></a>Spearman’s ρ Analysis</h4><ul><li>该文的主要目的就是希望在 没有gt sentence 的情况下，对 unseen-image 进行描述时，可以给出一个caption 的评分。或者是说，该captioning与 人类的描述的相近程度。</li><li>该任务也是希望提出一个 machine learning metric similar to human evaluation （trained-metric），则对该模型好坏的一个的评判就是这个模型给出的评分与人类评分的相近程度。</li><li>predict： 模型对image-caption pair 的评分， Gt:  人类给出的评分</li><li>指标：Spearman’s correlation.  <a href="https://github.com/ShiYaya/spearman-rank" target="_blank" rel="noopener">my github explanation</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pth: save in py2, but load in py3</title>
      <link href="/2019/12/11/pth-save-in-py2-but-load-in-py3/"/>
      <url>/2019/12/11/pth-save-in-py2-but-load-in-py3/</url>
      
        <content type="html"><![CDATA[<h3 id="在torch-load-pth-时出现的问题：UnicodeDecodeError-39-utf-8-39-codec-can-39-t-decode-byte-0xba-in-position-0-invalid-start-byte"><a href="#在torch-load-pth-时出现的问题：UnicodeDecodeError-39-utf-8-39-codec-can-39-t-decode-byte-0xba-in-position-0-invalid-start-byte" class="headerlink" title="在torch.load(*.pth) 时出现的问题：UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xba in position 0: invalid start byte"></a>在torch.load(*.pth) 时出现的问题：<code>UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xba in position 0: invalid start byte</code></h3><ul><li><p>经过网络查询，发现是由于该文件是在 python2 下保存的，但是现在却是在python3下读取，而导致的错误</p></li><li><p>有的人给出了下面的解决方案(但是对于我是无效的)：    </p><p>来自：<a href="https://github.com/CSAILVision/places365/issues/25" target="_blank" rel="noopener">https://github.com/CSAILVision/places365/issues/25</a></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools import partial</span><br><span class="line">import pickle</span><br><span class="line">pickle.load = partial(pickle.load, <span class="attribute">encoding</span>=<span class="string">"latin1"</span>)</span><br><span class="line">pickle.Unpickler = partial(pickle.Unpickler, <span class="attribute">encoding</span>=<span class="string">"latin1"</span>)</span><br><span class="line">model = torch.load(model_file, <span class="attribute">map_location</span>=lambda storage, loc: storage, <span class="attribute">pickle_module</span>=pickle)</span><br></pre></td></tr></table></figure></li></ul><ul><li><p><strong>这里给出我的解决办法</strong>   </p><p>（1） 在python2 环境下读取该文件，然后用 pickle来保存   </p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tmp_data = torch.<span class="built_in">load</span>(model_file)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'tmp.pickle'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> <span class="built_in">file</span>:</span><br><span class="line">    pickle.dump(tmp_data, <span class="built_in">file</span>, protocol=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>（2）换到python3环境下，再读取pickle文件，再用torch.load来保存（这一点或可以省略）</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'tmp.pickle'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> <span class="built_in">file</span>:</span><br><span class="line">    tmp_data = pickle.<span class="built_in">load</span>(<span class="built_in">file</span>, encoding=<span class="string">'latin1'</span>)</span><br><span class="line">    </span><br><span class="line">torch.save(tmp_data, tmp_model.pth)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video</title>
      <link href="/2019/12/02/Weakly-Supervised-Spatio-Temporally-Grounding-Natural-Sentence-in-Video/"/>
      <url>/2019/12/02/Weakly-Supervised-Spatio-Temporally-Grounding-Natural-Sentence-in-Video/</url>
      
        <content type="html"><![CDATA[<ul><li><strong>ACL 2019</strong></li></ul><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li><p>image grounding取得了很大的进步，但是将该任务迁移到视频上，需要对视频中的每帧都标注region，这个工程量是巨大的。</p></li><li><p>为了避免这种标注工作的工程量，一些<strong>weakly-supervised</strong> video grouding工作【1】【2】被提出来，他们只提供了video-sentence pairs，没有提供 fine-grained regional annotations。在他们的 video grounding任务中，他们仅仅对名词和代词在 视频的静态帧进行grounding。</p></li><li><p>但是这种 grounding存在问题，比如sentence: A brown and white dog is lying on the grass and then it stands up. 但是帧中出现了多个狗，而我们给出的要搜索的对象仅仅是一个名词： ‘dog’，没有其他更多的信息，来进行更加具体地定位，那么就有可能定位错误。另外只对一张静态帧进行定位，也无法捕捉到object在时域上的动态变化。</p></li><li><p>基于上述的分析，本文提出了一个在video grounding上 weakly-supervised 的新任务：<strong>weakly-supervised spatio-temporally grounding sentence in video (WSSTG).</strong>    </p></li></ul><h3 id="Weakly-supervised-spatio-temporally-grounding-sentence-in-video"><a href="#Weakly-supervised-spatio-temporally-grounding-sentence-in-video" class="headerlink" title="Weakly-supervised spatio-temporally grounding sentence in video"></a>Weakly-supervised spatio-temporally grounding sentence in video</h3><ul><li>Specifically, given a natural sentence and a video, we aim to localize a spatio-temporal tube (i.e., a sequence of bounding boxes) ,（本文中作者将tube 称作 instance）</li><li>yaya: 相比于之前的video-grounding任务，同是 weakly-supervised，但是有两点不同：（1）是句子级别的描述，对要定位的对象的描述更加具体，而不是仅仅是个noun。（2）是要定位出一个 spatial-temporal tube，而不是仅在一张静态帧中定位出一个bbox。</li><li>这两点不同同时带来了优势和挑战</li><li>（1）细节性的描述可以消除歧义，但是如何捕捉句子中的语义并在video中定位出来是一个难题；（2）相比于在静态帧中定位一个bbox, 而是在video中定位一个tube,更能呈现出一个object在时域上的动态。但是，如何利用和建模tube的时空特性以及它们与句子的复杂关系提出了另一个挑战。</li><li>compared with 【2】: different from 【2】，whose text input consists of nouns/pronouns and output is a bounding box in a specific frame, we aim  to ground a natural sentence and output a spatio-temporal tube in the video. </li></ul><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul><li>提出了一个新任务：weakly-supervised spatio-temporally grounding sentence in video</li><li>针对该任务提出了一个method：提出了一个Attentive interactor利用 tube(instance) 与 sentence之间的细粒度的关系来计算 匹配度；提出了一个diversity loss来加强 reliable instance-sentence pairs 并惩罚 unreliable ones。</li><li>在VID object detection dataset 数据集的基础上，对tube(instance) 增加了description</li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><ul><li>该任务是 给出一个 a natural sentence query <strong>q</strong> and a video <strong>v</strong> 来定位一个spatial-temporal tube，作者也将这个tube 称作 instance。</li><li>由于是弱监督，因此仅仅只给出 video-sentence pair，细粒度的regional annotations不给出！</li><li>将该任务转为一个 Multiple instance learning problem。给定一个video，首先由instance generator【3】来生成一组instance proposals，然后再根据语义相似性来匹配 natural sentence query 和 instance。  </li></ul><h4 id="Instance-Extraction"><a href="#Instance-Extraction" class="headerlink" title="Instance Extraction"></a>Instance Extraction</h4><ul><li><strong>Instance Generation</strong> ：  先由faster rcnn提取object proposals，假设每帧提取N个proposal ， 然后根据【3】得到N个spatial-temporal tube</li><li><strong>Feature Representation</strong> ：I3D-RGB， I3D-Flow， frame-level RoI pooled feature   </li></ul><h4 id="Attentive-Interactor"><a href="#Attentive-Interactor" class="headerlink" title="Attentive Interactor"></a>Attentive Interactor</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g9io2sodynj30i40hk0v9.jpg" alt="搜狗截图20191202204720.png"></p><ul><li>（1）分别对 sequential visual features 和 sequential textual features 经过LSTM进行编码，LSTM每个step输出的隐层状态作为新的representation，得到新的visual feature 和 sentence representation</li><li>（2）依次以visual feature中的每个隐状态作为查询，以 sentence 所有隐状态作为key 和 value，输入Attention中，则得到了<strong>visual guided sentence feature</strong>。（直观的理解：在给定某一个视觉特征，用attention去分析要关注哪一个word）  </li></ul><h4 id="Matching-Behavior-Characterization"><a href="#Matching-Behavior-Characterization" class="headerlink" title="Matching Behavior Characterization"></a>Matching Behavior Characterization</h4><ul><li>用余弦函数计算 <code>i-th</code> visual feature 和 visual guided sentence features 之间的 匹配度</li><li>对所有的step 加和，则得到instance proposal 与 sentence 之间的匹配度</li></ul><h3 id="Training-Loss"><a href="#Training-Loss" class="headerlink" title="Training Loss"></a>Training Loss</h3><ul><li><p>论文对这里介绍的比较详细，参见论文。</p></li><li><p><strong>ranking loss</strong>： aiming at distinguishing aligned video-sentence pairs from the unaligned ones.  这个损失是希望不匹配的video-sentence之间计算出来的匹配度差一些，比如给网络输入不与该视频对应的句子。</p></li><li><p><strong>novel diversity loss</strong> ：to strengthen the matching behaviors between reliable instance-sentence pairs and penalize the unreliable ones from the aligned video-sentence pair.  这个损失主要是希望对一个video，在计算tube 与 sentence之间的匹配度时，希望不同的 tube之间的差异性（diversity）大一些！</p></li></ul><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul><li>一个video 给出了N个 tube proposal，当计算完匹配度之后，选取匹配度最大的那个proposal，然后计算与GT之间的 overlap【4】，若overlap 大于一个阈值，则任务预测正确。</li></ul><h3 id="Yaya-Analysis："><a href="#Yaya-Analysis：" class="headerlink" title="Yaya Analysis："></a>Yaya Analysis：</h3><ul><li><p><strong>此类任务可提升的point</strong></p></li><li><p>更好的 detector来获取 object proposal</p></li><li><p>更好的算法来获取 tube proposal</p></li><li><p>设计算法更好滴计算 sentence 与 tube proposal 匹配度！</p></li><li><p>对 rank loss 给予更多的约束，像此文：提出了一个novel  diversity loss</p></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li>【1】De-An Huang, Shyamal Buch, Lucio Dery, Animesh Garg, Li Fei-Fei, and Juan Carlos Niebles. 2018. <strong>Finding “it”: Weakly-supervised reference-aware visual grounding in instructional videos</strong>. In CVPR. </li><li>【2】Luowei Zhou, Nathan Louis, and Jason J Corso. 2018. <strong>Weakly-supervised video object grounding from text by loss weighting and object interaction</strong>. BMVC. </li><li>【3】Georgia Gkioxari and Jitendra Malik. 2015. <strong>Finding action tubes</strong>. In CVPR, pages 759–768. </li><li>【4】Masataka Yamaguchi, Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. 2017. <strong>Spatio-temporal person retrieval via natural language queries</strong>. In ICCV. </li></ul>]]></content>
      
      
      <categories>
          
          <category> Visual Grounding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Viusal Grounding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Finding It: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos</title>
      <link href="/2019/12/02/Finding-It-Weakly-Supervised-Reference-Aware-Visual-Grounding-in-Instructional-Videos/"/>
      <url>/2019/12/02/Finding-It-Weakly-Supervised-Reference-Aware-Visual-Grounding-in-Instructional-Videos/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Learning to Generate Grounded Visual Captions without Localization Supervision</title>
      <link href="/2019/12/01/Learning-to-Generate-Grounded-Visual-Captions-without-Localization-Supervision/"/>
      <url>/2019/12/01/Learning-to-Generate-Grounded-Visual-Captions-without-Localization-Supervision/</url>
      
        <content type="html"><![CDATA[<h3 id="ICLR-2020-under-view"><a href="#ICLR-2020-under-view" class="headerlink" title="ICLR 2020 under view"></a>ICLR 2020 under view</h3><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li><p>问题：在captioning 任务中，当前的评价指标并不能很好的反应生成的句子与该视频之间的契合度（Groud），有可能生成的句子只是基于在训练过程中学习到的priors（一种统计特性，而不是基于该视频本身）</p></li><li><p>当前模型对于 groud 这个任务，存在的困难：（1）由于当前的 language model 常使用 attention 机制来关注某一个 region，以此来预测下一个生成的单词。换句话说，就是在不知道将会生成什么单词的情况下，却要先定位region， 另外，一篇论文 [1] 提出，attention机制关注的region与人类所关注的并不一致（2）更难的是：传入 attention网络的是 RNN 的 hidden_state，由于 RNN 的记录历史的特性，这个输入包括的是过去所有的信息，而不是针对于某一个individual word。  </p></li></ul><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul><li>不同于 GVD，该文不使用 annotation bbox 作为监督信号，而是使用了 decoder + localizer + redecoder的结构来自我监督（self-supervision）</li><li>由于其自监督的特性，在一些infrequent word上该文的方法比监督的方法，效果更好</li><li>不仅使用一般的为每个 object class 计算 grounding accuracy， 还提出了一个新的指标：为每个sentence 计算grounding accuracy。</li></ul><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ul><li><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g9heo6n8glj318k0lp0ze.jpg" alt="搜狗截图20191201183637.png"></p></li><li><p><strong>分阶段训练</strong></p></li><li><p>（1）正常的 encoder-decoder先训练 ~30个epoch</p></li><li><p>（2）在正常的基础上进行添加。 （a）<strong>re-localize</strong>: language_lstm 会得到y1, y2, …, yT 个预测的序列，将这些序列作为attention机制中的 查询向量，赋给每个region一个attention系数，这样就可以在每一个step重构attention系数分配，这样也解决了motivation中提到的问题，即attention是由某一个individual word 而计算得来的。（b）每个step 有了attention对齐之后的attention_region， 再输入到language_LSTM中，得到<strong>再次预测的sequence of word</strong>。</p></li><li><p>在这第二阶段，就是两个loss 交叉熵损失进行权重加和来训练</p></li><li><p>可以发现一个问题，对于visual-words 和 non-visual-words都进行了re-localize。实际上对于，on a 等这类词汇，并不需要在image中找到 grounded region。 该文作者在补充材料里给出了一些额外的实验， eg, 将这些non-visual words 进行抑制，不计算reconstruction loss, 或者给这些localized region representation重新赋给invalid representaion。但是实验表明，在Flickr30 上性能（caption and ground）有提升，但是在 activity上（caption 没变化，ground下降）。</p></li><li><p>但是作者并没有给出分析，我个人总觉得实验设计的不完善，分析的也不多。</p></li></ul><h3 id="Measuring-grounding-per-generated-sentence"><a href="#Measuring-grounding-per-generated-sentence" class="headerlink" title="Measuring grounding per generated sentence"></a>Measuring grounding per generated sentence</h3><ul><li>提该指标的原因：（Such metrics （F1all, F1loc） are extremely stringent as captioning models are generally biased toward certain words in the vocabulary, given the long-tailed distribution of words. ）</li></ul><h3 id="Analysis-Grounding-performance-when-using-a-better-object-detector"><a href="#Analysis-Grounding-performance-when-using-a-better-object-detector" class="headerlink" title="Analysis:  Grounding performance when using a better object detector."></a>Analysis:  Grounding performance when using a better object detector.</h3><ul><li>在 Flickr30k Entities 上进行实验，分析 better detector 对 grounding性能的影响</li><li>（1）使用 GT box (ubrealistically) ，进行实验，发现 caption metric 和 grounding accuracy都有提升</li><li>（2）在 Flickr30k上训练一个detector（之前使用的是在 visual genome上训练好的），进行实验，发现，使得caption metirc下降，（作者分析：由于在本数据集上进行训练，得到的 the ROI features and their associated object predictions 更偏向于 该数据中的  the annotated object words 却不能很好地泛化以预测 diverse captions， 从而导致了captioning 指标下降）</li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv Batra.  <strong>Human attention in visual question answering: Do humans and deep networks look at the same regions?</strong>  Computer Vision and Image Understanding, 163:90–100, 2017. </p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vatex_challenge_solutions</title>
      <link href="/2019/10/20/vatex-challenge-solutions/"/>
      <url>/2019/10/20/vatex-challenge-solutions/</url>
      
        <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch 索引 切片</title>
      <link href="/2019/10/10/pytorch-%E7%B4%A2%E5%BC%95-%E5%88%87%E7%89%87/"/>
      <url>/2019/10/10/pytorch-%E7%B4%A2%E5%BC%95-%E5%88%87%E7%89%87/</url>
      
        <content type="html"><![CDATA[<h2 id="给出：四维张量，三维的索引。根据索引得到张量中对应的数据"><a href="#给出：四维张量，三维的索引。根据索引得到张量中对应的数据" class="headerlink" title="给出：四维张量，三维的索引。根据索引得到张量中对应的数据"></a>给出：四维张量，三维的索引。根据索引得到张量中对应的数据</h2><ul><li><p>四维张量，object_feats.shape = [bs, 28, 5, 1024]。一个video中截取 28帧，每帧提取5个object, 其特征向量维度为1024</p></li><li><p>三维索引，traj_idx.shape = [bs, 28, 5]。以第一帧 frame上的 5个object 作为anchor，找到以该anchor 作为开头的轨迹（即，在其余帧上的对应的objects的索引），</p></li><li><p>目标：由索引，提取对应的 object feature</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">traj_feats = object_feats.gather(<span class="number">2</span>, traj_idx.unsqueeze(<span class="number">3</span>).expand_as(object_feats))</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用 objects position 作为特征的论文</title>
      <link href="/2019/10/10/%E4%BD%BF%E7%94%A8-objects-position-%E4%BD%9C%E4%B8%BA%E7%89%B9%E5%BE%81%E7%9A%84%E8%AE%BA%E6%96%87/"/>
      <url>/2019/10/10/%E4%BD%BF%E7%94%A8-objects-position-%E4%BD%9C%E4%B8%BA%E7%89%B9%E5%BE%81%E7%9A%84%E8%AE%BA%E6%96%87/</url>
      
        <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">U2FsdGVkX1/BL63FS7u6Mnaa91muuTfk/28t4y6E1Fd60mjTkZ7gKn90JptLlKgRsni8QXCnTjbJi0tLArOq5F73YSBCrhMysd+PWIEgzWydMofi5nS8kZR8RHDVDKDeypOxJQRlSoMWedDyJR8zmQlgMm8Dn8YMshPPF4mL9k5DBhyVkjbd/xu6SCmT1WeF0tC2TrY9jX4/cqLbSGQWnl4dzYzPs3n8tdzc6623mTxS/4FLf+sb/+p9I4H4J9IEITrzAm9uYQYi41EMDAW4UFl5A7MNQ04/IuMxRg3PukMEIBgfYdcgxTAl8mQ87wzSjTmgj3U/If9oLk8o6Lak0R+/fvE0PN0Utt9L0BftX+3yqaYRPtLwFjx2ZYhTfMPpmcf+0Ln7952LCPZPKvYyxzfnZ9U4xXHhN44wkSFrD593G8VK2IrcW3zelLeehWh0u90+CyX55stqAR0/AK55+k5M+hVdw/GoQjOPbVFFujSnz8WrCGYToCEdNT79L9CpAPs3eT1cRcHx0Swku72y6DZlBO8y2UxdifqRQaOSpPUn0Pi886uIqEew3VJS6wDwb/vs6OCcZQ0s7RvtzpV+b+mGWoJWu6/NUnuR2xbbMieOIgHQCkG+U7kxT1qI5lhL3p12RpvfEK0J+CLLJ5HK7YkxoAtiPGDQ+eqFseV+ltrw3QYHG7/AnfDoQhYbYUJcYr6hky/tGWpLyYvuMAAp75su6FTqS+Lt7U+YrIM3Xk3GAOcA5I9xrHaz9Ko1MtcLLWl+nZKXEWQmQDieHTCsTZNdFGaWzTrNgLTbvdHjD30S5QDsx25ljGEoGNHzbOYzFdiidZbu8VIzaBBYcE88Ug+ULavlYCZiSSwMbMo2Q9dyw0ChT/vjfAEjkk71sj15PEYy6Q8oJ4T5so+A5+/jbCHDZ+EHVJB4s9Gx1TK/CdbhCAe7gEjoUj8tZgFZWc3PMXoPb1prOBJX2H4Q0/jwwgfbAXiLlYap7aNlA3bobsIK7TG3BT7VcFyo8wOaChMHtvOe24jbcKKyZOSFg3xev6nlK9wlOA9S557coDGUK0mCrO23RKvftK3w9qldEVIxDgbAlnx3g5JYUHoD/bGSBS2UgDfsSwhPek9+/0Lrm5fhAozrZAtgXIFuZENNqaN+kZDkJK5DhIGAg2y4HF8UbUXu1ophfNTkPkN7qwpz303fWwKwj/faYPyz+DwGG3oqW/w41kpxGdMjWZTiu2y1l6L/UMJcDaaqX4YzHc4sapPQ6BCOfS5J3j1ljQsCsS/LsDG5cO9r53ppwoYpZzYZaTk275hoslQzQzoQdnKmXu1Vlo8duGeL8KnZQFUN3CROBhCiR4TOzcEWddjos4V1LU891BmqVlOPYlimY4pOm9iPWKmGL2aitrDXtVICe6I4iZs4IQm1S90AeMYLvykpvtQyOGkEFqyPJesiTw4SB8ZQGI6R5b1g9ELtJinoaTY0++fzqGno+a+MvkEz3zbpNt6lGfrZwTfxm+/sillAt6/geRHzJfaa2h2KHePpGzz3DXZ8HU/DkWzCJ/9BpRVgCo21OkVMSp/BPznMe4khOoaE9YcQSiS5o8VGz9YUy44YncS1HfJm6UuXDlahMV6DBFJVJkeXx+C3bnrPeyH3ptMsqx6tZYDJAKUTjo/LdOkSp+X+KBgglaGidqxOIwhVbBYI32GfJIvIbDRCYkgWUrtEak/BASUyErijqlRP/0F6GW+RpNK21ZmFqxZJrCXjOAoyKGcCNbCcwTnZ/CDBZAAIiKautoaw+AVr0c7+Y0Ht0JEG1vnN84IFF5CJuSCtft/FPNullm64XN/F61pjKQl2RghtpY4cHuMZmowZ9qZuxF3nkGvF39W7tlKzOL6hsWrEYkjTrq/SqcQRAvL8JQHBKWMBk2o4ImR41LttZx5Ah7lbllpU9nE/Wr8QQkDrl1t9czhpYc+a9zZCaGgZpuwDqdX+MjcnzRym08FVqoUxJFDiSLaf4z96RppWD+N12NXSX/t6ZSa15FZWB/eXv+jEk7c+CO9TqfUnbZCBgdB8CjsnT0gSPVTRosuWnw667nKxrZQlefPcuiSfyAP8Gu2t06g81AaT+50FWaXc47RSdupIFjjt3jb3LM8cfmgOQUFThDXGU4Ql+5oj7eaelwORP+Z+QyFFtvlQeEOTJsWnbr1MkMuks3xuE4c+wHIkVyxubHD3AnVuxyQliweyWZtRrE5BQeNLCAZHc7Ogva3wCHUISyvOsZ2BdGSCxwB4QUf27K+EmPBmemggzQshdt+xe7QqTsTA+5Ypz2ZQLkI7LpBzNMi3qaFHYC+OtrETKg6zWpoS8kzeNepF0h2BjJQkRITlDEP65RFl4gv/YStcJFmK7aeIGxT7jc+nkR2Jpz3m+De2DSjIcUiwdtwXjlJZwqWZe4YWCROVfzH+3lirl/eYY3vagFwBk5jVvLrO7vHsBMMsfipFr9vJ/KkWNTGjrEho2bCl1vZFsl2u2svwBYqW4ksHWPgbTx0HBBTnoTM4v2L44WjiQMfg/w9q9LFzW/y0h/B+GN24i83MHUcdLE6i7yQ1D0SakPxzTN0MxIpUCKgYXGAfTh9Vp1/eluPkcxs5dngVM9sWrfa4lYcSLqfpS7BTZNbdfFYTjw3VooBhJjHcr2BSfGMnJc476QMvpDAU82KVFWLeqp46kAPSyGe/z3Y5QnQvvSsOPW1HF8UTyC9l5RYnNDXa8cJcjC/McGfhEORHih7Kk22rQMSx4rIeMRVPg4Lf+vyznO9VHSRCnuCR0vexNWoWRshstJxc3tQVo6WLwj5ggdlQK1m1wBlTa8VtAaGahvoz+UgWbnojAgDlHo9k4IH8HTNosyOzGvo4oxrkYMEsP4/iVxKVfjEOTGQAVa2PkZ4uATSXIHRx3bUIE95fggXy4Mow7zIe6RK53SX9CEPEmEA6QcNdQGZ8xVEpDerFdIDvjGpJFYLBvTBRcjsKc5QGsbMd5CgjH99GTavj6NUYfqvVliUy87aiUl0VLcLlHfyzexmrjUwx8I/zThG7uncd+VNkW3t8oXmOoO3CGK2KF1NPmOjM4cHexZH6InmOe+yX/Ew98huiTLQGIwrNE7D2uXO3A/5IrFngW0e/dG55KV/ch506IAqyxldaNJLbtjU6rg95pL3ZvHnUbXHzjkQ7Cg3mudLTD7Nr3U+RYNZAoc/ZXUQ0gu2SiuNXfDFqNTIaPUtB6/AtReC6lJNJMIxVHjnEIXaVPLV/YZS8EfiBDWVRDBUc3elIXfe3KU8tkN0uG7pqpVnoh3X8udVMbPAPYKH/mDj2oTFJ2eyFMvd+kdHeHUX2iyvA18nsurv6o216vewq/3f9ZF/IAfjPaftn6YalbfKPw6OSqnMYTS/lNHb6OB/K5eGzhOhP+ScwoyDz7/ntMUvmmIhPZO2UUZsWsdGbiJPl4bQURmHu9JBlecqkuPK0OI6KZqVYdcTKXhusgXd26kEQhapRTyQVYai++nNelb745c+7UcYdACDtUhQofz2gefAFHAlk7Pj/xuE/EZpCpOYNGj0Mu64c3lSsF/7HErHA1narbFvk0A0WUxntODz4B+2aSz8foFCQ+oIoXSFl//8aYDTGnv5qinsO65MrWSSMvv32vdemlkOJf6EZ7RUFORJCff9g6mG6iz3N0UIXbm9s3VAWGz2ebXr5mZFlAA0Ze7PU9bOtoWfSbKGDqS+4+eTIJm9Y13JQ+9ccSQokBcXIsTnV18LqO3CkmkBxBsN/3JJspvZZ7m8winunE/mkbd9yRmdySksFJg37Ub7KxzDpUFKk7Ng2Yq/eTMUFWkChZR6wszJyJMtluWRxD0dlAEalz4W9BvOactVqdZxjAoHw39eJ8TXAti8LNBOqLLKpdTOPWoEhlcBUHrbFZ5QW/6DtDuohUxRzTI188LocWKX5WC7TOHe107j6FpDgKzjL77PwCuBclO5mM4zy+u45mFhP8+6VIA/hHDfqfqBXOXoTFEp4GAeaMgm9VNXVL49ksloOrh0nuaXEzaFHKsVX/ubCRuG/BIFaHpjtdSMFVJDnjv9KkuivXx+eoD2XyGLYRRgPNXqlwc522DfOxpAz0AEI/1GDV3+BmtwAxcbvqvoWX/AR8qN4Kcwcmtl3vGLAG7D60F6Ws0xAEj4ATneJ81QYvOmRt62EQCd6xIfClKrWnfHOj9z5FF54AjM3PHjxr671RfDOApOk67iqjBmx8AtlE0iTddSfFXMhBe5jXIagEyvfXpBZMEsJh+0ScseD6SWdbU33tyDqQvmS+Xfop7avJnM95mcuGCEy7QeP++yS+9QAO9xWSkh2HJFChcyB5zOlwGpG0ce19i70z9p6EuAEOQIQNrPJI0Xx9V7taHsVZzGiw3V2IX3ndjlkPGKbYH47QEMf39VC4ALbPoHjDAuruhNSP/9rOgB4pVwA5TZwrMr6IcvMfMlApYvYuVkHi/Vn+SDhJBrOMXkaZ/gmcJ6M4mzypN4MOOnN6flBdhTm7POHqhIIX3X5pnhfsXKyRoXlv2cz5+AFJKo73ITQb02lwIpDi3jtbfVfqK5mj3ZmkgCQTPYdpFgGWAgjrUepDsqoXWWQGuDnm/9AUksnmIvCmHXjU2Gf9wa19XGb8ZJbyBWcW7u0ZLf6XAFvDS1fzJds7s44GmvXDpZBy2AGdvCcVrNoY3P9ZTIQS+lL/MUx9wYONejT2/lZ4VcE8Nbr+ZbUBARE5ZCQLAjlEFleU/1K/68fFjO0YiY2SnyRMRpoDW1qblaEIFGEaPsocvkP8Omoy6YfdfIwC5lj4XWixmkkdolWDb5RC+wU2DlQ1m8dB95BzaOh8UjNFsHCS32yuicwu5y/IztciNfwQ6O6WXDTBf4IZJIyBRhSuCV+aZ8r1psvACrzbQ05/ZYw1hyNTn9jcy3ugbZacjcNl0BdlUWb6CnUqG6WQ/ifDoyNoCKqZcD4SrKfXi6cNUpZngThQvJNMZeoJjLJdSimrMPE1wGvaKXSPYbLxAaud6ZYAuA53EyfJbnRnwpWbwyNe/VCVBtuLFqBiWIEddhZfuldjTCwE2yFt69PG2e2yhXuRvuDDPHHsQj96xgVcX1nVRHMZLeYXFkaIXszxpA5v8dmgr/cmvPg1iefiEFu/N55Rh/ODNNOwBugSuSCMlLfdJ96vDVZN9Em3AtauV2+UPmpznxvtz2Gk1qfGpWf8F46bKzA9K8+4tcK0MMvHIqg4S5FzmUI+dAXIRh/x7BK2xil5kxUva8oaLT+nSH5FbPm583NEaoFs0+99lq42Ve2t7UHubGhb6NlUPepFN+QJ49An3om9/hcu6Ozfd+CKtvIIQqCP2mliJRCzVUGvUvfrARVe+ZCaUNXHoMvwI0pUBlJz+9qwHBHHSsQqGnM6B34FxJil8sDseRMxi+aEM9WPdPUwBaoUXnFFd5Xhc/Zf3KT+Dbzr/OyDYvJJU9wyQgYP6LEJ1dy4mQ2u1lj26ZeHphrK00lt+7OOT6qmy5P7TVrz+bffBd1Ll1jUceA3AHQdNFdx5G0f0FINjCvXpMJiGAA+/801i64F5K1PYsuTtqgov8Cntpxmj9jIxASzl5S6F9cOy+XOnq3Ho/UDcxmbS3+EKtPN49IjW7WkycOQi3WugLAeiZ5oNSIFsjXg8rreVBnfk5BU6xRQJmQQEnkxFp9jITYXmJE1uu+/Ele2zq6ZyMPw9JUxCBhCvpEwYEltDEjnpUSpuv48MfMj43sk2ozfDTZR2zZAyRvazy3HFYKyJLvWWDTE6j2RFOyJyDaxZ+iPrIgpqx/GJK1WvrnGc5XEtEi6VUcwdkr/Z3Gm9bobbq4yLGjUscWn5mlMRnbyn8O80Gw+l3dA2ncTI925jprRKKhSB8z+PiQLaPMAAtIpH/Li+J7OmSynvkaoUFU1Eoq9lwFqlRHYymBSiWd+/guOsyEqc22QOLpfSWvO6ODxn7253xlZBS0JSZDDPdb3LThZDCRB65hVek/C6ptT6ZyIxfRD9Ek7EZKuVEnqkxfoGsYM9xuV8T7WhiYflkXrmgjvTJET5rsrK944cTE5ZxFCc53Ux7Rua8Aav4d+BBWUDM36LSGFSbCdJ+Ob7TeMtVk4zryumtK8yx/kJGqYcmRDnp2Dc1gn7XfB4jJQwsYOs1sWPjhky8Tzuj84isyf6HAjK/k/6oNy8+THTGPpwWG4pTY91fFmpo8vDBaUNBV5/fctbVG0p4atBC3kWW4OZTbSdyecK2Uwj+6pLtuRHr2W9LTUXkk96rUcMgHkWkJUuL/UZDDI9+ezElyXCheZSkwTDOXYfNkvYfwBezxGTwx0hC6pkPrVt7gfd0H2h4k7AzW6nzQscZ2cF4oMlojOvmp0G9gHieCsAmUUc1KNJY8wQvn1Ick6PKt5ogwNUvPZNukzrDNSxDO8/pV8veGs38no4jSsxD8IiQFjz6BUSt4Elxfm5E96IvXrNX5GZA/dGPq8tSxOqILLHj1ACOe/aCEiWj0McIIja08prKvz/+iJCVq3qEL+7LIHpREMj2o8aBiIHxNMAQv8VRWAFwie7TajuWIkCzfr5jRCf67Q8ocnShiSUKd1QDbFmU/AzP4z7PCIqnHCxJbxqbmfHIUP8vLW7SOsKpdlKUBBhgNcgX/3FUQbr4iyRA1rO05RYsZ6gCJm6hO0Vl/CueKZp2zJK+kweNQUn/fIwtHQMqdMQwpqt9xYUiu29ydr3bdEDooCv0uBrZKm51FeofX16lAefga0B2yw7CKEVH2f0CJuVK+3RPI7DkeswZZWqxlwNzjUORtVayNTviGAuzWbhJzXimtEXE+44ruQ0sTXZwtE1Jrl3mX+rYDdWs8Sq6xk09S3GsurP6kHyS7MoYt0E343FVKaZ51G02NuhLSNbYPc7EXbU46sQBt5327zAEXD5s9gbj8Gf/Iljjbm5Y13uHiSkX15WjmyOnPgrahOSMwpRXT4qD5MFkQQVZY6vQAlZ6TjWKGk/A/G329CrHfgIXZ9YYE1f5bE2ZmeFnz1Ul8qwl7mIPWOgDai6z4GbNYuYryAVzB9+VlvkKpgUVW8F9y73nHRXbAA+QWrSoPB3zcpWCJmjKv7RgrU74L/Z/oCK7D0zqSpZh4+/8AXbJcQrbAwCSL1tf4vHpRMWuN1UaR75P64NEkOR7Uo10Qq/DsdVFM2OBcMydXNSgUtJX+x44ZAeWgfGqgVfjXamp8e1R5+LtXui1Q9tpN8I46mB6O5fmxGeOoq/T9XbsG22Ss+3T/DkAIAJg1+cQEwlOLiS+GLgZCvEANeblHX+3n8FNHHfConZ9C2GT06WhJD+m75l7hTqLdXBt0MU3fRKi0cHaET2LcmDMKtb6wdUlQJXUEwGefzXpf9n4vUNQ2b9M72YLRZbvO8pBX0dd9/mvSVNUKozAjIp1IxK2/Fnp/WzVjgowNuqp0ovMjpHollgiRDASJl83zOrirqXfO7L6L7s2my+Y3R6gUf0Z9UfWmmt0tPywxo4x1Pwa2zSAF2oP5wew4Uovg16kCDxdd86KPq+cLrw3cA+O619K0+ayTYrTa0WxMW/B/c9iPcPHo55rUdOjRbBrqspWPvaHUH3lF2TqXPdKWqJDDfa9USB4olDsykE+atAkMn2CKOm+J4sL65d2knu5tR4wJ9apSX2WMYdhzIo5fPowe7Ue8HiUhk6kO2/WHYLrnxqYoGB1QTlteVSEWbTDVhopvbmZqBw/svXOyash9fUfMxtUepnYyKlBr763kb6kNFmfF2jXKuu/5NrO3eZFppXv3vk7HYiUjBmp2VLcjC8YjU3W1mN0kC2EM7JJa5ASNZuvFZYUo92tNrIt4UrjRwUowSD/oNN3k0tSZYNz4DDgOlEHxazCRdWJ0716Q6FB4gvBExpiZkbHCpvlVgMCJddpX2uPGsA9q0jNSDeiFpL6C0kffYUgb0BefVjiYJXuf7dFj7AxOLPDPFSTzhsMkiaMhZQvXfHjMWbJ5SBDFnzLfJhOazSzZbJml3r0nj6JNFkF9/iSgPUIPBnKemnrGvdoBrh5JWFAQjkZ6gzUI5earSG/D+TzvzFAQN17Bh1F6S6EqLqadLYQNnj7bj6QW+nJH+A4jmaUynzMvMjRLap5GmVGXcY/94oEWF1Gt/7bXvikFePSMJv0ywPFFmhYodiAIbfu5bg/2ZUBh7JKz2ttbBuZoW+V9yiUWbJzWuTR8EQDZvxCt/j8Jh0IerUb2Y3sxm7dV/exAsjlmSCa19j3LMgAcU7rShsp08sbQOAGeQi2B/wOixrYurFhqWw0hLllkwaOMYLpmiftztoWsgfoZ4VEW3dFjFUzjMtlbMDwy8X9zH/MaMEMJH4jYQ8uP0YR/rR5C2ieWR7sOzyjlBt5icDok783geZyHlNm+ThNsfOj/6YcwpVt8p9CLvGU2mqSHr+ODVU/ID9e5i2HQk1ylA4zplO5jV57rBQE4Zj9RA6zVdCqaGTt3cunoe9LUs3eALpoAEmKxY8fbP66PPSxDgPJcb8bhb5qgcnw1zvLZfXngt33SRZfkHOvnKoAg30LKJW1j7Dpcf6BVCWdwUE3gpSbuG0Q6wW5OJY1B9TMLkdpVe86y8LteMFX12sO4lFNDzhr0FNcc/tRIZEK0aM7z47kddYhsGQ1CeEHf7x/SJxr0BUgf5dnhVrht5jQiqxyHccdoqhQ7XH099JJ9+cmzZ3ul9D232U9pN1sO8YPP5sVbogM3b0W2Ee9SdXKcNW+c3aMGosMcXjwpdQR8BVIZbogt4j53eZ9m0CR4LLXCN3PbX6aD/K/CcrWPMZBGnpL34IIEE2OfKxYA8P7Aq62w6tCinSh+FZi82HXRS85LsvFqoxzrOaAoChqXKF7H9iL/R1t+Om6kyjlazP5Oj+wXSu4b27fT/Jkdvo+npV5o2PWIjeZX8AFCs4tDaz3Zv9Ec/WiMOrUl+9UHUUhyMSYSCpAnEnhSuta5srLuvJcMxQMm3Et19E89bTq6fz9yQ/IfKE5fca2vV/NiOWZkoxHRKTcBPQInMcnTUumrvGK/KMDoRuxLBwwf+0e6nIWdiJgWAwGZVhnbPIhMh4UcK6Lx/NHG/8AHlsNDwEJcM0rk0YKjeRFik3fSX6I5JeRItj0sjruGvH0EVzIYLaoc0Y2xmIdoCFtRZkglUbG/Rp5K+5bt7d1pC5cl/i8MY3wjiLdpKDmzUrAL9whMt1RntbYz8gwAUNc6ZcDLpqarTeZP6fnJwFXoEwCSDKjQJNqBzjNExoUGXJ1DzIZc5GHh0zCHadFdyfy1QmG7PLoqG3Xit8/hbCyX+TysXW1uzT4e/EojHAeR5JComQELuvThBNPhjzrCDa3EFtMDuXcxMiV2CbjQ1aTm9W1/ygf9FYlUlg2WrIkL4dIXx3KS44EZZeJnaHJZjodt1TZMxOk40Az0sZ1rfrTBfU+3sdNRl2nrCKwLYJC/Agc+enoRoFZkwFtsJCBjElFgYIdLz70lnTvmPTUmet475kVtdsEEVYkC3v0Jeb5bQ9fWrP0/YFkfvvw2gps2dZyebJwf6ANvsHDJJxt8nYkJo4/rR9A3kDTr49Wc2OaRT7cqcqbkjtsDrUA1Xi9dnLkF7QtytHIW1h8qnwaYVGG7yxIMYBWU/MXgnmKMz8EliSrQC/OrmMarr56ktXGqmzoGkEsM0MRvUQWRCKk1pY1mhT2ZKmlYD9JlG531CKLttvzkGOMNJUYp7YJ8cbvSidIRpds0wt5Pv7rTkqgmTIrh8cvNSn7c92ZGv9knO92IO+hBYF212ehiQG3rLtDWJeLELb8KFIaeVrYMgga+hNPpQoj2UKBt834MGkPs/fiMX4xDq62WHRigURDw6njNxH6SWe0wgK3+IXuG+yrkPPqPXJ+JB2kth1UHBLqsZoG8zZcovrjBHSzhoVnLz71rK+ISEtZ5lcnDasgfffOwIeZuYm222sHphj7s681mdcKAQZu8TfYQD2GyGT0pEyU5PjGAzoH48XgOttipklhCOqg75ilu4Btzq2SxylM5cp/41BhRVC7zDAObZgKYTDn8y7Ve3811mouvwpOzG21dTSTDRPTJ4M7b4UD1GM5JnesqqxEn3deCExsZFrOwzFCQC4g7Q928OIl7PeP2wIsRLddNotjQTnb+GuhjdQBZj8pfHWjE4JVu0d/QTcJm1NO9OqZxPiRQHILEq7RcHEdZCMFJ6snOkIJpkRGQ7yvovp8sKEAB15gQDyHgZ1704apwQRRHaKyDfUd5KWS1zsKWXCeAE7uvLHYS0oWDq0GmjpNy6eY3ZejYQn+zc/1EGPhc8nRsWIOyR4n+xlyOLp8I/V5OWQ5VNrL/7ForISMnSZ0WrwZJdUqkLZ2LnvsLircNNsu01F9LKrhIb2sT1uTPuaTdwWgx/h9vCEmczkZfPTwy1niPFxyqrdZJyxm4dYwymoctqqhDdoBvRLY/tZ2Wik5r3rsSN4MxsoL3DYQZjnT1FStgGCkn/NOLWj/F8Yj0js3GLa4gzlC6gdipE9B8PL0RzIdFjJXuRua0plIRfkUHZSdXWz1CL1yoKPFASqNEZOXzOaiMgg99TinStbE67BqkJ95tvTfstdz1Jg1Urtdn/0Mupv0SD2ite9JXTa1wGTVimOVZAN7JfmJ65aWinejTx4tm8Zqa5cTZiX9SAQzXlCx4QAriX2HsTDYRptqq18BIp+jOzERZR4HjpLPJRj3m55brkNL8qCCuGftpFvRNKoo4JKb8n/MX5OofOGs0f8mijHknMO/Y2R9ndqpYJZ4UXvV9PcAFj4KlI5CeUE+m3g49nYehKPOFOmxdjzyHg2GC8C58u3F1GhtjkPaXEZcYxN/ERIW/gpJtsutSzpl911e9i3cnSXXqAVDQG9G8av4i8rZA8Q0Dl7wo4jfpfdSy/JIEEfyNKtIbK5tlaeqd0ma7dSTS4/nE2JgIbjVJ6I1px5G2pA7ZIeyjYkAjBsZm5nv8gZ5RJMarUn/igHnzo5Kg9j2/2RzFVIr23QjNHQPC6XUsdoInhioUJ2mC25atBRSS5UrQsXKYE8Pas5bVSNTzsDH0YoqyneOTjPSaeF+l5pnSgxhMx51WGh7TtTW94ZC+G6GNkKszW+Bn/GUIAJRSA3tRZZKlMrgoYlGqHkLHn9C7kV6k5ScL6s4YDXeQmAsJ60QNPATqYROYE9qdQ7SDKQAowPRUZ9oXzR7h0WsH6YN1yBNdIpzhySulGiX6JKNH6RjzZupW9UJO+XbbqTutJfe1rmfiDjweEav82lj000f2QZsBCSJxWE1+SmcLPIjz3dFsVyOpaEqAYc3btTX9+CIwVCht3SP8EENCKYxnkS15BnaX/rgeeTRWPY1T0huGeW9W3STNfOD9LDZ/Pdc1B1v21h6SCqpZdGIIweC/T44MyPd99IPhoJcNe01d6Ily/4SJ5/lodXmjztATKIuNV+BMlWzMK54Y/NyBRajfGAQBqdrlggFFYMJTbU7Dr3BMqsgac9Hv1/vsM0EiOnF+O8fwlUowDvk2pDYSd9ocIuJGYuQV3XrQSvGI9c0+Gpwm4vCqiMyK/p05Dqh2xxqxKw9MWdSgEXdXEoQ64Q3LyxvSH6kG4chByWWXSXNFEmtsynbuzB1wd6fPkU1R1e33jtvI5q9r4ULmiCRseqYpJ3PuZq/JK03vFcGiJgJtLnqF97GBUG3HJCSwfT6xbsizxvXvnQXr5A72v4uuUhX2+Dc9Cj84NUJPGxM7rvltttvb40ntIKWhQZziMNJueF2HbbtPcuKDpJKU4GhXlO6/6HQULUzvI0z/X+Ou7wF/8KHOd9tiAlKntySF0slwvasBjPqx4dj2qXBnc7gJYmlDLjBM6PO+5fz00PeCjqBGoXI5i8Uda7wb22UI+wWg1B1R5LqkhISaIElNGIsW3gQpPhAs30gv5fXKkzFhMJ14JhRmKze4pNApJWHXGfUAiX90FLuMisV4vNuieDPrRKL20Ly4OnpcWzXmSmwo93JvmTmwVqmAWDX89iYeCN31Tu0GVg+/cExVw4bHaiyPgDShp/4+nwMHfvu6DqmmojTMuPG+qIRIPE8HYuuBvlrvcFtoDCru8lwjSaWFn61eh0guCnc1TaYSTWNWMw5fEkGUGMboS2RH1V4LeS8v6QiXHTc3Zl0gTTs+iozmrdkpujikcxaesKMTkiIMXcgzNccYQKG8YrUe+8NyD1QC+gEP2lN9Om3KqpEr0k2j27cRrBa1hHSaKUwkI0zjNXvlRjNWVA6Z+4LGvjrpjQH1iMH6FV4vkov4BEzlcQd4vGU4e1h32jKQ/O4G3yBHUtSBDhf6LRbQPnWH7jITw8xxwHMYvVND356nwspEQXnRG+ibexJKQwv6cVIauX/tQqrXdWLW9tjoplhQ/i5n5U6urDxrMUi/JJsJgB34yJNS2V0GWJIZ5f3JqmiqADzKnXmKkf/mMbntNNNnUi7WensvX2xGgpst0qaao7eC9kFbcroaW6TD9ZhIDe0XwYAhY/VTmWBCujH0W7StHBkYfEUTfLPEoc7nUQF1PpuKQmisfEPR3DPIx9nGZHmvnw/TzhZRtyKWsbC0soVICYUniMC/7t7cqrbf2rayqjyYXRqKkPs9CbaqxhlL79PBBZgW2nAQIxiAhP3Vcj5vUwWuLDpZfxBfbtbk6zn7ArMTwbtvCl/oULxa9k3we8mp4Teel0WxWgfTMCR73mePT5OhLhkxqOitwN3IGGS22Ar6OJVQHP61Cpp+JP0W5Km61qc6HnGZBNcWGudKpzcr9aZgEZMmJy5XPKTpQy4PkQ4LcxjO/RmzUh6vJ+41eRvp7jQZYQC6MB0BHv0Edgbi3PWZxtn7nyLPUfeNoAM7/W7X+kSLlhS3jyK4FNt3ADDLhtDASDybST08qHPrTVvTqosnOMcSL4t+n4kgRRO2dk1dAibYU+V8gIWWs2lV/1e0czeJuAkXqee/fgcdMqAgAOsaJfPcQ/Ih/5Wb8GDn1zfTFdvbHtxxs7g9UcRMZxP9AQeNAS+xhBvglZEazX5spy9XOFt6LhWiZ2CEilOWPvQ=</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对分类损失的优化</title>
      <link href="/2019/09/23/%E5%AF%B9%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%98%E5%8C%96/"/>
      <url>/2019/09/23/%E5%AF%B9%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>视频描述评价指标的分析</title>
      <link href="/2019/09/21/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E7%9A%84%E5%88%86%E6%9E%90/"/>
      <url>/2019/09/21/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E7%9A%84%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Motifs: Scene Graph Parsing with Global Context</title>
      <link href="/2019/09/18/Neural-Motifs-Scene-Graph-Parsing-with-Global-Context/"/>
      <url>/2019/09/18/Neural-Motifs-Scene-Graph-Parsing-with-Global-Context/</url>
      
        <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>给出了一个对 repeated structures 的量化分析</li><li>分析显示，（1）由object label对预测 relation label是有效的，但是反之却不成立。（2）在一个image graph中会出现重复的模板（eg：大象有耳朵，大象有鼻子，  XX has YY）</li></ul><h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><ul><li>给定 object feature 和 object label 来去预测 relation label</li></ul>]]></content>
      
      
      <categories>
          
          <category> 场景图解析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 场景图解析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>video captioning 任务的难点？</title>
      <link href="/2019/09/07/video-captioning-%E4%BB%BB%E5%8A%A1%E7%9A%84%E9%9A%BE%E7%82%B9%EF%BC%9F/"/>
      <url>/2019/09/07/video-captioning-%E4%BB%BB%E5%8A%A1%E7%9A%84%E9%9A%BE%E7%82%B9%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>视频描述任务中用到objects的论文总结</title>
      <link href="/2019/09/01/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%94%A8%E5%88%B0objects%E7%9A%84%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/"/>
      <url>/2019/09/01/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%94%A8%E5%88%B0objects%E7%9A%84%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<ul><li><p>CVPR 2018</p><ol><li>Fine-grained Video Captioning for Sports Narrative</li></ol></li><li><p>CVPR 2019</p><ol><li>Grounded Video Description</li><li>Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning</li><li>Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning 【再去读一遍】</li><li>Adversarial Inference for Multi-Sentence Video Description</li></ol></li><li><p>ACM 2019</p><ol><li>Hierarchical Global-Local Temporal Modeling for Video Captioning</li></ol></li></ul><h3 id="Grounded-Video-Description"><a href="#Grounded-Video-Description" class="headerlink" title="Grounded Video Description"></a>Grounded Video Description</h3><p><img src="https://i.loli.net/2019/09/02/Hvtk4BJVNQ2WwdM.png" alt="搜狗截图20190902104324.png"></p><ol><li><p>如何使用region feature？</p><p> 仅在language lstm 用到了 region featrue, attention 加权求和之后 与 cat[ fc, motion] features 对应元素相加（cat[fc, motion]也是在经过attention加权求和之后的）</p><p> 但我个人认为对应元素相加，并没有道理，相当于在 cat[ fc, motion] 的基础上增加了一个 bias，没有什么道理</p></li><li><p>region feature 的构成？</p><p>R：是 object detector  在 fc6 输出的 feature</p><p>Ms(R)：是 object detector 在 fc7 输出的feature (这里有一些细节的修改，具体见论文)</p><p>Ml： 是 position embedding</p></li></ol><p><img src="https://i.loli.net/2019/09/02/y4JkxlmLQpqaj5c.png" alt="搜狗截图20190902105022.png"></p><h3 id="Object-aware-Aggregation-with-Bidirectional-Temporal-Graph-for-Video-Captioning"><a href="#Object-aware-Aggregation-with-Bidirectional-Temporal-Graph-for-Video-Captioning" class="headerlink" title="Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning"></a>Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning</h3><p><img src="https://i.loli.net/2019/09/02/TIP7Ww3FnLNKzvu.png" alt="搜狗截图20190902144125.png"></p><ol><li><p>简要介绍本文的结构</p><p> 在encoder 部分，使用 object feature 和 frame feature，分别经过设计的VALD 得到更新的特征向量</p><p> 在 decoder 部分，对object feature 使用两层的attention, 先对 <strong>一个轨迹</strong>上的objects 进行attention 的加权求和，再对N different objects instances进行 attention 的加权求和，这样就可以得到对所有objects 的聚合表达</p><p> 轨迹：对于第一帧的ojects, 根据相似性分别去找其他帧与其对应的objects，而构成的时域轨迹。</p><p>  这里采用了前向轨迹，和后向轨迹两种，在decoder 输出预测的单词之后，进行融合。</p></li><li><p>如何使用region feature？<br> 仅有一个lstm ，在输入lstm前对objects features进行两层attention 加权求和后，与同样经过attention的frames feature进行加和（sum）。<br> 本文没有使用 motion feeture</p></li><li><p>region feature 的构成？<br> 非常简单，只有 appearance feature，但是经过了 obejct VLAD module！</p></li><li><p><font color="#0099ff" size="5" face="黑体">object feature 的 hierarchical attention 值得借鉴呢！<br>计算object 相似性的部分也不错</font></p></li></ol><h3 id="Spatio-Temporal-Dynamics-and-Semantic-Attribute-Enriched-Visual-Encoding-for-Video-Captioning"><a href="#Spatio-Temporal-Dynamics-and-Semantic-Attribute-Enriched-Visual-Encoding-for-Video-Captioning" class="headerlink" title="Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning"></a>Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning</h3><p><img src="https://i.loli.net/2019/09/02/T5AzpW8DHkVL2Oy.png" alt="搜狗截图20190902152617.png"></p><ul><li>此文没有太看懂</li></ul><ol><li><p>如何使用region feature？</p><p> 得到 obejcts sematics embeddding 一起其他三个信息，经过聚合之后得到特征向量v，再经过一个线性变换得到v，再送入decoder中</p></li><li><p>region feature 的构成？</p><p> 由 object detector 输出的特征，以及其他输出（objetcs 存现的频率、概率），来构建semantics</p></li></ol><h3 id="Hierarchical-Global-Local-Temporal-Modeling-for-Video-Captioning"><a href="#Hierarchical-Global-Local-Temporal-Modeling-for-Video-Captioning" class="headerlink" title="Hierarchical Global-Local Temporal Modeling for Video Captioning"></a>Hierarchical Global-Local Temporal Modeling for Video Captioning</h3><p><img src="https://i.loli.net/2019/09/02/m5xLQnzCJGsjWVc.png" alt="搜狗截图20190902161552.png"></p><ol><li><p>如何使用region features ?</p><p> encoder 部分由两层LSTM，第一层LSTM 构建 frames features 和 c3d features的 隐层状态，并送入第二层LSTM，</p><p>在第二层LSTM 的每一个step, 都对该step 对应帧上的 objetcs进行attention 加权求和，并送入LSTM中，得到该帧的objects 的聚合特征的隐层状态   </p><p> <img src="https://i.loli.net/2019/09/02/q6XNP8iSVzekyCE.png" alt="搜狗截图20190902165813.png"></p></li><li><p>region feature 的构成？</p><p>   每帧 objects features 的加权求和，再经过LSTM得到隐层状态</p></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>当前基于 objects feature 的论文，decoder 部分没有太大的新颖（一般都是Top-Down或者是 Soft-Attention），主要的新颖的地方是在 encoder 部分</li><li>encoder部分有的使用LSTM 以及attention 来更新 objects features；有的使用VLAD 来构建 行为特征，使用 objects 的时域轨迹和两层attention 来聚合特征；使用objetcs 的其他信息，比如 position 以及 label 等信息</li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>目标检测模型中的性能评估——MAP(Mean Average Precision))</title>
      <link href="/2019/08/31/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E2%80%94%E2%80%94MAP-Mean-Average-Precision/"/>
      <url>/2019/08/31/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E2%80%94%E2%80%94MAP-Mean-Average-Precision/</url>
      
        <content type="html"><![CDATA[<p>参考：<a href="https://blog.csdn.net/katherine_hsr/article/details/79266880" target="_blank" rel="noopener">https://blog.csdn.net/katherine_hsr/article/details/79266880</a></p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多标签图像分类任务的评价方法-mAP</title>
      <link href="/2019/08/31/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95-mAP/"/>
      <url>/2019/08/31/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95-mAP/</url>
      
        <content type="html"><![CDATA[<p>转载 from: <a href="http://blog.sina.com.cn/s/blog_9db078090102whzw.html" target="_blank" rel="noopener">http://blog.sina.com.cn/s/blog_9db078090102whzw.html</a></p><p>多标签图像分类（Multi-label   Image  Classification）任务中图片的标签不止一个，因此评价不能用普通单标签图像分类的标准，即mean  accuracy，该任务采用的是和信息检索中类似的方法—mAP（mean  Average  Precision）。mAP虽然字面意思和mean  accuracy看起来差不多，但是计算方法要繁琐得多，以下是mAP的计算方法：</p><p>首先用训练好的模型得到所有测试样本的confidence  score，每一类（如car）的confidence   score保存到一个文件中（如comp1_cls_test_car.txt）。假设共有20个测试样本，每个的id，confidence  score和ground  truth  label如下：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPgy6XQd58yJ15f" target="_blank" rel="noopener"><img src="http://s16.sinaimg.cn/mw690/002T2ChPgy6XQd58yJ15f" alt="img"></a> </p><p>接下来对confidence  score排序，得到：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPgy6XQd86isc4c" target="_blank" rel="noopener"><img src="http://s13.sinaimg.cn/mw690/002T2ChPgy6XQd86isc4c" alt="img"></a><em>这张表很重要，接下来的precision和recall都是依照这个表计算的</em>﻿</p><p>然后计算precision和recall，这两个标准的定义如下：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPgy6XQdjij4Ae8" target="_blank" rel="noopener"><img src="http://s9.sinaimg.cn/mw690/002T2ChPgy6XQdjij4Ae8" alt="img"></a></p><p>上图比较直观，圆圈内（true   positives + false  positives）是我们选出的元素,它对应于分类任务中我们取出的结果，比如对测试样本在训练好的car模型上分类，我们想得到top-5的结果，即：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPgy6XQdbTpla5c" target="_blank" rel="noopener"><img src="http://s13.sinaimg.cn/mw690/002T2ChPgy6XQdbTpla5c" alt="img"></a></p><p>在这个例子中，true   positives就是指第4和第2张图片，false   positives就是指第13，19，6张图片。方框内圆圈外的元素（false   negatives和true  negatives）是相对于方框内的元素而言，在这个例子中，是指confidence   score排在top-5之外的元素，即：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPgy6XQdcMwKCea" target="_blank" rel="noopener"><img src="http://s11.sinaimg.cn/mw690/002T2ChPgy6XQdcMwKCea" alt="img"></a> </p><p>其中，false   negatives是指第9，16，7，20张图片，true   negatives是指第1,18,5,15,10,17,12,14,8,11,3张图片。</p><p>那么，这个例子中Precision=2/5=40%，意思是对于car这一类别，我们选定了5个样本，其中正确的有2个，即准确率为40%；Recall=2/6=30%，意思是在所有测试样本中，共有6个car，但是因为我们只召回了2个，所以召回率为30%。</p><p>实际多类别分类任务中，我们通常不满足只通过top-5来衡量一个模型的好坏，而是需要知道从top-1到top-N（N是所有测试样本个数，本文中为20）对应的precision和recall。显然随着我们选定的样本越来也多，recall一定会越来越高，而precision整体上会呈下降趋势。把recall当成横坐标，precision当成纵坐标，即可得到常用的precision-recall曲线。这个例子的precision-recall曲线如下：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPgy6XQddBz7ze9" target="_blank" rel="noopener"><img src="http://s10.sinaimg.cn/mw690/002T2ChPgy6XQddBz7ze9" alt="img"></a></p><p>接下来说说AP的计算，此处参考的是PASCAL  VOC  CHALLENGE的计算方法。首先设定一组阈值，[0, 0.1, 0.2, …, 1]。然后对于recall大于每一个阈值（比如recall&gt;0.3），我们都会得到一个对应的最大precision。这样，我们就计算出了11个precision。AP即为这11个precision的平均值。这种方法英文叫做11-point interpolated average precision。</p><p>当然PASCAL VOC CHALLENGE自2010年后就换了另一种计算方法。新的计算方法假设这N个样本中有M个正例，那么我们会得到M个recall值（1/M, 2/M, …, M/M）,对于每个recall值r，我们可以计算出对应（r’ &gt; r）的最大precision，然后对这M个precision值取平均即得到最后的AP值。计算方法如下：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPzy76AuWjHOp29" target="_blank" rel="noopener"><img src="http://s10.sinaimg.cn/mw690/002T2ChPzy76AuWjHOp29" alt="img"></a></p><p>相应的Precision-Recall曲线（这条曲线是单调递减的）如下：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPzy76AuH9Z6010" target="_blank" rel="noopener"><img src="http://s1.sinaimg.cn/mw690/002T2ChPzy76AuH9Z6010" alt="img"></a></p><p>AP衡量的是学出来的模型在每个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏，得到AP后mAP的计算就变得很简单了，就是取所有AP的平均值。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>到底ResNet在解决一个什么问题呢</title>
      <link href="/2019/08/17/%E5%88%B0%E5%BA%95ResNet%E5%9C%A8%E8%A7%A3%E5%86%B3%E4%B8%80%E4%B8%AA%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%E5%91%A2/"/>
      <url>/2019/08/17/%E5%88%B0%E5%BA%95ResNet%E5%9C%A8%E8%A7%A3%E5%86%B3%E4%B8%80%E4%B8%AA%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%E5%91%A2/</url>
      
        <content type="html"><![CDATA[<p>对知乎上回答的简单总结</p><hr><p><strong>一、引言：为什么会有ResNet？Why ResNet？</strong></p><ul><li><p>过拟合？<br>  不是！因为深层网络表现为训练误差和测试误差都比较高，所以不是过拟合</p></li><li><p>梯度消失？梯度爆炸？<br>  不是！因为已经使用了 batch normalization ，在很大程度上解决了梯度消失、爆炸的问题，（yaya：我个人认为对梯度消失问题有一定的帮助，毕竟梯度值为1）</p></li><li><p>深层网络退化的原因？</p><p>  由于非线性激活函数的存在，使得信息被丢失，而不能完整保留，所以，应该在网络中加入恒等映射</p></li></ul><p>*<em>二、关于resnet网络结构 【没看懂为什么要有两层】  *</em></p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g62hrnrs6nj30h9048aax.jpg" alt></p><ul><li>yaya 分析：<br>一层：  relu(x +  w1 x)<br>两层：  relu(x +w2 relu(w1 x))</li></ul><p>​       既然非线性激活函数会把信息丢失，为什么不这样：relu(wx) + x ，因为这样是错误的，本身relu是需要放在输出后面，起到非线性的作用，但是这样，就不算作对输出的非线</p><p>*<em>三、更多的理解    *</em></p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g62hpvudvxj30iu0cc3zi.jpg" alt></p><hr><p>yaya 的总结/理解</p><ol><li>resnet 解决的不是过拟合的问题，因为过拟合的现象是，train loss 小，但是val loss大，但是当前深层网络的问题是train loss大，val loss也大</li><li>resnet 提供了一个梯度为1的反向传播，在一定程度上解决了梯度消失的问题</li><li>FPN中指出，不同深度的网络的结合可以结合不同的分辨率，但是当前resnet 只跨越了一种分辨率，因此，没能很好地利用这一特点，因此desnet便被提出来</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>深入理解Batch Normalization批标准化</title>
      <link href="/2019/08/15/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Batch-Normalization%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96/"/>
      <url>/2019/08/15/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Batch-Normalization%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<ul><li>转载 from：<a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/8724433.html</a></li></ul><blockquote><p>这几天面试经常被问到BN层的原理，虽然回答上来了，但还是感觉答得不是很好，今天仔细研究了一下Batch Normalization的原理，以下为参考网上几篇文章总结得出。</p></blockquote><p>　　Batch Normalization作为最近一年来DL的重要成果，已经广泛被证明其有效性和重要性。虽然有些细节处理还解释不清其理论原因，但是实践证明好用才是真的好，别忘了DL从Hinton对深层网络做Pre-Train开始就是一个<strong>经验领先于理论分析</strong>的偏经验的一门学问。本文是对论文《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》的导读。</p><p>　　机器学习领域有个很重要的假设：<strong>IID独立同分布假设</strong>，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。那BatchNorm的作用是什么呢？<strong>BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。</strong></p><p>　　接下来一步一步的理解什么是BN。</p><p>　　为什么深度神经网络<strong>随着网络深度加深，训练起来越困难，收敛越来越慢？</strong>这是个在DL领域很接近本质的好问题。很多论文都是解决这个问题的，比如ReLU激活函数，再比如Residual Network，BN本质上也是解释并从某个不同的角度来解决这个问题的。</p><h2 id="一、“Internal-Covariate-Shift”问题"><a href="#一、“Internal-Covariate-Shift”问题" class="headerlink" title="一、“Internal Covariate Shift”问题"></a>一、“Internal Covariate Shift”问题</h2><p>　　从论文名字可以看出，BN是用来解决“Internal Covariate Shift”问题的，那么首先得理解什么是“Internal Covariate Shift”？</p><p>　　论文首先说明Mini-Batch SGD相对于One Example SGD的两个优势：梯度更新方向更准确；并行计算速度快；（为什么要说这些？因为BatchNorm是基于Mini-Batch SGD的，所以先夸下Mini-Batch SGD，当然也是大实话）；然后吐槽下SGD训练的缺点：超参数调起来很麻烦。（作者隐含意思是用BN就能解决很多SGD的缺点）</p><p>　　接着引入<strong>covariate shift的概念</strong>：<strong>如果ML系统实例集合&lt;X,Y&gt;中的输入值X的分布老是变，这不符合IID假设</strong>，网络模型很难<strong>稳定的学规律</strong>，这不得引入迁移学习才能搞定吗，我们的ML系统还得去学习怎么迎合这种分布变化啊。对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是<strong>在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。</strong></p><p>　　然后提出了BatchNorm的基本思想：能不能<strong>让每个隐层节点的激活输入分布固定下来呢</strong>？这样就避免了“Internal Covariate Shift”问题了。</p><p>　　BN不是凭空拍脑袋拍出来的好点子，它是有启发来源的：之前的研究表明如果在图像处理中对输入图像进行白化（Whiten）操作的话——所谓<strong>白化</strong>，<strong>就是对输入数据分布变换到0均值，单位方差的正态分布</strong>——那么神经网络会较快收敛，那么BN作者就开始推论了：图像是深度神经网络的输入层，做白化能加快收敛，那么其实对于深度网络来说，其中某个隐层的神经元是下一层的输入，意思是其实深度神经网络的每一个隐层都是输入层，不过是相对下一层来说而已，那么能不能对每个隐层都做白化呢？这就是启发BN产生的原初想法，而BN也确实就是这么做的，<strong>可以理解为对深层神经网络每个隐层神经元的激活值做简化版本的白化操作。</strong></p><h2 id="二、BatchNorm的本质思想"><a href="#二、BatchNorm的本质思想" class="headerlink" title="二、BatchNorm的本质思想"></a><strong>二、</strong>BatchNorm的本质思想</h2><p>　　BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的<strong>激活输入值</strong>（就是那个x=WU+B，U是输入）<strong>随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近</strong>（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这<strong>导致反向传播时低层神经网络的梯度消失</strong>，这是训练深层神经网络收敛越来越慢的<strong>本质原因</strong>，<strong>而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</strong>，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是<strong>这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong></p><p>　　THAT’S IT。其实一句话就是：<strong>对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。</strong>因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。BN说到底就是这么个机制，方法很简单，道理很深刻。</p><p>　　上面说得还是显得抽象，下面更形象地表达下这种调整到底代表什么含义。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180405225246905-37854887.png" alt="img"></p><p>  图1  几个正态分布</p><p>　　假设某个隐层神经元原先的激活输入x取值符合正态分布，正态分布均值是-2，方差是0.5，对应上图中最左端的浅蓝色曲线，通过BN后转换为均值为0，方差是1的正态分布（对应上图中的深蓝色图形），意味着什么，意味着输入x的取值正态分布整体右移2（均值的变化），图形曲线更平缓了（方差增大的变化）。这个图的意思是，BN其实就是把每个隐层神经元的激活输入分布从偏离均值为0方差为1的正态分布通过平移均值压缩或者扩大曲线尖锐程度，调整为均值为0方差为1的正态分布。</p><p>　　那么把激活输入x调整到这个正态分布有什么用？首先我们看下均值为0，方差为1的标准正态分布代表什么含义：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180405225314624-527885612.png" alt="img"></p><p>图2  均值为0方差为1的标准正态分布图</p><p>　　这意味着在一个标准差范围内，也就是说64%的概率x其值落在[-1,1]的范围内，在两个标准差范围内，也就是说95%的概率x其值落在了[-2,2]的范围内。那么这又意味着什么？我们知道，激活值x=WU+B,U是真正的输入，x是某个神经元的激活值，假设非线性函数是sigmoid，那么看下sigmoid(x)其图形：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407143109455-1460017374.png" alt="img"></p><p>图3. Sigmoid(x)</p><p>及sigmoid(x)的导数为：G’=f(x)*(1-f(x))，因为f(x)=sigmoid(x)在0到1之间，所以G’在0到0.25之间，其对应的图如下：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407142351924-124461667.png" alt="img"></p><p>图4  Sigmoid(x)导数图</p><p>　　假设没有经过BN调整前x的原先正态分布均值是-6，方差是1，那么意味着95%的值落在了[-8,-4]之间，那么对应的Sigmoid（x）函数的值明显接近于0，这是典型的梯度饱和区，在这个区域里梯度变化很慢，为什么是梯度饱和区？请看下sigmoid(x)如果取值接近0或者接近于1的时候对应导数函数取值，接近于0，意味着梯度变化很小甚至消失。而假设经过BN后，均值是0，方差是1，那么意味着95%的x值落在了[-2,2]区间内，很明显这一段是sigmoid(x)函数接近于线性变换的区域，意味着x的小变化会导致非线性函数值较大的变化，也即是梯度变化较大，对应导数函数图中明显大于0的区域，就是梯度非饱和区。</p><p>　　从上面几个图应该看出来BN在干什么了吧？其实就是把隐层神经元激活输入x=WU+B从变化不拘一格的正态分布通过BN操作拉回到了均值为0，方差为1的正态分布，即原始正态分布中心左移或者右移到以0为均值，拉伸或者缩减形态形成以1为方差的图形。什么意思？就是说<strong>经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。</strong></p><p>　　但是很明显，看到这里，稍微了解神经网络的读者一般会提出一个疑问：如果都通过BN，那么不就跟把非线性函数替换成线性函数效果相同了？这意味着什么？我们知道，如果是多层的线性函数变换其实这个深层是没有意义的，因为多层线性网络跟一层线性网络是等价的。这意味着网络的<strong>表达能力</strong>下降了，这也意味着深度的意义就没有了。<strong>所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale*x+shift)</strong>，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把这个值从标准正态分布左移或者右移一点并长胖一点或者变瘦一点，每个实例挪动的程度不一样，这样等价于非线性函数的值从正中心周围的线性区往非线性区动了动。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。当然，这是我的理解，论文作者并未明确这样说。但是很明显这里的scale和shift操作是会有争议的，因为按照论文作者论文里写的理想状态，就会又通过scale和shift操作把变换后的x调整回未变换的状态，那不是饶了一圈又绕回去原始的“Internal Covariate Shift”问题里去了吗，感觉论文作者并未能够清楚地解释scale和shift操作的理论原因。</p><h2 id="三、训练阶段如何做BatchNorm"><a href="#三、训练阶段如何做BatchNorm" class="headerlink" title="三、训练阶段如何做BatchNorm"></a>三、训练阶段如何做BatchNorm</h2><p>　　上面是对BN的抽象分析和解释，具体在Mini-Batch SGD下做BN怎么做？其实论文里面这块写得很清楚也容易理解。为了保证这篇文章完整性，这里简单说明下。</p><p>　　假设对于一个深层神经网络来说，其中两层结构如下：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180405213859690-1933561230.png" alt="img"></p><p>  图5  DNN其中两层</p><p>　　要对每个隐层神经元的激活值做BN，可以想象成每个隐层又加上了一层BN操作层，它位于X=WU+B激活值获得之后，非线性函数变换之前，其图示如下：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180405213955224-1791925244.png" alt="img"></p><p>  图6. BN操作</p><p>　　对于Mini-Batch SGD来说，一次训练过程里面包含m个训练实例，其具体BN操作就是对于隐层内每个神经元的激活值来说，进行如下变换：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407142802238-1209499294.png" alt="img"></p><p>　　要注意，这里t层某个神经元的x(k)不是指原始输入，就是说不是t-1层每个神经元的输出，而是t层这个神经元的线性激活x=WU+B，这里的U才是t-1层神经元的输出。变换的意思是：某个神经元对应的原始的激活x通过减去mini-Batch内m个实例获得的m个激活x求得的均值E(x)并除以求得的方差Var(x)来进行转换。</p><p>　　上文说过经过这个<strong>变换后某个神经元的激活x形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大导数值，增强反向传播信息流动性，加快训练收敛速度。**</strong>但是这样会导致网络表达能力下降，为了防止这一点，每个神经元增加两个调节参数（scale和shift），这两个参数是通过训练来学习到的，用来对变换后的激活反变换，使得网络表达能力增强，即对变换后的激活进行如下的scale和shift操作，这其实是变换的反操作：**</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407142923190-79595046.png" alt="img"></p><p>　　BN其具体操作流程，如论文中描述的一样：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407142956288-903484055.png" alt="img"></p><p>　　过程非常清楚，就是上述公式的流程化描述，这里不解释了，直接应该能看懂。</p><h2 id="四、BatchNorm的推理-Inference-过程"><a href="#四、BatchNorm的推理-Inference-过程" class="headerlink" title="四、BatchNorm的推理(Inference)过程"></a>四、BatchNorm的推理(Inference)过程</h2><p>　　BN在训练的时候可以根据Mini-Batch里的若干训练实例进行激活数值调整，但是在推理（inference）的过程中，很明显输入就只有一个实例，看不到Mini-Batch其它实例，那么这时候怎么对输入做BN呢？因为很明显一个实例是没法算实例集合求出的均值和方差的。这可如何是好？</p><p>　　既然没有从Mini-Batch数据里可以得到的统计量，那就想其它办法来获得这个统计量，就是均值和方差。可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量，因为本来就打算用全局的统计量，只是因为计算量等太大所以才会用Mini-Batch这种简化方式的，那么在推理的时候直接用全局统计量即可。</p><p>　　决定了获得统计量的数据范围，那么接下来的问题是如何获得均值和方差的问题。很简单，因为每次做Mini-Batch训练时，都会有那个Mini-Batch里m个训练实例获得的均值和方差，现在要全局统计量，只要把每个Mini-Batch的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量，即：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407143405654-1995556833.png" alt="img"></p><p>　　有了均值和方差，每个隐层神经元也已经有对应训练好的Scaling参数和Shift参数，就可以在推导的时候对每个神经元的激活数据计算NB进行变换了，在推理过程中进行BN采取如下方式：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407143658338-63450857.png" alt="img"></p><p>　　这个公式其实和训练时</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407143807788-1841864822.png" alt="img"></p><p>　　是等价的，通过简单的合并计算推导就可以得出这个结论。那么为啥要写成这个变换形式呢？我猜作者这么写的意思是：在实际运行的时候，按照这种变体形式可以减少计算量，为啥呢？因为对于每个隐层节点来说：</p><p>　　　　　　　　<img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407144519480-1024698421.png" alt="img">　　<img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407144549010-487189588.png" alt="img"></p><p>　　都是固定值，这样这两个值可以事先算好存起来，在推理的时候直接用就行了，这样比原始的公式每一步骤都现算少了除法的运算过程，乍一看也没少多少计算量，但是如果隐层节点个数多的话节省的计算量就比较多了。</p><h2 id="五、BatchNorm的好处"><a href="#五、BatchNorm的好处" class="headerlink" title="五、BatchNorm的好处"></a>五、BatchNorm的好处</h2><p>　　BatchNorm为什么NB呢，关键还是效果好。<strong>①**</strong>不仅仅极大提升了训练速度，收敛过程大大加快；②还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；③另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。**总而言之，经过这么简单的变换，带来的好处多得很，这也是为何现在BN这么快流行起来的原因。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux 文件名中有空格、括号 时如何操作</title>
      <link href="/2019/08/14/linux-%E6%96%87%E4%BB%B6%E5%90%8D%E4%B8%AD%E6%9C%89%E7%A9%BA%E6%A0%BC%E3%80%81%E6%8B%AC%E5%8F%B7-%E6%97%B6%E5%A6%82%E4%BD%95%E6%93%8D%E4%BD%9C/"/>
      <url>/2019/08/14/linux-%E6%96%87%E4%BB%B6%E5%90%8D%E4%B8%AD%E6%9C%89%E7%A9%BA%E6%A0%BC%E3%80%81%E6%8B%AC%E5%8F%B7-%E6%97%B6%E5%A6%82%E4%BD%95%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h3 id="如何处理-cd-cp"><a href="#如何处理-cd-cp" class="headerlink" title="如何处理 cd cp"></a>如何处理 <code>cd</code> <code>cp</code></h3><ul><li><p>将文件名用<strong>双引号</strong> 包起来</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmd = 'cp -r <span class="string">"&#123;&#125;"</span> <span class="string">"&#123;&#125;"</span>'.format(source_path, target_path)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python 3.3.3 字面量,正则,反斜杠和原始字符串</title>
      <link href="/2019/08/14/python-3-3-3-%E5%AD%97%E9%9D%A2%E9%87%8F-%E6%AD%A3%E5%88%99-%E5%8F%8D%E6%96%9C%E6%9D%A0%E5%92%8C%E5%8E%9F%E5%A7%8B%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
      <url>/2019/08/14/python-3-3-3-%E5%AD%97%E9%9D%A2%E9%87%8F-%E6%AD%A3%E5%88%99-%E5%8F%8D%E6%96%9C%E6%9D%A0%E5%92%8C%E5%8E%9F%E5%A7%8B%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
      
        <content type="html"><![CDATA[<ul><li>注明：转载 from <a href="https://www.cnblogs.com/xiangnan/p/3446904.html" target="_blank" rel="noopener">https://www.cnblogs.com/xiangnan/p/3446904.html</a></li></ul><h1 id="两个不起眼但是比较重要的设定"><a href="#两个不起眼但是比较重要的设定" class="headerlink" title="两个不起眼但是比较重要的设定"></a>两个不起眼但是比较重要的设定</h1><ul><li>Python str类型的字面量解释器</li></ul><p>当反斜杠及其紧接字符无法构成一个具有特殊含义的序列(‘recognized escape sequences’)时,Python选择保留全部字符.直接看例子:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\c'</span></span><br><span class="line"><span class="string">'\\c'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\d'</span></span><br><span class="line"><span class="string">'\\d'</span></span><br></pre></td></tr></table></figure><p>官方管’\c’这种序列叫’unrecognized escape sequences’.官方文档相应部分:</p><p>Unlike Standard C, all unrecognized escape sequences are left in the string unchanged, i.e., <em>the backslash is left in the string</em>. (This behavior is useful when debugging: if an escape sequence is mistyped, the resulting output is more easily recognized as broken.) </p><p>按这段英文的意思,估计C语言里面,’c’和’\c’是等同的.Python是’\c’和’\c’等同.这个等以后学C语言再确定.</p><p>与上面对应的是,如果紧接字符能够和反斜杠构成’recognized escape sequences’的<strong>全部</strong>或者<strong>起始部分</strong>,中文就叫’被承认的转义序列’吧.比如:</p><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\b'</span></span><br><span class="line"><span class="string">'\x08'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\n'</span></span><br><span class="line"><span class="string">'\n'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\x'</span></span><br><span class="line"><span class="symbol">SyntaxError:</span> (unicode error) <span class="string">'unicodeescape'</span> codec can<span class="string">'t decode bytes in position 0-1: truncated \xXX escape</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; '</span>\N<span class="string">'</span></span><br><span class="line"><span class="string">SyntaxError: (unicode error) '</span>unicodeescape<span class="string">' codec can'</span>t decode bytes <span class="keyword">in</span> position <span class="number">0</span>-<span class="number">1</span>: malformed \N character escape</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\U'</span></span><br><span class="line"><span class="symbol">SyntaxError:</span> (unicode error) <span class="string">'unicodeescape'</span> codec can<span class="string">'t decode bytes in position 0-1: truncated \UXXXXXXXX escape</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; '</span>\u<span class="string">'</span></span><br><span class="line"><span class="string">SyntaxError: (unicode error) '</span>unicodeescape<span class="string">' codec can'</span>t decode bytes <span class="keyword">in</span> position <span class="number">0</span>-<span class="number">1</span>: truncated \uXXXX escape</span><br></pre></td></tr></table></figure><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><ul><li>Python re模块正则表达式解释器</li></ul><p>当反斜杠及其紧接字符无法构成一个具有特殊含义的序列(special sequences)时,re选择忽略反斜杠,例如:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\e'</span>,<span class="string">'eee'</span>)</span><br><span class="line">[<span class="string">'e'</span>, <span class="string">'e'</span>, <span class="string">'e'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'e'</span>,<span class="string">'eee'</span>)</span><br><span class="line">[<span class="string">'e'</span>, <span class="string">'e'</span>, <span class="string">'e'</span>]</span><br></pre></td></tr></table></figure><p>可见,’\e’和’e’起到了完全一样的效果.Python相关文档描述是:</p><p>If the ordinary character is not on the list, then the resulting RE will match the second character. For example, <code>\$</code> matches the character <code>&#39;$&#39;</code>.</p><p>与上面对应的是,如果能够构成special sequences,那么re会解释为相应含义.例如:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\w'</span>,<span class="string">'abcdefghijklmnopqrstuvwxyz'</span>)</span><br><span class="line">[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>, <span class="string">'g'</span>, <span class="string">'h'</span>, <span class="string">'i'</span>, <span class="string">'j'</span>, <span class="string">'k'</span>, <span class="string">'l'</span>, <span class="string">'m'</span>, <span class="string">'n'</span>, <span class="string">'o'</span>, <span class="string">'p'</span>, <span class="string">'q'</span>, <span class="string">'r'</span>, <span class="string">'s'</span>, <span class="string">'t'</span>, <span class="string">'u'</span>, <span class="string">'v'</span>, <span class="string">'w'</span>, <span class="string">'x'</span>, <span class="string">'y'</span>, <span class="string">'z'</span>]</span><br></pre></td></tr></table></figure><h1 id="字面量"><a href="#字面量" class="headerlink" title="字面量"></a>字面量</h1><p>字面量(Literals),是用于表示一些Python内建类型的常量的符号.最常见的字面量类型是str literals 和 bytes literals.</p><p>比如:</p><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'abc'</span></span><br><span class="line"><span class="string">'abc'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">"abc"</span></span><br><span class="line"><span class="string">'abc'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'啊哦额'</span></span><br><span class="line"><span class="string">'啊哦额'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'abc'</span></span><br><span class="line"><span class="string">b'abc'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">r'\n'</span></span><br><span class="line"><span class="string">'\\n'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'啊哦额'</span></span><br><span class="line">SyntaxError: bytes can only contain ASCII literal characters.</span><br></pre></td></tr></table></figure><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><p>反斜杠\的用途按紧接其后的字符种类可划分为3类:</p><p>1.将特殊字符转换为字面量.这特殊字符包括(单引号,双引号,反斜杠):’”\</p><p>2.将普通字符转换为特殊序列.包括:abfNnrtuUvx0123456789.</p><p>(注意,bytes字面量中,NuU这三个普通字符无法被转义成特殊序列)</p><p>3.将”新行”和自身忽略掉.这个比较抽象,举例说明:py文件中,某个字符串太长了,以至于需要分两行写,那么你可以插个反斜杠,紧接着换行,然后写剩余字符串.</p><p>下面是官方文档归纳的表:</p><table><thead><tr><th>Escape Sequence</th><th>Meaning</th><th>Notes</th></tr></thead><tbody><tr><td><code>\newline</code></td><td>Backslash and newline ignored</td><td></td></tr><tr><td><code>\\</code></td><td>Backslash (<code>\</code>)</td><td></td></tr><tr><td><code>\&#39;</code></td><td>Single quote (<code>&#39;</code>)</td><td></td></tr><tr><td><code>\&quot;</code></td><td>Double quote (<code>&quot;</code>)</td><td></td></tr><tr><td><code>\a</code></td><td>ASCII Bell (BEL)</td><td></td></tr><tr><td><code>\b</code></td><td>ASCII Backspace (BS)</td><td></td></tr><tr><td><code>\f</code></td><td>ASCII Formfeed (FF)</td><td></td></tr><tr><td><code>\n</code></td><td>ASCII Linefeed (LF)</td><td></td></tr><tr><td><code>\r</code></td><td>ASCII Carriage Return (CR)</td><td></td></tr><tr><td><code>\t</code></td><td>ASCII Horizontal Tab (TAB)</td><td></td></tr><tr><td><code>\v</code></td><td>ASCII Vertical Tab (VT)</td><td></td></tr><tr><td><code>\ooo</code></td><td>Character with octal value <em>ooo</em></td><td>(1,3)</td></tr><tr><td><code>\xhh</code></td><td>Character with hex value <em>hh</em></td><td>(2,3)</td></tr></tbody></table><p>Escape sequences only recognized in string literals are:</p><table><thead><tr><th>Escape Sequence</th><th>Meaning</th><th>Notes</th></tr></thead><tbody><tr><td><code>\N{name}</code></td><td>Character named <em>name</em> in the Unicode database</td><td>(4)</td></tr><tr><td><code>\uxxxx</code></td><td>Character with 16-bit hex value <em>xxxx</em></td><td>(5)</td></tr><tr><td><code>\Uxxxxxxxx</code></td><td>Character with 32-bit hex value <em>xxxxxxxx</em></td><td>(6)</td></tr></tbody></table><p>举例:</p><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\N&#123;END OF LINE&#125;'</span></span><br><span class="line"><span class="string">'\n'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\N&#123;HORIZONTAL TABULATION&#125;'</span></span><br><span class="line"><span class="string">'\t'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\u9f6a'</span>==<span class="string">'齪'</span></span><br><span class="line">True</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\1'</span>==<span class="string">'\01'</span></span><br><span class="line">True</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\1'</span>==<span class="string">'\001'</span></span><br><span class="line">True</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\1'</span>==<span class="string">'\0000001'</span></span><br><span class="line">False</span><br></pre></td></tr></table></figure><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><h1 id="正则"><a href="#正则" class="headerlink" title="正则"></a>正则</h1><ul><li>正则表达式的反斜杠的作用</li></ul><p>一种是使紧跟在后面的元字符(special characters或metacharacters)失去特殊含义,变为字面量.这些元字符有14个:</p><p>.^$*+?{}<a href></a>|</p><p>另一种是使紧跟在后面的普通字符变得具有特殊含义.这些普通字符是:</p><p>AbBdDsSwWZ0123456789</p><p>以及在str字面量中能被反斜杠转义的字符:</p><p>&#39;“abfnrtuUvx0123456789</p><p>例如:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\"'</span>,<span class="string">'"'</span>)</span><br><span class="line">[<span class="string">'"'</span>]</span><br></pre></td></tr></table></figure><p>正则pattern的反斜杠的作用和Python字面量的反斜杠类似,这据说是带来”反斜杠灾难”的根源.最典型的莫过于你需要用正则’\\‘才能匹配字面量反斜杠’\‘.</p><p>为方便说明,我们假设re.search(pattern,string)中,pattern表示正则表达式字符串,string表示待匹配的字符串.</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; re.search(<span class="string">'\\\\'</span>,<span class="string">'\\'</span>)</span><br><span class="line">&lt;_sre<span class="selector-class">.SRE_Match</span> <span class="selector-tag">object</span> at <span class="number">0</span>x02858528&gt;</span><br></pre></td></tr></table></figure><p>详细来说就是一个文本层级的反斜杠’&#39;(比如你在txt文件中看到的反斜杠),对应Python str 字面量的’\‘,对应正则pattern的’\\‘.这个确实比较难以理解,实在不行就住这点就好:<strong>如果不是最简单的正则类型(比如’ab’),强烈推荐对pattern使用r前缀符</strong>.这样容易理解:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>re.search(<span class="string">r'\\'</span>,<span class="string">'\\'</span>)</span><br><span class="line">&lt;_sre.SRE_Match object at <span class="number">0x02858448</span>&gt;</span><br></pre></td></tr></table></figure><p>注意:</p><ul><li>1.多重含义的特殊序列处理机制</li></ul><p>b0123456789比较特殊,它们在Python字面量和re正则中都能和反斜杠构成作用不同的特殊序列.例如\b,在python 字面量中解释为”退格键”.re正则中解释为’单词边界’.<strong>python 字面量有优先解释权</strong>,如下可证:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\b'</span>,<span class="string">'\b'</span>)  <span class="comment">#'\b'被优先解释为退格键,而不是单词边界</span></span><br><span class="line">[<span class="string">'\x08'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\b'</span>,<span class="string">'\b'</span>)</span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\b'</span>,<span class="string">'b'</span>) </span><br><span class="line">[<span class="string">''</span>, <span class="string">''</span>]</span><br></pre></td></tr></table></figure><p>再比如:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'(a)\1\1'</span>,<span class="string">'aaa'</span>) <span class="comment">#\1按字面量优先解释为八进制字符串,因此无匹配结果</span></span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'(a)\\1\\1'</span>,<span class="string">'aaa'</span>)  <span class="comment">#\\1按正则引擎层级的反斜杠解释为第一个匹配组提取到的字符,相当于'(a)aa'</span></span><br><span class="line">[<span class="string">'a'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'a\1\1'</span>,<span class="string">'a\1\1'</span>) <span class="comment">#\1按字面量优先解释为八进制字符串,所以有匹配结果</span></span><br><span class="line">[<span class="string">'a\x01\x01'</span>]</span><br></pre></td></tr></table></figure><p>了解这个设置有什么用?</p><p>1.当你想使用正则层级的特殊序列\1时,如果你没有使用r作为前缀,那么你必须使用\1才能如愿.</p><p>2.当你想使用字面量层级的特殊序列\1时,则不能使用r作为pattern前缀.</p><p>想想,你有可能在一个r前缀的字符串中写出能够匹配值为1的八进制字符串的pattern吗?</p><p>也许我太较真了,因为实践中好像从没遇到过需要匹配值为1的八进制字符串的情况,但理论上就是这样的.</p><ul><li><strong>2.正则表达式中特殊序列的准确定义的猜想</strong></li></ul><p>官方文档下面的一句话值得推敲:</p><p>Note that <code>\b</code> is used to represent word boundaries, and means “backspace” only inside character classes</p><p>意思是说\b只有在[…]里面时才表示退格键,这显然是错的.比如下面这个例子,\b没有在[]之内,但它是按”退格键”解释的,并非”单词边界”:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\b'</span>,<span class="string">'\b'</span>)</span><br><span class="line">[<span class="string">'\x08'</span>]</span><br></pre></td></tr></table></figure><p>除非官方文档描述的\b是指文本层面的数据(比如你在txt文档里看到的\b).</p><p>由此引出了一个猜想,re的正则pattern中”反斜杠+普通字符”构成特殊序列或”反斜杠+特殊字符”构成字面量–这种描述中的反斜杠准确来说是指两个反斜杠!</p><p>仍然是举例说明:</p><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\b\w+\\b'</span>,<span class="string">'one two three'</span>)  <span class="comment">#必须用\\b才能表示单词边界</span></span><br><span class="line">[<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\b\\w+\\b'</span>,<span class="string">'one two three'</span>)  <span class="comment">#想想,为什么\w和\\w都一样</span></span><br><span class="line">[<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\d'</span>,<span class="string">'123'</span>)</span><br><span class="line">[<span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\d'</span>,<span class="string">'123'</span>)</span><br><span class="line">[<span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>]</span><br></pre></td></tr></table></figure><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><ul><li>3.u和U只在str字面量中才能被转义,bytes字面量中是普通字符.</li></ul><p>以下是我猜测的正则表达式分析器和Python字面量分析器的传递规则表格:</p><table><thead><tr><th>Python string literal</th><th>values passed to regular expression</th><th>number of characters</th><th>what regular expression engine does</th><th>real meaning for regular expression</th></tr></thead><tbody><tr><td>\e</td><td>\e</td><td>2</td><td>ignore the backslash</td><td>e</td></tr><tr><td>\e</td><td>\e</td><td>2</td><td>ignore the backslash</td><td>e</td></tr><tr><td>e</td><td>e</td><td>1</td><td>nothing spacial</td><td>e</td></tr><tr><td>\n</td><td>\n</td><td>1</td><td>nothing spacial</td><td>换行符</td></tr><tr><td>\n</td><td>\n</td><td>2</td><td>\n is special</td><td>换行符</td></tr><tr><td>\b</td><td>\b</td><td>1</td><td>nothing spacial</td><td>退格键</td></tr><tr><td>\b</td><td>\b</td><td>2</td><td>\b is special</td><td>word boundary</td></tr><tr><td>\s</td><td>\s</td><td>2</td><td>\s is special</td><td>Unicode whitespace characters</td></tr><tr><td>\</td><td>\</td><td>1</td><td>must followed by a charcter</td><td>Can’t form any meaning</td></tr><tr><td>\\</td><td>\</td><td>2</td><td>remove all special meanning of \</td><td>\</td></tr><tr><td>*</td><td>*</td><td>1</td><td>* is special</td><td>repeat the left characters 0 or more times</td></tr><tr><td>*</td><td>*</td><td>2</td><td>remove all special meanning of *</td><td>*</td></tr></tbody></table><p>最后是待探究的例子:</p><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\n'</span>,<span class="string">'\n\n'</span>)</span><br><span class="line">[<span class="string">'\n'</span>, <span class="string">'\n'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\n'</span>,<span class="string">'\n\n'</span>)</span><br><span class="line">[<span class="string">'\n'</span>, <span class="string">'\n'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\\n'</span>,<span class="string">'\n\n'</span>)</span><br><span class="line">[<span class="string">'\n'</span>, <span class="string">'\n'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\\\n'</span>,<span class="string">'\n\n'</span>)</span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\b'</span>,<span class="string">'\b\b'</span>)</span><br><span class="line">[<span class="string">'\x08'</span>, <span class="string">'\x08'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\b'</span>,<span class="string">'\b\b'</span>)</span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\\b'</span>,<span class="string">'\b\b'</span>)</span><br><span class="line">[<span class="string">'\x08'</span>, <span class="string">'\x08'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\\\b'</span>,<span class="string">'\b\b'</span>)</span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\c'</span>,<span class="string">'\c\c'</span>)</span><br><span class="line">[<span class="string">'c'</span>, <span class="string">'c'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\c'</span>,<span class="string">'\c\c'</span>)</span><br><span class="line">[<span class="string">'c'</span>, <span class="string">'c'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\\c'</span>,<span class="string">'\c\c'</span>)</span><br><span class="line">[<span class="string">'\\c'</span>, <span class="string">'\\c'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\\\c'</span>,<span class="string">'\c\c'</span>)</span><br><span class="line">[<span class="string">'\\c'</span>, <span class="string">'\\c'</span>]</span><br></pre></td></tr></table></figure><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><p>参考:</p><p>Python 3.3.3 官方文档</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>glob 之 **</title>
      <link href="/2019/08/13/glob-%E4%B9%8B/"/>
      <url>/2019/08/13/glob-%E4%B9%8B/</url>
      
        <content type="html"><![CDATA[<ul><li>该篇主要介绍glob的一些使用小技巧</li></ul><h3 id="想要获得某个文件目录下所有-指定文件格式-的所有文件"><a href="#想要获得某个文件目录下所有-指定文件格式-的所有文件" class="headerlink" title="想要获得某个文件目录下所有 指定文件格式 的所有文件"></a>想要获得某个文件目录下所有 <strong><em>指定文件格式</em></strong> 的所有文件</h3><ul><li><p>假设有一个文件环境如下图所示</p><p><img src="https://i.loli.net/2019/08/14/sjTANPfDuV6cord.png" alt="搜狗截图20190814100532.png"></p></li></ul><ul><li><p>比如想要获得<code>/userhome/dataset/MSVD/YouTubeClips/YouTubeClips</code> 下 <code>.avi</code>格式的所有文件</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">path</span> = <span class="string">'/userhome/dataset/MSVD/YouTubeClips/YouTubeClips/'</span></span><br><span class="line">glob.glob(<span class="built_in">path</span> + <span class="string">'*.avi'</span>)</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>想要获得某目录下的所有子目录中的所有指定文件格式的所有文件</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">path</span> = <span class="string">'/userhome/dataset/MSVD/YouTubeClips/'</span></span><br><span class="line">glob.glob(<span class="built_in">path</span> + <span class="string">'**/'</span> + <span class="string">'*.avi'</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">path</span> = <span class="string">'/userhome/dataset/MSVD/'</span></span><br><span class="line">glob.glob(<span class="built_in">path</span> + <span class="string">'**/'</span> + <span class="string">'**/'</span> + <span class="string">'*.avi'</span>)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title> pytorch clone() vs copy_()</title>
      <link href="/2019/08/06/pytorch-clone-vs-copy/"/>
      <url>/2019/08/06/pytorch-clone-vs-copy/</url>
      
        <content type="html"><![CDATA[<p><code>clone</code>() → Tensor</p><ul><li>反向传播时，将会返回到原来的变量上<br>Returns a copy of the <code>self</code> tensor. The copy has the same size and data type as <code>self</code>.</li><li>NOTE</li><li>Unlike copy_(), this function is recorded in the computation graph. Gradients propagating to the cloned tensor will propagate to the original tensor.</li></ul><hr><p><code>copy_</code>(<em>src</em>, <em>non_blocking=False</em>) → Tensor</p><ul><li><p>只是值得复制<br>Copies the elements from <code>src</code> into <code>self</code> tensor and returns <code>self</code>.</p></li><li><p>The <code>src</code> tensor must be <a href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics" target="_blank" rel="noopener">broadcastable</a> with the <code>self</code> tensor. It may be of a different data type or reside on a different device.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实验中遇到的问题及解决</title>
      <link href="/2019/08/05/%E5%AE%9E%E9%AA%8C%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3/"/>
      <url>/2019/08/05/%E5%AE%9E%E9%AA%8C%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3/</url>
      
        <content type="html"><![CDATA[<h3 id="问题-1"><a href="#问题-1" class="headerlink" title="问题 1"></a>问题 1</h3><ul><li>问题描述：首先表现为：在pycharm debug下和在running模式下的实验结果不一致，<br><br>后来，在训练阶段将预训练的模型保存下来，载入evaluate.py 文件中再次进行评估，得到的分数与在训练阶段评估的分数不一致</li><li>解决思路：由于第二个现象，更加容易解决，因此先解决他，师兄提出一个办法，将保存的模型再次载入，这样就可以有两个网络，然后比较两个网络的数据是在哪里出现差异的，这样可以找到问题。</li><li>解决办法：</li></ul><ol><li>在训练一个epoch 后，将模型保存了下来，然后用两个网络，一个时train.py中重新加载这个网络，一个是在evaluate.py中加载这个网络，将得到的结果，进行比较，（看输出的结果是否一致），然后发现，在一些video 输出的结果是一样的，在一些video是不一样的。<br></li><li>找到那些video对应的结果不一样的所对应的iteration，在该iteration打印出了网络中的部分变量的数据，发现，在dataloader的数据就是不一样的.<br></li><li>那么问题就是出现在数据加载上。通过对数据加载部分的代码进行调试，发现，仅在num_workers=0时，两个dataloader的数据才一样，而采用多线程的话，两个dataloader的数据不完全一样。而又在其他的代码上测试，多线程不会影响数据加载，那么问题就是出现在，自己设计的dataset上，<br></li><li>又发现在加载h5py文件时，没有取切片，而self.critical pytorch代码时加上了的，通过加上切片 <code>[:]</code> 发现在多线程时，是正常的。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 问题总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 问题总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bridging the Gap between Training and Inference for Neural Machine Translation</title>
      <link href="/2019/08/04/Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation/"/>
      <url>/2019/08/04/Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Multi-Label Image Recognition with Graph Convolutional Networks</title>
      <link href="/2019/08/02/Multi-Label-Image-Recognition-with-Graph-Convolutional-Networks/"/>
      <url>/2019/08/02/Multi-Label-Image-Recognition-with-Graph-Convolutional-Networks/</url>
      
        <content type="html"><![CDATA[<h3 id="Motivation：建模-label-之间的依赖"><a href="#Motivation：建模-label-之间的依赖" class="headerlink" title="Motivation：建模  label 之间的依赖"></a>Motivation：建模  label 之间的依赖</h3><ul><li>使用GCN来建模label之间的依赖</li><li>有向图</li><li>每个节点用 label 的词向量来表达</li></ul><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><h4 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h4><ul><li><p>GCN 的输入：GCN 的 输入是label的word embedding，使用预训练的glove vector，如果label 是含有多个词的，那么对这多个词的词向量取平均，</p></li><li><p>GCN的输出<code>C*D</code>是为了得到一个分类器，<code>C</code>是类别数，<code>D</code>是image representation的维度，</p></li><li><p>邻接矩阵：a<sub>ij</sub>用条件概率来表示：当label<sub>i</sub>出现时，label<sub>j</sub>出现的概率，因此这不是一个对称矩阵，具体地论文中还给出了更加细节的修改。</p></li></ul><h4 id="image-representation"><a href="#image-representation" class="headerlink" title="image representation"></a>image representation</h4><ul><li>使用 ResNet101 得到 conv5层的输出，再经过全局池化得到一个<code>D</code>维度的特征向量</li></ul><h4 id="multi-label-classifier"><a href="#multi-label-classifier" class="headerlink" title="multi-label classifier"></a>multi-label classifier</h4><ul><li>将上两步的输出进行矩阵相乘，就可以得到 计算的multi-label</li></ul><p><img src="https://i.loli.net/2019/08/03/cdwYEWSF9q6tk3p.png" alt="搜狗截图20190802221229.png"></p><h3 id="不同点-vs-semi-supervised-gcn"><a href="#不同点-vs-semi-supervised-gcn" class="headerlink" title="不同点 vs semi-supervised gcn"></a>不同点 vs semi-supervised gcn</h3><p>1.</p><ul><li>不同于一般的GCN，输入节点的特征，和边，经过GCN之后，得到的是更新后的节点特征</li><li>本文GCN的输出<code>C*D</code>是为了得到一个分类器，<code>C</code>是类别数，<code>D</code>是image representation的维度，</li><li>GCN 的 输入是label的word embedding，使用预训练的glove vector，如果label 是含有多个词的，那么对这多个词的词向量取平均</li></ul><p>2.</p><ul><li>一般的GCN的邻接矩阵是预先定义好的，</li><li>但是本文的邻接矩阵：need to construct the <code>A</code> from scrach</li></ul>]]></content>
      
      
      <categories>
          
          <category> 图卷积网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图卷积网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>word2vec</title>
      <link href="/2019/08/02/word2vec-1/"/>
      <url>/2019/08/02/word2vec-1/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>word2vec</title>
      <link href="/2019/08/01/word2vec/"/>
      <url>/2019/08/01/word2vec/</url>
      
        <content type="html"><![CDATA[<h3 id="使用one-hot-来作为词向量"><a href="#使用one-hot-来作为词向量" class="headerlink" title="使用one-hot 来作为词向量"></a>使用one-hot 来作为词向量</h3><ul><li>存在一个缺点，即，两个单词之间的余弦相似度为0，因为one-hot是两两正交的形式。</li><li>但是相似度为0，显然是不对的</li></ul><h3 id="word2vet"><a href="#word2vet" class="headerlink" title="word2vet"></a>word2vet</h3><ul><li><p>跳字模型：中心词生成背景词</p></li><li><p>连续词袋模型：背景词生成中心词</p></li><li><p>这两个模型存在的问题：在softmax中，由于分母是对整个vocab进行求和，导致反向传播的计算量非常大</p></li><li><p><a href="https://www.bilibili.com/video/av18512944/" target="_blank" rel="noopener">相关教程</a></p></li></ul><p>预训练模型</p><ul><li>glove</li><li>fasttext</li><li><a href="https://www.bilibili.com/video/av18795160/?spm_id_from=333.788.videocard.0" target="_blank" rel="noopener">相关教程</a></li><li>spacy</li><li><a href="https://shiyaya.github.io/2019/07/16/Spacy工具包/" target="_blank" rel="noopener">https://shiyaya.github.io/2019/07/16/Spacy%E5%B7%A5%E5%85%B7%E5%8C%85/</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>FPN</title>
      <link href="/2019/08/01/FPN/"/>
      <url>/2019/08/01/FPN/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="https://vision.cornell.edu/se3/wp-content/uploads/2017/07/fpn-poster.pdf" target="_blank" rel="noopener">poster</a></li><li><a href="https://blog.csdn.net/WZZ18191171661/article/details/79494534" target="_blank" rel="noopener">某篇博客</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>VideoGraph: Recognizing Minutes-Long Human Activities in Videos</title>
      <link href="/2019/07/30/VideoGraph-Recognizing-Minutes-Long-Human-Activities-in-Videos/"/>
      <url>/2019/07/30/VideoGraph-Recognizing-Minutes-Long-Human-Activities-in-Videos/</url>
      
        <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>当前基于CNN或者non-lcoal的方法，可以建模 temporal concepts，但是却不能建模分钟级长的时域依赖。</li><li>学习一个无向图，节点和边都是直接从video中得到，而不需要进行单独的节点标注。</li><li>这里的节点是：组成activity的一个unit-action，比如 “煎鸡蛋” 这个activity里的 “打破鸡蛋” 。</li><li>边，表示 (units-action) 运动单元之间的时域关系</li></ul><h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><ul><li>建模长范围的activity</li><li>捕捉到细节信息</li></ul><h3 id="Vs-Video-as-space-time-region-graph"><a href="#Vs-Video-as-space-time-region-graph" class="headerlink" title="Vs  Video as space-time region graph"></a>Vs  <code>Video as space-time region graph</code></h3><ul><li>Video as space-time region graph： 需要提取 key objects</li><li>Video graph：自动的从video中学到 nodes</li></ul>]]></content>
      
      
      <categories>
          
          <category> 行为识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 行为识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装pytorch_geometricc</title>
      <link href="/2019/07/30/%E5%AE%89%E8%A3%85pytorch-geometricc/"/>
      <url>/2019/07/30/%E5%AE%89%E8%A3%85pytorch-geometricc/</url>
      
        <content type="html"><![CDATA[<ul><li><p><a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html#frequently-asked-questions" target="_blank" rel="noopener">官方链接</a></p></li><li><p>下面是截取自官方</p></li></ul><h2 id="Directly-Installation"><a href="#Directly-Installation" class="headerlink" title="Directly Installation"></a>Directly Installation</h2><p>We have outsourced a lot of functionality of PyTorch Geometric to other packages, which needs to be installed in advance. These packages come with their own CPU and GPU kernel implementations based on the newly introduced <a href="https://github.com/pytorch/extension-cpp/" target="_blank" rel="noopener">C++/CUDA extensions</a> in PyTorch 0.4.0.</p><p>Note</p><p>We do not recommend installation as root user on your system python. Please setup an <a href="https://conda.io/docs/user-guide/install/index.html/" target="_blank" rel="noopener">Anaconda/Miniconda</a> environment or create a <a href="https://www.docker.com/" target="_blank" rel="noopener">Docker image</a>.</p><p>Please follow the steps below for a successful installation:</p><ol start="0"><li><p>Added  by yaya:</p><ul><li><p>may be you can select a conda environments, will be more fine</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3<span class="number">-5.0</span><span class="number">.0</span>-Linux-x86_64.sh</span><br><span class="line">conda create -n pytorch_geometric python=<span class="number">3.7</span> -y</span><br><span class="line">source activate pytorch_geometric</span><br></pre></td></tr></table></figure></li><li><p>after into env: pytorch_geometric</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">pip</span> <span class="selector-tag">install</span> <span class="selector-tag">torch-1</span><span class="selector-class">.1</span><span class="selector-class">.0-cp37-cp37m-manylinux1_x86_64</span><span class="selector-class">.whl</span>  # <span class="selector-tag">download</span> <span class="selector-tag">at</span> <span class="selector-tag">first</span></span><br><span class="line"><span class="selector-tag">pip</span> <span class="selector-tag">installl</span> <span class="selector-tag">numpy-1</span><span class="selector-class">.17</span><span class="selector-class">.0-cp37-cp37m-manylinux1_x86_64</span><span class="selector-class">.whl</span>  # <span class="selector-tag">download</span> <span class="selector-tag">at</span> <span class="selector-tag">first</span></span><br><span class="line"><span class="selector-tag">pip</span> <span class="selector-tag">install</span> <span class="selector-tag">scipy-1</span><span class="selector-class">.3</span><span class="selector-class">.0-cp37-cp37m-manylinux1_x86_64</span><span class="selector-class">.whl</span>  # <span class="selector-tag">download</span> <span class="selector-tag">at</span> <span class="selector-tag">first</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><ol><li><p>Ensure that at least PyTorch 1.1.0 is installed:</p><blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; $ python -c <span class="string">"import torch; print(torch.__version__)"</span></span><br><span class="line">&gt; <span class="meta">&gt;&gt;&gt; </span><span class="number">1.1</span>.<span class="number">0</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote></li><li><p>Ensure CUDA is setup correctly (optional):</p><blockquote><ol><li><p>Check if PyTorch is installed with CUDA support:</p><blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;    &gt; $ python -c <span class="string">"import torch; print(torch.cuda.is_available())"</span></span><br><span class="line">&gt;    &gt; <span class="meta">&gt;&gt;&gt; </span>True</span><br><span class="line">&gt;    &gt;</span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote><blockquote><ol start="2"><li><p>Add CUDA to <code>$PATH</code> and <code>$CPATH</code> (note that your actual CUDA path may vary from <code>/usr/local/cuda</code>):</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda/bin:<span class="variable">$PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">echo</span> <span class="variable">$PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; &gt;&gt;&gt; /usr/<span class="built_in">local</span>/cuda/bin:...</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">export</span> CPATH=/usr/<span class="built_in">local</span>/cuda/include:<span class="variable">$CPATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">echo</span> <span class="variable">$CPATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; &gt;&gt;&gt; /usr/<span class="built_in">local</span>/cuda/include:...</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt;</span></span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote><blockquote><ol start="3"><li><p>Add CUDA to <code>$LD_LIBRARY_PATH</code> on Linux and to <code>$DYLD_LIBRARY_PATH</code> on macOS (note that your actual CUDA path may vary from <code>/usr/local/cuda</code>):</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda/lib64:<span class="variable">$LD_LIBRARY_PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">echo</span> <span class="variable">$LD_LIBRARY_PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; &gt;&gt;&gt; /usr/<span class="built_in">local</span>/cuda/lib64:...</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">export</span> DYLD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda/lib:<span class="variable">$DYLD_LIBRARY_PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">echo</span> <span class="variable">$DYLD_LIBRARY_PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; &gt;&gt;&gt; /usr/<span class="built_in">local</span>/cuda/lib:...</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt;</span></span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote><blockquote><ol start="4"><li><p>Verify that <code>nvcc</code> is accessible from terminal:</p><blockquote><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="quote">&gt;    &gt; $ nvcc --version</span></span><br><span class="line"><span class="quote">&gt;    &gt; &gt;&gt;&gt; 10.0</span></span><br><span class="line"><span class="quote">&gt;    &gt;</span></span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote><blockquote><ol start="5"><li><p>Ensure that PyTorch and system CUDA versions match:</p><blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;    &gt; $ python -c <span class="string">"import torch; print(torch.version.cuda)"</span></span><br><span class="line">&gt;    &gt; <span class="meta">&gt;&gt;&gt; </span><span class="number">10.0</span></span><br><span class="line">&gt;    &gt; </span><br><span class="line">&gt;    &gt; $ nvcc --version</span><br><span class="line">&gt;    &gt; <span class="meta">&gt;&gt;&gt; </span><span class="number">10.0</span></span><br><span class="line">&gt;    &gt;</span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote></li><li><p>Install all needed packages:</p><blockquote><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="symbol">$</span> you can see <span class="number">4.</span> first (optional)</span><br><span class="line">&gt; <span class="symbol">$</span> pip install --verbose --<span class="keyword">no</span>-cache-dir torch-scatter</span><br><span class="line">&gt; <span class="symbol">$</span> pip install --verbose --<span class="keyword">no</span>-cache-dir torch-sparse</span><br><span class="line">&gt; <span class="symbol">$</span> pip install --verbose --<span class="keyword">no</span>-cache-dir torch-cluster</span><br><span class="line">&gt; <span class="symbol">$</span> pip install --verbose --<span class="keyword">no</span>-cache-dir torch-spline-conv (optional)</span><br><span class="line">&gt; <span class="symbol">$</span> pip install torch-geometric</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote></li><li><p>added by yaya:<br>may be you can pip install scipy at first ,because above need it.</p></li></ol><h2 id="Docker-install"><a href="#Docker-install" class="headerlink" title="Docker install"></a>Docker install</h2><ul><li><a href="https://github.com/rusty1s/pytorch_geometric/tree/master/docker" target="_blank" rel="noopener">https://github.com/rusty1s/pytorch_geometric/tree/master/docker</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>GCN_LSTM  vs  SGAE</title>
      <link href="/2019/07/30/gcn-on-captioning/"/>
      <url>/2019/07/30/gcn-on-captioning/</url>
      
        <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Video Description: A Survey of Methods, Datasets and Evaluation Metrics</title>
      <link href="/2019/07/29/Video-Description-A-Survey-of-Methods-Datasets-and-Evaluation-Metrics/"/>
      <url>/2019/07/29/Video-Description-A-Survey-of-Methods-Datasets-and-Evaluation-Metrics/</url>
      
        <content type="html"><![CDATA[<h3 id="视频描述仍然处于起步阶段的原因"><a href="#视频描述仍然处于起步阶段的原因" class="headerlink" title="视频描述仍然处于起步阶段的原因"></a>视频描述仍然处于起步阶段的原因</h3><ul><li>对视频描述模型的分析是困难的，很难去判别是visual feature 亦或是 language model 哪个做的贡献大</li><li>当前的数据集，既没有包含足够的视觉多样性，也没有复杂的语言结构</li><li>当前的凭据指标并不能非常正确的去评估生成的句子与人类生成的句子之间的一致程度</li></ul><h3 id="the-difficulty-of-video-caption"><a href="#the-difficulty-of-video-caption" class="headerlink" title="the difficulty of video caption"></a>the difficulty of video caption</h3><ul><li>并不是在video中的所有object 都是与description相关的，可能其只是背景中的一个元素。    </li><li>此外，还需要objects的运动信息，以及 事件，动作，对象之间的因果关系。   </li><li>视频中的action可能有不同的长度，不同的action之间，可能有重叠。    </li></ul><h3 id="Sequence-Learning-based-Video-Captioning-Methods"><a href="#Sequence-Learning-based-Video-Captioning-Methods" class="headerlink" title="Sequence Learning based Video Captioning Methods"></a>Sequence Learning based Video Captioning Methods</h3><h4 id="CNN-RNN-based"><a href="#CNN-RNN-based" class="headerlink" title="CNN-RNN-based"></a>CNN-RNN-based</h4><ul><li><p>第一个 end-to-end：</p><p>S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. Mooney, and K. Saenko. 2014. Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729, (2014).    </p><img src="https://i.loli.net/2019/07/29/5d3ea016090c918345.png" alt="图片1.png" title="图片1.png"></li><li><p>S2VT （变长输入，变长输出）</p><p>I. Sutskever, O. Vinyals, and Q. V. Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems. 3104-3112.    </p><img src="https://i.loli.net/2019/07/29/5d3ea01536b3144846.png" alt="图片2.png" title="图片2.png">   </li><li><p>TA ( 加入C3D[1] )</p><p>L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and A.Courville. 2015. Describing videos by exploiting temporal structure. In IEEE ICCV    </p><img src="https://i.loli.net/2019/07/29/5d3ea016a248c95582.png" alt="图片3.png" title="图片3.png">  </li><li><p>LSTM-E （making a common visual-semantic-embedding ）</p><p>Y. Pan, T. Mei, T. Yao, H. Li, and Y. Rui. 2016. Jointly modeling embedding and translation to bridge video and language. In IEEE CVPR. </p><img src="https://i.loli.net/2019/07/29/5d3ea421aaf9013065.png" alt="图片4.png" title="图片4.png"></li></ul><ul><li><p>GRU-EVE  ( short fourier transform)</p><p>N. Aafaq, N. Akhtar, W. Liu, S. Z. Gilani and A. Mian. 2019. Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning. In IEEE CVPR.    </p><img src="https://i.loli.net/2019/07/29/5d3ea0163113561600.png" alt="搜狗截图20190729152752.png" title="搜狗截图20190729152752.png">   </li><li><p>h-RNN<br>H. Yu, J. Wang, Z. Huang, Y. Yang, and W. Xu. 2016. Video paragraph captioning using hierarchical recurrent neural networks. In IEEE CVPR.</p><img src="https://i.loli.net/2019/07/29/5d3ea63af2e0354548.png" alt="图片5.png" title="图片5.png"></li></ul><h4 id="RL-based"><a href="#RL-based" class="headerlink" title="RL-based"></a>RL-based</h4><ul><li><p>Z. Ren, X. Wang, N. Zhang, X. Lv, and L. Li. 2017. Deep reinforcement learning-based image captioning with embedding reward. arXiv preprint arXiv:1704.03899, (2017).</p></li><li><p>Y. Chen, S. Wang, W. Zhang, and Q. Huang. 2018.  ==Less Is More: Picking Informative Frames for Video Captioning.==  arXiv preprint arXiv:1803.01457, (2018).</p><p>提出了一个基于强化学习的方法，来选择 key informative frames 来表达一个 complete video ，希望这样的操作可以忽略掉噪声和不必要的计算。</p></li><li><p>L. Li and B. Gong. 2018. End-to-End Video Captioning with Multitask Reinforcement Learning. arXiv preprint arXiv:1803.07950,<br>(2018).</p></li><li><p>R. Pasunuru and M. Bansal. 2017. Reinforced video captioning with entailment rewards. arXiv preprint arXiv:1708.02300, (2017).</p></li><li><p>S. Phan, G. E. Henter, Y. Miyao, and S. Satoh. 2017. Consensusbased Sequence Training for Video Captioning. arXiv preprint arXiv:1712.09532, (2017).</p></li><li><p>X. Wang, W. Chen, J. Wu, Y. Wang, and W. Y. Wang. 2017.  ==Video Captioning via Hierarchical Reinforcement Learning.==  arXiv preprint arXiv:1711.11135, (2017).</p><p>在 decoder阶段，使用 深度强化学习，这个方法证明可以捕捉到视频内容中的细节，并生成细粒度的description，但是！这个方法相对于当前的baseline 没有多大的提高。（我自己还需要再看看， 使用DRL的motivation）</p></li></ul><h3 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h3><ul><li><p><a href="https://blog.csdn.net/joshuaxx316/article/details/58696552" target="_blank" rel="noopener">参考链接</a></p></li><li><p>BLEU、ROUGE、METEOR  来源于 机器翻译</p></li><li><p>CIDEr、SPICE 来源于图像描述   </p></li></ul><h4 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a>BLEU</h4><ul><li><a href="https://blog.csdn.net/allocator/article/details/79657792" target="_blank" rel="noopener">BLEU参考链接</a></li><li>==BLEU实质是对两个句子的共现词频率计算==，但计算过程中使用好些技巧，追求计算的数值可以衡量这两句话的一致程度。 </li><li>BLEU容易陷入常用词和短译句的陷阱中，而给出较高的评分值。本文主要是对解决BLEU的这两个弊端的优化方法介绍。</li><li>缺点</li></ul><ol><li>　不考虑语言表达（语法）上的准确性； </li><li>　 测评精度会受常用词的干扰； </li><li>　 短译句的测评精度有时会较高； </li><li>　没有考虑同义词或相似表达的情况，可能会导致合理翻译被否定；</li></ol><h4 id="ROUGE"><a href="#ROUGE" class="headerlink" title="ROUGE"></a>ROUGE</h4><img src="https://i.loli.net/2019/07/29/5d3ed71f2086769963.png" alt="20170228224903951.png" title="20170228224903951.png"><h4 id="METEOR"><a href="#METEOR" class="headerlink" title="METEOR"></a>METEOR</h4><img src="https://i.loli.net/2019/07/29/5d3edcce1761442736.png" alt="20170228225011405.png" title="20170228225011405.png">   <h4 id="CIDEr"><a href="#CIDEr" class="headerlink" title="CIDEr"></a>CIDEr</h4><img src="https://i.loli.net/2019/07/29/5d3edcce646d089162.png" alt="20170228225056046.png" title="20170228225056046.png"><h4 id="SPICE"><a href="#SPICE" class="headerlink" title="SPICE"></a>SPICE</h4><ul><li>基于 gt 和 pred 的场景图解析，来对预测结果进行评价，</li><li>不被广泛使用的原因是，当前sentence scene graph 的能力还比较若，很容易解析错误(eg:dog swimming through river”, the failure case could be the word “swimming” being parsed as “object” and the word “dog” parsed as “attribute” )</li><li>对句子解析错误了，那么给出的评价指标也不会很好！！！</li></ul><h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><img src="https://i.loli.net/2019/07/29/5d3edd503479c20027.png" alt="搜狗截图20190729194921.png" title="搜狗截图20190729194921.png">    <h3 id="当前的瓶颈："><a href="#当前的瓶颈：" class="headerlink" title="当前的瓶颈："></a>当前的瓶颈：</h3><h4 id="缺乏有效的评价指标"><a href="#缺乏有效的评价指标" class="headerlink" title="缺乏有效的评价指标"></a>缺乏有效的评价指标</h4><ul><li><p>我们的调查显示，阻碍这一研究进展的一个主要瓶颈是缺乏有效和有目的设计的视频描述评价指标。目前，无论是从机器翻译还是从图像字幕中，都采用了现有的度量标准，无法衡量机器生成的视频字幕的质量及其与人类判断的一致性。改进这些指标的一种方法是增加引用语句的数量。我们认为，从数据本身学习的目的构建的度量标准是推进视频描述研究的关键。    </p></li><li><p>王鑫也曾说：human evaluation在video captioning任务中是有必要的       </p><h4 id="视觉特征部分的瓶颈"><a href="#视觉特征部分的瓶颈" class="headerlink" title="视觉特征部分的瓶颈"></a>视觉特征部分的瓶颈</h4></li><li><p>在一个video中，可能出现多个activity，但是caption model只能检测出部分几个，导致性能下降。   </p></li><li><p>可能这个video中 action 的持续时间较长，但是，当前的video representation方法只能捕捉时域较短的运动信息（eg:C3D），因此不能很好地提取视频特征。   </p></li><li><p>大多数特征提取器只适用于静态或平稳变化的图像，因此难以处理突然的场景变化。目前的方法通过表示整体视频或帧来简化视觉编码部分。可能需要进一步探索注意力模型，以关注视频中具有重要意义的空间和时间部分。   </p></li><li><p>当前的encoder 与 decoder 部分，并 ==不是端到端的==，需要先提取 video representation再进行decoder，这样分布进行，而不是端到端的训练是不好的！    </p></li></ul><h3 id="captioning-model-的可解释性不足"><a href="#captioning-model-的可解释性不足" class="headerlink" title="captioning model 的可解释性不足"></a>captioning model 的可解释性不足</h3><ul><li>举个例子：当我们从包含“白色消防栓”的帧中看到视频描述模型生成的标题“红色消防栓”时，很难确定颜色特征是视觉特征提取器编码错误还是由于使用的语言模型bias( 由于有过多的训练数据是“红色消防栓)。<img src="https://i.loli.net/2019/07/29/5d3ee4996cf7480633.png" alt="搜狗截图20190729202028.png" title="搜狗截图20190729202028.png"></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li>[1] D. Tran, L. D. Bourdev, R. Fergus, L. Torresani, and M. Paluri. 2014. C3D: Generic Features for Video Analysis. CoRR abs/1412.0767, (2014). </li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Temporal Deformable Convolutional Encoder-Decoder Networks for Video Captioning</title>
      <link href="/2019/07/28/Temporal-Deformable-Convolutional-Encoder-Decoder-Networks-for-Video-Captioning/"/>
      <url>/2019/07/28/Temporal-Deformable-Convolutional-Encoder-Decoder-Networks-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="当前存在的问题"><a href="#当前存在的问题" class="headerlink" title="当前存在的问题"></a>当前存在的问题</h3><ul><li>RNN 存在梯度消失和梯度下降的问题</li><li>RNN 的本质的循环依赖，限制了其并行计算</li><li>因此本文提出了 ==Temporal Deformable Convolutional Encoder-Decoder Networks (dubbed as TDConvED) ==that fully employ convolutions in both encoder and decoder networks for video captioning. </li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Motion Guided Spatial Attention for Video Captioning</title>
      <link href="/2019/07/28/Motion-Guided-Spatial-Attention-for-Video-Captioning/"/>
      <url>/2019/07/28/Motion-Guided-Spatial-Attention-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="当前的问题"><a href="#当前的问题" class="headerlink" title="当前的问题"></a>当前的问题</h3><ul><li>spatial attention 很少有人去探索</li><li>motion information 被利用通常是使用3D-CNNs来作为另外一种模态</li></ul><h3 id="本文的工作"><a href="#本文的工作" class="headerlink" title="本文的工作"></a>本文的工作</h3><ul><li>两个贡献： MGSA、GARU</li><li>The proposed MGSA utilize motion information between consecutive frames by applying CNN to stacked optical flows. </li><li>In addition, a gated recurrent unit named GARU is designed to adaptively relate spatial attention maps across time. <img src="https://i.loli.net/2019/07/28/5d3d877e9c0d546970.png" alt="搜狗截图20190728193057.png" title="搜狗截图20190728193057.png">    </li></ul><h3 id="Encoder-部分我的理解"><a href="#Encoder-部分我的理解" class="headerlink" title="Encoder 部分我的理解"></a>Encoder 部分我的理解</h3><ul><li>对一个video 采取N帧，对这N帧提取appearences feature，得到<code>N*H*W*D</code>的特征向量</li><li>以每帧为中心，采取连续的M帧，这M帧计算optical flow，并将这个<code>N*M</code>帧的optical flow images送入CNN中，得到<code>N*H*W*1</code>的特征向量。</li><li>==构造一个长度为N的GRU时域序列，每次送入一帧==  appearence feature 和 optical flow cnn feature，并得到一个输出,维度为<code>H*W</code>，</li><li>该输出作为一个attention系数，并与 ==当前帧== frame feature 相乘。得到一个为该帧的每个像素点（<code>H*W</code>）分配的权重系数。即进行加权求和，则可以得到该帧的spatial attention</li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><img src="https://i.loli.net/2019/07/28/5d3d87b91afaa36288.png" alt="搜狗截图20190728193156.png" title="搜狗截图20190728193156.png" width="440px" height="400px">    <h3 id="Experimental-results"><a href="#Experimental-results" class="headerlink" title="Experimental results"></a>Experimental results</h3><ul><li>这里只是想提一点，就是有一些论文在MSR-VTT上的实验结果，是使用了==音频信息==。<img src="https://i.loli.net/2019/07/28/5d3d80835814750581.png" alt="搜狗截图20190728190107.png" title="搜狗截图20190728190107.png"></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><p>基于 spatial attention的 video captioning model</p></li><li><p>Li, X.; Zhao, B.; and Lu, X. 2017. MAM-RNN: multi-level attention model based RNN for video captioning. In IJCAI, 2208–2214. </p></li><li><p>Yang, Z.; Han, Y.; and Wang, Z. 2017. Catching the temporal regions-of-interest for video captioning. In ACM MM, 146–153. attention, spatial. </p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning</title>
      <link href="/2019/07/27/Object-aware-Aggregation-with-Bidirectional-Temporal-Graph-for-Video-Captioning/"/>
      <url>/2019/07/27/Object-aware-Aggregation-with-Bidirectional-Temporal-Graph-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h3><ul><li>本文旨在于捕捉基于object的运动信息(capture object-based trajectory)，以前向流为例，以第一帧中的object regions 作为anchor， 来寻找在其他帧中相对应的regions， 计算该anchor 与 第i帧中的regions的相似性【相似性不仅考虑了特征相似性，还考虑了空间位置相似性】，然后相似性最大的那个region，认为是与anchor一致的objects， 然后将他们组成一组。反向流类似。  </li><li>这个捕捉运动信息的思想与 【Learning Video Representations from Correspondence Proposals】中的很相似。   </li></ul><h3 id="当前存在的问题"><a href="#当前存在的问题" class="headerlink" title="当前存在的问题"></a>当前存在的问题</h3><ul><li>当前的工作主要使用 global frame 或者是 salient regions而不是使用specific objects，那么将不能捕捉到每个object 的细节的时域动态。</li></ul><h3 id="文章的主要工作"><a href="#文章的主要工作" class="headerlink" title="文章的主要工作"></a>文章的主要工作</h3><h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><ul><li>constructs ==bidirectional temporal graph== to extract  the temporal trajectories for each object instance, which captures the detailed temporal dynamics in video content.    </li><li>==aggregation process on object regions==, which can capture the object-aware semantic information， 这里主要是得到了 VLAD[5, 6] representation   </li></ul><h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><ul><li><p>对object VLAD representation实施了temporal attention 和 object attention</p></li><li><p>对 frames VLAD representation 实施了 temporal attention</p></li><li><p>然后分别进行nn.linear 线性变换后，相加</p></li><li><p>再与word_embedding相加送入GRU</p></li><li></li><li><p><font color="#0099ff" size="5" face="黑体">前向流和后向流的融合：</font>在分别得到两流输出的word score 之后，进行 sum</p><img src="https://i.loli.net/2019/07/28/5d3d37324d6d283934.png" alt="搜狗截图20190728134820.png" title="搜狗截图20190728134820.png">   </li></ul><h3 id="本文的性能分析"><a href="#本文的性能分析" class="headerlink" title="本文的性能分析"></a>本文的性能分析</h3><ul><li>可以准确的描述video，比如关键的objects。</li><li><strong>但是！不能很好地去描述 objects 之间的交互</strong></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li>【NetVLAD】Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: Cnn architecture for weakly supervised place recognition. In CVPR, pages 5297–5307, 2016.</li><li>【SeqVLAD】Youjiang Xu, Yahong Han, Richang Hong, and Qi Tian. Sequential video vlad: Training the aggregation locally and temporally. IEEE Transactions on Image Processing (TIP), 27(10):4933–4944, 2018</li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pointing Novel Objects in Image Captioning</title>
      <link href="/2019/07/26/Pointing-Novel-Objects-in-Image-Captioning/"/>
      <url>/2019/07/26/Pointing-Novel-Objects-in-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><img src="https://i.loli.net/2019/07/27/5d3c1676f301a18995.png" alt="搜狗截图20190727171628.png" title="搜狗截图20190727171628.png"><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><ul><li>当前的模型，都是以image-caption对来进行训练，因此训练模型只能输出in-domain objects，但是，在实际应用中有些图片含有丰富的信息，但是用现有的模型却不能充分的表达。   </li></ul><h4 id="解决1"><a href="#解决1" class="headerlink" title="解决1"></a>解决1</h4><ul><li>希望可以生成新的words,(没有出现在training dataset)   </li><li>本文提出了解决办法：用object learner 来扩增标准的deep caption 结构。即，由一个图像分类任务，则可以得到该图像中出现的obects。这可以作为一个补充信息，加入到当前现有的deep caption Model 中。   </li><li>具体地：（1）标准的LSTM decoder 会输出一个predicted word,  （2）objects learner 通过一个copying layer 也可以得到一个预测单词。那么该选谁，本文并不硬选择，而是软选择，即给一个系数，来给这两个分配个概率，然后加和。这个选择的过程称为 <strong>Pointing Mechanism</strong>   </li><li>loss:   <img src="https://i.loli.net/2019/07/27/5d3c1c012cccc48971.png" alt="搜狗截图20190727173939.png" title="搜狗截图20190727173939.png">   </li></ul><h4 id="解决2"><a href="#解决2" class="headerlink" title="解决2:"></a>解决2:</h4><ul><li>希望将image中的所有信息，在句子中都可以覆盖到</li><li>提出了一个新的损失。target caption中含有 n词，即对应到image 中的 objects。那么希望生成的句子中含有的n词信息能够包含image中所有出现到的objects（即 target caption中的所有名词）</li><li>那么可以根据预测的单词是否生成了 target caption 中的名词，来计算损失（文章中这里在计算损失的时候忽略了语法结构，即不要求名词出现的在句子中的位置，只要求出现就可以）.   </li><li>loss:</li></ul><img src="https://i.loli.net/2019/07/27/5d3c1c754b56a83488.png" alt="搜狗截图20190727174134.png" title="搜狗截图20190727174134.png">    <img src="https://i.loli.net/2019/07/27/5d3c1c7536e6674709.png" alt="搜狗截图20190727174147.png" title="搜狗截图20190727174147.png"> ]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vision to language 大牛</title>
      <link href="/2019/07/26/vision-to-language-%E5%A4%A7%E7%89%9B/"/>
      <url>/2019/07/26/vision-to-language-%E5%A4%A7%E7%89%9B/</url>
      
        <content type="html"><![CDATA[<h3 id="王鑫"><a href="#王鑫" class="headerlink" title="王鑫"></a>王鑫</h3><p>Papers can be found at <a href="https://sites.cs.ucsb.edu/~xwang" target="_blank" rel="noopener">https://sites.cs.ucsb.edu/~xwang</a><br>Email: <a href="mailto:xwang@cs.ucsb.edu" target="_blank" rel="noopener">xwang@cs.ucsb.edu</a></p><h4 id="video-captioning-via-hierarchical-reinforcement-learning"><a href="#video-captioning-via-hierarchical-reinforcement-learning" class="headerlink" title="video captioning via hierarchical  reinforcement learning"></a>video captioning via hierarchical  reinforcement learning</h4><ol><li>强化学习</li><li>加入音频信号</li></ol><h4 id="zero-shot-video-captioning"><a href="#zero-shot-video-captioning" class="headerlink" title="zero-shot video captioning"></a>zero-shot video captioning</h4><ul><li>Topic-Aware Mixture of Experts (TAMoE)  <h4 id="evaluation"><a href="#evaluation" class="headerlink" title="evaluation"></a>evaluation</h4></li></ul><ol><li><p>如何去评判，本身就是一个问题，当前的评价指标并不是那么合理</p></li><li><p>human evaluation是一个必要的评测方法，尤其是对于生成story的</p></li></ol><h4 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h4><ul><li>利用强化学习直接对指标进行优化，很可能会造成，指标上去了，但是生成的句子语义并不好。所以提出了下篇论文</li></ul><ol><li>Adversarial REward Learning (AREL)</li></ol><h4 id="Connecting-Language-and-Vision-to-Actions"><a href="#Connecting-Language-and-Vision-to-Actions" class="headerlink" title="Connecting Language and Vision to Actions"></a>Connecting Language and Vision to Actions</h4><ul><li>Look Before You Leap: Model-based RL</li><li>Reinforced Cross-Modal Matching (RCM)</li></ul><h3 id="吴琦"><a href="#吴琦" class="headerlink" title="吴琦"></a>吴琦</h3><h4 id="从-Vision-到-Language-再到-Action，万字漫谈三年跨域信息融合研究"><a href="#从-Vision-到-Language-再到-Action，万字漫谈三年跨域信息融合研究" class="headerlink" title="从 Vision 到 Language 再到 Action，万字漫谈三年跨域信息融合研究"></a><a href="https://mp.weixin.qq.com/s/lnoL1TpKY8HQqCMaBqWA5Q" target="_blank" rel="noopener">从 Vision 到 Language 再到 Action，万字漫谈三年跨域信息融合研究</a></h4><h4 id="一文纵览-Vision-and-Language-领域最新研究与进展"><a href="#一文纵览-Vision-and-Language-领域最新研究与进展" class="headerlink" title="一文纵览 Vision-and-Language 领域最新研究与进展"></a><a href="https://mp.weixin.qq.com/s/dyY64QrvPWbjGvJw5H51OA" target="_blank" rel="noopener">一文纵览 Vision-and-Language 领域最新研究与进展</a></h4>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CMU课程上新：Neural Networks for NLP</title>
      <link href="/2019/07/26/CMU%E8%AF%BE%E7%A8%8B%E4%B8%8A%E6%96%B0%EF%BC%9ANeural-Networks-for-NLP/"/>
      <url>/2019/07/26/CMU%E8%AF%BE%E7%A8%8B%E4%B8%8A%E6%96%B0%EF%BC%9ANeural-Networks-for-NLP/</url>
      
        <content type="html"><![CDATA[<h1 id="CMU课程上新：Neural-Networks-for-NLP（18年视频课件放出）"><a href="#CMU课程上新：Neural-Networks-for-NLP（18年视频课件放出）" class="headerlink" title="CMU课程上新：Neural Networks for NLP（18年视频课件放出）"></a>CMU课程上新：Neural Networks for NLP（18年视频课件放出）</h1><p>今天文摘菌给大家推荐一门非常棒的课程《Neural Networks for NLP》。这个课程首先简要介绍一下神经网络的基本知识，然后课程的大部讲分如何将神经网络应用于自然语言处理。</p><p>课程中的每一节都会介绍自然语言中的特定问题和现象，并描述建模的难点，当然，并也会绍几个解决这些问题的模型。</p><p>总的来说，这个“神课”会涉及用神经网络建模过程中所使用的各种技术，包括如何处理结构化句子，如何处理大数据，以及半监督和无监督学习，结构化预测和多语言建模等等。</p><p>注意，修读本门课程需要有一定的自然语言处理的知识储备，按照课程的要求，就是应该上过《17-711，NLP算法》。</p><p>2018年的课程视频已经公开，无法上外网的同学，国内也有热心的小伙伴将课程搬到了国内的B站，通过下面的链接可以打开哟</p><p><a href="https://www.bilibili.com/video/av31156700/" target="_blank" rel="noopener">https://www.bilibili.com/video/av31156700/</a></p><p>19年的春季新课程新增了ELMo/BERT上下文词表示、模型可解释性等内容，PyTorch/DyNet代码示例。</p><p>19年课程的课程目录等详细信息，可以去课程主页去查看哟~~文摘菌在下面给大家简单介绍一下这门课程的师资以及作业等情况。</p><p><a href="https://phontron.com/class/nn4nlp2019/schedule.html" target="_blank" rel="noopener">https://phontron.com/class/nn4nlp2019/schedule.html</a></p><p><strong>师资力量</strong></p><p><img src="https://image.jiqizhixin.com/uploads/editor/6e56a929-53db-4396-bd4d-e42e2bd166cd/640.png" alt="img"></p><p>本课程有两位主讲教师，分别是：Graham Neubig、Antonios Anastasopoulos。其中Graham Neubig是卡内基梅陇大学的教授，主要研究自然语言处理，他对机器学习非常感兴趣。Antonios Anastasopoulos我是圣母大学的在读博士，目前David Chiang自然语言处理技术组的成员。专注于“濒危”语言的机器翻译和语音识别。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/db527776-dbf8-4611-a415-1c94e99db32e/640.png" alt="img"></p><p>除了上课的教师外，还有一大批助教来解答同学们的疑惑。从助教安排，我们可以看出，课程在尽量做到有问必答。</p><h2 id="作业介绍及资料公开"><a href="#作业介绍及资料公开" class="headerlink" title="作业介绍及资料公开"></a>作业介绍及资料公开</h2><p>在课程的官网上，对课程的每一次作业都做了详细的说明，包括评分要求，完成作业的条件等等。除此之外还给出了作业示例。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/0cf289f4-3ea8-43ad-b318-c97703e921ac/640.png" alt="img"></p><p>课程官网也贴心的给大家准备好了每一次讲课的PPT，在上课之前，大家多多预习哟~</p><p>PPT下载地址：<a href="https://phontron.com/class/nn4nlp2019/schedule.html" target="_blank" rel="noopener">https://phontron.com/class/nn4nlp2019/schedule.html</a></p><p>此次课程，初步是线下课程。请大家关注大数据文摘，如果后期有视频放出，文摘菌也一定会为大家更新的。</p><p>最后，再次给出课程主页：<a href="https://phontron.com/class/nn4nlp2019/schedule.html" target="_blank" rel="noopener">https://phontron.com/class/nn4nlp2019/schedule.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 课程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 课程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Video Representations from Correspondence Proposals</title>
      <link href="/2019/07/26/Learning-Video-Representations-from-Correspondence-Proposals/"/>
      <url>/2019/07/26/Learning-Video-Representations-from-Correspondence-Proposals/</url>
      
        <content type="html"><![CDATA[<h3 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h3><ul><li>与Non-local 类似，都是在现有CNN（2D， 3D）模型中加入一个设计的模块</li></ul><h3 id="CPNet-介绍"><a href="#CPNet-介绍" class="headerlink" title="CPNet 介绍"></a>CPNet 介绍</h3><p>（1）在CNN的某一层，得到了<code>T*H*W*d</code>的特征，这<code>T*H*W</code>个特征，是经过conv来的，即一个特征，返回到原图对应的是一个块（区域）的特征。   </p><p>（2）类似于graph 中的邻接矩阵的操作，计算这个<code>T*H*W</code>个节点之间的相似性，相似性近的前K个（且不在同一帧），认为他们之间存在对应关系，即找到了一个区域对应到其他帧的对应区域。   </p><p>（3）将原区域，与对应区域的特征，与他们之间的位置关系，输入到MLP中，得到了一个更新的特征。对每个对应区域都采取这样的操作，得到K个特征。取max，得到了一个鲁棒的特征（可以去掉不是对应块区域的特征，即去掉噪声）。   </p><img src="https://i.loli.net/2019/07/26/5d3a7afee6b0178187.png" alt="搜狗截图20190726120054.png" title="搜狗截图20190726120054.png"><ul><li>是不是跟Non-Local很像，==CP Module就是融合了相似区域的特征，对原区域的特征进行更新。==</li></ul><h3 id="Non-local-vs-CPNet"><a href="#Non-local-vs-CPNet" class="headerlink" title="Non-local  vs   CPNet"></a>Non-local  vs   CPNet</h3><ul><li><p>在toy dataset （figure4）上设计了toy model（两层 CNN）,将现有的三个SOTA model以及自己设计的CPNet上进行试验</p></li><li><p>可以看到 I3D，ARTNet ，TRN三个模型的效果都不是很好</p></li><li><p>ARTNet ，TRN 是由于只使用了两个卷积层，不能捕捉长范围的运动信息</p></li><li><p>Non-local 可以捕捉长范围的运动信息，但是为什么效果还是不好：==NL block 没有加进去位置信息==（作者这么说的原因，就是因为在他们的CP module中有position information）</p></li></ul><img src="https://i.loli.net/2019/07/26/5d3a6c44659ae30922.png" alt="搜狗截图20190726102546.png" title="搜狗截图20190726102546.png"><h3 id="CPNet-可以退化成这三个model-没看懂-之后可以结合代码再深入分析"><a href="#CPNet-可以退化成这三个model-没看懂-之后可以结合代码再深入分析" class="headerlink" title="CPNet 可以退化成这三个model: ( 没看懂 ,之后可以结合代码再深入分析)"></a>CPNet 可以退化成这三个model: ( 没看懂 ,之后可以结合代码再深入分析)</h3><img src="https://i.loli.net/2019/07/26/5d3a889e2ce0b69830.png" alt="搜狗截图20190726125811.png" title="搜狗截图20190726125811.png">]]></content>
      
      
      <categories>
          
          <category> 行为识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 行为识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Memory-Attended Recurrent Network for Video Captioning</title>
      <link href="/2019/07/25/Memory-Attended-Recurrent-Network-for-Video-Captioning/"/>
      <url>/2019/07/25/Memory-Attended-Recurrent-Network-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="词频"><a href="#词频" class="headerlink" title="词频"></a>词频</h3><p>“&gt; =3”的保留</p><p>MSR-VTT :11K   MSVD:4K</p><h3 id="Attention-Decoder"><a href="#Attention-Decoder" class="headerlink" title="Attention Decoder"></a>Attention Decoder</h3><ul><li><p>采用SA-LSTM的结构</p></li><li><p>细节：</p><ul><li><p>==共享attention==<br>由于需要对frames_feature ==(L帧)==  与 C3D_feature ==（L帧 -&gt; L/16个特征向量）== 都进行attention，这里进行了共享attention，好处：   <br>  </p><p>（1）将2D 和 3D 特征映射到相似的特征空间  <br></p><p>（2）像是一种正则化，减少了参数，避免过拟合  <br>  </p></li><li><p>降维<br>将2D 和 3D 的2048维度的特征，降维到512</p></li></ul></li></ul><h3 id="Attended-Memory-Decoder"><a href="#Attended-Memory-Decoder" class="headerlink" title="Attended Memory Decoder"></a>Attended Memory Decoder</h3><ul><li><p>当前模型的不足：</p><ul><li>现有的模型在生成word的时候，只依赖于当前video的信息，而不能依赖于那些，出现过该单词的其他video的信息</li><li>生成下一个单词，仅依赖于video信息和当前单词，没有建模相邻两个单词之间的兼容性（没看懂）</li></ul></li><li><p>具体的memeory设计详见论文</p></li></ul><h3 id="Attention-Coherent-Loss-AC-Loss"><a href="#Attention-Coherent-Loss-AC-Loss" class="headerlink" title="Attention-Coherent Loss (AC Loss)"></a>Attention-Coherent Loss (AC Loss)</h3><ul><li>将C3D 输入的L帧作为1个time interval,希望对一个time interval 中的frames feature 的attention系数值相近</li><li>仅对frames_features 的attention 系数，计算这样的一个loss</li></ul><img src="https://i.loli.net/2019/07/25/5d397582d36f640160.png" alt="搜狗截图20190725172350.png" title="搜狗截图20190725172350.png">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>深度层次化图卷积神经网络</title>
      <link href="/2019/07/25/%E6%B7%B1%E5%BA%A6%E5%B1%82%E6%AC%A1%E5%8C%96%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2019/07/25/%E6%B7%B1%E5%BA%A6%E5%B1%82%E6%AC%A1%E5%8C%96%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h3 id="池化-可以扩大感受野"><a href="#池化-可以扩大感受野" class="headerlink" title="池化 可以扩大感受野"></a>池化 可以扩大感受野</h3><h3 id="GCN（两层）-node-classification"><a href="#GCN（两层）-node-classification" class="headerlink" title="GCN（两层）( node classification )"></a>GCN（两层）( node classification )</h3><ul><li>基于邻域聚合的</li><li><code>H= AXW</code><h3 id="deeper-insight-into-graph-convolutional-networks-for-semi-supervised-learning"><a href="#deeper-insight-into-graph-convolutional-networks-for-semi-supervised-learning" class="headerlink" title="deeper insight into graph convolutional networks for semi-supervised learning"></a>deeper insight into graph convolutional networks for semi-supervised learning</h3></li><li>GCN层数多效果不好：特征之间过于平滑<h3 id="GAT-（两层）-GraphSAGE"><a href="#GAT-（两层）-GraphSAGE" class="headerlink" title="GAT （两层）  GraphSAGE"></a>GAT （两层）  GraphSAGE</h3></li><li>两层，感受野小，2-hop</li></ul><h3 id="Hierarchical-Graph-Representation-Learning-with-Differentiable-Pooling-（graph-classification）"><a href="#Hierarchical-Graph-Representation-Learning-with-Differentiable-Pooling-（graph-classification）" class="headerlink" title="Hierarchical Graph Representation Learning with Differentiable Pooling （graph classification）"></a>Hierarchical Graph Representation Learning with Differentiable Pooling （graph classification）</h3><ol><li>优点</li></ol><ul><li>简单的两层GCN 的感受野只有2-hop</li><li>但是如果GCN- clusterpooling，把相同的节点聚类在一起，再进行GCN，那么感受野就会扩大，</li><li>捕捉到了graph 中的Hierarchical  structure</li></ul><ol start="2"><li>缺点</li></ol><ul><li>但是由于他自身网络设计的，一次池化，就需要一个全连接层，使得想要设计一个很深的网络，就需要很多的参数，容易过拟合</li><li>很难去训练pooling matrix，这是由于不能保证，经过这一个池化层，就可以把相似的objects聚类到一起。本文作者在每层都增加了两个正则项</li></ul><h3 id="Hierarchical-Graph-Convolutional-Networks-for-Semi-supervised-Node-Classification-9层-IJCAI-2019"><a href="#Hierarchical-Graph-Convolutional-Networks-for-Semi-supervised-Node-Classification-9层-IJCAI-2019" class="headerlink" title="Hierarchical Graph Convolutional Networks for Semi-supervised Node Classification ( 9层 ) ( IJCAI 2019)"></a>Hierarchical Graph Convolutional Networks for Semi-supervised Node Classification ( 9层 ) ( IJCAI 2019)</h3><ul><li>粗化 coarsening</li><li>结构一致粗化</li><li>结构相似粗化</li></ul>]]></content>
      
      
      <categories>
          
          <category> 图卷积网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图卷积网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hierarchical Global-Local Temporal Modeling for Video Captioning</title>
      <link href="/2019/07/23/Hierarchical-Global-Local-Temporal-Modeling-for-Video-Captioning/"/>
      <url>/2019/07/23/Hierarchical-Global-Local-Temporal-Modeling-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>使用object features能够更好地检测出action 和 关键的Object</li></ul><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul><li><p>两个LSTM层</p></li><li><p>global : frame features and C3D features</p></li><li><p>local : objects </p></li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul><li><p>Top Down decoder</p><ul><li>Bottom LSTM：mean of regions</li><li>Top LSTM : attention of  regions</li></ul></li><li><p>Grounded video description的decoder：</p><ul><li>Bottom LSTM：mean of  fc+motion</li><li>Top LSTM: attention of  regions and attention of  fc+motion</li></ul></li><li><p>==Hierarchical Global-Local Temporal Modeling（本文） ==</p><ul><li>Bottom LSTM：attention of fc+motion</li><li>Top LSTM: attention of regions</li><li>本文不一样的地方是在Bottom LSTM的输入也加入了attention</li></ul></li></ul><h3 id="提取特征"><a href="#提取特征" class="headerlink" title="提取特征"></a>提取特征</h3><ul><li>等间隔提取帧的特征</li><li>由于帧之间的间隔，会使得没有运动信息，所以再使用C3D来补充运动信息（以该该为中心，提取16帧，输入C3D中）</li><li>object features: faster rcnn 去掉rcnn部分的类别/分数预测，提取head_to_heal处的pooled_feats</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
            <tag> [object Object] </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VATEX: a video caption dataset</title>
      <link href="/2019/07/23/VATEX-a-video-caption-dataset/"/>
      <url>/2019/07/23/VATEX-a-video-caption-dataset/</url>
      
        <content type="html"><![CDATA[<h2 id="VATEX数据集"><a href="#VATEX数据集" class="headerlink" title="VATEX数据集"></a>VATEX数据集</h2><ul><li>一个新的数据集，41269个video， 时长大约10s, 每个video有10个中文，10个英文，同时这10个之中，中英文之间有5个是两两配对的</li><li>提出了两个新的任务：（1）一个encoder-decoder模型，在两种语言之间共享参数，即希望一个模型，可以得到两种语言的描述。（2）提出了一种新的机器翻译任务，即当进行中英文的机器翻译任务时，可以添加视频的视觉特征作为辅助信息</li><li>数据集的来源：来自于kinetics的validation dataset, 然后它们找人进行了caption的标注。它们将这41269个video 分成了4部分，train, validation, public test, secret test(不公开，用于比赛)</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g8e7blhuobj30js0lfwkf.jpg" alt="搜狗截图20191028204431.png"></p><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><ul><li>encoder-decoder 就是 TopDown的形式</li><li>视觉特征：通过I3D（在kinetics train上预训练且不再fine-tune）来提取视觉特征，应该是把video分成了很多segments，对每个segment都提取I3D的特征，每个特征作为vi。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>faster_rcnn various box head</title>
      <link href="/2019/07/22/faster-rcnn-various-box-head/"/>
      <url>/2019/07/22/faster-rcnn-various-box-head/</url>
      
        <content type="html"><![CDATA[<h4 id="Ground-video-description"><a href="#Ground-video-description" class="headerlink" title="Ground video description"></a>Ground video description</h4><ul><li>在阅读这篇论文的时候，由于作者提取了objects，说是提取的fc6的特征，但是不太懂是哪里，在issue中，他说是借鉴这里的代码，于是乎，我就来看了看<a href="https://github.com/facebookresearch/Detectron/blob/8170b25b425967f8f1c7d715bea3c5b8d9536cd8/detectron/modeling/fast_rcnn_heads.py" target="_blank" rel="noopener">box_head</a>，哈哈哈哈哈哈 </li><li>fc6 是 box_head里边的，box_head就是 类似于faster_rcnn中的_head_to_tail</li></ul><h4 id="那么box-head-是干嘛的？"><a href="#那么box-head-是干嘛的？" class="headerlink" title="那么box_head 是干嘛的？"></a>那么box_head 是干嘛的？</h4><ul><li>由于经过roi_pooling 之后得到的是 7*7的一个pooled_feats，还要 ==再进行池化或者拍平，或者再进行全连接层等== ，以便于后边的预测，分类任务。</li></ul><ul><li><p>faster_rcnn 中的box_head就是 resnet layer4</p></li><li><p>mmdetection 中的 faster_rcnn 现将7*7  排成49 ，再送入两个全连接层，可以将这两个全连接层命名为fc6, fc7.   完美!！!！</p></li><li><p>这里展示了各种各样的 box_head</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mmdetection的configs中的各项参数具体解释</title>
      <link href="/2019/07/21/mmdetection%E7%9A%84configs%E4%B8%AD%E7%9A%84%E5%90%84%E9%A1%B9%E5%8F%82%E6%95%B0%E5%85%B7%E4%BD%93%E8%A7%A3%E9%87%8A/"/>
      <url>/2019/07/21/mmdetection%E7%9A%84configs%E4%B8%AD%E7%9A%84%E5%90%84%E9%A1%B9%E5%8F%82%E6%95%B0%E5%85%B7%E4%BD%93%E8%A7%A3%E9%87%8A/</url>
      
        <content type="html"><![CDATA[<p>一、简介<br>在使用mmdetection对模型进行调优的过程中总会遇到很多参数的问题，不知道参数在代码中是什么作用，会对训练产生怎样的影响，这里我以faster_rcnn_r50_fpn_1x.py和cascade_rcnn_r50_fpn_1x.py为例，简单介绍一下mmdetection中的各项参数含义</p><p>二、faster_rcnn_r50_fpn_1x.py配置文件<br>首先介绍一下这个配置文件所描述的框架，它是基于resnet50的backbone，有着5个fpn特征层的faster-RCNN目标检测网络，训练迭代次数为标准的12次epoch，下面逐条解释其含义</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model settings</span></span><br><span class="line">model = dict(</span><br><span class="line">type=<span class="string">'FasterRCNN'</span>,                         <span class="comment"># model类型</span></span><br><span class="line">    pretrained=<span class="string">'modelzoo://resnet50'</span>,          <span class="comment"># 预训练模型：imagenet-resnet50</span></span><br><span class="line">    backbone=dict(</span><br><span class="line">        type=<span class="string">'ResNet'</span>,                         <span class="comment"># backbone类型</span></span><br><span class="line">        depth=<span class="number">50</span>,                              <span class="comment"># 网络层数</span></span><br><span class="line">        num_stages=<span class="number">4</span>,                          <span class="comment"># resnet的stage数量</span></span><br><span class="line">        out_indices=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>),              <span class="comment"># 输出的stage的序号</span></span><br><span class="line">        frozen_stages=<span class="number">1</span>,                       <span class="comment"># 冻结的stage数量，即该stage不更新参数，-1表示所有的stage都更新参数</span></span><br><span class="line">        style=<span class="string">'pytorch'</span>),                      <span class="comment"># 网络风格：如果设置pytorch，则stride为2的层是conv3x3的卷积层；如果设置caffe，则stride为2的层是第一个conv1x1的卷积层</span></span><br><span class="line">    neck=dict(</span><br><span class="line">        type=<span class="string">'FPN'</span>,                            <span class="comment"># neck类型</span></span><br><span class="line">        in_channels=[<span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>, <span class="number">2048</span>],    <span class="comment"># 输入的各个stage的通道数</span></span><br><span class="line">        out_channels=<span class="number">256</span>,                      <span class="comment"># 输出的特征层的通道数</span></span><br><span class="line">        num_outs=<span class="number">5</span>),                           <span class="comment"># 输出的特征层的数量</span></span><br><span class="line">    rpn_head=dict(</span><br><span class="line">        type=<span class="string">'RPNHead'</span>,                        <span class="comment"># RPN网络类型</span></span><br><span class="line">        in_channels=<span class="number">256</span>,                       <span class="comment"># RPN网络的输入通道数</span></span><br><span class="line">        feat_channels=<span class="number">256</span>,                     <span class="comment"># 特征层的通道数</span></span><br><span class="line">        anchor_scales=[<span class="number">8</span>],                     <span class="comment"># 生成的anchor的baselen，baselen = sqrt(w*h)，w和h为anchor的宽和高</span></span><br><span class="line">        anchor_ratios=[<span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span>],         <span class="comment"># anchor的宽高比</span></span><br><span class="line">        anchor_strides=[<span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>],     <span class="comment"># 在每个特征层上的anchor的步长（对应于原图）</span></span><br><span class="line">        target_means=[<span class="number">.0</span>, <span class="number">.0</span>, <span class="number">.0</span>, <span class="number">.0</span>],         <span class="comment"># 均值</span></span><br><span class="line">        target_stds=[<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>],      <span class="comment"># 方差</span></span><br><span class="line">        use_sigmoid_cls=<span class="literal">True</span>),                 <span class="comment"># 是否使用sigmoid来进行分类，如果False则使用softmax来分类</span></span><br><span class="line">    bbox_roi_extractor=dict(</span><br><span class="line">        type=<span class="string">'SingleRoIExtractor'</span>,                                   <span class="comment"># RoIExtractor类型</span></span><br><span class="line">        roi_layer=dict(type=<span class="string">'RoIAlign'</span>, out_size=<span class="number">7</span>, sample_num=<span class="number">2</span>),   <span class="comment"># ROI具体参数：ROI类型为ROIalign，输出尺寸为7，sample数为2</span></span><br><span class="line">        out_channels=<span class="number">256</span>,                                            <span class="comment"># 输出通道数</span></span><br><span class="line">        featmap_strides=[<span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>]),                             <span class="comment"># 特征图的步长</span></span><br><span class="line">    bbox_head=dict(</span><br><span class="line">        type=<span class="string">'SharedFCBBoxHead'</span>,                     <span class="comment"># 全连接层类型</span></span><br><span class="line">        num_fcs=<span class="number">2</span>,                                   <span class="comment"># 全连接层数量</span></span><br><span class="line">        in_channels=<span class="number">256</span>,                             <span class="comment"># 输入通道数</span></span><br><span class="line">        fc_out_channels=<span class="number">1024</span>,                        <span class="comment"># 输出通道数</span></span><br><span class="line">        roi_feat_size=<span class="number">7</span>,                             <span class="comment"># ROI特征层尺寸</span></span><br><span class="line">        num_classes=<span class="number">81</span>,                              <span class="comment"># 分类器的类别数量+1，+1是因为多了一个背景的类别</span></span><br><span class="line">        target_means=[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],               <span class="comment"># 均值</span></span><br><span class="line">        target_stds=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.2</span>],            <span class="comment"># 方差</span></span><br><span class="line">        reg_class_agnostic=<span class="literal">False</span>))                   <span class="comment"># 是否采用class_agnostic的方式来预测，class_agnostic表示输出bbox时只考虑其是否为前景，后续分类的时候再根据该bbox在网络中的类别得分来分类，也就是说一个框可以对应多个类别</span></span><br><span class="line"><span class="comment"># model training and testing settings</span></span><br><span class="line">train_cfg = dict(</span><br><span class="line">    rpn=dict(</span><br><span class="line">        assigner=dict(</span><br><span class="line">            type=<span class="string">'MaxIoUAssigner'</span>,            <span class="comment"># RPN网络的正负样本划分</span></span><br><span class="line">            pos_iou_thr=<span class="number">0.7</span>,                  <span class="comment"># 正样本的iou阈值</span></span><br><span class="line">            neg_iou_thr=<span class="number">0.3</span>,                  <span class="comment"># 负样本的iou阈值</span></span><br><span class="line">            min_pos_iou=<span class="number">0.3</span>,                  <span class="comment"># 正样本的iou最小值。如果assign给ground truth的anchors中最大的IOU低于0.3，则忽略所有的anchors，否则保留最大IOU的anchor</span></span><br><span class="line">            ignore_iof_thr=<span class="number">-1</span>),               <span class="comment"># 忽略bbox的阈值，当ground truth中包含需要忽略的bbox时使用，-1表示不忽略</span></span><br><span class="line">        sampler=dict(</span><br><span class="line">            type=<span class="string">'RandomSampler'</span>,             <span class="comment"># 正负样本提取器类型</span></span><br><span class="line">            num=<span class="number">256</span>,                          <span class="comment"># 需提取的正负样本数量</span></span><br><span class="line">            pos_fraction=<span class="number">0.5</span>,                 <span class="comment"># 正样本比例</span></span><br><span class="line">            neg_pos_ub=<span class="number">-1</span>,                    <span class="comment"># 最大负样本比例，大于该比例的负样本忽略，-1表示不忽略</span></span><br><span class="line">            add_gt_as_proposals=<span class="literal">False</span>),       <span class="comment"># 把ground truth加入proposal作为正样本</span></span><br><span class="line">        allowed_border=<span class="number">0</span>,                     <span class="comment"># 允许在bbox周围外扩一定的像素</span></span><br><span class="line">        pos_weight=<span class="number">-1</span>,                        <span class="comment"># 正样本权重，-1表示不改变原始的权重</span></span><br><span class="line">        smoothl1_beta=<span class="number">1</span> / <span class="number">9.0</span>,                <span class="comment"># 平滑L1系数</span></span><br><span class="line">        debug=<span class="literal">False</span>),                         <span class="comment"># debug模式</span></span><br><span class="line">    rcnn=dict(</span><br><span class="line">        assigner=dict(</span><br><span class="line">            type=<span class="string">'MaxIoUAssigner'</span>,            <span class="comment"># RCNN网络正负样本划分</span></span><br><span class="line">            pos_iou_thr=<span class="number">0.5</span>,                  <span class="comment"># 正样本的iou阈值</span></span><br><span class="line">            neg_iou_thr=<span class="number">0.5</span>,                  <span class="comment"># 负样本的iou阈值</span></span><br><span class="line">            min_pos_iou=<span class="number">0.5</span>,                  <span class="comment"># 正样本的iou最小值。如果assign给ground truth的anchors中最大的IOU低于0.3，则忽略所有的anchors，否则保留最大IOU的anchor</span></span><br><span class="line">            ignore_iof_thr=<span class="number">-1</span>),               <span class="comment"># 忽略bbox的阈值，当ground truth中包含需要忽略的bbox时使用，-1表示不忽略</span></span><br><span class="line">        sampler=dict(</span><br><span class="line">            type=<span class="string">'RandomSampler'</span>,             <span class="comment"># 正负样本提取器类型</span></span><br><span class="line">            num=<span class="number">512</span>,                          <span class="comment"># 需提取的正负样本数量</span></span><br><span class="line">            pos_fraction=<span class="number">0.25</span>,                <span class="comment"># 正样本比例</span></span><br><span class="line">            neg_pos_ub=<span class="number">-1</span>,                    <span class="comment"># 最大负样本比例，大于该比例的负样本忽略，-1表示不忽略</span></span><br><span class="line">            add_gt_as_proposals=<span class="literal">True</span>),        <span class="comment"># 把ground truth加入proposal作为正样本</span></span><br><span class="line">        pos_weight=<span class="number">-1</span>,                        <span class="comment"># 正样本权重，-1表示不改变原始的权重</span></span><br><span class="line">        debug=<span class="literal">False</span>))                         <span class="comment"># debug模式</span></span><br><span class="line">test_cfg = dict(</span><br><span class="line">    rpn=dict(                                 <span class="comment"># 推断时的RPN参数</span></span><br><span class="line">        nms_across_levels=<span class="literal">False</span>,              <span class="comment"># 在所有的fpn层内做nms</span></span><br><span class="line">        nms_pre=<span class="number">2000</span>,                         <span class="comment"># 在nms之前保留的的得分最高的proposal数量</span></span><br><span class="line">        nms_post=<span class="number">2000</span>,                        <span class="comment"># 在nms之后保留的的得分最高的proposal数量</span></span><br><span class="line">        max_num=<span class="number">2000</span>,                         <span class="comment"># 在后处理完成之后保留的proposal数量</span></span><br><span class="line">        nms_thr=<span class="number">0.7</span>,                          <span class="comment"># nms阈值</span></span><br><span class="line">        min_bbox_size=<span class="number">0</span>),                     <span class="comment"># 最小bbox尺寸</span></span><br><span class="line">    rcnn=dict(</span><br><span class="line">        score_thr=<span class="number">0.05</span>, nms=dict(type=<span class="string">'nms'</span>, iou_thr=<span class="number">0.5</span>), max_per_img=<span class="number">100</span>)   <span class="comment"># max_per_img表示最终输出的det bbox数量</span></span><br><span class="line">    <span class="comment"># soft-nms is also supported for rcnn testing</span></span><br><span class="line">    <span class="comment"># e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)            # soft_nms参数</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># dataset settings</span></span><br><span class="line">dataset_type = <span class="string">'CocoDataset'</span>                <span class="comment"># 数据集类型</span></span><br><span class="line">data_root = <span class="string">'data/coco/'</span>                    <span class="comment"># 数据集根目录</span></span><br><span class="line">img_norm_cfg = dict(</span><br><span class="line">    mean=[<span class="number">123.675</span>, <span class="number">116.28</span>, <span class="number">103.53</span>], std=[<span class="number">58.395</span>, <span class="number">57.12</span>, <span class="number">57.375</span>], to_rgb=<span class="literal">True</span>)   <span class="comment"># 输入图像初始化，减去均值mean并处以方差std，to_rgb表示将bgr转为rgb</span></span><br><span class="line">data = dict(</span><br><span class="line">    imgs_per_gpu=<span class="number">2</span>,                <span class="comment"># 每个gpu计算的图像数量</span></span><br><span class="line">    workers_per_gpu=<span class="number">2</span>,             <span class="comment"># 每个gpu分配的线程数</span></span><br><span class="line">    train=dict(</span><br><span class="line">        type=dataset_type,                                                 <span class="comment"># 数据集类型</span></span><br><span class="line">        ann_file=data_root + <span class="string">'annotations/instances_train2017.json'</span>,       <span class="comment"># 数据集annotation路径</span></span><br><span class="line">        img_prefix=data_root + <span class="string">'train2017/'</span>,                               <span class="comment"># 数据集的图片路径</span></span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),                                             <span class="comment"># 输入图像尺寸，最大边1333，最小边800</span></span><br><span class="line">        img_norm_cfg=img_norm_cfg,                                         <span class="comment"># 图像初始化参数</span></span><br><span class="line">        size_divisor=<span class="number">32</span>,                                                   <span class="comment"># 对图像进行resize时的最小单位，32表示所有的图像都会被resize成32的倍数</span></span><br><span class="line">        flip_ratio=<span class="number">0.5</span>,                                                    <span class="comment"># 图像的随机左右翻转的概率</span></span><br><span class="line">        with_mask=<span class="literal">False</span>,                                                   <span class="comment"># 训练时附带mask</span></span><br><span class="line">        with_crowd=<span class="literal">True</span>,                                                   <span class="comment"># 训练时附带difficult的样本</span></span><br><span class="line">        with_label=<span class="literal">True</span>),                                                  <span class="comment"># 训练时附带label</span></span><br><span class="line">    val=dict(</span><br><span class="line">        type=dataset_type,                                                 <span class="comment"># 同上</span></span><br><span class="line">        ann_file=data_root + <span class="string">'annotations/instances_val2017.json'</span>,         <span class="comment"># 同上</span></span><br><span class="line">        img_prefix=data_root + <span class="string">'val2017/'</span>,                                 <span class="comment"># 同上</span></span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),                                             <span class="comment"># 同上</span></span><br><span class="line">        img_norm_cfg=img_norm_cfg,                                         <span class="comment"># 同上</span></span><br><span class="line">        size_divisor=<span class="number">32</span>,                                                   <span class="comment"># 同上</span></span><br><span class="line">        flip_ratio=<span class="number">0</span>,                                                      <span class="comment"># 同上</span></span><br><span class="line">        with_mask=<span class="literal">False</span>,                                                   <span class="comment"># 同上</span></span><br><span class="line">        with_crowd=<span class="literal">True</span>,                                                   <span class="comment"># 同上</span></span><br><span class="line">        with_label=<span class="literal">True</span>),                                                  <span class="comment"># 同上</span></span><br><span class="line">    test=dict(</span><br><span class="line">        type=dataset_type,                                                 <span class="comment"># 同上</span></span><br><span class="line">        ann_file=data_root + <span class="string">'annotations/instances_val2017.json'</span>,         <span class="comment"># 同上</span></span><br><span class="line">        img_prefix=data_root + <span class="string">'val2017/'</span>,                                 <span class="comment"># 同上</span></span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),                                             <span class="comment"># 同上</span></span><br><span class="line">        img_norm_cfg=img_norm_cfg,                                         <span class="comment"># 同上</span></span><br><span class="line">        size_divisor=<span class="number">32</span>,                                                   <span class="comment"># 同上</span></span><br><span class="line">        flip_ratio=<span class="number">0</span>,                                                      <span class="comment"># 同上</span></span><br><span class="line">        with_mask=<span class="literal">False</span>,                                                   <span class="comment"># 同上</span></span><br><span class="line">        with_label=<span class="literal">False</span>,                                                  <span class="comment"># 同上</span></span><br><span class="line">        test_mode=<span class="literal">True</span>))                                                   <span class="comment"># 同上</span></span><br><span class="line"><span class="comment"># optimizer</span></span><br><span class="line">optimizer = dict(type=<span class="string">'SGD'</span>, lr=<span class="number">0.02</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.0001</span>)   <span class="comment"># 优化参数，lr为学习率，momentum为动量因子，weight_decay为权重衰减因子</span></span><br><span class="line">optimizer_config = dict(grad_clip=dict(max_norm=<span class="number">35</span>, norm_type=<span class="number">2</span>))          <span class="comment"># 梯度均衡参数</span></span><br><span class="line"><span class="comment"># learning policy</span></span><br><span class="line">lr_config = dict(</span><br><span class="line">    policy=<span class="string">'step'</span>,                        <span class="comment"># 优化策略</span></span><br><span class="line">    warmup=<span class="string">'linear'</span>,                      <span class="comment"># 初始的学习率增加的策略，linear为线性增加</span></span><br><span class="line">    warmup_iters=<span class="number">500</span>,                     <span class="comment"># 在初始的500次迭代中学习率逐渐增加</span></span><br><span class="line">    warmup_ratio=<span class="number">1.0</span> / <span class="number">3</span>,                 <span class="comment"># 起始的学习率</span></span><br><span class="line">    step=[<span class="number">8</span>, <span class="number">11</span>])                         <span class="comment"># 在第8和11个epoch时降低学习率</span></span><br><span class="line">checkpoint_config = dict(interval=<span class="number">1</span>)      <span class="comment"># 每1个epoch存储一次模型</span></span><br><span class="line"><span class="comment"># yapf:disable</span></span><br><span class="line">log_config = dict(</span><br><span class="line">    interval=<span class="number">50</span>,                          <span class="comment"># 每50个batch输出一次信息</span></span><br><span class="line">    hooks=[</span><br><span class="line">        dict(type=<span class="string">'TextLoggerHook'</span>),      <span class="comment"># 控制台输出信息的风格</span></span><br><span class="line">        <span class="comment"># dict(type='TensorboardLoggerHook')</span></span><br><span class="line">    ])</span><br><span class="line"><span class="comment"># yapf:enable</span></span><br><span class="line"><span class="comment"># runtime settings</span></span><br><span class="line">total_epochs = <span class="number">12</span>                               <span class="comment"># 最大epoch数</span></span><br><span class="line">dist_params = dict(backend=<span class="string">'nccl'</span>)              <span class="comment"># 分布式参数</span></span><br><span class="line">log_level = <span class="string">'INFO'</span>                              <span class="comment"># 输出信息的完整度级别</span></span><br><span class="line">work_dir = <span class="string">'./work_dirs/faster_rcnn_r50_fpn_1x'</span> <span class="comment"># log文件和模型文件存储路径</span></span><br><span class="line">load_from = <span class="literal">None</span>                                <span class="comment"># 加载模型的路径，None表示从预训练模型加载</span></span><br><span class="line">resume_from = <span class="literal">None</span>                              <span class="comment"># 恢复训练模型的路径</span></span><br><span class="line">workflow = [(<span class="string">'train'</span>, <span class="number">1</span>)]                       <span class="comment"># 当前工作区名称</span></span><br></pre></td></tr></table></figure><p> 三、cascade_rcnn_r50_fpn_1x.py配置文件<br>cascade-RCNN是cvpr2018的文章，相比于faster-RCNN的改进主要在于其RCNN有三个stage，这三个stage逐级refine检测的结果，使得结果达到更高的精度。下面逐条解释其config的含义，与faster-RCNN相同的部分就不再赘述。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model settings</span></span><br><span class="line">model = dict(</span><br><span class="line">    type=<span class="string">'CascadeRCNN'</span>,</span><br><span class="line">    num_stages=<span class="number">3</span>,                     <span class="comment"># RCNN网络的stage数量，在faster-RCNN中为1</span></span><br><span class="line">    pretrained=<span class="string">'modelzoo://resnet50'</span>,</span><br><span class="line">    backbone=dict(</span><br><span class="line">        type=<span class="string">'ResNet'</span>,</span><br><span class="line">        depth=<span class="number">50</span>,</span><br><span class="line">        num_stages=<span class="number">4</span>,</span><br><span class="line">        out_indices=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>),</span><br><span class="line">        frozen_stages=<span class="number">1</span>,</span><br><span class="line">        style=<span class="string">'pytorch'</span>),</span><br><span class="line">    neck=dict(</span><br><span class="line">        type=<span class="string">'FPN'</span>,</span><br><span class="line">        in_channels=[<span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>, <span class="number">2048</span>],</span><br><span class="line">        out_channels=<span class="number">256</span>,</span><br><span class="line">        num_outs=<span class="number">5</span>),</span><br><span class="line">    rpn_head=dict(</span><br><span class="line">        type=<span class="string">'RPNHead'</span>,</span><br><span class="line">        in_channels=<span class="number">256</span>,</span><br><span class="line">        feat_channels=<span class="number">256</span>,</span><br><span class="line">        anchor_scales=[<span class="number">8</span>],</span><br><span class="line">        anchor_ratios=[<span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span>],</span><br><span class="line">        anchor_strides=[<span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>],</span><br><span class="line">        target_means=[<span class="number">.0</span>, <span class="number">.0</span>, <span class="number">.0</span>, <span class="number">.0</span>],</span><br><span class="line">        target_stds=[<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>],</span><br><span class="line">        use_sigmoid_cls=<span class="literal">True</span>),</span><br><span class="line">    bbox_roi_extractor=dict(</span><br><span class="line">        type=<span class="string">'SingleRoIExtractor'</span>,</span><br><span class="line">        roi_layer=dict(type=<span class="string">'RoIAlign'</span>, out_size=<span class="number">7</span>, sample_num=<span class="number">2</span>),</span><br><span class="line">        out_channels=<span class="number">256</span>,</span><br><span class="line">        featmap_strides=[<span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>]),</span><br><span class="line">    bbox_head=[</span><br><span class="line">        dict(</span><br><span class="line">            type=<span class="string">'SharedFCBBoxHead'</span>,</span><br><span class="line">            num_fcs=<span class="number">2</span>,</span><br><span class="line">            in_channels=<span class="number">256</span>,</span><br><span class="line">            fc_out_channels=<span class="number">1024</span>,</span><br><span class="line">            roi_feat_size=<span class="number">7</span>,</span><br><span class="line">            num_classes=<span class="number">81</span>,</span><br><span class="line">            target_means=[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            target_stds=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.2</span>],</span><br><span class="line">            reg_class_agnostic=<span class="literal">True</span>),</span><br><span class="line">        dict(</span><br><span class="line">            type=<span class="string">'SharedFCBBoxHead'</span>,</span><br><span class="line">            num_fcs=<span class="number">2</span>,</span><br><span class="line">            in_channels=<span class="number">256</span>,</span><br><span class="line">            fc_out_channels=<span class="number">1024</span>,</span><br><span class="line">            roi_feat_size=<span class="number">7</span>,</span><br><span class="line">            num_classes=<span class="number">81</span>,</span><br><span class="line">            target_means=[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            target_stds=[<span class="number">0.05</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.1</span>],</span><br><span class="line">            reg_class_agnostic=<span class="literal">True</span>),</span><br><span class="line">        dict(</span><br><span class="line">            type=<span class="string">'SharedFCBBoxHead'</span>,</span><br><span class="line">            num_fcs=<span class="number">2</span>,</span><br><span class="line">            in_channels=<span class="number">256</span>,</span><br><span class="line">            fc_out_channels=<span class="number">1024</span>,</span><br><span class="line">            roi_feat_size=<span class="number">7</span>,</span><br><span class="line">            num_classes=<span class="number">81</span>,</span><br><span class="line">            target_means=[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            target_stds=[<span class="number">0.033</span>, <span class="number">0.033</span>, <span class="number">0.067</span>, <span class="number">0.067</span>],</span><br><span class="line">            reg_class_agnostic=<span class="literal">True</span>)</span><br><span class="line">    ])</span><br><span class="line"><span class="comment"># model training and testing settings</span></span><br><span class="line">train_cfg = dict(</span><br><span class="line">    rpn=dict(</span><br><span class="line">        assigner=dict(</span><br><span class="line">            type=<span class="string">'MaxIoUAssigner'</span>,</span><br><span class="line">            pos_iou_thr=<span class="number">0.7</span>,</span><br><span class="line">            neg_iou_thr=<span class="number">0.3</span>,</span><br><span class="line">            min_pos_iou=<span class="number">0.3</span>,</span><br><span class="line">            ignore_iof_thr=<span class="number">-1</span>),</span><br><span class="line">        sampler=dict(</span><br><span class="line">            type=<span class="string">'RandomSampler'</span>,</span><br><span class="line">            num=<span class="number">256</span>,</span><br><span class="line">            pos_fraction=<span class="number">0.5</span>,</span><br><span class="line">            neg_pos_ub=<span class="number">-1</span>,</span><br><span class="line">            add_gt_as_proposals=<span class="literal">False</span>),</span><br><span class="line">        allowed_border=<span class="number">0</span>,</span><br><span class="line">        pos_weight=<span class="number">-1</span>,</span><br><span class="line">        smoothl1_beta=<span class="number">1</span> / <span class="number">9.0</span>,</span><br><span class="line">        debug=<span class="literal">False</span>),</span><br><span class="line">    rcnn=[                    <span class="comment"># 注意，这里有3个RCNN的模块，对应开头的那个RCNN的stage数量</span></span><br><span class="line">        dict(</span><br><span class="line">            assigner=dict(</span><br><span class="line">                type=<span class="string">'MaxIoUAssigner'</span>,</span><br><span class="line">                pos_iou_thr=<span class="number">0.5</span>,</span><br><span class="line">                neg_iou_thr=<span class="number">0.5</span>,</span><br><span class="line">                min_pos_iou=<span class="number">0.5</span>,</span><br><span class="line">                ignore_iof_thr=<span class="number">-1</span>),</span><br><span class="line">            sampler=dict(</span><br><span class="line">                type=<span class="string">'RandomSampler'</span>,</span><br><span class="line">                num=<span class="number">512</span>,</span><br><span class="line">                pos_fraction=<span class="number">0.25</span>,</span><br><span class="line">                neg_pos_ub=<span class="number">-1</span>,</span><br><span class="line">                add_gt_as_proposals=<span class="literal">True</span>),</span><br><span class="line">            pos_weight=<span class="number">-1</span>,</span><br><span class="line">            debug=<span class="literal">False</span>),</span><br><span class="line">        dict(</span><br><span class="line">            assigner=dict(</span><br><span class="line">                type=<span class="string">'MaxIoUAssigner'</span>,</span><br><span class="line">                pos_iou_thr=<span class="number">0.6</span>,</span><br><span class="line">                neg_iou_thr=<span class="number">0.6</span>,</span><br><span class="line">                min_pos_iou=<span class="number">0.6</span>,</span><br><span class="line">                ignore_iof_thr=<span class="number">-1</span>),</span><br><span class="line">            sampler=dict(</span><br><span class="line">                type=<span class="string">'RandomSampler'</span>,</span><br><span class="line">                num=<span class="number">512</span>,</span><br><span class="line">                pos_fraction=<span class="number">0.25</span>,</span><br><span class="line">                neg_pos_ub=<span class="number">-1</span>,</span><br><span class="line">                add_gt_as_proposals=<span class="literal">True</span>),</span><br><span class="line">            pos_weight=<span class="number">-1</span>,</span><br><span class="line">            debug=<span class="literal">False</span>),</span><br><span class="line">        dict(</span><br><span class="line">            assigner=dict(</span><br><span class="line">                type=<span class="string">'MaxIoUAssigner'</span>,</span><br><span class="line">                pos_iou_thr=<span class="number">0.7</span>,</span><br><span class="line">                neg_iou_thr=<span class="number">0.7</span>,</span><br><span class="line">                min_pos_iou=<span class="number">0.7</span>,</span><br><span class="line">                ignore_iof_thr=<span class="number">-1</span>),</span><br><span class="line">            sampler=dict(</span><br><span class="line">                type=<span class="string">'RandomSampler'</span>,</span><br><span class="line">                num=<span class="number">512</span>,</span><br><span class="line">                pos_fraction=<span class="number">0.25</span>,</span><br><span class="line">                neg_pos_ub=<span class="number">-1</span>,</span><br><span class="line">                add_gt_as_proposals=<span class="literal">True</span>),</span><br><span class="line">            pos_weight=<span class="number">-1</span>,</span><br><span class="line">            debug=<span class="literal">False</span>)</span><br><span class="line">    ],</span><br><span class="line">    stage_loss_weights=[<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0.25</span>])     <span class="comment"># 3个RCNN的stage的loss权重</span></span><br><span class="line">test_cfg = dict(</span><br><span class="line">    rpn=dict(</span><br><span class="line">        nms_across_levels=<span class="literal">False</span>,</span><br><span class="line">        nms_pre=<span class="number">2000</span>,</span><br><span class="line">        nms_post=<span class="number">2000</span>,</span><br><span class="line">        max_num=<span class="number">2000</span>,</span><br><span class="line">        nms_thr=<span class="number">0.7</span>,</span><br><span class="line">        min_bbox_size=<span class="number">0</span>),</span><br><span class="line">    rcnn=dict(</span><br><span class="line">        score_thr=<span class="number">0.05</span>, nms=dict(type=<span class="string">'nms'</span>, iou_thr=<span class="number">0.5</span>), max_per_img=<span class="number">100</span>),</span><br><span class="line">    keep_all_stages=<span class="literal">False</span>)         <span class="comment"># 是否保留所有stage的结果</span></span><br><span class="line"><span class="comment"># dataset settings</span></span><br><span class="line">dataset_type = <span class="string">'CocoDataset'</span></span><br><span class="line">data_root = <span class="string">'data/coco/'</span></span><br><span class="line">img_norm_cfg = dict(</span><br><span class="line">    mean=[<span class="number">123.675</span>, <span class="number">116.28</span>, <span class="number">103.53</span>], std=[<span class="number">58.395</span>, <span class="number">57.12</span>, <span class="number">57.375</span>], to_rgb=<span class="literal">True</span>)</span><br><span class="line">data = dict(</span><br><span class="line">    imgs_per_gpu=<span class="number">2</span>,</span><br><span class="line">    workers_per_gpu=<span class="number">2</span>,</span><br><span class="line">    train=dict(</span><br><span class="line">        type=dataset_type,</span><br><span class="line">        ann_file=data_root + <span class="string">'annotations/instances_train2017.json'</span>,</span><br><span class="line">        img_prefix=data_root + <span class="string">'train2017/'</span>,</span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),</span><br><span class="line">        img_norm_cfg=img_norm_cfg,</span><br><span class="line">        size_divisor=<span class="number">32</span>,</span><br><span class="line">        flip_ratio=<span class="number">0.5</span>,</span><br><span class="line">        with_mask=<span class="literal">False</span>,</span><br><span class="line">        with_crowd=<span class="literal">True</span>,</span><br><span class="line">        with_label=<span class="literal">True</span>),</span><br><span class="line">    val=dict(</span><br><span class="line">        type=dataset_type,</span><br><span class="line">        ann_file=data_root + <span class="string">'annotations/instances_val2017.json'</span>,</span><br><span class="line">        img_prefix=data_root + <span class="string">'val2017/'</span>,</span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),</span><br><span class="line">        img_norm_cfg=img_norm_cfg,</span><br><span class="line">        size_divisor=<span class="number">32</span>,</span><br><span class="line">        flip_ratio=<span class="number">0</span>,</span><br><span class="line">        with_mask=<span class="literal">False</span>,</span><br><span class="line">        with_crowd=<span class="literal">True</span>,</span><br><span class="line">        with_label=<span class="literal">True</span>),</span><br><span class="line">    test=dict(</span><br><span class="line">        type=dataset_type,</span><br><span class="line">        ann_file=data_root + <span class="string">'annotations/instances_val2017.json'</span>,</span><br><span class="line">        img_prefix=data_root + <span class="string">'val2017/'</span>,</span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),</span><br><span class="line">        img_norm_cfg=img_norm_cfg,</span><br><span class="line">        size_divisor=<span class="number">32</span>,</span><br><span class="line">        flip_ratio=<span class="number">0</span>,</span><br><span class="line">        with_mask=<span class="literal">False</span>,</span><br><span class="line">        with_label=<span class="literal">False</span>,</span><br><span class="line">        test_mode=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># optimizer</span></span><br><span class="line">optimizer = dict(type=<span class="string">'SGD'</span>, lr=<span class="number">0.02</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.0001</span>)</span><br><span class="line">optimizer_config = dict(grad_clip=dict(max_norm=<span class="number">35</span>, norm_type=<span class="number">2</span>))</span><br><span class="line"><span class="comment"># learning policy</span></span><br><span class="line">lr_config = dict(</span><br><span class="line">    policy=<span class="string">'step'</span>,</span><br><span class="line">    warmup=<span class="string">'linear'</span>,</span><br><span class="line">    warmup_iters=<span class="number">500</span>,</span><br><span class="line">    warmup_ratio=<span class="number">1.0</span> / <span class="number">3</span>,</span><br><span class="line">    step=[<span class="number">8</span>, <span class="number">11</span>])</span><br><span class="line">checkpoint_config = dict(interval=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># yapf:disable</span></span><br><span class="line">log_config = dict(</span><br><span class="line">    interval=<span class="number">50</span>,</span><br><span class="line">    hooks=[</span><br><span class="line">        dict(type=<span class="string">'TextLoggerHook'</span>),</span><br><span class="line">        <span class="comment"># dict(type='TensorboardLoggerHook')</span></span><br><span class="line">    ])</span><br><span class="line"><span class="comment"># yapf:enable</span></span><br><span class="line"><span class="comment"># runtime settings</span></span><br><span class="line">total_epochs = <span class="number">12</span></span><br><span class="line">dist_params = dict(backend=<span class="string">'nccl'</span>)</span><br><span class="line">log_level = <span class="string">'INFO'</span></span><br><span class="line">work_dir = <span class="string">'./work_dirs/cascade_rcnn_r50_fpn_1x'</span></span><br><span class="line">load_from = <span class="literal">None</span></span><br><span class="line">resume_from = <span class="literal">None</span></span><br><span class="line">workflow = [(<span class="string">'train'</span>, <span class="number">1</span>)]</span><br></pre></td></tr></table></figure><p>参考链接：</p><p><a href="https://www.jiqizhixin.com/articles/2018-10-17-10" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2018-10-17-10</a></p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Visual Genome 数据集</title>
      <link href="/2019/07/21/Visual-Genome-%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
      <url>/2019/07/21/Visual-Genome-%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<ul><li>数据集介绍<br><a href="https://cloud.tencent.com/developer/article/1391855" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1391855</a></li></ul><p><a href="https://visualgenome.org/" target="_blank" rel="noopener">Visual Genome 主页</a></p><p><a href="https://visualgenome.org/api/v0/api_home.html" target="_blank" rel="noopener">Visual Genome Data</a></p><p><a href="https://visualgenome.org/api/v0/api_readme" target="_blank" rel="noopener">Visual Genome Readme</a></p><p>Visual Genome 数据集总览：</p><ul><li>108077 张图片</li><li>5.4 Million Region Descriptions</li><li>1.7 Million Visual Question Answers</li><li>3.8 Million Object Instances</li><li>2.8 Million Attributes</li><li>2.3 Million Relationships</li><li>Everything Mapped to Wordnet Synsets  </li><li>标注数据：  objects，attributes，图片内的 relationships</li><li>共 108K 张图片，每张图片平均有， 35 个 objects，26 个 attributes，21对 objects 见的成对 relationships.</li></ul><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/rex8eso6p5.png?imageView2/2/w/1620" alt="img"></p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/vtmiv1yyr6.png?imageView2/2/w/1620" alt="img"></p><h2 id="1-Visual-Genome-数据标注"><a href="#1-Visual-Genome-数据标注" class="headerlink" title="1. Visual Genome 数据标注"></a>1. Visual Genome 数据标注</h2><p>数据集主要包括七个主要部分：</p><ul><li>region descriptions</li><li>objects</li><li>attributes</li><li>relationships</li><li>region graphs</li><li>scene graphs</li><li>question answer pairs</li></ul><h3 id="1-1-Region-Descriptions"><a href="#1-1-Region-Descriptions" class="headerlink" title="1.1. Region Descriptions"></a>1.1. Region Descriptions</h3><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/8kgo0p0qim.png?imageView2/2/w/1620" alt="img"></p><p>数据集标注了图片的 regions descriptions，每个 region 有一个 bounding box. </p><p>如上图中，图片有三个 regions descriptions： “man jumping over a fire hydrant,”，“yellow fire hydrant,” 和   “woman in shorts is standing behind the man.”.</p><h3 id="1-2-Objects"><a href="#1-2-Objects" class="headerlink" title="1.2. Objects"></a>1.2. Objects</h3><p>数据集中每张图片平均有 35 个 objects，每个 object 采用 bounding box 标注.</p><p>如图：   </p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/ih1qpz1p3s.png?imageView2/2/w/1620" alt="img"></p><p><a href="http://blog.csdn.net/zziahgf/article/details/72819043" target="_blank" rel="noopener">MS-COCO 数据集</a> 只标注了 80 个 object categories，没有描述图片中的所有 objects. 实际场景中，可能有更多的 objects 类别.</p><p>Visual Genome 数据集旨在对图片里出现的所有视觉 objects 进行标注，objects categories 类别达到 33877 种.</p><h3 id="1-3-Attributes"><a href="#1-3-Attributes" class="headerlink" title="1.3. Attributes"></a>1.3. Attributes</h3><p>数据集中每张图片平均有 26 个 attributes. Objects 可能没有或者有更多的相关 attributes. </p><p>Attributes 可以是 color(如 yellow)，states(如 standing) 等，如图：   </p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/k1n26a1mdt.png?imageView2/2/w/1620" alt="img"></p><p>Attributes 能够对 objects 进行更容易的描述、对比与分类. 即使以前未见到某 object，根据 attributes 仍能推断出与 object 相关的东西. 如，“yellow and brown spotted with long neck(长脖子上有黄色和棕色的斑点)”，很可能推断出 object 是 giraffe(长颈鹿).</p><p>关于 attributes 的研究：</p><ul><li>采用examplar SVMs，利用相似特征来寻找 objects；</li><li>采用纹理(textures) 研究 objects，或者预测颜色.</li><li>采用 attributes 来提高目标分类结果. 如 fine-grained 识别.</li></ul><p>Attributes 一般被定义为 parts(如 has legs)、shapes(如，spherical球形的)、materials(如 furry毛皮的)；用于对新的 objects 类别进行分类.</p><p>Visual Genome 数据集对于 attributes 进行扩展，其 attributes 不是 image-specific 的，而是真实场景中 object-specific 的. attributes 类型包括：size(如 small), pose(如bent), state (如 transparent), emotion (如 happy)等等.</p><ul><li>基于 VGG16 的 attributes 预测结果：   </li></ul><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/373ih7qquc.png?imageView2/2/w/1620" alt="img"></p><h3 id="1-4-Relationships"><a href="#1-4-Relationships" class="headerlink" title="1.4. Relationships"></a>1.4. Relationships</h3><p>Relationships 是两个 objects 的连接关系.</p><p>Relationships 可以是 actions(如 jumping over)，spatial(如 is build)，comparative(如 taller than)，prepositional phrases (如 drive on). 如图：   </p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/5ljbd3m2av.png?imageView2/2/w/1620" alt="img"></p><ul><li>Relationship 预测结果：   </li></ul><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/pgbhzj5ui4.png?imageView2/2/w/1620" alt="img"></p><h3 id="1-5-Region-Graphs"><a href="#1-5-Region-Graphs" class="headerlink" title="1.5. Region Graphs"></a>1.5. Region Graphs</h3><p>结合 objects、attributes 以及  region descriptions 提取的 relationships，创建每个 regions 的 graph representation. </p><h3 id="1-6-Scene-Graphs"><a href="#1-6-Scene-Graphs" class="headerlink" title="1.6. Scene Graphs"></a>1.6. Scene Graphs</h3><p>Region graphs 是图片的局部区域表示，将 region graphs 结合，生成单个 scene graph来表示整张图片.</p><p>Scene graph 是全部 region graphs 的统一，包含了全部的 objects、attributes以及每个 region description 的 relationships.</p><p>Scene Graph 将多种不同层次的 scene 信息以更加一致的方式结合在一起.</p><h3 id="1-7-Question-Answer-QA-Pairs"><a href="#1-7-Question-Answer-QA-Pairs" class="headerlink" title="1.7. Question Answer(QA) Pairs"></a>1.7. Question Answer(QA) Pairs</h3><p>数据集中每张图片有两种类型的 QA pairs：</p><ul><li>freeform QAs - 基于整张图片；</li><li>region-based QAs - 基于图片的选择区域. </li></ul><p>每张图片标注了 6 中不同类型的问题：what, where, how, when, who, why.</p><p>如图：   </p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/vbxbgpfi92.png?imageView2/2/w/1620" alt="img"></p><p>Figure . Visual Genome 数据集. 每张图片包括：region descriptions - 描述了图像的局部信息；两种类型的 question answer pairs(QAs) - free form QAs 和 region-based QAs. 每个 region 转化为 objects、attributes 和 pairwise relationships region 构成的 region graph 表示. 最终， 结合 region graphs 以形成图片内全部 objects 的 scene graph.</p><h2 id="2-Visual-Genome-数据集应用"><a href="#2-Visual-Genome-数据集应用" class="headerlink" title="2. Visual Genome 数据集应用"></a>2. Visual Genome 数据集应用</h2><p>基本应用：</p><ul><li>attribute classification 属性分类</li><li>relationship classification 关系分类</li><li>description generation 描述生成</li><li>question answering QA</li></ul><p>更多应用：</p><ul><li>Dense image captioning</li><li>Visual question answering</li><li>Image understanding</li><li>Relationship extraction</li><li>Semantic image retrieval</li><li>Completing the Set of Annotations</li></ul><p>注 - 与其它数据集对比：   </p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/cjqe5v7i44.png?imageView2/2/w/1620" alt="img"></p><h2 id="3-Reference"><a href="#3-Reference" class="headerlink" title="3. Reference"></a>3. Reference</h2><p>[1] - <a href="https://visualgenome.org/" target="_blank" rel="noopener">Visual Genome Home</a></p><p>[1] - <a href="https://visualgenome.org/static/paper/Visual_Genome.pdf" target="_blank" rel="noopener">Visual Genome Doc</a></p><p>[2] - <a href="https://arxiv.org/pdf/1701.02426.pdf" target="_blank" rel="noopener">Scene Graph Generation by Iterative Message Passing</a></p><p>本文参与<a href="https://cloud.tencent.com/developer/support-plan" target="_blank" rel="noopener">腾讯云自媒体分享计划</a>，欢迎正在阅读的你也加入，一起分享。</p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>设置随机种子</title>
      <link href="/2019/07/21/%E8%AE%BE%E7%BD%AE%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90/"/>
      <url>/2019/07/21/%E8%AE%BE%E7%BD%AE%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90/</url>
      
        <content type="html"><![CDATA[<p>def set_random_seed(seed):<br>    random.seed(seed)<br>    np.random.seed(seed)<br>    torch.manual_seed(seed)<br>    torch.cuda.manual_seed_all(seed)</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>mmdetection的安装</title>
      <link href="/2019/07/20/mmdetection%E7%9A%84%E5%AE%89%E8%A3%85/"/>
      <url>/2019/07/20/mmdetection%E7%9A%84%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><ul><li>选择镜像：py36-pytorch0.4.0-cu90-ctc （即，准备一个cuda9.0的环境）</li><li>note: 需要一个pytorch1.1.0（pytorch1.2测试不可以用，推荐使用1.1.0），后文有讲如何在anaconda下安装pytorch</li><li>进入容器，安装anaconda</li></ul><h3 id="按着Github-install的步骤进行安装如下："><a href="#按着Github-install的步骤进行安装如下：" class="headerlink" title="按着Github install的步骤进行安装如下："></a>按着<a href="https://github.com/open-mmlab/mmdetection/blob/master/INSTALL.md" target="_blank" rel="noopener">Github install</a>的步骤进行安装如下：</h3><ul><li>Create a conda virtual environment and activate it. Then install Cython.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n open-mmlab python=<span class="number">3.7</span> -y</span><br><span class="line">source activate open-mmlab</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>** 以下的操作都是在进入open-mmlab环境之后进行的**</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda <span class="keyword">install</span> cython</span><br></pre></td></tr></table></figure></li><li><p>安装 numpy</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> numpy</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>安装<a href="https://github.com/open-mmlab/mmcv" target="_blank" rel="noopener">mmcv</a> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install mmcv</span><br></pre></td></tr></table></figure></li><li><p>安装pytorch<br>最好是离线下载，然后再安装，因为conda install 或者 pip install 可能连接不上（细节：pip install torch  就会出现下载链接，然后自己复制链接去网页下载即可），下载之后：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">pip</span> <span class="selector-tag">install</span> <span class="selector-tag">torch-1</span><span class="selector-class">.1</span><span class="selector-class">.0-cp37-cp37m-manylinux1_x86_64</span><span class="selector-class">.whl</span></span><br></pre></td></tr></table></figure></li><li><p>安装opencv</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda <span class="keyword">install</span> -c menpo opencv</span><br></pre></td></tr></table></figure></li></ul><ul><li>安装matplotlib<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> matplotlib</span><br></pre></td></tr></table></figure></li></ul><ul><li>安装 terminaltables<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> terminaltables</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>安装 pip install pycocotools</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> pycocotools</span><br></pre></td></tr></table></figure></li><li><p>选择一个看的顺眼的位置：Clone the mmdetection repository.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/open-mmlab/mmdetection.git</span><br><span class="line">cd mmdetection</span><br></pre></td></tr></table></figure></li><li><p>Install mmdetection</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python setup.py develop</span><br></pre></td></tr></table></figure></li><li><p>大功告成</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>torch.no_grad</title>
      <link href="/2019/07/17/torch-no-grad/"/>
      <url>/2019/07/17/torch-no-grad/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/21" target="_blank" rel="noopener">https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/21</a></li><li>with torch.no_grad()</li><li>可以减少内存，加快运行速度，同时可以使得batch_size 增大</li><li>但不是说非得必要</li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spacy工具包</title>
      <link href="/2019/07/16/Spacy%E5%B7%A5%E5%85%B7%E5%8C%85/"/>
      <url>/2019/07/16/Spacy%E5%B7%A5%E5%85%B7%E5%8C%85/</url>
      
        <content type="html"><![CDATA[<h2 id="spacy的主要操作："><a href="#spacy的主要操作：" class="headerlink" title="spacy的主要操作："></a>spacy的主要操作：</h2><h3 id="1、分词断句"><a href="#1、分词断句" class="headerlink" title="1、分词断句"></a>1、分词断句</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import spacy</span><br><span class="line">nlp = spacy.<span class="built_in">load</span>(<span class="string">'en'</span>)</span><br><span class="line">doc = nlp(<span class="string">'Hello World! My name is HanXiaoyang'</span>)</span><br><span class="line"><span class="comment"># 分词</span></span><br><span class="line"><span class="keyword">for</span> <span class="keyword">token</span> <span class="keyword">in</span> doc:</span><br><span class="line">    print(<span class="string">'"'</span> + <span class="keyword">token</span>.<span class="keyword">text</span> + <span class="string">'"'</span>)</span><br><span class="line"><span class="comment"># 断句</span></span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> doc.sents:</span><br><span class="line">    print(sent)</span><br></pre></td></tr></table></figure><p>每个token对象有着非常丰富的属性，如下的方式可以取出其中的部分属性。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">doc</span> <span class="string">=</span> <span class="string">nlp("Next</span> <span class="string">week</span> <span class="string">I'll</span>   <span class="string">be</span> <span class="string">in</span> <span class="string">Shanghai.")</span></span><br><span class="line"><span class="string">for</span> <span class="string">token</span> <span class="string">in</span> <span class="attr">doc:</span></span><br><span class="line">    <span class="string">print("&#123;0&#125;\t&#123;1&#125;\t&#123;2&#125;\t&#123;3&#125;\t&#123;4&#125;\t&#123;5&#125;\t&#123;6&#125;\t&#123;7&#125;".format(</span></span><br><span class="line">        <span class="string">token.text,</span></span><br><span class="line">        <span class="string">token.idx,</span></span><br><span class="line">        <span class="string">token.lemma_,</span></span><br><span class="line">        <span class="string">token.is_punct,</span></span><br><span class="line">        <span class="string">token.is_space,</span></span><br><span class="line">        <span class="string">token.shape_,</span></span><br><span class="line">        <span class="string">token.pos_,</span></span><br><span class="line">        <span class="string">token.tag_</span></span><br><span class="line">    <span class="string">))</span></span><br><span class="line"><span class="string">输出结果如下：</span></span><br><span class="line"><span class="string">Next</span>    <span class="number">0</span>   <span class="string">next</span>    <span class="literal">False</span>   <span class="literal">False</span>   <span class="string">Xxxx</span>    <span class="string">ADJ</span> <span class="string">JJ</span></span><br><span class="line"><span class="string">week</span>    <span class="number">5</span>   <span class="string">week</span>    <span class="literal">False</span>   <span class="literal">False</span>   <span class="string">xxxx</span>    <span class="string">NOUN</span>    <span class="string">NN</span></span><br><span class="line"><span class="string">I</span>   <span class="number">10</span>  <span class="bullet">-PRON-</span>  <span class="literal">False</span>   <span class="literal">False</span>   <span class="string">X</span>   <span class="string">PRON</span>    <span class="string">PRP</span></span><br><span class="line"><span class="string">'ll 11  will    False   False   '</span><span class="string">xx</span> <span class="string">VERB</span>    <span class="string">MD</span></span><br><span class="line">    <span class="number">15</span>      <span class="literal">False</span>   <span class="literal">True</span>        <span class="string">SPACE</span>   <span class="string">_SP</span></span><br><span class="line"><span class="string">be</span>  <span class="number">17</span>  <span class="string">be</span>  <span class="literal">False</span>   <span class="literal">False</span>   <span class="string">xx</span>  <span class="string">VERB</span>    <span class="string">VB</span></span><br><span class="line"><span class="string">in</span>  <span class="number">20</span>  <span class="string">in</span>  <span class="literal">False</span>   <span class="literal">False</span>   <span class="string">xx</span>  <span class="string">ADP</span> <span class="string">IN</span></span><br><span class="line"><span class="string">Shanghai</span>    <span class="number">23</span>  <span class="string">shanghai</span>    <span class="literal">False</span>   <span class="literal">False</span>   <span class="string">Xxxxx</span>   <span class="string">PROPN</span>   <span class="string">NNP</span></span><br><span class="line"><span class="string">.</span>   <span class="number">31</span>  <span class="string">.</span>   <span class="literal">True</span>    <span class="literal">False</span>   <span class="string">.</span>   <span class="string">PUNCT</span>   <span class="string">.</span></span><br></pre></td></tr></table></figure><h3 id="2、词性标注"><a href="#2、词性标注" class="headerlink" title="2、词性标注"></a>2、词性标注</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 词性标注</span></span><br><span class="line">doc = nlp(<span class="string">"Next week I'll be in Shanghai."</span>)</span><br><span class="line">print([(<span class="keyword">token</span>.<span class="keyword">text</span>, <span class="keyword">token</span>.tag_) <span class="keyword">for</span> <span class="keyword">token</span> <span class="keyword">in</span> doc])</span><br></pre></td></tr></table></figure><p>[(‘Next’, ‘JJ’), (‘week’, ‘NN’), (‘I’, ‘PRP’), (“‘ll”, ‘MD’), (‘be’, ‘VB’), (‘in’, ‘IN’), (‘Shanghai’, ‘NNP’), (‘.’, ‘.’)]</p><h3 id="3、组块分析"><a href="#3、组块分析" class="headerlink" title="3、组块分析"></a>3、组块分析</h3><p>spaCy可以自动检测名词短语，并输出根(root)词，比如下面的”Journal”,”piece”,”currencies”</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">doc = nlp(<span class="string">"Wall Street Journal just published an interesting piece on crypto currencies"</span>)</span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> doc<span class="selector-class">.noun_chunks</span>:</span><br><span class="line">    print(chunk<span class="selector-class">.text</span>, chunk<span class="selector-class">.label_</span>, chunk<span class="selector-class">.root</span><span class="selector-class">.text</span>)</span><br></pre></td></tr></table></figure><p>输出结果：<br>Wall Street Journal NP Journal<br>an interesting piece NP piece<br>crypto currencies NP currencies</p><h3 id="4、命名实体识别"><a href="#4、命名实体识别" class="headerlink" title="4、命名实体识别"></a>4、命名实体识别</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">doc = nlp(<span class="string">"Two years ago, I lived in my Beijing."</span>)</span><br><span class="line"><span class="keyword">for</span> ent <span class="keyword">in</span> doc<span class="selector-class">.ents</span>:</span><br><span class="line">    print(ent<span class="selector-class">.text</span>, ent.label_)</span><br></pre></td></tr></table></figure><p>输出结果：<br>Two years ago DATE<br>BeijingGPE</p><p>还可以用非常漂亮的可视化做显示：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> spacy import displacy</span><br><span class="line">displacy.render(doc, <span class="attribute">style</span>=<span class="string">'ent'</span>, <span class="attribute">jupyter</span>=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="//upload-images.jianshu.io/upload_images/11681023-77f9837fa7e661dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/454/format/webp" alt></p><p>输出结果.png</p><h3 id="5、句法依存解析"><a href="#5、句法依存解析" class="headerlink" title="5、句法依存解析"></a>5、句法依存解析</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">doc = nlp(<span class="string">'Wall Street Journal just published an interesting piece on crypto currencies'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    print(<span class="string">"&#123;0&#125;/&#123;1&#125; &lt;--&#123;2&#125;-- &#123;3&#125;/&#123;4&#125;"</span>.format(</span><br><span class="line">        token<span class="selector-class">.text</span>, token<span class="selector-class">.tag_</span>, token<span class="selector-class">.dep_</span>, token<span class="selector-class">.head</span><span class="selector-class">.text</span>, token<span class="selector-class">.head</span><span class="selector-class">.tag_</span>))</span><br></pre></td></tr></table></figure><p>输出结果：<br>Wall/NNP &lt;–compound– Street/NNP<br>Street/NNP &lt;–compound– Journal/NNP<br>Journal/NNP &lt;–nsubj– published/VBD<br>just/RB &lt;–advmod– published/VBD<br>published/VBD &lt;–ROOT– published/VBD<br>an/DT &lt;–det– piece/NN<br>interesting/JJ &lt;–amod– piece/NN<br>piece/NN &lt;–dobj– published/VBD<br>on/IN &lt;–prep– piece/NN<br>crypto/JJ &lt;–compound– currencies/NNS<br>currencies/NNS &lt;–pobj– on/IN</p><h3 id="6、-词向量"><a href="#6、-词向量" class="headerlink" title="6、==词向量=="></a>6、==词向量==</h3><p>NLP中有一个非常强大的文本表示学习方法叫做==word2vec==，通过词的上下文学习到词语的稠密向量化表示，同时在这个表示形态下，语义相关的词在向量空间中会比较接近。也有类似v(爷爷)-v(奶奶) ≈ v(男人)-v(女人)的关系。<br>在spaCy中，要使用英文的词向量，需先下载预先训练好的结果。</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">python3</span> -m spacy download en_core_web_lg</span><br></pre></td></tr></table></figure><p>词向量的应用：</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">nlp = spacy.<span class="built_in">load</span>(<span class="string">'en_core_web_lg'</span>)</span><br><span class="line"><span class="built_in">from</span> scipy import spatial</span><br><span class="line"></span><br><span class="line"><span class="comment"># 余弦相似度计算</span></span><br><span class="line">cosine_similarity = lambda x, y: <span class="number">1</span> - spatial.distance.cosine(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 男人、女人、国王、女王 的词向量</span></span><br><span class="line">man = nlp.vocab[<span class="string">'man'</span>].vector</span><br><span class="line">woman = nlp.vocab[<span class="string">'woman'</span>].vector</span><br><span class="line">queen = nlp.vocab[<span class="string">'queen'</span>].vector</span><br><span class="line">king = nlp.vocab[<span class="string">'king'</span>].vector</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 我们对向量做一个简单的计算，"man" - "woman" + "queen"</span></span><br><span class="line">maybe_king = man - woman + queen</span><br><span class="line">computed_similarities = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 扫描整个词库的词向量做比对，召回最接近的词向量</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">word</span> <span class="keyword">in</span> nlp.vocab:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">word</span>.has_vector:</span><br><span class="line">        continue</span><br><span class="line"> </span><br><span class="line">    similarity = cosine_similarity(maybe_king, <span class="built_in">word</span>.vector)</span><br><span class="line">    computed_similarities.append((<span class="built_in">word</span>, similarity))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 排序与最接近结果展示</span></span><br><span class="line">computed_similarities = sorted(computed_similarities, key=lambda <span class="keyword">item</span>: -<span class="keyword">item</span>[<span class="number">1</span>])</span><br><span class="line">print([w[<span class="number">0</span>].<span class="keyword">text</span> <span class="keyword">for</span> w <span class="keyword">in</span> computed_similarities[:<span class="number">10</span>]])</span><br></pre></td></tr></table></figure><p>输出结果：<br>[‘Queen’, ‘QUEEN’, ‘queen’, ‘King’, ‘KING’, ‘king’, ‘KIng’, ‘Kings’, ‘KINGS’, ‘kings’]</p><h3 id="6、词汇与文本相似度"><a href="#6、词汇与文本相似度" class="headerlink" title="6、词汇与文本相似度"></a>6、词汇与文本相似度</h3><p>在词向量的基础上，spaCy提供了从词到文档的相似度计算的方法，下面的例子是它的使用方法。</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 词汇语义相似度(关联性)</span></span><br><span class="line">banana = nlp.vocab['banana']</span><br><span class="line">dog = nlp.vocab['dog']</span><br><span class="line">fruit = nlp.vocab['fruit']</span><br><span class="line">animal = nlp.vocab['animal']</span><br><span class="line"> </span><br><span class="line">print(dog.similarity(animal), dog.similarity(fruit)) <span class="comment"># 0.6618534 0.23552845</span></span><br><span class="line">print(banana.similarity(fruit), banana.similarity(animal)) <span class="comment"># 0.67148364 0.2427285</span></span><br></pre></td></tr></table></figure><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文本语义相似度(关联性)</span></span><br><span class="line">target = nlp(<span class="string">"Cats are beautiful animals."</span>)</span><br><span class="line"> </span><br><span class="line">doc1 = nlp(<span class="string">"Dogs are awesome."</span>)</span><br><span class="line">doc2 = nlp(<span class="string">"Some gorgeous creatures are felines."</span>)</span><br><span class="line">doc3 = nlp(<span class="string">"Dolphins are swimming mammals."</span>)</span><br><span class="line"> </span><br><span class="line">print(target.similarity(doc1))  <span class="comment"># 0.8901765218466683</span></span><br><span class="line">print(target.similarity(doc2))  <span class="comment"># 0.9115828449161616</span></span><br><span class="line">print(target.similarity(doc3))  <span class="comment"># 0.7822956752876101</span></span><br></pre></td></tr></table></figure><p>作者：还是那个没头脑<br>链接：<a href="https://www.jianshu.com/p/74e6c5376bc0" target="_blank" rel="noopener">https://www.jianshu.com/p/74e6c5376bc0</a><br>来源：简书<br>简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言理解 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch-gather</title>
      <link href="/2019/07/12/pytorch-gather/"/>
      <url>/2019/07/12/pytorch-gather/</url>
      
        <content type="html"><![CDATA[<p>函数<code>torch.gather(input, dim, index, out=None) → Tensor</code><br> 沿给定轴 dim ,将输入索引张量 index 指定位置的值进行聚合.<br> 对一个 3 维张量,输出可以定义为:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out[<span class="string">i</span>][<span class="symbol">j</span>][<span class="string">k</span>] = input[<span class="string">index[i</span>][<span class="symbol">j</span>][<span class="string">k</span>]][<span class="string">j</span>][<span class="symbol">k</span>]  # if dim == 0</span><br><span class="line">out[<span class="string">i</span>][<span class="symbol">j</span>][<span class="string">k</span>] = input[<span class="string">i</span>][<span class="symbol">index[i</span>][<span class="string">j</span>][<span class="symbol">k</span>]][<span class="symbol">k</span>]  # if dim == 1</span><br><span class="line">out[<span class="string">i</span>][<span class="symbol">j</span>][<span class="string">k</span>] = input[<span class="string">i</span>][<span class="symbol">j</span>][<span class="string">index[i</span>][<span class="symbol">j</span>][<span class="string">k</span>]]  # if dim == 2</span><br></pre></td></tr></table></figure><p>Parameters:</p><ul><li>input (Tensor) – 源张量</li><li>dim (int) – 索引的轴</li><li>index (LongTensor) – 聚合元素的下标(index需要是torch.longTensor类型)</li><li>out (Tensor, optional) – 目标张量</li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/07/11/hello-world/"/>
      <url>/2019/07/11/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>numpy 函数</title>
      <link href="/2019/06/16/numpy-%E5%87%BD%E6%95%B0/"/>
      <url>/2019/06/16/numpy-%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="np-round"><a href="#np-round" class="headerlink" title="np.round "></a><font color="#0099ff" size="7" face="黑体">np.round </font></h2><h3 id="round函数概念："><a href="#round函数概念：" class="headerlink" title="round函数概念："></a>round函数概念：</h3><p>英文：圆，四舍五入<br>是python内置函数，它在哪都能用，对数字取四舍五入。<br>round(number[, ndigits])<br>round 对传入的数据进行四舍五入，如果ngigits不传，默认是0（就是说保留整数部分）.ngigits&lt;0 的时候是来对整数部分进行四舍五入，返回的结果是浮点数.</p><h3 id="round-负数"><a href="#round-负数" class="headerlink" title="round 负数"></a>round 负数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 四舍五入是围绕着0来计算的，  </span></span><br><span class="line">round(<span class="number">0.5</span>) <span class="comment"># 1.0  </span></span><br><span class="line">round(<span class="number">-0.5</span>) <span class="comment">#-1.0</span></span><br></pre></td></tr></table></figure><h3 id="round-的陷阱"><a href="#round-的陷阱" class="headerlink" title="round 的陷阱"></a>round 的陷阱</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">round(<span class="number">1.675</span>, <span class="number">2</span>) <span class="comment">#1.68  </span></span><br><span class="line">round(<span class="number">2.675</span>, <span class="number">2</span>) <span class="comment">#2.67</span></span><br></pre></td></tr></table></figure><h3 id="举例："><a href="#举例：" class="headerlink" title="举例："></a>举例：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">round(<span class="number">3.4</span>) <span class="comment"># 3.0  </span></span><br><span class="line">round(<span class="number">3.5</span>) <span class="comment"># 4.0  </span></span><br><span class="line">round(<span class="number">3.6</span>) <span class="comment"># 4.0  </span></span><br><span class="line">round(<span class="number">3.6</span>, <span class="number">0</span>) <span class="comment"># 4.0  </span></span><br><span class="line">round(<span class="number">1.95583</span>, <span class="number">2</span>) <span class="comment"># 1.96  </span></span><br><span class="line">round(<span class="number">1241757</span>, <span class="number">-3</span>) <span class="comment"># 1242000.0  </span></span><br><span class="line">round(<span class="number">5.045</span>, <span class="number">2</span>) <span class="comment"># 5.05  </span></span><br><span class="line">round(<span class="number">5.055</span>, <span class="number">2</span>) <span class="comment"># 5.06</span></span><br></pre></td></tr></table></figure><h2 id="np-clip"><a href="#np-clip" class="headerlink" title="np.clip"></a><font color="#0099ff" size="7" face="黑体">np.clip</font></h2><p>numpy.clip(a, a_min, a_max, out=None)[source]<br>其中a是一个数组，后面两个参数分别表示最小和最大值，怎么用呢，老规矩，我们看代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x=np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>])</span><br><span class="line">np.clip(x,<span class="number">3</span>,<span class="number">8</span>)</span><br><span class="line">Out[<span class="number">88</span>]:</span><br><span class="line">array([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>self-attention</title>
      <link href="/2019/06/16/self-attention/"/>
      <url>/2019/06/16/self-attention/</url>
      
        <content type="html"><![CDATA[<ul><li>一篇解读：<a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">https://kexue.fm/archives/4765</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自然语言理解 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>优先级队列式分支限界法---最小重量机器设计问题--python实现</title>
      <link href="/2019/05/22/%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97%E5%BC%8F%E5%88%86%E6%94%AF%E9%99%90%E7%95%8C%E6%B3%95-%E6%9C%80%E5%B0%8F%E9%87%8D%E9%87%8F%E6%9C%BA%E5%99%A8%E8%AE%BE%E8%AE%A1%E9%97%AE%E9%A2%98-python%E5%AE%9E%E7%8E%B0/"/>
      <url>/2019/05/22/%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97%E5%BC%8F%E5%88%86%E6%94%AF%E9%99%90%E7%95%8C%E6%B3%95-%E6%9C%80%E5%B0%8F%E9%87%8D%E9%87%8F%E6%9C%BA%E5%99%A8%E8%AE%BE%E8%AE%A1%E9%97%AE%E9%A2%98-python%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<p>这里给出两个解决方案：</p><p>1）不使用优先级，简单使用队列式分支限界法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 普通的FIFO 队列式分支限界法</span></span><br><span class="line"><span class="comment">## 当 不满足总价格不超过d的要求时，则剪枝</span></span><br><span class="line"><span class="comment">## 当搜索到深度n时，即搜索到了叶节点，不再进行扩展节点的操作，而是针对于叶节点所对应的最小值，</span></span><br><span class="line"><span class="comment"># 反向求得该节点所对应的的路径</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># coding : utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> queue</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">n = <span class="number">3</span></span><br><span class="line">m = <span class="number">3</span></span><br><span class="line">d = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">price = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]]</span><br><span class="line">weight = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到当前扩展节点所在的Level</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getlevel</span><span class="params">(m, currrent)</span>:</span></span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    level = <span class="number">0</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> currrent == <span class="number">0</span>:</span><br><span class="line">        level = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> level</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        level = level+<span class="number">1</span></span><br><span class="line">        sum = m**level + sum  <span class="comment"># sum=m</span></span><br><span class="line">        <span class="keyword">if</span> sum-m**level &lt; currrent &lt;= sum:  <span class="comment"># m-m^0 = m-1</span></span><br><span class="line">            <span class="keyword">return</span> level</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到当前扩展节点，所对应的首个子节点所在的索引</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_idx</span><span class="params">(m, current)</span>:</span></span><br><span class="line">    level = getlevel(m, current)</span><br><span class="line">    <span class="comment"># 求current 在该level中的相对索引，即 相对于该level第一个元素的位置</span></span><br><span class="line">    current_level_idx = current - sum([m**i <span class="keyword">for</span> i <span class="keyword">in</span> range(level)])</span><br><span class="line">    <span class="comment"># 子节点所在层的开始绝对索引</span></span><br><span class="line">    start_idx  = sum([m**i <span class="keyword">for</span> i <span class="keyword">in</span> range(level+<span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> start_idx + current_level_idx*m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到最优解之后，反向查找其路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_path</span><span class="params">(m, current)</span>:</span></span><br><span class="line">    path = []</span><br><span class="line">    path.append(current%m)  <span class="comment"># from 1, not from 0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        level = getlevel(m, current)</span><br><span class="line">        <span class="keyword">if</span> level == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> path[::<span class="number">-1</span>]</span><br><span class="line">        current_level_idx = current - sum([m ** i <span class="keyword">for</span> i <span class="keyword">in</span> range(level)])  <span class="comment"># # 求current 在该level中的相对索引，即 相对于该level第一个元素的位置</span></span><br><span class="line">        path.append(current_level_idx // m + <span class="number">1</span>)  <span class="comment"># 得到上一级的索引位置</span></span><br><span class="line">        current = sum([m ** i <span class="keyword">for</span> i <span class="keyword">in</span> range(level<span class="number">-1</span>)]) + current_level_idx // m  <span class="comment">#得到上一级的值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MinWighet</span><span class="params">(n,m,d,price,weight)</span>:</span></span><br><span class="line">    minweight = float(<span class="string">"inf"</span>)</span><br><span class="line">    <span class="comment"># 子集树中的节点数</span></span><br><span class="line">    vec_len = sum([m ** i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>)]) + <span class="number">1</span></span><br><span class="line">    que = queue.Queue()</span><br><span class="line">    que.put(<span class="number">0</span>)</span><br><span class="line">    vec_price = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(vec_len)]</span><br><span class="line">    vec_weight = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(vec_len)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(<span class="keyword">not</span> que.empty()):</span><br><span class="line">        current = que.get()  <span class="comment"># 得到当前扩展节点（索引号）</span></span><br><span class="line">        level = getlevel(m, current)  <span class="comment"># 当前 扩展节点所在的level</span></span><br><span class="line"></span><br><span class="line">        idx = get_idx(m, current)   <span class="comment"># 得到当前扩展节点，所对应的首个子节点所在的索引</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 若搜索完了整棵树</span></span><br><span class="line">        <span class="keyword">if</span> getlevel(m, current) == getlevel(m, vec_len)<span class="number">-1</span>:</span><br><span class="line">            minweight = vec_price[current]</span><br><span class="line">            min_at_idx = current</span><br><span class="line">            <span class="keyword">while</span> (<span class="keyword">not</span> que.empty()):</span><br><span class="line">                <span class="comment"># minweight = min(minweight, vec_price[que.get()])</span></span><br><span class="line">                tmp = que.get()</span><br><span class="line">                <span class="keyword">if</span> minweight &gt; vec_price[tmp]:</span><br><span class="line">                    minweight = vec_price[tmp]</span><br><span class="line">                    min_at_idx = tmp</span><br><span class="line">            path = get_path(m, min_at_idx)</span><br><span class="line">            <span class="keyword">return</span> minweight, path</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断当前的扩展结点下的所有子节点是否可以加入活结点队列中</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            vec_price[idx] = int(vec_price[current] + price[level][i])</span><br><span class="line">            <span class="keyword">if</span> vec_price[idx] &lt;= d:</span><br><span class="line">                vec_weight[idx] = int(vec_weight[current] + weight[level][i])</span><br><span class="line">                que.put(idx)</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(MinWighet(n,m,d,price,weight))</span><br></pre></td></tr></table></figure><p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>2） 优先级队列式分支限界法</p><p>解空间：子集树，每个分支节点的分支数为m<br>解向量：x[1:n]  n为部件数量， x[i] 表示第i个部件使用哪个供应商。</p><p>算法：采用优先队列式分支限界法。<br>类似于单源最短路径，使用当前节点所确定下的采购方案对应的机器重量和最为优先级。<br>由于wij不是负值，当前节点所对应的当前机器重量和是解空间中以该节点为根的子树的中所有节点所对应的重量和的下界。</p><p>算法代码实现：</p><p>1）使用列表来代表队列，通过对列表中的活结点按照其当前重量和进行从小到大排序（实现了最小堆的维护）<br>2）定义一个节点类，属性有：节点所在的索引，以及节点当前的重量和<br>3）取出一个扩展节点：由于对活结点表进行了某种规则的排序，则直接取出列表的第一个元素即可<br>4）加入活结点表：将满足条件的子节点加入到活结点表中</p><p>失活当前扩展节点：删掉列表中的第一个元素即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 普通的FIFO 队列式分支限界法</span></span><br><span class="line"><span class="comment">## 当 不满足总价格不超过d的要求时，则剪枝</span></span><br><span class="line"><span class="comment">## 当搜索到深度n时，即搜索到了叶节点，不再进行扩展节点的操作，而是针对于叶节点所对应的最小值，</span></span><br><span class="line"><span class="comment"># 反向求得该节点所对应的的路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入优先级--使用当前节点的重量作为优先级，重量小优先级高</span></span><br><span class="line"><span class="comment"># 将队列改成列表，以append的方式加入到列表中，再以排序的方式维护当前列表的首个元素为最小权值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># coding : utf-8</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到当前扩展节点所在的Level</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getlevel</span><span class="params">(m, currrent)</span>:</span></span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    level = <span class="number">0</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> currrent == <span class="number">0</span>:</span><br><span class="line">        level = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> level</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        level = level+<span class="number">1</span></span><br><span class="line">        sum = m**level + sum  <span class="comment"># sum=m</span></span><br><span class="line">        <span class="keyword">if</span> sum-m**level &lt; currrent &lt;= sum:  <span class="comment"># m-m^0 = m-1</span></span><br><span class="line">            <span class="keyword">return</span> level</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到当前扩展节点，所对应的首个子节点所在的索引</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_idx</span><span class="params">(m, current)</span>:</span></span><br><span class="line">    level = getlevel(m, current)</span><br><span class="line">    <span class="comment"># 求current 在该level中的相对索引，即 相对于该level第一个元素的位置</span></span><br><span class="line">    current_level_idx = current - sum([m**i <span class="keyword">for</span> i <span class="keyword">in</span> range(level)])</span><br><span class="line">    <span class="comment"># 子节点所在层的开始绝对索引</span></span><br><span class="line">    start_idx  = sum([m**i <span class="keyword">for</span> i <span class="keyword">in</span> range(level+<span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> start_idx + current_level_idx*m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到最优解之后，反向查找其路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_path</span><span class="params">(m, current)</span>:</span></span><br><span class="line">    path = []</span><br><span class="line">    path.append(current%m)  <span class="comment"># from 1, not from 0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        level = getlevel(m, current)</span><br><span class="line">        <span class="keyword">if</span> level == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> path[::<span class="number">-1</span>]</span><br><span class="line">        current_level_idx = current - sum([m ** i <span class="keyword">for</span> i <span class="keyword">in</span> range(level)])  <span class="comment"># # 求current 在该level中的相对索引，即 相对于该level第一个元素的位置</span></span><br><span class="line">        path.append(current_level_idx // m + <span class="number">1</span>)  <span class="comment"># 得到上一级的索引位置</span></span><br><span class="line">        current = sum([m ** i <span class="keyword">for</span> i <span class="keyword">in</span> range(level<span class="number">-1</span>)]) + current_level_idx // m  <span class="comment">#得到上一级的值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为活结点表中的节点 定义了一个类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, idx, weight)</span>:</span></span><br><span class="line">        self.idx = idx</span><br><span class="line">        self.weight = weight</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MinWighet</span><span class="params">(n,m,d,price,weight)</span>:</span></span><br><span class="line">    <span class="comment"># 子集树中的节点数</span></span><br><span class="line">    vec_len = sum([m ** i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>)]) + <span class="number">1</span></span><br><span class="line">    que = []</span><br><span class="line">    que.append(Node(<span class="number">0</span>,<span class="number">0</span>))  <span class="comment"># 在活结点表中加入根节点</span></span><br><span class="line">    <span class="comment"># vec_price = [0 for _ in range(vec_len)]</span></span><br><span class="line">    <span class="comment"># vec_weight = [0 for _ in range(vec_len)]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(que):  <span class="comment"># 当活结点表非空时</span></span><br><span class="line">        que = sorted(que, key=<span class="keyword">lambda</span> node: node.weight)  <span class="comment"># 类似于最小堆的维护</span></span><br><span class="line">        current = que[<span class="number">0</span>]  <span class="comment"># 得到当前扩展节点（索引号）</span></span><br><span class="line">        level = getlevel(m, current.idx)  <span class="comment"># 当前 扩展节点所在的level</span></span><br><span class="line"></span><br><span class="line">        new_node_idx = get_idx(m, current.idx)   <span class="comment"># 得到当前扩展节点，所对应的首个子节点所在的索引</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 若搜索完了整棵树</span></span><br><span class="line">        <span class="keyword">if</span> getlevel(m, current.idx) == getlevel(m, vec_len)<span class="number">-1</span>:</span><br><span class="line">            minweight = current.weight</span><br><span class="line">            min_at_idx = current.idx</span><br><span class="line"></span><br><span class="line">            path = get_path(m, min_at_idx)</span><br><span class="line">            <span class="keyword">return</span> minweight, path</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断当前的扩展结点下的所有子节点是否可以加入活结点队列中</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="keyword">if</span> int(current.weight + price[level][i]) &lt;= d:</span><br><span class="line">                new_node = Node(new_node_idx, int(current.weight + weight[level][i]))</span><br><span class="line">                que.append(new_node)</span><br><span class="line">            new_node_idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将当前的扩展节点失活</span></span><br><span class="line">        <span class="keyword">del</span> que[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    n = <span class="number">3</span></span><br><span class="line">    m = <span class="number">3</span></span><br><span class="line">    d = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    price = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]]</span><br><span class="line">    weight = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]]</span><br><span class="line">    result = MinWighet(n,m,d,price,weight)</span><br><span class="line">    print(MinWighet(n,m,d,price,weight))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Grounded Video Description</title>
      <link href="/2019/05/10/Grounded-Video-Description/"/>
      <url>/2019/05/10/Grounded-Video-Description/</url>
      
        <content type="html"><![CDATA[<h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li>这是CVPR 2019 做视频描述的一篇文章，该文的主要贡献是对已有的ActivityNet dense caption数据集进行加强—对其中的帧进行了object bbox的标注，这就为视频描述任务增加了非常有用的信息。</li><li>总的来说，<font color="#dd00dd">该文的出发点是：1. 利用object 信息来生成句子. 2. 希望生成的句子中的名词，在video中可以找到相对应的证据(object)。</font><br></li><li>grounded-based video description model ：联合生成的单词，并微调在description中生成的object。可以探索这种显式的监督对视频描述带来的益处，并与无监督（可能利用region feature，但是没有 penalize grounding）的方法进行对比。</li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>video输出的句子中提到的object，并没有在该video中实际存在。那么为什么有这种现象，是因为前的视频描述模型是基于先验知识，在之前的训练过程中，存在一个视频与该视频有 similar contexts，从而使得输出的单词中涵盖了训练video中的obejct，而不是该video本身的object，导致了该现象。</li><li>因此本文的工作： 将句子中的名词与视频中的object一 一对应起来，这样就可以建立sentence 与 evidence of video 之间的联系。<br>这样可以<font color="#0099ff" size="5">在视频描述模型中使用这些object 信息</font>，同时，<font color="#0099ff" size="5">还可以评估模型</font>（“teaching models to explicitly rely on the corresponding evidence in the video frame  when generating words and evaluating how well models   are doing in grounding individual words or phrases they  generated.”）。<br>如：该模型生成的句子中的名词与已经标注的object是否是一 一对应的（即便一个模型说出争取的sentence，比如一个男孩正在玩耍，但是如果video中有多个个男孩，那么该sentence输出的一个男孩是指向哪一个呢？）。</li></ul><h2 id="标注bbox时的细节"><a href="#标注bbox时的细节" class="headerlink" title="标注bbox时的细节"></a>标注bbox时的细节</h2><ul><li>“we collect ActivityNet-Entities (short as ANet-Entities) which  grounds or links noun phrases in sentences with bounding  boxes in the video frames.”<br>“we only  annotate a single frame of the video for each noun phrase” 。<br>即， 是对照着已有的sentence中的名词对其进行加框(bbox)，而不是对该video中的所有示例进行标注。对于sentence中的一个名词只在一帧上进行标注（稀疏标注） </li></ul><h2 id="调研工作"><a href="#调研工作" class="headerlink" title="调研工作"></a>调研工作</h2><h3 id="1-结合object-feature-做captioning任务"><a href="#1-结合object-feature-做captioning任务" class="headerlink" title="1. 结合object feature 做captioning任务"></a>1. 结合object feature 做captioning任务</h3><ul><li><strong>当前的方法</strong>，主要是两步：（1）使用off-the-shelf 或者是fine-tuned 的 object detector 来得到 object proposals （2）对object features采用动态attention，或者是对region进行分类，然后送入 decoder中。</li><li><strong>存在的问题</strong>，使用现成的object detector 将会使得到的object proposals 更偏向于 source dataset , 而不是偏向于当前的视频描述target dataset。一种解决方案是针对于target dataset 来fine-tune object detector。但是这种那个方案需要大量的标注，尤其是对于video，数据量会更大，</li><li>因此提出了<font color="#0099ff" size="5" face="黑体">本文的方法(给出了fine-tune obejct detector的改进方案)</font>：“Instead of  fine-tuning a general detector, we transfer the object classification knowledge from off-the-shelf object detectors to  our model and then fine-tune this representation as part of  our generation model with sparse box annotations. ”。</li><li>在文章中的4.4节开头给出了具体的实现：已经得到了bbox，现在的目的是想要得到the class probability distribution for each region. 将在visual genome上预训练的detector迁移到我们的<strong>object classifier</strong> 任务上，另外关于classes集合，假定我们已经有了K个类别，则我们在Visual Genome中根据最近距离找到与其对应的classes。  定义一个softmax( Wx+b )的分类层，W 和 b 的初始化是预训练的detector的最后一个线性层（分类层）的参数值（W应该是根据找出的K个类别按照索引抽出的一个矩阵）。<h3 id="2-object-attention"><a href="#2-object-attention" class="headerlink" title="2. object attention"></a>2. object attention</h3></li><li>某些作者指出，attention model关注的region和人类的关注点并不一致，增加attention supervision几乎不能提高性能。另一方面，在feature map attention 上增加监督，是有益处的。</li><li>在该文作者的实验中，region attention with supervision 并不能带来性能的提升，作者分析，这可能是由于缺少object context 信息，因此<font color="#0099ff" size="5" face="黑体">该文作者在attention model中引入了基于context encoding 的self-attention</font>，这将会使得信息能够在被采样的视频帧中的regions 传递(我理解的是，region feature 不仅仅是单纯的从fc层中提取到的信息，同时也结合了其余信息来得到 grounding-aware region encoding， 在文章的4.3 以及 4.4节有关于<strong>R<sup>~</sup></strong> 的定义)。 </li></ul><h2 id="Description-with-Grounding-Supervision"><a href="#Description-with-Grounding-Supervision" class="headerlink" title="Description with Grounding Supervision"></a>Description with Grounding Supervision</h2><ul><li>这个框架包括三个模块: <strong>grounding</strong>, <strong>region attention</strong> and <strong>language generation</strong>.<br>grounding： 对于生成的word， 从video中检测到对应的visual clue。<br>region attention: 动态的将visual clue 形成一个high-level的视觉内容的表达，并将其送入decoder。   </li><li>这里包括三种方式来结合object-level supervision: <strong>region classification</strong>,  <strong>object grounding (localization)</strong>, and <strong>supervised attention</strong>.  </li><li><strong>（我的理解，supervised attention直接针对attention中的系数，查看与真实的对应关系，设计的这个loss对于视频描述生成由益处；object grounding 涉及到了region attention中的系数，因此与描述生成有一定的关联，反向传播可能是有益处的；region classification中设计的loss： 它的反向传播会更新M<sub>s</sub>( R ), 进一步作用于region encoding, 进一步作用于region attention 和 language generate；）</strong></li></ul><h3 id="Language-Generation-Module"><a href="#Language-Generation-Module" class="headerlink" title="Language Generation Module"></a>Language Generation Module</h3><ul><li>本文的decoder 部分采用 [1] 中提到的decoder，与bottom up[3] 的结构大致近似，但是<strong>在[1]中</strong>第二层 language lstm的输入部分，不仅包括attention of region feature， 还包括attention of 最后一个卷积层k girds的特征。对应到本文的视频描述任务上，第二层language lstm的输入，不仅包括attention of region features ，还包括 attention of frames features。即region attention 和 temporal attention</li><li>需要注意的是该文中使用的<font color="#0099ff" size="5" face="黑体">temporal attention</font>是[2]中提到的self-attention context encoder with Bidirectional GRU (Bi-GRU)，而不是[1]中使用的attention机制。</li><li><font color="#0099ff" size="5" face="黑体">region attention</font>采用的就是bottom up 中的attention结构</li><li>下面将[1] 中的原图贴一下：   <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2x5hlq801j30l20ezq4v.jpg">   </li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Region-proposal-and-features"><a href="#Region-proposal-and-features" class="headerlink" title="Region proposal and features"></a>Region proposal and features</h3><p>For each frame, we use a Faster  R-CNN detector [24] with ResNeXt-101 backbone [30] for  region proposal and feature extraction (fc6). The detector is  pretrained on Visual Genome。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Neural baby talk. In CVPR 2018.<br>[2] End-to-end dense video captioning with masked transformer. In CVPR 2018.<br>[3] Bottom-up and top-down attention for image captioning and  visual question answering. In CVPR 2018.</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning（CVPR2019）</title>
      <link href="/2019/05/10/Spatio-Temporal-Dynamics-and-Semantic-Attribute-Enriched-Visual-Encoding-for-Video-Captioning-1/"/>
      <url>/2019/05/10/Spatio-Temporal-Dynamics-and-Semantic-Attribute-Enriched-Visual-Encoding-for-Video-Captioning-1/</url>
      
        <content type="html"><![CDATA[<p><img src="https://i.loli.net/2019/09/02/T5AzpW8DHkVL2Oy.png" alt="搜狗截图20190902152617.png"></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>我们的方法丰富视觉特征的<strong>时域动态temporal dynamics</strong>，通过在整个video上分层对CNN特征应用短的fourier 变换</li><li>从object detector 中提取高层语义，来丰富被检测object 的<strong>空间动态 spatial dynamics</strong></li><li>最终的表达映射到一个压缩的空间</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>object detector YOLO[1]</li><li>目标检测和C3D的输出层被用来得到高层语义属性，</li><li>提出的视觉特征包含检测的目标属性、目标发生的频率</li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>现有的video captioning model 一种使用平均池化得到特征，一种使用attention得到high level特征，但是这些视觉特征都是直接的被使用，则，这些方法没有充分利用CNN在视频字幕框架中的最新特性。我们的模型丰富了视觉特征，实验结果证明，该视觉特征与任意一个简单的语言模型相结合，可以提高其性能。</p><h2 id="Visual-Representation"><a href="#Visual-Representation" class="headerlink" title="Visual Representation"></a>Visual Representation</h2><ul><li>the visual representation of a  video V as v = [α; β; γ; η]</li><li>α; β; γ; η 是四个列向量，下面具体介绍如何得到这四个列向量<h3 id="Encoding-temporal-dynamics"><a href="#Encoding-temporal-dynamics" class="headerlink" title="Encoding temporal dynamics"></a>Encoding temporal dynamics</h3></li><li>首先已经有 f 帧 对应的CNN[2]特征向量，和c个clip对应的C3D[3]得到的特征向量</li><li>对某个video而言，其所有帧再某一个维度的神经元，组成了一个特征向量a，利用<strong>傅里叶变换</strong>得到一个p维度的特征向量，将a分成两半，分别进行傅里叶变换，得到一个p维度的特征向量，再次进行分半，等等一系列操作，可以得到 p×7的矩阵。则对于所有的神经元m 则得到m×p×7的张量。至此得到<strong>α</strong></li><li><strong>β</strong>同理，只是对clips对应的C3D特征进行处理</li><li>目前已经有将傅里叶变换应用在行为识别上的文章吗，但是本文是第一篇将傅里叶变换应用在视频描述上的文章。</li><li><font color="#0099ff" size="4" face="黑体">但是需要注意的是，该文并没有说明使用傅里叶变换的动机（rich temporal dynamics?），但是为什么使用傅里叶变换可以丰富？？</font></li></ul><h3 id="Encoding-Semantics-and-Spatial-Evolution"><a href="#Encoding-Semantics-and-Spatial-Evolution" class="headerlink" title="Encoding Semantics and Spatial Evolution"></a>Encoding Semantics and Spatial Evolution</h3><ul><li>比较复杂，利用object detector YOLO 来提取Object 以及C3D来加强语义信息，具体看论文吧</li></ul><h2 id="Experimental-Results-on-MSVD"><a href="#Experimental-Results-on-MSVD" class="headerlink" title="Experimental Results on MSVD"></a>Experimental Results on MSVD</h2><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2wd8q6kegj30dh0ig0wc.jpg">    <ul><li>GRU-MP - (C3D) 与 使用了傅里叶变换的GRU-EVEhft - (C3D)  相比，可知，使用傅里叶变换是有小鬼的</li><li>GRU-EVEhft - (CI) 与GRU-EVEhft+sem - (CI)相比，可得增加的senmatic 效果是不显著的。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Yolo9000: better, faster, stronger.  In IEEE CVPR, 2017<br>[2] Inception-v4, inception-resnet and the impact of residual  connections on learning. In AAAI, volume 4, page 12, 2017.<br>[3] Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference  on computer vision, pages 4489–4497, 2015.</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python-dict</title>
      <link href="/2019/05/08/python-dict/"/>
      <url>/2019/05/08/python-dict/</url>
      
        <content type="html"><![CDATA[<h1 id="dict-的get-函数"><a href="#dict-的get-函数" class="headerlink" title="dict 的get 函数"></a>dict 的get 函数</h1><h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Python 字典(Dictionary) get() 函数返回指定键的值，如果值不在字典中返回默认值。</p><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><p>get()方法语法：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dict.<span class="builtin-name">get</span>(key, <span class="attribute">default</span>=None)</span><br></pre></td></tr></table></figure><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><ul><li>key – 字典中要查找的键。</li><li>default – 如果指定键的值不存在时，返回该默认值值。</li></ul><h2 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h2><p>返回指定键的值，如果值不在字典中返回默认值None。</p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><p>get函数的作用是返回指定key的值，若key不存在，则返回default值，default值，默认为None，也可以自己指定</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
      <link href="/2019/05/06/Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention/"/>
      <url>/2019/05/06/Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention/</url>
      
        <content type="html"><![CDATA[<ul><li>encoder  attention<br>本文的出发点是利用低层次的特征，并结合了attention 机制<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2re7nfbn3j30q70430sx.jpg"></li></ul><p>参考链接：<a href="https://blog.csdn.net/shenxiaolu1984/article/details/51493673" target="_blank" rel="noopener">https://blog.csdn.net/shenxiaolu1984/article/details/51493673</a></p>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>池化层的反向传播</title>
      <link href="/2019/04/21/%E6%B1%A0%E5%8C%96%E5%B1%82%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
      <url>/2019/04/21/%E6%B1%A0%E5%8C%96%E5%B1%82%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
      
        <content type="html"><![CDATA[<ul><li>参考： <a href="https://blog.csdn.net/qq_21190081/article/details/72871704" target="_blank" rel="noopener">https://blog.csdn.net/qq_21190081/article/details/72871704</a></li><li>总结，<br>（1）对于平均池化层，比如2×2-&gt;1， 梯度的反向传递，是1-&gt;2×2，若梯度为x, 则，反向传播的梯度为4个 1/4<br>（1）对于最大池化层，比如2×2-&gt;1， 梯度的反向传递，是1-&gt;2×2，若梯度为x, 则，反向传播只赋值给最大值所在的元素，其余三个元素的梯度为0</li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An End-to-End Baseline for Video Captioning</title>
      <link href="/2019/04/20/An-End-to-End-Baseline-for-Video-Captioning/"/>
      <url>/2019/04/20/An-End-to-End-Baseline-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="积累梯度那里没有看明白-–-解决内存占用多的问题"><a href="#积累梯度那里没有看明白-–-解决内存占用多的问题" class="headerlink" title="积累梯度那里没有看明白 – 解决内存占用多的问题"></a>积累梯度那里没有看明白 – 解决内存占用多的问题</h3><p>我认为可能是积累 loss, 直到达到某一个batch size才进行反向传播<br>说是为了解决内存占用多的问题，可是这样，就解决了吗？？？</p><h3 id="灵魂反问"><a href="#灵魂反问" class="headerlink" title="灵魂反问"></a>灵魂反问</h3><p>为什么要是end-to-end，我最终需要的是一个效果比较好的模型，但是为了只得到这样的一点提升，反而会需要很多的GPU计算资源。这个end-to-end fine-tune 是否有必要。</p><h3 id="目前方法存在的问题"><a href="#目前方法存在的问题" class="headerlink" title="目前方法存在的问题"></a>目前方法存在的问题</h3><p>encoder：比如 CNN， 一般是在不同任务上的其他数据集上进行预训练的，之后，在训练video captioning任务时，得到的video/image feature就不再fine tune。这样得到的结果是次优的。<br>目前改进这一缺陷的文章有：[1][2][3]，他们尝试捕捉不同帧之间的动态时域，但是，他们没有从根本上改变一个事实：视频描述任务需要一个与该任务相关的特征。</p><h4 id="当前没有人去fine-tune-encoder的原因"><a href="#当前没有人去fine-tune-encoder的原因" class="headerlink" title="当前没有人去fine-tune encoder的原因"></a>当前没有人去fine-tune encoder的原因</h4><p>（1）because of the amount of memory required to process  video data for each batch。是因为每个批次都要处理视频数据，所需要的存储空间会很大。<br>（2）batch sizes for video captioning can become very high (e.g. 512), making training  prohibitive on a small number of GPUs。同时，视频描述的批次一般都比较大，使得所需要的GPU数量会很多。</p><h4 id="本文提取出的解决方案—-即训练过程"><a href="#本文提取出的解决方案—-即训练过程" class="headerlink" title="本文提取出的解决方案—-即训练过程"></a>本文提取出的解决方案—-即训练过程</h4><p>In this paper we address this issue by accumulating gradients over multiple steps, to update parameters only after  the required effective batch size is achieved.<br>在多步积累梯度，并只在达到有效的批次大小之后(当神经网络训练完512个examples)，才进行梯度更新。<br>这种训练方案相比于分别训练两部分收敛速度会慢，因为所需的迭代次数增加了。但是这里采用了一个加速训练过程的方案，先分别训练encoder和decoder，然后再end-to-end进行fine-tune。</p><h4 id="主要的贡献"><a href="#主要的贡献" class="headerlink" title="主要的贡献"></a>主要的贡献</h4><ol><li>可以得到与具体任务（视频描述）相关的特征</li><li>积累梯度来限制GPU存储消耗，因此可以处理大批次，这是基于RNN的decoders所需的。</li><li>使用了两阶段的训练来加速训练</li><li>为未来的工作，创建了一个简单的baseline</li></ol><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><ol><li>先预训练encoder，例如利用图像识别、行为识别等</li><li>冻结encoder的参数，在视频描述任务上训练decoder，直到在验证数据集上表现出比较好的效果</li><li>整个网络，端到端的训练，冻结Inception-ResNet-v2中的BN层，由于该过程中占用的内存较多，作者采用了一种方法：accumulating gradients over multiple steps, to update parameters only after  the required effective batch size is achieved. </li></ol><ul><li>需要注意的是，在2. 3. 阶段的训练过程中，SA-LSTM都是使用target words(ground truth)作为输入，而不是使用之前的预测。</li></ul><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p> Inception-ResNet-v2[5] as an encoder，and a modified version of  Soft-Attention LSTM as a decoder.<br> <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2ad637ykjj30hy0h677d.jpg">    </p><ul><li>decoder<ul><li>输入LSTM的input: <strong>x<sup>t</sup></strong> ：<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2aeagi7o3j30a30280sl.jpg">    - 该step 生成的word：<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2aeiriwanj30jd014mx7.jpg"></li></ul></li></ul><p>soft-attention这里，原文是采用attention机制进行加权求和 ，这里与原soft-attention[2] 略有不同。</p><ul><li><p>原soft-attention论文：</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2aeagheogj308o03ct8l.jpg">     </li><li><p>现修改为：（增加了β<sub>t</sub>）</p></li></ul><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2aeagka0rj30h504dq37.jpg">   <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2aeaggzpdj309302xdfq.jpg">   <ul><li><font color="#0099ff" size="5" face="黑体">这里与Figure2 图中显示的结构并不一致，这里是全权求和，但是在图中却是concatenate !</font></li></ul><ul><li>还有一些其他的修改<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2aes6w62rj30il09xwgp.jpg"></li></ul><h3 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h3><ul><li>step1 : encoder先训练，然后固定encoder的参数，训练decoder的参数，不进行联合训练</li><li>step2：在step1的基础上，联合训练encoder-decoder。</li><li>MSR-VTT的实验结果<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2ajqfodpkj30kl0b9acc.jpg">  </li><li>MSVD的实验结果<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2ajqfqhbjj30ks0icn0q.jpg"></li></ul><h4 id="yaya：-本文step1训练效果比较好的原因（相较于soft-attention-lstm-原论文）"><a href="#yaya：-本文step1训练效果比较好的原因（相较于soft-attention-lstm-原论文）" class="headerlink" title=" yaya： 本文step1训练效果比较好的原因（相较于soft-attention lstm 原论文）"></a><font color="#0099ff" size="5" face="黑体"> yaya： 本文step1训练效果比较好的原因（相较于soft-attention lstm 原论文）</font></h4><p>这里step1的意思是：encoder、decoder 分开训练，并不进行联合训练</p><ol><li>该文使用的 Inception-ResNet-v2作为encoder来提取特征。会比其他论文中使用的encoder更复杂。</li><li>初始化的细节<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2ak8as8u4j309703pglo.jpg"></li><li>对SA-LSTM[2]而言，进行了一些修改：<br>（1）frame features 转为一个特征向量，使用的注意力机制，但是该文在soft-attention的基础上，还增加了一个系数β<sub>t</sub><br>（2）LSTM的内部结构的计算公式增加了一项，如下图<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2ajg7vn8rj31fi0ci427.jpg">（3）在生成word时，主要差别就是E[y<sub>t-1</sub>]前边是否有权重的问题 - soft-attention 使用的公式：<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2aeiriwanj30jd014mx7.jpg"> - 但是在本文中使用的公式为：<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2ajjxtsplj30d103v74i.jpg">- 但是就该文3.3.2节中说：These changes are inspired by  the original code repository by Yao et al [2]，也就是有可能人家的源代码和在论文中提到的不一致。</li></ol><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Video captioning with  transferred semantic attributes. CVPR, 2017<br>[2] Describing videos by exploiting temporal  structure. ICCV, 2015.<br>[3] Task-driven dynamic fusion: Reducing ambiguity in video description. CVPR, 2017.<br>[4] Show and  tell: Lessons learned from the 2015 mscoco image captioning challenge. TPAMI, 2016.<br>[5] Inception-v4, inception-resnet and the impact of residual  connections on learning. AAAI, 2017</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python 关于list的操作</title>
      <link href="/2019/04/18/python-%E5%85%B3%E4%BA%8Elist%E7%9A%84%E6%93%8D%E4%BD%9C/"/>
      <url>/2019/04/18/python-%E5%85%B3%E4%BA%8Elist%E7%9A%84%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h3 id="list-的置换"><a href="#list-的置换" class="headerlink" title="list 的置换"></a>list 的置换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">nums[<span class="number">0</span>],nums[<span class="number">1</span>] = nums[<span class="number">1</span>],nums[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># nums = [2,1,3]</span></span><br></pre></td></tr></table></figure><h3 id="判断是否为空列表"><a href="#判断是否为空列表" class="headerlink" title="判断是否为空列表"></a>判断是否为空列表</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = []</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> a:</span><br><span class="line">    print(<span class="string">"a is a null list"</span>)</span><br><span class="line"><span class="comment"># 输出：a is a null list</span></span><br></pre></td></tr></table></figure><h3 id="列表的连接"><a href="#列表的连接" class="headerlink" title="列表的连接"></a>列表的连接</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b = [<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]</span><br><span class="line">c = a + b</span><br><span class="line">print(c)  <span class="comment"># [1, 2, 3, 6, 7, 8]</span></span><br><span class="line">d = a.extend(b)  <span class="comment"># extend()传入的参数需要是一个迭代对象 ：列表、元组、集合</span></span><br><span class="line">print(d) </span><br><span class="line"><span class="comment"># d 输出为None ,因为extend 无返回值，但是此时 a更改为[1, 2, 3, 6, 7, 8]</span></span><br></pre></td></tr></table></figure><h3 id="列表的排序函数"><a href="#列表的排序函数" class="headerlink" title="列表的排序函数"></a>列表的排序函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = [2,4,6,5]</span><br><span class="line">b = a.sort()  # a = [2,4,5,6] , b = None</span><br><span class="line">a= [2,4,6,5]  </span><br><span class="line">b = sorted(a)  # a = [2, 4, 6, 5] , b = [2, 4, 5, 6]</span><br><span class="line"></span><br><span class="line">## sorted() 输出的是排序的结果，但是不更改传入的列表</span><br><span class="line">## sort() 直接对列表进行排序操作，并更改列表值</span><br></pre></td></tr></table></figure><h3 id="列表的置换顺序函数"><a href="#列表的置换顺序函数" class="headerlink" title="列表的置换顺序函数"></a>列表的置换顺序函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">b = a.reverse() <span class="comment"># a=[4,3,2,1] b = None</span></span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对于列表 nums<br>凡是可以进行 nums.function()，这样的函数，一般是无返回值的，直接对列表本身进行操作。<br>比如 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">nums.extend([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])     <span class="comment"># [1, 2, 3, 4, 1, 2, 3]</span></span><br><span class="line">nums.append(<span class="number">2</span>)           <span class="comment"># [1, 2, 3, 4, 1, 2, 3, 2]</span></span><br><span class="line">nums.sort()              <span class="comment"># [1, 1, 2, 2, 2, 3, 3, 4]</span></span><br><span class="line">nums.reverse()           <span class="comment"># [4, 3, 3, 2, 2, 2, 1, 1]</span></span><br><span class="line">nums.insert(<span class="number">5</span>,<span class="number">100</span>)       <span class="comment"># [4, 3, 3, 2, 2, 100, 2, 1, 1]    在索引为5的位置插入元素100</span></span><br></pre></td></tr></table></figure><ul><li>注意<br>nums.index(100)          # <strong>有返回值5</strong>, 查找对应元素<strong>首次出现</strong>所在位置的索引</li></ul><h3 id="数字转为列表-123-gt-“1”-“2”-“3”"><a href="#数字转为列表-123-gt-“1”-“2”-“3”" class="headerlink" title="数字转为列表  123 -&gt; [“1”, “2”, “3”]"></a>数字转为列表  123 -&gt; [“1”, “2”, “3”]</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num = <span class="number">123</span></span><br><span class="line">A = list(str(num))  <span class="comment"># A = ['1', '2', '3']</span></span><br><span class="line">B = int(<span class="string">""</span>.join(A))  <span class="comment"># B = 123</span></span><br></pre></td></tr></table></figure><h3 id="中括号-for循环生成列表，，并使用判断"><a href="#中括号-for循环生成列表，，并使用判断" class="headerlink" title="中括号 for循环生成列表，，并使用判断"></a>中括号 for循环生成列表，，并使用判断</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">2</span>,<span class="number">3</span>,<span class="number">-7</span>,<span class="number">0</span>,]</span><br><span class="line">a = [<span class="number">1</span> <span class="keyword">if</span> num &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> num <span class="keyword">in</span> nums]</span><br><span class="line">print(a)   <span class="comment"># [1, 1, 0, 0]</span></span><br></pre></td></tr></table></figure><h2 id="python内置函数的复杂度"><a href="#python内置函数的复杂度" class="headerlink" title="python内置函数的复杂度"></a>python内置函数的复杂度</h2><p><a href="https://wiki.python.org/moin/TimeComplexity" target="_blank" rel="noopener">https://wiki.python.org/moin/TimeComplexity</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>双指针(two pointers)</title>
      <link href="/2019/04/18/%E5%8F%8C%E6%8C%87%E9%92%88-two-pointers/"/>
      <url>/2019/04/18/%E5%8F%8C%E6%8C%87%E9%92%88-two-pointers/</url>
      
        <content type="html"><![CDATA[<ul><li>荷兰分区问题<br>可参考 <a href="https://blog.csdn.net/sylar_d/article/details/52742598" target="_blank" rel="noopener">https://blog.csdn.net/sylar_d/article/details/52742598</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>pytorch0.4.0学习</title>
      <link href="/2019/04/18/pytorch0-4-0%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/04/18/pytorch0-4-0%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="不会的"><a href="#不会的" class="headerlink" title="不会的"></a>不会的</h2><ol><li>SGD 、 Adam中 的weight_decay 是干嘛的</li><li>SGD 中的 momentum是干嘛的，一般设置为多大</li><li>pytorch 中的初始化函数 </li><li>y = y.permute(0, 2, 1).contiguous()<br>这是干嘛的</li></ol><h3 id="学会的"><a href="#学会的" class="headerlink" title="学会的"></a>学会的</h3><ul><li>累加loss<br>以前（0.3.0）了累加loss(为了看loss的大小)一般是用total_loss+=loss.data[0] , 比较诡异的是, 为啥是.data[0]? 这是因为, 这是因为loss是一个Variable,<br>所以以后累加loss, 用loss.item().这个是必须的, 如果直接加, 那么随着训练的进行, 会导致后来的loss具有非常大的graph, 可能会超内存.<br>然而total_loss只是用来看的, 所以没必要进行维持这个graph!</li></ul><h3 id="pytorch-中-对tensor的一些函数"><a href="#pytorch-中-对tensor的一些函数" class="headerlink" title="pytorch 中 对tensor的一些函数"></a>pytorch 中 对tensor的一些函数</h3><ul><li><p>生成正态分布的随机张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)  <span class="comment"># 正态分布的随机张量</span></span><br><span class="line">a.sum()                 <span class="comment"># 对a中的元素求和</span></span><br></pre></td></tr></table></figure></li><li><p>对tensor 求最大值</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">a</span> = torch.randn(<span class="number">10</span>,<span class="number">8</span>)</span><br><span class="line">max_value, max_index = <span class="keyword">a</span>.<span class="built_in">max</span>(<span class="number">1</span>)   </span><br><span class="line"><span class="comment"># 按照维度对a求最大值 ，此处为1，即得到（10,1）的张量，</span></span><br><span class="line"><span class="comment"># 有两个返回值，第一个返回值为具体的最大值为多少，第二个返回值为该最大值所在的索引</span></span><br></pre></td></tr></table></figure></li><li><p>判断两数有多少个元素相等</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>])</span><br><span class="line">a.eq(b)   # 输出 tensor([ <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">0</span>], dtype=torch.uint8)</span><br></pre></td></tr></table></figure></li><li><p>批矩阵相乘</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 批矩阵相乘 pytorch <span class="number">0.3</span><span class="number">.0</span></span><br><span class="line">output = torch.bmm(W, x)</span><br><span class="line"></span><br><span class="line"># 批矩阵相乘 pytorch <span class="number">0.4</span><span class="number">.0</span></span><br><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = torch.rand(<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">c = torch.matmul(a, b)</span><br><span class="line">print(c.shape)  # torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"># 在pytorch <span class="number">0.4</span><span class="number">.0</span>中使用torch.matmul 输入的参数是两个<span class="number">3</span>d的tensor ,tensor的首个维度是batch_size</span><br></pre></td></tr></table></figure></li><li><p>tensor 两个维度转置</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># tensor 两个维度转置</span><br><span class="line">x = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">x = x.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">print(x.shape)  # torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure></li><li><p>chunk  cat </p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.cat(inputs, <span class="attribute">dimension</span>=0) → Tensor</span><br><span class="line"><span class="comment"># cat 是将多个tensor按照指定的维度拼接起来</span></span><br><span class="line">torch.chunk(tensor, chunks, <span class="attribute">dim</span>=0)</span><br><span class="line"><span class="comment"># chunk是将某个tensor按照指定的维度进行拆分成指定的块数</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="初始化函数"><a href="#初始化函数" class="headerlink" title="初始化函数"></a>初始化函数</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">self.W = nn.Conv2d(in_channels=<span class="number">512</span>, out_channels=<span class="number">1024</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>), \</span><br><span class="line">                   padding=(<span class="number">1</span>,<span class="number">1</span>), stride=(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">nn.init.kaiming.normal(self.W.weight)</span><br><span class="line">nn.init.kaiming.uniform(self.W.weight)</span><br><span class="line">nn.init.constant(self.W.bias, <span class="number">0</span>)</span><br><span class="line"># 输出 tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>动态规划(dynamic programming)</title>
      <link href="/2019/04/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-dynamic-programming/"/>
      <url>/2019/04/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-dynamic-programming/</url>
      
        <content type="html"><![CDATA[<h3 id="从起始点，走到终点"><a href="#从起始点，走到终点" class="headerlink" title="从起始点，走到终点"></a>从起始点，走到终点</h3><p>（1）共有多少路径<br>（2）哪条路径最短<br>对于grid 走路，只有两种走走法，这类问题，需要：   </p><ul><li>分析最后终点的结果，是怎么得来的：是由左邻和上邻的结果，进行某种运算得来的   </li><li>先将第一行、第一列进行初始化（结合具体问题）   </li><li>分析，递推公式，并采用自底向上的方式，因此，需要先高度的想，最后终点的递推公式，在结合这个公式，分析，在初始化之后，接下来的点，如何根据初始化的值，以及递推公式来计算得到。   </li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>分治递归： 一步一步的化解为小问题，最终由小问题再反向计算各大问题。自上而下   </li><li>动态规划：也需要得到递推公式，但是需要先将小问题的值写出来（初始化阶段），再根据递推公式，写for循环   </li></ul><h3 id="对于求解任意两个节点之间的值，并求全局最优解的问题，还需要再总结"><a href="#对于求解任意两个节点之间的值，并求全局最优解的问题，还需要再总结" class="headerlink" title="对于求解任意两个节点之间的值，并求全局最优解的问题，还需要再总结"></a>对于求解任意两个节点之间的值，并求全局最优解的问题，还需要再总结</h3><ul><li><a href="https://leetcode.com/problems/maximum-subarray/" target="_blank" rel="noopener">https://leetcode.com/problems/maximum-subarray/</a>  </li><li><a href="https://leetcode.com/problems/maximum-product-subarray/" target="_blank" rel="noopener">https://leetcode.com/problems/maximum-product-subarray/</a>  </li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>排序(sort)</title>
      <link href="/2019/04/18/%E6%8E%92%E5%BA%8F-sort/"/>
      <url>/2019/04/18/%E6%8E%92%E5%BA%8F-sort/</url>
      
        <content type="html"><![CDATA[<ul><li>快速排序与冒泡排序均是进行交换操作，使用的空间复杂度为O(1)，而插入排序的空间复杂度为O(n)</li><li>快速排序的平均时间复杂度为O(nlogn)，最坏情况复杂度为O(n^2)</li><li>冒泡排序的时间复杂度为O(n^2)</li><li>插入排序的时间复杂度为O(n^2)</li></ul><h2 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h2><blockquote><p>待补充</p></blockquote><h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partition</span><span class="params">(nums, left, right)</span>:</span></span><br><span class="line">    tmp = left</span><br><span class="line">    reference = nums[left]  <span class="comment"># 以最左端的nums[left] 作为中位数</span></span><br><span class="line">    left = left</span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> nums[left] &lt;= reference:</span><br><span class="line">            left = left + <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> nums[right] &gt; reference:</span><br><span class="line">            right = right - <span class="number">1</span></span><br><span class="line">        nums[left], nums[right] = nums[right], nums[left]</span><br><span class="line">    <span class="keyword">if</span> nums[left] &lt; reference:</span><br><span class="line">        nums[left], nums[tmp] = nums[tmp], nums[left]</span><br><span class="line">        <span class="keyword">return</span> left</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        nums[left<span class="number">-1</span>], nums[tmp] = nums[tmp], nums[left<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">return</span> left<span class="number">-1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">QuickSort</span><span class="params">(nums , left, right)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> left &lt; right:</span><br><span class="line">        index = partition(nums, left, right)</span><br><span class="line">        QuickSort(nums, left, index<span class="number">-1</span>)</span><br><span class="line">        QuickSort(nums, index+<span class="number">1</span>, right)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">nums = [<span class="number">6</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>]</span><br><span class="line"><span class="comment"># nums = [1,2,3,4]</span></span><br><span class="line"><span class="comment"># nums = [3,2,5,6,4,4,4,5,6]</span></span><br><span class="line">left = <span class="number">0</span></span><br><span class="line">right = len(nums)<span class="number">-1</span></span><br><span class="line">QuickSort(nums, left, right)</span><br><span class="line">print(nums)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法</title>
      <link href="/2019/04/18/%E7%AE%97%E6%B3%95/"/>
      <url>/2019/04/18/%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<ul><li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/19131887" target="_blank" rel="noopener">别人的博客</a></p></li><li><p><a href="https://shiyaya.github.io/2019/04/18/%E5%8F%8C%E6%8C%87%E9%92%88-two-pointers/" target="_blank" rel="noopener">双指针</a></p></li><li><p><a href="https://shiyaya.github.io/2019/04/18/%E6%8E%92%E5%BA%8F-sort/" target="_blank" rel="noopener">排序</a></p></li><li><p><a href="https://shiyaya.github.io/2019/04/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-dynamic-programming/" target="_blank" rel="noopener">动态规划</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python 函数</title>
      <link href="/2019/04/10/python-%E5%87%BD%E6%95%B0/"/>
      <url>/2019/04/10/python-%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>numpy广播</title>
      <link href="/2019/04/09/numpy%E5%B9%BF%E6%92%AD/"/>
      <url>/2019/04/09/numpy%E5%B9%BF%E6%92%AD/</url>
      
        <content type="html"><![CDATA[<ul><li>末尾有彩蛋</li></ul><h1 id="NumPy-广播-Broadcast"><a href="#NumPy-广播-Broadcast" class="headerlink" title="NumPy 广播(Broadcast)"></a>NumPy 广播(Broadcast)</h1><p>广播(Broadcast)是 numpy 对不同形状(shape)的数组进行数值计算的方式， 对数组的算术运算通常在相应的元素上进行。</p><p>如果两个数组 a 和 b 形状相同，即满足 <strong>a.shape == b.shape</strong>，那么 a*b 的结果就是 a 与 b 数组对应位相乘。这要求维数相同，且各维度的长度相同。</p><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>import numpy as np    a = np.array([1,2,3,4])  b = np.array([10,20,30,40])  c = a * b  print (c)</p><p>输出结果为：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ <span class="number">10</span>  <span class="number">40</span>  <span class="number">90</span> <span class="number">160</span>]</span><br></pre></td></tr></table></figure><p>当运算中的 2 个数组的形状不同时，numpy 将自动触发广播机制。如：</p><h2 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np    </span><br><span class="line">a = np.array([[ <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],            [<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>],            [<span class="number">20</span>,<span class="number">20</span>,<span class="number">20</span>],            [<span class="number">30</span>,<span class="number">30</span>,<span class="number">30</span>]]) </span><br><span class="line">b = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) print(a + b)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[ <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line"> [<span class="number">11</span> <span class="number">12</span> <span class="number">13</span>]</span><br><span class="line"> [<span class="number">21</span> <span class="number">22</span> <span class="number">23</span>]</span><br><span class="line"> [<span class="number">31</span> <span class="number">32</span> <span class="number">33</span>]]</span><br></pre></td></tr></table></figure><p>下面的图片展示了数组 b 如何通过广播来与数组 a 兼容。</p><p><img src="http://www.runoob.com/wp-content/uploads/2018/10/image0020619.gif" alt="img"></p><p>4x3 的二维数组与长为 3 的一维数组相加，等效于把数组 b 在二维上重复 4 次再运算。</p><h2 id="yayay实例"><a href="#yayay实例" class="headerlink" title="yayay实例"></a>yayay实例</h2><p>在few-shot gnn任务中，想要计算邻接矩阵A，其公式为：<strong>a<sub>ij</sub> = fc(v<sub>i</sub>-v<sub>j</sub>)</strong><br>那么问题来了得到的邻接矩阵是N×N的，则计算的差值矩阵也应该是N×N的。那么该如何高效的计算出来这个差值矩阵。</p><pre><code class="python"><span class="keyword">import</span> numpy <span class="keyword">as</span> npN = <span class="number">10</span>D = <span class="number">7</span>X = np.ones((N,D))X1 = np.expand_dims(X, axis=<span class="number">0</span>)X2 = np.expand_dims(X, axis=<span class="number">1</span>)X_abs = np.abs(X1-X2)X_abs = np.reshape(X_abs, (N,N,D))X_T = X_abs ?????????????????需要考虑一下这个转置问题</code></pre>]]></content>
      
      
      <categories>
          
          <category> numpy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>少样本学习(few-shot learning)</title>
      <link href="/2019/04/09/%E5%B0%91%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0-few-shot-learning/"/>
      <url>/2019/04/09/%E5%B0%91%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0-few-shot-learning/</url>
      
        <content type="html"><![CDATA[<h2 id="小样本学习的概念"><a href="#小样本学习的概念" class="headerlink" title="小样本学习的概念"></a>小样本学习的概念</h2><ul><li>少样本学习(few-shot learning)的目标是在<strong>已知类别(Seen Class)训练一个分类模型，使它能够在只有少量数据的未知类别(Unseen Class)上面具有很好的泛化性能</strong>。</li><li>少样本学习面临两个重要的问题：<br>（1）已知类别和未知类别之间没有交集，导致它们的数据分布差别很大，不能直接通过训练分类器和微调(finetune)的方式得到很好的性能；<br>（2）未知类别只有极少量数据(每个类别1或者5个训练样本)，导致分类器学习不可靠。</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>指数加权平均</title>
      <link href="/2019/04/09/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87/"/>
      <url>/2019/04/09/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87/</url>
      
        <content type="html"><![CDATA[<p>参考：<a href="https://zhuanlan.zhihu.com/p/29895933" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29895933</a><br><a href="https://www.jianshu.com/p/41218cb5e099" target="_blank" rel="noopener">https://www.jianshu.com/p/41218cb5e099</a></p><h2 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h2><p><strong>v<sub>t</sub></strong> 是要代替  θ_t  的估计值，代表第t天的指数平均温度值<br><strong>θ<sub>t</sub></strong> 代表第t天的实际温度值<br><strong>β</strong> 代表可调节的超参数值  </p><p>则第t天的指数平均温度，可用如下公式表示<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1wnugwmkrj30ne0ggjsp.jpg" width="50%" height="50%">    </p><p>将<strong>v<sub>100</sub></strong> 展开可得:<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1wonisescj30j201o0sr.jpg"><br>v<sub>t</sub> 是对每天温度的加权平均，之所以称之为指数加权，是因为加权系数是随着时间以指数形式递减的，<strong>时间越靠近，权重越大</strong>，越靠前，权重越小。</p><p><img src="https://upload-images.jianshu.io/upload_images/1667471-485da343fbd96353.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/665/format/webp" alt><br>再来看下面三种情况：</p><p>当 β = 0.9 时，指数加权平均最后的结果如图<strong>红色线</strong>所示，代表的是最近 10 天的平均温度值；<br>当 β = 0.98 时，指结果如图<strong>绿色线</strong>所示，代表的是最近 50 天的平均温度值；<br>当 β = 0.5 时，结果如下图<strong>黄色线</strong>所示，代表的是最近 2 天的平均温度值；</p><p><img src="//upload-images.jianshu.io/upload_images/1667471-7d82e7b89e860299.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/473/format/webp" alt></p><p><img src="//upload-images.jianshu.io/upload_images/1667471-6fd989467bcb6121.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/475/format/webp" alt></p><p><strong>β 越小，噪音越多</strong>，虽然能够很快的适应温度的变化，但是更容易出现奇异值。</p><p><strong>β 越大，得到的曲线越平坦</strong>，因为多平均了几天的温度，这个曲线的波动更小。<br>但有个缺点是，因为只有 0.02 的权重给了当天的值，而之前的数值权重占了 0.98 ，<br>曲线进一步右移，在温度变化时就会适应地更缓慢一些，会出现一定延迟。</p><p>通过上面的内容可知，β 也是一个很重要的超参数，不同的值有不同的效果，需要调节来达到最佳效果，<strong>一般 0.9 的效果就很好</strong>。</p><p>作者：不会停的蜗牛<br>链接：<a href="https://www.jianshu.com/p/41218cb5e099" target="_blank" rel="noopener">https://www.jianshu.com/p/41218cb5e099</a><br>来源：简书  </p><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><p>我们可以看到指数加权平均的求解过程实际上是一个递推的过程，那么这样就会有一个非常大的好处，每当我要求从0到某一时刻（n）的平均值的时候，我并不需要像普通求解平均值的作为，保留所有的时刻值，类和然后除以n。</p><p>而是只需要保留0-(n-1)时刻的平均值和n时刻的温度值即可。也就是每次只需要保留常数值，然后进行运算即可，这对于深度学习中的海量数据来说，是一个很好的减少内存和空间的做法。</p><h2 id="为什么在优化算法中使用指数加权平均"><a href="#为什么在优化算法中使用指数加权平均" class="headerlink" title="为什么在优化算法中使用指数加权平均"></a>为什么在优化算法中使用指数加权平均</h2><p>上面提到了一些 指数加权平均 的应用，这里我们着重看一下在优化算法中的作用。</p><p>以 Momentum 梯度下降法为例，</p><p><strong>Momentum 梯度下降法</strong>，就是计算了梯度的指数加权平均数，并以此来更新权重，它的运行<strong>速度几乎总是快于标准的梯度下降算法</strong>。</p><p><strong>这是为什么呢？</strong></p><p>让我们来看一下这个图，</p><p><img src="//upload-images.jianshu.io/upload_images/1667471-07d825d3e2624537.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/745/format/webp" alt></p><p>例如这就是我们要优化的成本函数的形状，图中红点就代表我们要达到的最小值的位置，<br>假设我们<strong>从左下角这里出发开始用梯度下降法</strong>，那么蓝色曲线就是一步一步迭代，一步一步向最小值靠近的轨迹。</p><p>可以看出<strong>这种上下波动，减慢了梯度下降法的速度</strong>，而且无法使用更大的学习率，因为如果用较大的学习率，可能会偏离函数的范围。</p><p>如果有一种方法，可以使得在纵轴上，学习得慢一点，减少这些摆动，但是在横轴上，学习得快一些，快速地从左向右移移向红点最小值，那么训练的速度就可以加快很多。</p><p>这个方法就是动量 Momentum 梯度下降法，它<strong>在每次计算梯度的迭代中，对 dw 和 db 使用了指数加权平均法的思想</strong>，</p><p><img src="//upload-images.jianshu.io/upload_images/1667471-eedf9342a4bce813.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/272/format/webp" alt></p><p>这样我们就可以得到如图红色线的轨迹：</p><p><img src="//upload-images.jianshu.io/upload_images/1667471-f9e70b57daae0359.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/718/format/webp" alt></p><p>可以看到：<br><strong>纵轴方向</strong>，平均过程中正负摆动相互抵消，平均值接近于零，摆动变小，学习放慢。<br><strong>横轴方向</strong>，因为所有的微分都指向横轴方向，因此平均值仍然较大，向最小值运动更快了。<br>在抵达最小值的路上减少了摆动，加快了训练速度。</p><p>作者：不会停的蜗牛<br>链接：<a href="https://www.jianshu.com/p/41218cb5e099" target="_blank" rel="noopener">https://www.jianshu.com/p/41218cb5e099</a><br>来源：简书  </p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>模型剪枝 Model Pruning</title>
      <link href="/2019/04/09/%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D-Model-Pruning/"/>
      <url>/2019/04/09/%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D-Model-Pruning/</url>
      
        <content type="html"><![CDATA[<p>参考：<a href="https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/" target="_blank" rel="noopener">https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/</a><br>L1正则化项，加入损失函数中，可以对特征进行选择。<br>L1也可以应用到模型压缩任务中，选择某个filter是否有存在的必要，从而决定是否要剪掉。</p>]]></content>
      
      
      <categories>
          
          <category> 模型压缩 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型压缩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>视频分类总结</title>
      <link href="/2019/04/09/%E8%A7%86%E9%A2%91%E5%88%86%E7%B1%BB%E6%80%BB%E7%BB%93/"/>
      <url>/2019/04/09/%E8%A7%86%E9%A2%91%E5%88%86%E7%B1%BB%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h3 id="Two-Stream"><a href="#Two-Stream" class="headerlink" title="Two-Stream"></a>Two-Stream</h3><ul><li>训练<br>（1）spatial net：每个video中随机提取1帧：224<em>224</em>3，进行训练，<br>（2）temporal net：每个video中选取L=10帧光流，224<em>224</em>2L，进行训练<br>由于含有x,y 方向，因此10帧光流，对应的5帧图像，该5帧是连续的</li><li>测试：每个video中平均采25帧，并通过crop and flip等操作扩增10倍，整个video的得分，是这250帧的平均得分。</li></ul><h3 id="TSN"><a href="#TSN" class="headerlink" title="TSN"></a>TSN</h3><ul><li>训练，将video分段，默认为3段<br>（1）spatial net：每个video分成N段，每段随机提取1帧，则得到N帧，这N帧共享参数，一起训练，N帧分别得到的score进行平均，作为video的分数，并反向传播，训练。<br>（2）temporal net ,同理，每个video分成N段，每段随机提取L=10帧，这N段共享参数，一起训练，N段分别得到的score进行平均，作为video的分数，并反向传播，训练。</li><li>测试，将video分段，默认为25段</li><li>同训练过程</li></ul><h3 id="C3D"><a href="#C3D" class="headerlink" title="C3D"></a>C3D</h3><ul><li>在Sports-1M上进行训练，训练完成之后便得到一个video feature extractor </li><li>训练：在每个video中随机剪切5个2s长的clip，对clip进行训练 </li><li>测试：对于一个video，含有N帧，则将这N帧分成16帧的clips，每相邻的两个clips重叠8帧，然后将这些clips得到的fc6 activations 进行<strong>平均得到video feature</strong>，进而送入分类层得到video class label</li></ul><h3 id="I3D"><a href="#I3D" class="headerlink" title="I3D"></a>I3D</h3><ul><li>video 以25帧/秒的帧率来提取关键帧</li><li>训练：以64帧组成的snippets进行训练。</li><li>测试：同样以64帧组成的snippets进行测试，但是对于该video上的所有的snippet的<strong>预测结果进行取平均</strong>作为该video的预测结果</li></ul>]]></content>
      
      
      <categories>
          
          <category> 行为识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 行为识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>I3D</title>
      <link href="/2019/04/09/I3D/"/>
      <url>/2019/04/09/I3D/</url>
      
        <content type="html"><![CDATA[<ul><li>参考:<a href="https://zhuanlan.zhihu.com/p/34919655" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34919655</a></li></ul><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1wgkib11aj310b0ew77m.jpg">  <h3 id="训练和预测"><a href="#训练和预测" class="headerlink" title="训练和预测"></a>训练和预测</h3><ul><li>写在前面，方便看，哈哈哈哈</li><li>video 以25帧/秒的帧率来提取关键帧</li><li>在训练时，以64帧组成的snippets进行训练。</li><li>在测试时，同样以64帧组成的snippets进行测试，但是对于该video上的所有的snippet的预测结果进行取平均作为该video的预测结果</li></ul><h3 id="方法1：ConvNet-LSTM"><a href="#方法1：ConvNet-LSTM" class="headerlink" title="方法1：ConvNet+LSTM"></a><strong>方法1：ConvNet+LSTM</strong></h3><ul><li>该方案主要考虑到cnn在图像分类领域的成功以及LSTM对于序列建模的能力，很自然提出将两者结合起来的方案。如果只是用cnn的话，需要对每一帧都提取特征，然后将视频的所有帧特征进行总汇，以此来表达对视频的表示，显然这样做忽略的时间结构特征。而LSTM可以处理长时间依赖的问题，可以对视频从第一帧开始建模直到最后一帧，使用cnn网络对每帧提取特征，然后将特征送入LSTM来捕捉时间特征，最后一帧的输出用来对视频特征表示。<br>  LSTM往往依赖cnn的最后一层特征最为输入，因此不能够捕捉到<code>low-level motion</code>的特征，而且对于遍历整个视频也很难去训练。<h3 id="方法2：3D-ConvNets"><a href="#方法2：3D-ConvNets" class="headerlink" title="方法2：3D ConvNets"></a><strong>方法2：3D ConvNets</strong></h3></li><li><code>3D ConvNets</code>是对视频建模最自然的方式，和标准cnn区别在于由<code>2d conv</code>变为<code>3d conv</code>，来捕捉<code>spatio-temporal feature</code>。想法很好，但目前遇到一些问题，问题一：<code>3D ConvNets</code>的<code>3d conv</code>多了一个维度，参数量有较大增加，这将会很难去训练。问题二：没有利用那些Imagenet上成功的预训练模型来迁移学习，往往使用层数少的cnn在小数据集上从头训练。简要说就是要利用已有预训练模型，要减少参数或增大数据集。<br>  论文中实现了C3D（与原版略有差异）有8 conv layer、5 pooling layer 和 2 fc layer，并在所有圈卷积层和fc层加bn。输入是16×112×112（通过crop方法），将第一个pooling layer对时间的stride由1变成2，为了可以减少memory和允许更大batch。<h3 id="方法3：Two-Stream-Networks"><a href="#方法3：Two-Stream-Networks" class="headerlink" title="方法3：Two-Stream Networks"></a><strong>方法3：Two-Stream Networks</strong></h3></li><li>该方案利用短的视频段来建模，用每个clip的预测分数平均的方式（其实C3D也是类似），但不同的是输入，包括一张RGB和10张<code>optical flow</code>(其实是5张，x/y两个方向，运动特征)。模型能使用<code>two-branch</code>方式，利用预训练的imagenet模型，最后将预测结果平均下（最原始的，或者在最后softmax做融合），这样建模的模型比较好训练，同时也能获得更高的分数。  </li><li>模型的两个输入流也可以在后面的cnn层来进行融合，以提升相同，同时可以<code>end-to-end</code>训练。论文实现一个类似的two-stream方案，在最后一层用<code>3d conv</code>将spatial和flow特征进行融合。<h3 id="方法4：Two-Stream-Inflated-3D-ConvNets"><a href="#方法4：Two-Stream-Inflated-3D-ConvNets" class="headerlink" title="方法4：Two-Stream Inflated 3D ConvNets"></a><strong>方法4：Two-Stream Inflated 3D ConvNets</strong></h3></li><li>该方案是论文提出的，出发点是要利用imagenet的预训练模型，同时利用<code>3d conv</code>来提取<code>RGB stream</code>的<code>temporal feature</code>，最后再利用<code>optical-flow stream</code>提升网络性能，也就大融合的方案（把有效的技巧都用上）。  </li><li>通过对预训练的<code>2D conv</code>增加temporal维度，把N×N的filter变为N×N×N。简单的办法就是对N×N的filter重复复制N遍，并归一化，这样多的出发点是短期内时间不变性的假设，姑且把这当成<code>3D filter</code>初始化的一种策略吧。  </li><li>池化操作怎么膨胀？stride怎么选？主要依赖感受野尺寸，如果图像水平方向和竖直方向相等，那么stride也相等，而且越深的特征感受野越大。但是考虑到时间因素，对称感受野不是必须的，这主要还是依赖帧率和图片大小。时间相对于空间变化过快，将合并不同object的边信息，过慢将不能捕捉场景变化。  </li><li>虽然3D conv能够捕捉motion信息，但是与光流优化的方式和效果还是不一样，因此使用<code>two-stream</code>的方式构建，并分开训练两个网络。</li></ul><p><img src="https://pic2.zhimg.com/80/v2-34f1d3ac14884d5c9114d4e9383c2e89_hd.jpg" alt></p><hr><ul><li><p>数据集不同，评测结果也不同。flow在UCF-101上效果比HMDB-51、kinetics上好（有更多camera运动的原因）。<br><img src="https://pic2.zhimg.com/80/v2-e719a0a3a022e348838d4b6a5c0b8a55_hd.jpg" alt></p></li><li><p>在imagenet上训练后迁移到kinetics和直接在kinetics上的对比，迁移后的效果好，说明RGB流起的作用大。整体上I3D模型参数更少，更深，训练输入在时间和空间维度上都比C3D大。<br><img src="https://pic3.zhimg.com/80/v2-b358535638c000de801577fc84296252_hd.jpg" alt></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 行为识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 行为识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C3D</title>
      <link href="/2019/04/09/C3D/"/>
      <url>/2019/04/09/C3D/</url>
      
        <content type="html"><![CDATA[<h2 id="pytorch-中"><a href="#pytorch-中" class="headerlink" title="pytorch 中"></a>pytorch 中</h2><h3 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h3><p><a href="https://pytorch.org/docs/stable/nn.html#conv2d" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html#conv2d</a><br>the shape of input:  <font size="5," color="#0099ff">batch×channel×height×width</font></p><h2 id="Conv3d"><a href="#Conv3d" class="headerlink" title="Conv3d"></a>Conv3d</h2><p><a href="https://pytorch.org/docs/stable/nn.html#conv3d" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html#conv3d</a><br>the shape of input: <font size="5," color="#0099ff">batch×channel×depth×height×width</font></p><h2 id="C3D-用于行为识别-1-2"><a href="#C3D-用于行为识别-1-2" class="headerlink" title="C3D 用于行为识别[1][2]"></a>C3D 用于行为识别[1][2]</h2><p>[1] C3D 的网络结构</p><ul><li>输入： bs×3×<font size="5," color="#0099ff">16</font>×H×W，即输入一个长为16的视频序列clip， 实际是 bs×3×16×12×12</li><li>输出： bs×feature_size</li></ul><p>对于一个video，含有N帧，则将这N帧分成16帧的clips，每相邻的两个clips重叠8帧，然后将这些clips得到的fc6 activations 进行平均得到video feature，进而送入分类层得到video class label</p><h2 id="解读论文Learning-Spatio-Temporal-Features-with-3D-Residual-Networks-for-Action-Recognition"><a href="#解读论文Learning-Spatio-Temporal-Features-with-3D-Residual-Networks-for-Action-Recognition" class="headerlink" title="解读论文Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition"></a>解读论文<code>Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition</code></h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>当前行为识别的方法，参数量不能过多的原因是，现在可以用于训练的数据集较小，一旦参数量过大，使得模型过拟合</li><li>但是，现在有了kinetics 这样的大型数据集，这时，便可以提出一个参数量大的model</li></ul><h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><ul><li>生成一个用于 spatio-temporal recognition的标准预训练模型</li></ul><h3 id="关于image-size-部分的介绍"><a href="#关于image-size-部分的介绍" class="headerlink" title="关于image size 部分的介绍"></a>关于image size 部分的介绍</h3><ul><li>截取帧时，固定长宽比，并设置高为360（resneXt是240），</li></ul><h4 id="train"><a href="#train" class="headerlink" title="train"></a>train</h4><ul><li>以一个 16 连续帧组成的clip来代表 video</li><li>先以最小边为尺寸进行裁剪（裁剪为一个正方形），然后再进行resize 112</li><li>为了数据增强，这里有三个trick  (1) 最小边=min(height, weight) * sacle (2) 选取哪个区域进行裁剪，有五种选择 四个角和中心 (3)以50%的概率进行随机水平翻转</li></ul><h4 id="test"><a href="#test" class="headerlink" title="test"></a>test</h4><ul><li>一个video 去选择 没有重叠的所有 16 连续帧，然后对这所有的clip的得分进行取平均作为video的得分</li><li>不再进行数据增强</li><li>先按照最小边，固定长宽比进行缩放</li><li>再 中心裁剪成指定的 <code>112*112</code></li></ul><h3 id="分析-resnet34-c3d-比-rgb-i3d-差的原因"><a href="#分析-resnet34-c3d-比-rgb-i3d-差的原因" class="headerlink" title="分析 resnet34-c3d 比 rgb-i3d 差的原因"></a>分析 resnet34-c3d 比 rgb-i3d 差的原因</h3><ol><li><p>rgb-i3d 使用了64个GPU，可能他们使用的batch size 也比较大。而 resnet34-c3d 仅使用了4个GPU，256 batch size。</p></li><li><p>rgb-i3d 使用的 clip 分辨率为：3 × 64 × 224 × 224.  而 resnet34-c3d 的 clip 分辨率为 3 × 16 × 112 × 112 。    </p><p>即，i3d 使用连续的 <strong>64</strong> 帧组成一个clip，并且 image 分辨率为 <strong>224</strong>，</p><p>而， resnet c3d 使用连续的 <strong>16</strong> 帧组成一个clip，并且 image 分辨率为 <strong>112</strong>，</p></li></ol><h2 id="解读论文Can-Spatiotemporal-3D-CNNs-Retrace-the-History-of-2D-CNNs-and-ImageNet"><a href="#解读论文Can-Spatiotemporal-3D-CNNs-Retrace-the-History-of-2D-CNNs-and-ImageNet" class="headerlink" title="解读论文Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?"></a>解读论文<code>Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?</code></h2><h3 id="Goal-1"><a href="#Goal-1" class="headerlink" title="Goal"></a>Goal</h3><ul><li>本文的目的是为了验证当前存在的数据集是否足够支撑训练一个很深的3d网络</li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition .  ICCV 2017<br>[2] Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?  CVPR 2018</p>]]></content>
      
      
      <categories>
          
          <category> 行为识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 行为识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>视频格式</title>
      <link href="/2019/04/09/%E8%A7%86%E9%A2%91%E6%A0%BC%E5%BC%8F/"/>
      <url>/2019/04/09/%E8%A7%86%E9%A2%91%E6%A0%BC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[ <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1w907erjdj30800ffq31.jpg">  <h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li>为了更好的对视频进行传输，需要对视频进行压缩编码</li><li>当前的对video中<strong>图像</strong>的压缩方法有两大阵营，MPEG系列和H.26x系列，</li><li>得到压缩后的图像和音频之后，需要组合封装起来，这时就需要一个容器，可以使用AVI、MPEG等进行封装</li><li>最后为了将该video用系统上的某个视频播放器来开，还需要给其添加个后缀名，一般采用了某个封装格式就会有相应的后缀名，但是若我们更改某个video的文件名，也不会改变video的实质。</li><li>如在上图中视频流压缩采用的是H.26系列，但是文件类型却是mp4，即容器采用的是MPEG封装格式</li></ul><h2 id="转载"><a href="#转载" class="headerlink" title="转载"></a>转载</h2><p>作者：袁园<br>链接：<a href="https://www.zhihu.com/question/20997688/answer/30720197" target="_blank" rel="noopener">https://www.zhihu.com/question/20997688/answer/30720197</a><br>来源：知乎<br>在生活语境里所说的“视频格式”，在学术上有两个概念与之对应：Container format (封装格式)和Codec (暂且译为“编解码格式”)。</p><h3 id="1-Container-format-封装格式"><a href="#1-Container-format-封装格式" class="headerlink" title="1. Container format (封装格式)"></a><strong>1. Container format (封装格式)</strong></h3><p>Container format 描述了视频文件的结构。正如它的字面含义所说，它是对一个“容器”的规范。一个视频文件往往会包含图像和音频，还有一些配置信息(如图像和音频的关联，如何解码它们等)：这些内容需要按照一定的规则组织、存储起来，Container format就是这些规则。<br>如果一个视频文件是以某个Container format封装起来的，那么它的后缀名一般会体现出来。所以，后缀名只是形式，只是为了便于识别(例如，windows系统会根据文件的后缀名决定以什么程序打开它)，无决定性的意义。</p><h3 id="2-Codec-编解码格式"><a href="#2-Codec-编解码格式" class="headerlink" title="2. Codec (编解码格式)"></a><strong>2. Codec (编解码格式)</strong></h3><p>Codec是一种压缩标准。而文件的压缩/还原是通过编/解码实现的，所以Codec也可理解成编/解码标准。要知道，未经过处理的原始视频和音频文件十分巨大，不好存储、传输。为了节省磁盘空间和网络带宽，原始的视频和音频文件都会通过编码压缩体积，然后需要播放时再通过逆向过程解码还原。Codec就是规定编/解码实现细节(数字存储空间、帧速率、比特率、分辨率等)的标准，不同的标准对于压缩的质量和效率有影响。<br>世界上有两大制定这套标准的阵营：ITU-T VCEG(Visual Coding Experts Group，国际电联旗下的标准化组织)和MPEG(Moving Picture Experts Group, ISO旗下的组织)。MPEG系列标准是MPEG制定的，H.26x系列标准是ITU-T制定的。</p><h3 id="3-Container-format-封装格式-和Codec-编解码格式-有关系吗？"><a href="#3-Container-format-封装格式-和Codec-编解码格式-有关系吗？" class="headerlink" title="3. Container format (封装格式)**和Codec** (编解码格式)**有关系吗？**"></a><strong>3. Container format</strong> <strong>(封装格式)**</strong>和Codec** <strong>(编解码格式)**</strong>有关系吗？**</h3><p>不妨将视频文件看作容器(Container)，那么这个容器里盛放的就是遵循某种Codec的内容(Content)。一个容器里应该能放下视频、音频、数据信息，即使它们遵循的Codec不相同。例如，QuickTime File Format (.MOV)支持几乎所有的Codec，MPEG(.MP4)也支持相当广的Codec。所以，单从视频文件的格式是无法获知它的质量细节的，这些细节取决与采用的Codec。比较专业的说法是，“给我一个H.264 Quicktime文件(.mov)”。</p><h3 id="4-为何还是有点迷糊？"><a href="#4-为何还是有点迷糊？" class="headerlink" title="4. 为何还是有点迷糊？"></a><strong>4. 为何还是有点迷糊？</strong></h3><p>以上的解释是从学术角度出发的，在学术上区分它们没有一点障碍。但现实生活中人们不会一丝不苟地区分“Container format ”“Codec”，往往只会说“这是一个mov格式的文件”。这是将日常用语与学术术语混为一谈导致的理解混乱。</p><p>另外，Container format和Codec的命名法有点奇葩，明白人有时也会晕。例如，“MPEG-4”既是“Container format ”，也是“Codec”，这确实够烦的。</p>]]></content>
      
      
      <categories>
          
          <category> 视频处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>视频解码</title>
      <link href="/2019/04/09/%E8%A7%86%E9%A2%91%E8%A7%A3%E7%A0%81/"/>
      <url>/2019/04/09/%E8%A7%86%E9%A2%91%E8%A7%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<ul><li><p>opencv提取关键帧（banet）</p></li><li><p>ffmpeg提取关键帧 （video to text）</p></li><li><p>i帧、P帧、B帧</p></li><li><p>参考：<a href="https://blog.csdn.net/huangblog/article/details/8739876" target="_blank" rel="noopener">https://blog.csdn.net/huangblog/article/details/8739876</a><br>I帧，Intra-coded frame：是一张完整的图片<br>P帧，predictive frame: 记录了与之前真的差别，在解码P帧之前需要参考之前的图片帧<br>B帧，Bi-Predictive frame: 不仅需要参考之前的图片帧，还需要参考之后的图片帧，才能完整解码。<br>因此解码P帧、B帧的速度相对较慢，直接解码I帧可以获得更快的速度。</p></li></ul><p><strong>简单地讲，I帧是一个完整的画面，而P帧和B帧记录的是相对于I帧的变化。没有I帧，P帧和B帧就无法解码</strong>  </p><h3 id="GOP"><a href="#GOP" class="headerlink" title="GOP"></a>GOP</h3><p>所谓GOP，意思是画面组，一个GOP就是一组连续的画面。从一个I帧到下一个I帧之间的所有帧的组合称为一个GOP。</p><ul><li><p><strong>I帧</strong><br>I帧是参考帧，一个GOP中必须含有I帧，它是一个全帧压缩编码帧。它将全帧图像信息进行JPEG压缩编码及传输。</p></li><li><p><strong>P帧的预测与重构:</strong><br>P帧是以I帧为参考帧,在I帧中找出P帧“某点”的预测值和运动矢量,取预测差值和运动矢量一起传送。在接收端根据运动矢量从I帧中找出P帧“某点”的预测值并与差值相加以得到P帧“某点”样值,从而可得到完整的P帧。</p></li><li><p><strong>B帧的预测与重构：</strong><br>B帧以前面的I或P帧和后面的P帧为参考帧,“找出”B帧“某点”的预测值和两个运动矢量,并取预测差值和运动矢量传送。接收端根据运动矢量在两个参考帧中“找出(算出)”预测值并与差值求和,得到B帧“某点”样值,从而可得到完整的B帧。</p></li><li><p><strong>用下图中的1234567帧来表达</strong><br>首先由1：I帧，<br>再由1、4帧得到第4帧所在位置处的图像信息<br>最后由1、4、2得到第2帧所在位置处的图像信息，由1、4、3得到第3帧所在位置处的图像信息</p></li></ul><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1w7ki6x02j30ra0ge0st.jpg">  <ul><li>另外一张图  </li></ul><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1w7ki8pfvj30nm0fk77r.jpg"><hr><h3 id="视频压缩：I帧、P帧、B帧"><a href="#视频压缩：I帧、P帧、B帧" class="headerlink" title="视频压缩：I帧、P帧、B帧"></a>视频压缩：I帧、P帧、B帧</h3><ul><li>来源： <a href="https://blog.csdn.net/huangblog/article/details/8739876" target="_blank" rel="noopener">https://blog.csdn.net/huangblog/article/details/8739876</a></li></ul><p>视频压缩中，每帧代表一幅静止的图像。而在实际压缩时，会采取各种算法减少数据的容量，其中IPB就是最常见的。</p><pre><code>简单地说，I帧是关键帧，属于帧内压缩。就是和AVI的压缩是一样的。P是向前搜索的意思。B是双向搜索。他们都是基于I帧来压缩数据。</code></pre><p>   I帧表示关键帧，你可以理解为这一帧画面的完整保留；解码时只需要本帧数据就可以完成（因为包含完整画面）</p><p>   P帧表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。（也就是差别帧，P帧没有完整画面数据，只有与前一帧的画面差别的数据）</p><p>   B帧是双向差别帧，也就是B帧记录的是本帧与前后帧的差别（具体比较复杂，有4种情况），换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的与本帧数据的叠加取得最终的画面。B帧压缩率高，但是解码时CPU会比较累~。</p><pre><code>采用的压缩方法: 分组:把几帧图像分为一组(GOP),为防止运动变化,帧数不宜取多。    1.定义帧:将每组内各帧图像定义为三种类型,即I帧、B帧和P帧;    2.预测帧:以I帧做为基础帧,以I帧预测P帧,再由I帧和P帧预测B帧;    3.数据传输:最后将I帧数据与预测的差值信息进行存储和传输。</code></pre><p>一、I帧  </p><pre><code>I图像（帧）是靠尽可能去除图像空间冗余信息来压缩传输数据量的帧内编码图像。I帧又称为内部画面 (intra picture)，I 帧通常是每个 GOP（MPEG 所使用的一种视频压缩技术）的第一个帧，经过适度地压缩（做为随机访问的参考点）可以当成图象。在MPEG编码的过程中部分视频帧序列压缩成为I帧，部分压缩成P帧，还有部分压缩成B帧。I帧法是帧内压缩法（P、B为帧间），也称为“关键帧”压缩法。I帧法是基于离散余弦变换DCT（Discrete Cosine Transform）的压缩技术，这种算法与JPEG压缩算法类似。采用I帧压缩可达到1/6的压缩比而无明显的压缩痕迹。I帧特点：    1.它是一个全帧压缩编码帧。它将全帧图像信息进行JPEG压缩编码及传输;    2.解码时仅用I帧的数据就可重构完整图像;    3.I帧描述了图像背景和运动主体的详情;    4.I帧不需要参考其他画面而生成;    5.I帧是P帧和B帧的参考帧(其质量直接影响到同组中以后各帧的质量);    6.I帧是帧组GOP的基础帧(第一帧),在一组中只有一个I帧;    7.I帧不需要考虑运动矢量;    8.I帧所占数据的信息量比较大。I帧编码流程：    (1)进行帧内预测，决定所采用的帧内预测模式。    (2)像素值减去预测值，得到残差。    (3)对残差进行变换和量化。    (4)变长编码和算术编码。    (5)重构图像并滤波，得到的图像作为其它帧的参考帧。</code></pre><p>二、P帧</p><pre><code> P图像（帧）是通过充分降低于图像序列中前面已编码帧的时间冗余信息来压缩传输数据量的编码图像，也叫预测帧。在针对连续动态图像编码时，将连续若干幅图像分成P,B,I三种类型，P帧由在它前面的P帧或者I帧预测而来，它比较与它前面的P帧或者I帧之间的相同信息或数据，也即考虑运动的特性进行帧间压缩。P帧法是根据本帧与相邻的前一帧（I帧或P帧）的不同点来压缩本帧数据。采取P帧和I帧联合压缩的方法可达到更高的压缩且无明显的压缩痕迹。P帧的预测与重构:    P帧是以I帧为参考帧,在I帧中找出P帧“某点”的预测值和运动矢量,取预测差值和运动矢量一起传送。在接收端根据运动矢量从I帧中找出P帧“某点”的预测值并与差值相加以得到P帧“某点”样值,从而可得到完整的P帧。P帧特点：    ①P帧是I帧后面相隔1-2帧的编码帧。      ②P帧采用运动补偿的方法传送它与前面的I或P帧的差值及运动矢量（预测误差）。      ③解码时必须将I帧中的预测值与预测误差求和后才能重构完整的P帧图像。      ④P帧属于前向预测的帧间编码。它只参考前面最靠近它的I帧或P帧。      ⑤P帧可以是其后面P帧的参考帧，也可以是其前后的B帧的参考帧。    ⑥由于P帧是参考帧，它可能造成解码错误的扩散。     ⑦由于是差值传送，P帧的压缩比较高。</code></pre><p>三、B帧</p><pre><code>B图像（帧）是既考虑与源图像序列前面已编码帧，也顾及源图像序列后面已编码帧之间的时间冗余信息来压缩传输数据量的编码图像，也叫双向预测帧。   B帧法是双向预测的帧间压缩算法。当把一帧压缩成B帧时，它根据相邻的前一帧、本帧以及后一帧数据的不同点来压缩本帧，也即仅记录本帧与前后帧的差值。只有采用B帧压缩才能达到200：1的高压缩。一般地，I帧压缩效率最低，P帧较高，B帧最高。B帧的预测与重构：    B帧以前面的I或P帧和后面的P帧为参考帧,“找出”B帧“某点”的预测值和两个运动矢量,并取预测差值和运动矢量传送。接收端根据运动矢量在两个参考帧中“找出(算出)”预测值并与差值求和,得到B帧“某点”样值,从而可得到完整的B帧。B帧特点：    1.B帧是由前面的I或P帧和后面的P帧来进行预测的;    2.B帧传送的是它与前面的I或P帧和后面的P帧之间的预测误差及运动矢量;    3.B帧是双向预测编码帧;    4.B帧压缩比最高,因为它只反映2参考帧间运动主体的变化情况,预测比较准确;    5.B帧不是参考帧,不会造成解码错误的扩散。 P 帧和 B 帧编码的基本流程为：    (1)进行运动估计，计算采用帧间编码模式的率失真函数(节)值。P 帧 只参考前面的帧，B 帧可参考后面的帧。    (2)进行帧内预测，选取率失真函数值最小的帧内模式与帧间模式比较，确定采用哪种编码模式。    (3)计算实际值和预测值的差值。    (4)对残差进行变换和量化。    (5)若编码，如果是帧间编码模式，编码运动矢量。注:I、B、P各帧是根据压缩算法的需要,是人为定义的,它们都是实实在在的物理帧,至于图像中的哪一帧是I帧,是随机的,一但确定了I帧,以后的各帧就严格按规定顺序排列。 </code></pre><p>四、实际应用</p><pre><code>从上面的解释看，我们知道I和P的解码算法比较简单，资源占用也比较少，I只要自己完成就行了，P呢，也只需要解码器把前一个画面缓存一下，遇到P时就使用之前缓存的画面就好了，如果视频流只有I和P，解码器可以不管后面的数据，边读边解码，线性前进，大家很舒服。但网络上的电影很多都采用了B帧，因为B帧记录的是前后帧的差别，比P帧能节约更多的空间，但这样一来，文件小了，解码器就麻烦了，因为在解码时，不仅要用之前缓存的画面，还要知道下一个I或者P的画面（也就是说要预读预解码），而且，B帧不能简单地丢掉，因为B帧其实也包含了画面信息，如果简单丢掉，并用之前的画面简单重复，就会造成画面卡（其实就是丢帧了），并且由于网络上的电影为了节约空间，往往使用相当多的B帧，B帧用的多，对不支持B帧的播放器就造成更大的困扰，画面也就越卡。一般平均来说，I的压缩率是7（跟JPG差不多），P是20，B可以达到50，可见使用B帧能节省大量空间，节省出来的空间可以用来保存多一些I帧，这样在相同码率下，可以提供更好的画质。</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>python类的继承</title>
      <link href="/2019/04/08/python%E7%B1%BB%E7%9A%84%E7%BB%A7%E6%89%BF/"/>
      <url>/2019/04/08/python%E7%B1%BB%E7%9A%84%E7%BB%A7%E6%89%BF/</url>
      
        <content type="html"><![CDATA[<ul><li>参考：<a href="https://www.cnblogs.com/bigberg/p/7182741.html" target="_blank" rel="noopener">https://www.cnblogs.com/bigberg/p/7182741.html</a>  </li></ul><h2 id="类的继承"><a href="#类的继承" class="headerlink" title="类的继承"></a>类的继承</h2><h3 id="1-继承的定义"><a href="#1-继承的定义" class="headerlink" title="1. 继承的定义"></a>1. 继承的定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">(object)</span>:</span>   <span class="comment"># 定义一个父类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">talk</span><span class="params">(self)</span>:</span>    <span class="comment"># 父类中的方法</span></span><br><span class="line">        print(<span class="string">"person is talking...."</span>)  </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Chinese</span><span class="params">(Person)</span>:</span>    <span class="comment"># 定义一个子类， 继承Person类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">walk</span><span class="params">(self)</span>:</span>      <span class="comment"># 在子类中定义其自身的方法</span></span><br><span class="line">        print(<span class="string">'is walking...'</span>)</span><br><span class="line"> </span><br><span class="line">c = Chinese()</span><br><span class="line">c.talk()      <span class="comment"># 调用继承的Person类的方法</span></span><br><span class="line">c.walk()     <span class="comment"># 调用本身的方法</span></span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>person is talking….<br>is walking…</p></blockquote><h2 id="2-构造函数的继承"><a href="#2-构造函数的继承" class="headerlink" title="2. 构造函数的继承"></a>2. 构造函数的继承</h2><p>如果我们要给实例 c 传参，我们就要使用到构造函数，那么构造函数该如何继承，同时子类中又如何定义自己的属性？</p><p>继承类的构造方法：<br>1.经典类的写法： 父类名称.<strong>init</strong>(self,参数1，参数2，…)  </p><ol start="2"><li><font color="#0059ff" size="5" face="黑体"> 新式类的写法：super(子类，self).<strong>init</strong>(参数1，参数2，….)</font></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">(object)</span>:</span>  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span>  </span><br><span class="line">        self.name = name  </span><br><span class="line">        self.age = age  </span><br><span class="line">        self.weight = <span class="string">'weight'</span>  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">talk</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        print(<span class="string">"person is talking...."</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Chinese</span><span class="params">(Person)</span>:</span>  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age, language)</span>:</span>  <span class="comment"># 先继承，在重构  </span></span><br><span class="line">Person.__init__(self, name, age)  </span><br><span class="line"><span class="comment"># 继承父类的构造方法，也可以写成：</span></span><br><span class="line"><span class="comment"># super(Chinese,self).__init__(name,age)  </span></span><br><span class="line">        self.language = language  <span class="comment"># 定义类的本身属性  </span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">walk</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        print(<span class="string">'is walking...'</span>)  </span><br><span class="line">        </span><br><span class="line">c = Chinese(<span class="string">'bigberg'</span>, <span class="number">22</span>, <span class="string">'Chinese'</span>)  </span><br><span class="line">print(c.name)  </span><br><span class="line">print(c.language)</span><br><span class="line">c.talk()</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>bigberg<br>Chinese<br>person is talking….</p></blockquote><h3 id="3-子类对父类方法的重写"><a href="#3-子类对父类方法的重写" class="headerlink" title="3.子类对父类方法的重写"></a>3.子类对父类方法的重写</h3><p>如果我们对基类/父类的方法需要修改，可以在子类中重构该方法。如下的talk()方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">(object)</span>:</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span>  </span><br><span class="line">        self.name = name  </span><br><span class="line">        self.age = age  </span><br><span class="line">        self.weight = <span class="string">'weight'</span>  </span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">talk</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        print(<span class="string">"person is talking...."</span>)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Chinese</span><span class="params">(Person)</span>:</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age, language)</span>:</span>  </span><br><span class="line">        Person.__init__(self, name, age)  </span><br><span class="line">        self.language = language  </span><br><span class="line">        print(self.name, self.age, self.weight, self.language)  </span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">talk</span><span class="params">(self)</span>:</span>  <span class="comment"># 子类 重构方法  </span></span><br><span class="line">  print(<span class="string">'%s is speaking chinese'</span> % self.name)  </span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">walk</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        print(<span class="string">'is walking...'</span>)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">c = Chinese(<span class="string">'bigberg'</span>, <span class="number">22</span>, <span class="string">'Chinese'</span>)  </span><br><span class="line">c.talk()</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>bigberg 22 weight Chinese<br>bigberg is speaking chinese</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谱聚类</title>
      <link href="/2019/04/08/%E8%B0%B1%E8%81%9A%E7%B1%BB/"/>
      <url>/2019/04/08/%E8%B0%B1%E8%81%9A%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<ul><li><p>先占位置</p></li><li><p><a href="https://www.cnblogs.com/pinard/p/6221564.html" target="_blank" rel="noopener">参考某博客</a>  </p></li><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" target="_blank" rel="noopener">sklearn.cluster.SpectralClustering</a></p><ul><li>References<br>Normalized cuts and image segmentation, 2000 Jianbo Shi, Jitendra Malik <a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324" target="_blank" rel="noopener">http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324</a><br>A Tutorial on Spectral Clustering, 2007 Ulrike von Luxburg <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323" target="_blank" rel="noopener">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323</a><br>Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi <a href="http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf" target="_blank" rel="noopener">http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图卷积网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图卷积网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spectral Networks and Deep Locally Connected Networks on Graphs</title>
      <link href="/2019/04/08/Spectral-Networks-and-Deep-Locally-Connected-Networks-on-Graphs/"/>
      <url>/2019/04/08/Spectral-Networks-and-Deep-Locally-Connected-Networks-on-Graphs/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>(SCST)Self-critical Sequence Training for Image Captioning</title>
      <link href="/2019/04/08/SCST-Self-critical-Sequence-Training-for-Image-Captioning/"/>
      <url>/2019/04/08/SCST-Self-critical-Sequence-Training-for-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<p>强化学习中的策略梯度法可以针对那些不可微分的度量进行优化，<br>本文中，使用强化学习的方法来优化图像描述任务，将这个新的优化方法称为self-critical sequence training (SCST)。</p><p><strong>sequence models for image captioning的理想训练过程， 应该是避免 exposure bias 并且可以直接优化任务中的度量</strong></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="图像描述方面的现状"><a href="#图像描述方面的现状" class="headerlink" title="图像描述方面的现状"></a><strong>图像描述方面的现状</strong></h3><p>[1] <strong>show attend and tell</strong> 证明在caption任务中，使用attention机制是有益处的。<br>[2] <strong>Teacher-Forcing</strong> 用于文本的deep generative models 的训练方法一般是：给定上一步word的ground truth 来最大化该步生成word的最大似然，来反向传播。这个方法称为“Teacher-Forcing”  。但是这种方法导致在训练和测试时很不匹配。因为在测试时，该步生成的单词是在给定上一步预测出的单词的前提下。  </p><ul><li><p>这个exposure  bias [2]导致在测试时产生误差累积，因为模型从未暴露于其自己的预测中。<br>This exposure  bias [2], results in error accumulation during generation at  test time,<br>since the model has never been exposed to its own  predictions</p><p>一些克服exposure bias的方法[3] [4]<br>[3] <strong>Scheduled sampling</strong> 他们表明，反馈模型自己的预测，并在训练过程中缓慢地增加反馈概率p，可以显着地提高测试时间的性能。<br>[4] <strong>Professor forcing</strong> a  technique that uses adversarial training to encourage the dynamics of the recurrent network to be the same when training conditioned on ground truth previous words and when  sampling freely from the network</p></li></ul><h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><h3 id="实验结果证明"><a href="#实验结果证明" class="headerlink" title="实验结果证明"></a>实验结果证明</h3><p>we find that directly optimizing the CIDEr metric with  SCST and greedy decoding at test-time is highly effective.</p><h2 id="Image-Features"><a href="#Image-Features" class="headerlink" title="Image Features"></a>Image Features</h2><ul><li>FC Models<br>由CNN+FC得到image 的 特征向量，并送入LSTM中，来生成caption<br>但是需要注意的是仅在first step 输入该特征向量，其余步输入上一步生成的word (embedding)   </li><li>Spatial CNN features for Attention models<br>在不缩放也不裁剪图片的基础上，使用resnet-101来提取最后一个卷积层的特征，并应用apply spatially adaptive max-pooling来得到一个固定的尺寸14 × 14 × 2048。在每一个time step，attention model在这14 × 14=196个位置上计算一个<font color="#0099ff" size="6" face="黑体"> attention mask</font>（注意力系数/权重）。由这个mask 计算所有位置的加权求和，以此得到image feature。  </li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Show, attend and tell: Neural image caption generation with visual attention. In ICML,  2015.<br>[2] Sequence level training with recurrent neural networks. ICLR, 2015.<br>[3] Scheduled sampling for sequence prediction with recurrent neural networks. In NIPS, 2015.<br>[4] Professor forcing: A  new algorithm for training recurrent networks. Neural Information Processing Systems. (NIPS) 2016.  </p>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</title>
      <link href="/2019/04/07/Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering/"/>
      <url>/2019/04/07/Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在object级别上计算 attention<br><strong>bottom-up</strong>: 使用Faster R-CNN 来提取 object，并得到相对应的特征向量。<br><strong>top-down</strong>: 用来计算attention 的系数，作为bottom-up中得到的每个object feature 的权重。</p><h2 id="概括："><a href="#概括：" class="headerlink" title="概括："></a>概括：</h2><p><strong>（1）Bottom-Up</strong><br>使用Faster R-CNN 中的R-CNN来得到object feature。<br><strong>（2）Top-Down Attention</strong><br>得到了该层的隐层状态，并与object features  中的每一个<strong>v<sub>i</sub></strong>来计算一个attention 系数。<br><strong>（3）对object features 进行attention 权重求和</strong><br>得到image feature<br><strong>（4）Decoder：language LSTM</strong><br>输出预测单词</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g19d1x7kiij30im0bvaat.jpg" style="zoom:60%">  <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14xg3jwvdj30mo0h2wfi.jpg" style="zoom:60%"><h2 id="Bottom-Up"><a href="#Bottom-Up" class="headerlink" title="Bottom-Up"></a>Bottom-Up</h2><ul><li><strong>主要介绍一下Faster R-CNN 的训练过程</strong><br>（1）首先Resnet-101 是在ImageNet上预训练的<br>（2）Faster R-CNN在MS COCO上进行预训练<br>rpn 的score classification loss，bbox regression loss<br>r-cnn 的score classification loss，bbox regression loss<br>（3）Faster R-CNN在Visual Genome上再进行预训练<br>为了得到更好的特征表达，增加一个预测属性的输出： </li><li><strong>具体的网络：</strong><br>To predict attributes for region i, we concatenate the mean  pooled convolutional feature vi with a learned embedding  of the ground-truth object class, and feed this into an additional output layer defining a softmax distribution over each  attribute class plus a ‘no attributes’ class.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>self.training in pytorch</title>
      <link href="/2019/04/07/self-training-in-pytorch/"/>
      <url>/2019/04/07/self-training-in-pytorch/</url>
      
        <content type="html"><![CDATA[<ul><li>代码来源于：<a href="https://zhuanlan.zhihu.com/p/26893755" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26893755</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable, Function</span><br><span class="line"></span><br><span class="line">x_train = np.array([[<span class="number">3.3</span>], [<span class="number">4.4</span>], [<span class="number">5.5</span>], [<span class="number">6.71</span>], [<span class="number">6.93</span>], [<span class="number">4.168</span>],</span><br><span class="line">                    [<span class="number">9.779</span>], [<span class="number">6.182</span>], [<span class="number">7.59</span>], [<span class="number">2.167</span>], [<span class="number">7.042</span>],</span><br><span class="line">                    [<span class="number">10.791</span>], [<span class="number">5.313</span>], [<span class="number">7.997</span>], [<span class="number">3.1</span>]], dtype=np.float32)</span><br><span class="line"></span><br><span class="line">y_train = np.array([[<span class="number">1.7</span>], [<span class="number">2.76</span>], [<span class="number">2.09</span>], [<span class="number">3.19</span>], [<span class="number">1.694</span>], [<span class="number">1.573</span>],</span><br><span class="line">                    [<span class="number">3.366</span>], [<span class="number">2.596</span>], [<span class="number">2.53</span>], [<span class="number">1.221</span>], [<span class="number">2.827</span>],</span><br><span class="line">                    [<span class="number">3.465</span>], [<span class="number">1.65</span>], [<span class="number">2.904</span>], [<span class="number">1.3</span>]], dtype=np.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x_train = torch.from_numpy(x_train)</span><br><span class="line">y_train = torch.from_numpy(y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"---------------------------------------"</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LinearRegression, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># input and output is 1 dimension</span></span><br><span class="line">        print(<span class="string">"self.training: "</span> + str(self.training))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.linear(x)</span><br><span class="line">        print(<span class="string">"self.training；"</span> + str(self.training))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">print(<span class="string">"initialize"</span>)</span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"---------------------------------------"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"model.eval()"</span>)</span><br><span class="line">model.eval()</span><br><span class="line">inputs = Variable(x_train)</span><br><span class="line">target = Variable(y_train)</span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line">out = model(inputs) <span class="comment"># 前向传播</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"---------------------------------------"</span>)</span><br><span class="line">print(<span class="string">"model.train()"</span>)</span><br><span class="line">model.train()</span><br><span class="line">inputs = Variable(x_train)</span><br><span class="line">target = Variable(y_train)</span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line">out = model(inputs) <span class="comment"># 前向传播</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorboard_logger</title>
      <link href="/2019/04/06/tensorboard-logger/"/>
      <url>/2019/04/06/tensorboard-logger/</url>
      
        <content type="html"><![CDATA[<p>使用tensorboard_logger记录训练过程中的数据<br>（1）首先需要安装tensorflow</p><ul><li>可参考<a href="https://blog.csdn.net/love666666shen/article/details/77099843" target="_blank" rel="noopener">https://blog.csdn.net/love666666shen/article/details/77099843</a></li><li>不需要单独设置一个tensorflow的环境，直接pip install 一个CPU 版本的即可</li><li>pip install –ignore-installed –upgrade <a href="https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.1.0-cp35-cp35m-win_amd64.whl" target="_blank" rel="noopener">https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.1.0-cp35-cp35m-win_amd64.whl</a></li></ul><p>（2）安装tensorboard</p><ul><li>pip install tensorboard</li></ul><p>（3）No scalar data was found的解决<br>只需将cmd目录cd进入日志文件存放的目录，再加载日志文件便可解决：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">e:</span><br><span class="line">cd logdir</span><br><span class="line">tensorboard <span class="attribute">--logdir</span>=E:\logdir <span class="attribute">--host</span>=127.0.0.1</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/09/05/7YODLiJAZ6aUTG4.png" alt="搜狗截图20190905112015.png"></p>]]></content>
      
      
      
        <tags>
            
            <tag> tensorboard </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>range xrange np.arange np.linspace</title>
      <link href="/2019/04/05/range-xrange-np-arange-np-linspace/"/>
      <url>/2019/04/05/range-xrange-np-arange-np-linspace/</url>
      
        <content type="html"><![CDATA[<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1rmpadk75j309a07saa2.jpg"><p>python</p><ul><li>xrange 得到一个迭代器，（仅可以在python2中使用）</li><li>range 得到一个列表，（python2/python3均可）</li></ul><p>numpy</p><ul><li>numpy.arange 得到一份数组</li><li>numpy.linspace <strong>得到固定数量的等间隔数组，注意包含指定的尾部</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
          <category> numpy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>glob.glob vs os.listdir</title>
      <link href="/2019/04/05/glob-glob-vs-os-listdir/"/>
      <url>/2019/04/05/glob-glob-vs-os-listdir/</url>
      
        <content type="html"><![CDATA[<ul><li>现在想要得到某个文件夹下的一些图片，并按照顺序排列，如下图所示：  </li></ul><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1rl9d9xq9j30ia0cbjri.jpg">  <ul><li>第一种方法：（得到的frames_list是不包含路径的）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frames_list = sorted(os.listdir(video_path))</span><br></pre></td></tr></table></figure></li></ul><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1rlb8uq8aj309h0a8q2z.jpg"><ul><li>第二种方法：（得到的frames_list包含路径的）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frames_list = sorted(glob.glob(os.path.join(video_path, <span class="string">'*.jpg'</span>)))</span><br></pre></td></tr></table></figure></li></ul><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1rlbroiqmj30nt0b40u9.jpg"><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>os.listdir 仅可以得到对当前路径下文件名称，但是不包含路径信息<br>glob.glob 可以得到对当前路径下文件名称，并包含路径信息</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python list sort方法</title>
      <link href="/2019/04/04/python-list-sort%E6%96%B9%E6%B3%95/"/>
      <url>/2019/04/04/python-list-sort%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p><strong>sort()</strong> 函数用于对原列表进行排序，如果指定参数，则使用比较函数指定的比较函数。</p><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><p>sort()方法语法：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list.sort(<span class="attribute">cmp</span>=None, <span class="attribute">key</span>=None, <span class="attribute">reverse</span>=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><ul><li>cmp – 可选参数, 如果指定了该参数会使用该参数的方法进行排序。</li><li>key – 主要是用来进行比较的元素，只有一个参数，具体的函数的参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序。</li><li>reverse – 排序规则，<strong>reverse = True</strong> 降序， <strong>reverse = False</strong> 升序（默认）。</li></ul><h2 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h2><p>该方法没有返回值，但是会对列表的对象进行排序。</p><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">video_sort_lambda = <span class="keyword">lambda</span> x: int(x[<span class="number">3</span>:<span class="number">-4</span>]) <span class="comment"># 定义一个函数对元素x进行操作，并得到一个整数Int</span></span><br><span class="line">video_root = <span class="string">"/userhome/dataset/MSVD/Video-Description-with-Spatial-Temporal-Attention/youtube"</span></span><br><span class="line">videos = sorted(os.listdir(video_root), key=video_sort_lambda) <span class="comment"># 按得到的整数，对list进行排序</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytohn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>史雅雅的收藏夹</title>
      <link href="/2019/04/04/%E5%8F%B2%E9%9B%85%E9%9B%85%E7%9A%84%E6%94%B6%E8%97%8F%E5%A4%B9/"/>
      <url>/2019/04/04/%E5%8F%B2%E9%9B%85%E9%9B%85%E7%9A%84%E6%94%B6%E8%97%8F%E5%A4%B9/</url>
      
        <content type="html"><![CDATA[<p><a href="http://pygments.org/" target="_blank" rel="noopener">http://pygments.org/</a></p><p><a href="https://202.38.95.226:7443/view.html" target="_blank" rel="noopener">https://202.38.95.226:7443/view.html</a></p><p><a href="https://aideadlin.es/?sub=ML,RO,CV,SP,NLP,DM" target="_blank" rel="noopener">https://aideadlin.es/?sub=ML,RO,CV,SP,NLP,DM</a></p><p><a href="https://yjs.ustc.edu.cn/" target="_blank" rel="noopener">https://yjs.ustc.edu.cn/</a></p><p><a href="https://www.json.cn/" target="_blank" rel="noopener">https://www.json.cn/</a></p><p><a href="https://kevinj-huang.github.io/" target="_blank" rel="noopener">https://kevinj-huang.github.io/</a></p><p><a href="https://shiyaya.github.io/" target="_blank" rel="noopener">https://shiyaya.github.io/</a></p><p><a href="https://stackedit.io/app#" target="_blank" rel="noopener">https://stackedit.io/app#</a></p><p><a href="http://jsonviewer.stack.hu/" target="_blank" rel="noopener">http://jsonviewer.stack.hu/</a></p><p><a href="http://www.nlpjob.com/" target="_blank" rel="noopener">http://www.nlpjob.com/</a></p><p><a href="https://nlp.stanford.edu/software/scenegraph-parser.shtml#Usage" target="_blank" rel="noopener">https://nlp.stanford.edu/software/scenegraph-parser.shtml#Usage</a></p><p><a href="https://paperswithcode.com/sota" target="_blank" rel="noopener">https://paperswithcode.com/sota</a></p><p><a href="https://sm.ms/" target="_blank" rel="noopener">https://sm.ms/</a></p><p><a href="http://www.arxiv-sanity.com/" target="_blank" rel="noopener">http://www.arxiv-sanity.com/</a></p><p><a href="http://www.cvpapers.com/" target="_blank" rel="noopener">http://www.cvpapers.com/</a></p><h3 id="论文搜索"><a href="#论文搜索" class="headerlink" title="论文搜索"></a>论文搜索</h3><ul><li><p><a href="https://dblp.uni-trier.de/db/" target="_blank" rel="noopener">https://dblp.uni-trier.de/db/</a></p><ul><li>（可以进行筛选，eg:nips, iccv, cvpr）; (也可以对某些作者进行查询)</li></ul></li><li><p><a href="http://openaccess.thecvf.com/menu.py" target="_blank" rel="noopener">http://openaccess.thecvf.com/menu.py</a></p></li><li><p><a href="http://actionrecognition.net/files/paper.php" target="_blank" rel="noopener">http://actionrecognition.net/files/paper.php</a></p></li><li><p><a href="http://www.aaai.org/Library/AAAI/aaai19contents.php" target="_blank" rel="noopener">http://www.aaai.org/Library/AAAI/aaai19contents.php</a></p></li><li><p><a href="https://dl.acm.org/results.cfm?within=owners.owner%3DHOSTED&amp;srt=_score&amp;query=&amp;Go.x=26&amp;Go.y=1" target="_blank" rel="noopener">https://dl.acm.org/results.cfm?within=owners.owner%3DHOSTED&amp;srt=_score&amp;query=&amp;Go.x=26&amp;Go.y=1</a></p></li></ul><h3 id="翻墙"><a href="#翻墙" class="headerlink" title="翻墙"></a>翻墙</h3><ul><li><p><a href="https://github.com/vpncn/vpncn.github.io" target="_blank" rel="noopener">https://github.com/vpncn/vpncn.github.io</a></p></li><li><p><a href="https://flyzyblog.com/install-ss-ssr-bbr-in-one-command/#ss" target="_blank" rel="noopener">https://flyzyblog.com/install-ss-ssr-bbr-in-one-command/#ss</a></p></li><li><p><a href="https://www.banpie.info/shadowsocks-pac-gfw/" target="_blank" rel="noopener">https://www.banpie.info/shadowsocks-pac-gfw/</a></p></li><li><p>Vultr搭建SS</p></li><li><p><a href="https://github.com/sirzdy/shadowsocks/wiki/Vultr搭建SS（VPS搭建SS）" target="_blank" rel="noopener">http://wuzhangyang.com/2019/03/06/vultr-ss/</a></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget https:<span class="comment">//raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh</span></span><br><span class="line"></span><br><span class="line">chmod +x shadowsocks.sh</span><br><span class="line"></span><br><span class="line">./shadowsocks<span class="selector-class">.sh</span> <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>搬瓦工</p></li><li><p>推荐：<a href="https://www.bandwagonhost.net/1967.html" target="_blank" rel="noopener">https://www.bandwagonhost.net/1967.html</a></p></li></ul><h2 id="会议搜索"><a href="#会议搜索" class="headerlink" title="会议搜索"></a>会议搜索</h2><ul><li><a href="http://www.searchconf.net/conf/searchresule/" target="_blank" rel="noopener">http://www.searchconf.net/conf/searchresule/</a></li></ul><h3 id="iccv-2019-challenge"><a href="#iccv-2019-challenge" class="headerlink" title="iccv 2019 challenge"></a>iccv 2019 challenge</h3><ul><li><a href="https://sites.google.com/site/iccv19clvllsmdc/home" target="_blank" rel="noopener">https://sites.google.com/site/iccv19clvllsmdc/home</a></li></ul><h3 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h3><ul><li><a href="https://github.com/rusty1s/pytorch_geometric" target="_blank" rel="noopener">https://github.com/rusty1s/pytorch_geometric</a></li></ul><p>深度学习课程</p><ul><li><a href="https://discuss.gluon.ai/c/5-category" target="_blank" rel="noopener">https://discuss.gluon.ai/c/5-category</a></li><li><a href="http://zh.d2l.ai/chapter_preface/preface.html" target="_blank" rel="noopener">http://zh.d2l.ai/chapter_preface/preface.html</a></li></ul><h3 id="tensorboard-可视化"><a href="#tensorboard-可视化" class="headerlink" title="tensorboard 可视化"></a>tensorboard 可视化</h3><ul><li><a href="https://www.aiuai.cn/aifarm646.html" target="_blank" rel="noopener">https://www.aiuai.cn/aifarm646.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 杂类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch 减小显存消耗，优化显存使用，避免out of memory</title>
      <link href="/2019/04/03/pytorch-%E5%87%8F%E5%B0%8F%E6%98%BE%E5%AD%98%E6%B6%88%E8%80%97%EF%BC%8C%E4%BC%98%E5%8C%96%E6%98%BE%E5%AD%98%E4%BD%BF%E7%94%A8%EF%BC%8C%E9%81%BF%E5%85%8Dout-of-memory/"/>
      <url>/2019/04/03/pytorch-%E5%87%8F%E5%B0%8F%E6%98%BE%E5%AD%98%E6%B6%88%E8%80%97%EF%BC%8C%E4%BC%98%E5%8C%96%E6%98%BE%E5%AD%98%E4%BD%BF%E7%94%A8%EF%BC%8C%E9%81%BF%E5%85%8Dout-of-memory/</url>
      
        <content type="html"><![CDATA[<h3 id="本文是整理了大神的两篇博客："><a href="#本文是整理了大神的两篇博客：" class="headerlink" title="本文是整理了大神的两篇博客："></a>本文是整理了大神的两篇博客：</h3><ul><li><p>如何计算模型以及中间变量的显存占用大小：<br><a href="https://oldpan.me/archives/how-to-calculate-gpu-memory" target="_blank" rel="noopener">https://oldpan.me/archives/how-to-calculate-gpu-memory</a></p></li><li><p>如何在Pytorch中精细化利用显存：<br><a href="https://oldpan.me/archives/how-to-use-memory-pytorch" target="_blank" rel="noopener">https://oldpan.me/archives/how-to-use-memory-pytorch</a></p></li><li><p>还有知乎中大神的解答：<br><a href="https://zhuanlan.zhihu.com/p/31558973" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31558973</a></p></li><li><p>ppt<br><a href="https://www.zhihu.com/question/67209417" target="_blank" rel="noopener">https://www.zhihu.com/question/67209417</a></p></li><li><p>在说之前先推荐一个实时监控内存显存使用的小工具：</p></li></ul><blockquote><p>sudo apt-get install htop</p></blockquote><ul><li>监控内存（-d为更新频率，下为每0.1s更新一次）：</li></ul><blockquote><p>htop -d=0.1</p></blockquote><ul><li>监控显存（-n为更新频率，下为每0.1s更新一次）：</li></ul><blockquote><p>watch -n 0.1 nvidia-smi</p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>torch.backends.cudnn.benchmark = true 使用情形</title>
      <link href="/2019/04/03/torch-backends-cudnn-benchmark-true-%E4%BD%BF%E7%94%A8%E6%83%85%E5%BD%A2/"/>
      <url>/2019/04/03/torch-backends-cudnn-benchmark-true-%E4%BD%BF%E7%94%A8%E6%83%85%E5%BD%A2/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.pytorchtutorial.com/when-should-we-set-cudnn-benchmark-to-true/" target="_blank" rel="noopener">pytorch-torch.backends.cudnn.benchmark文档</a></p><ul><li>torch.backends.cudnn.benchmark<br>设置这个 flag 可以让内置的 cuDNN 的 auto-tuner 自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。</li></ul><p>应该遵循以下准则：</p><ul><li>如果网络的输入数据维度或类型上变化不大，设置  torch.backends.cudnn.benchmark = true  可以增加运行效率；</li><li>如果网络的输入数据在每次 iteration 都变化的话，会导致 cnDNN 每次都会去寻找一遍最优配置，这样反而会降低运行效率。</li><li>在程序刚开始加这条语句可以提升一点训练速度，没什么额外开销。</li><li>有时候可能是因为每次迭代都会引入点临时变量，会导致训练速度越来越慢，基本呈线性增长。<br>开发人员还不清楚原因，但如果周期性的使用torch.cuda.empty_cache()的话就可以解决这个问题。这个命令是清除没用的临时变量的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>刷题经验</title>
      <link href="/2019/04/02/%E5%88%B7%E9%A2%98%E7%BB%8F%E9%AA%8C/"/>
      <url>/2019/04/02/%E5%88%B7%E9%A2%98%E7%BB%8F%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h3 id="输入输出的部分，需要注意："><a href="#输入输出的部分，需要注意：" class="headerlink" title="输入输出的部分，需要注意："></a>输入输出的部分，需要注意：</h3><ul><li>输出的地方，需要看人家是否<strong>保留固定的位数</strong>，否则输出的结果也是错误的</li></ul>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 刷题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stanford Scene Graph Parser</title>
      <link href="/2019/03/26/Stanford-Scene-Graph-Parser/"/>
      <url>/2019/03/26/Stanford-Scene-Graph-Parser/</url>
      
        <content type="html"><![CDATA[<ul><li>官网：<a href="https://nlp.stanford.edu/software/scenegraph-parser.shtml#Usage" target="_blank" rel="noopener">https://nlp.stanford.edu/software/scenegraph-parser.shtml#Usage</a></li></ul><h2 id="下载相应的文件（官网有）"><a href="#下载相应的文件（官网有）" class="headerlink" title="下载相应的文件（官网有）"></a>下载相应的文件（官网有）</h2><ul><li>stanford-corenlp-full-2015-12-09.zip</li><li>scenegraph-1.0.jar<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2></li></ul><ol><li>将stanford-corenlp-full-2015-12-09.zip解压，然后按照博客<a href="https://shiyaya.github.io/2019/03/26/ubuntu-%E5%AE%89%E8%A3%85-Stanford-CoreNLP/" target="_blank" rel="noopener">ubuntu 安装 Stanford CoreNLP</a>来安装corenlp</li><li>需要将 scenegraph-1.0.jar 放入解压之后的文件夹stanford-corenlp-full-2015-12-09中，</li></ol><ul><li><p>需要注意版本</p></li><li><p>java  idk 1.8+ 按照博客来就可以</p></li><li><p>corenlp 使用人家给定的2015的，不要升级</p><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>法1：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -mx2g -cp <span class="string">"*"</span> edu.stanford.nlp.scenegraph.RuleBasedParser</span><br></pre></td></tr></table></figure></li><li><p>注意该命令是在stanford-corenlp-full-2015-12-09文件夹下执行的<br>该方法是交互式的，提示你输入句子，他给出相对应的解析出的scene graph</p></li></ul><p>法2：</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言理解 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ubuntu 安装 Stanford CoreNLP</title>
      <link href="/2019/03/26/ubuntu-%E5%AE%89%E8%A3%85-Stanford-CoreNLP/"/>
      <url>/2019/03/26/ubuntu-%E5%AE%89%E8%A3%85-Stanford-CoreNLP/</url>
      
        <content type="html"><![CDATA[<h3 id="安装java-jdk"><a href="#安装java-jdk" class="headerlink" title="安装java jdk"></a>安装java jdk</h3><ul><li><p>更新软件包列表：</p><figure class="highlight q"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-<span class="built_in">get</span> <span class="keyword">update</span></span><br></pre></td></tr></table></figure></li><li><p>安装openjdk-8-jdk：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-<span class="builtin-name">get</span> install openjdk-8-jdk</span><br></pre></td></tr></table></figure></li><li><p>查看java版本：</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -<span class="built_in">version</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="下载并解压Stanford-coreNLP-包："><a href="#下载并解压Stanford-coreNLP-包：" class="headerlink" title="下载并解压Stanford coreNLP 包："></a>下载并解压Stanford coreNLP 包：</h3><ul><li>从这里下载<br><a href="https://stanfordnlp.github.io/CoreNLP/download.html" target="_blank" rel="noopener">https://stanfordnlp.github.io/CoreNLP/download.html</a><br>或者以命令行方式下载<blockquote><p>wget <a href="http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip" target="_blank" rel="noopener">http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip</a>  </p></blockquote></li></ul><ul><li><p>解压</p><blockquote><p>unzip stanford-corenlp-full-2018-02-27.zip</p></blockquote></li><li><p>转到文件目录</p></li></ul><blockquote><p>cd stanford-corenlp-full-2018-02-27/</p></blockquote><h3 id="配置环境变量："><a href="#配置环境变量：" class="headerlink" title="配置环境变量："></a>配置环境变量：</h3><p>把下列这行代码加到你的.bashrc里面(vim .bashrc)</p><blockquote><p> cd ~<br>vim .bashrc<br>export CLASSPATH=/path/to/stanford-corenlp-full-2018-02-27/stanford-corenlp-3.9.1.jar<br>source ~/.bashrc  ## 使之生效<br>把/path/to/替换为你保存stanford-corenlp-full-2016-10-31的地方的路径</p></blockquote><h3 id="安装："><a href="#安装：" class="headerlink" title="安装："></a>安装：</h3><blockquote><p>pip install stanfordcorenlp</p></blockquote><p>处理中文还需要下载中文的模型jar文件，然后放到stanford-corenlp-full-2018-02-27根目录下即可</p><p>wget <a href="http://nlp.stanford.edu/software/stanford-chinese-corenlp-2018-02-27-models.jar" target="_blank" rel="noopener">http://nlp.stanford.edu/software/stanford-chinese-corenlp-2018-02-27-models.jar</a></p><h3 id="检查自己是否装好了stanfordcorenlp"><a href="#检查自己是否装好了stanfordcorenlp" class="headerlink" title="检查自己是否装好了stanfordcorenlp"></a>检查自己是否装好了stanfordcorenlp</h3><p>进入python2或者python3</p><p>命令行下输入：</p><blockquote><p>python</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> stanfordcorenlp <span class="keyword">import</span> StanfordCoreNLP</span><br></pre></td></tr></table></figure><p>能成功导入不报错，就是安装成功了。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>java</title>
      <link href="/2019/03/26/java%20%E7%9A%84%E7%BC%96%E8%AF%91%E4%B8%8E%E6%89%A7%E8%A1%8C/"/>
      <url>/2019/03/26/java%20%E7%9A%84%E7%BC%96%E8%AF%91%E4%B8%8E%E6%89%A7%E8%A1%8C/</url>
      
        <content type="html"><![CDATA[<h2 id="java-的编译与执行"><a href="#java-的编译与执行" class="headerlink" title="java 的编译与执行"></a>java 的编译与执行</h2><ol><li><p>用文本编辑器新建一个yumhtest.java文件，在其中输入以下代码并保存：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">yumhtest</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"hello world !"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>编译：在shell终端执行命令 <strong>javac yumhtest.java</strong></p></li><li><p>运行：在shell终端执行命令 <strong>java yumhtest</strong><br>当shell下出现“hello world !”字样</p></li></ol><p>注意事项：类名应和文件名相同。</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java 的编译与执行</title>
      <link href="/2019/03/26/java/"/>
      <url>/2019/03/26/java/</url>
      
        <content type="html"><![CDATA[<h2 id="java-的编译与执行"><a href="#java-的编译与执行" class="headerlink" title="java 的编译与执行"></a>java 的编译与执行</h2><ol><li><p>用文本编辑器新建一个yumhtest.java文件，在其中输入以下代码并保存：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">yumhtest</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"hello world !"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>编译：在shell终端执行命令 <strong>javac yumhtest.java</strong></p></li><li><p>运行：在shell终端执行命令 <strong>java yumhtest</strong><br>当shell下出现“hello world !”字样</p></li></ol><p>注意事项：类名应和文件名相同。</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SPICE</title>
      <link href="/2019/03/25/SPICE/"/>
      <url>/2019/03/25/SPICE/</url>
      
        <content type="html"><![CDATA[<h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><h3 id="Ubuntu下安装Stanford-CoreNLP"><a href="#Ubuntu下安装Stanford-CoreNLP" class="headerlink" title="Ubuntu下安装Stanford CoreNLP"></a><a href="https://blog.csdn.net/Hay54/article/details/82313535" target="_blank" rel="noopener">Ubuntu下安装Stanford CoreNLP</a></h3>]]></content>
      
      
      <categories>
          
          <category> 自然语言理解 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>faster r-cnn 解读</title>
      <link href="/2019/03/24/faster-r-cnn-%E8%A7%A3%E8%AF%BB/"/>
      <url>/2019/03/24/faster-r-cnn-%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>来自<a href="https://mp.weixin.qq.com/s/M_i38L2brq69BYzmaPeJ9w" target="_blank" rel="noopener">机器之心</a><br>可能机器之心的那个链接无法转到，<a href="http://tech.ifeng.com/a/20180223/44884976_0.shtml" target="_blank" rel="noopener">看这个</a><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e0ltm2y7j30u0083wev.jpg"><br>by yaya:<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e10a6tguj31fj0mw0vw.jpg"></p><h2 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h2><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e0p68j95j30t80gaaar.jpg" style="zoom:60%"><p>anchor: 定义anchor的长: scale=[4,8,16], 长宽比: ratio=[0.5, 1, 1.5, 2]，则在each position of conv feature 将会有k=len(scale)×len(ratio)=12个anchor</p><p>(1)对于分类层，我们对每个锚点输出两个预测值：它是背景（不是目标）的分数，和它是前景（实际的目标）的分数.<br><br>则经过该1×1的卷积层，输出的shape=N×2k×H×W  <br></p><p>(2)对于回归或边框调整层，我们输出四个预测值(偏移值)：<font color="#0099ff" size="5" face="黑体">Δxcenter、Δycenter、Δwidth、Δheight</font>，我们将会把这些值用到锚点中来得到最终的建议：(x1, y1, x2, y2)分别为左下角和右上角的坐标，即area=(x2-x1)*(y2-y1).<br><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e0p66yxyj312w066t91.jpg"></p><p>(3) 补充，<font color="#0099ff" size="5" face="黑体">RoI Pooling</font>  得到 pooled feats，输入的是base feats, 得到的pred proposals 以及 <font color="#0099ff" size="5" face="黑体">1/scale</font><br>因为pred proposals得到的坐标是在<del>原始的image上的</del> 输入到网络中的image，而当前的base feats 是相对于原图有尺度变化的，为了对应.</p><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e0p6ekybj30u00c175l.jpg"><p>有两个不同的目标：<br><br>(1) 将建议分到一个类中，加上一个背景类（用于删除不好的建议）。<br><br>(2) 根据预测的类别更好地调整建议的边框。<br><br>在最初的 Faster R-CNN 论文中，R-CNN 对每个建议采用特征图，将它平坦化并使用两个大小为 4096 的有 ReLU 激活函数的全连接层。然后，它对每个不同的目标使用两种不同的全连接层：<br><br>一个有 N+1 个单元的全连接层，其中 N 是类的总数，另外一个是背景类。<br><br>一个有 4N 个单元的全连接层。我们希望有一个回归预测，因此对 N 个类别中的每一个可能的类别，我们都需要 <font color="#0099ff" size="5" face="黑体">Δxcenter、Δycenter、Δwidth、Δheight</font>。<br><br>训练和目标<br><br>R-CNN 的目标与 RPN 的目标的计算方法几乎相同，但是考虑的是不同的可能类别。我们采用建议和真实边框，并计算它们之间的 IoU。<br></p><p>那些有任何真实边框的建议，只要其 IoU 大于 0.5，都被分配给那个真实数据。那些 IoU 在 0.1 和 0.5 之间的被标记为背景。与我们在为 RPN 分配目标时相反的是，我们忽略了没有任何交集的建议。这是因为在这个阶段，我们假设已经有好的建议并且我们对解决更困难的情况更有兴趣。当然，这些所有的值都是可以为了更好的拟合你想找的目标类型而做调整的超参数。<br></p><p>边框回归的目标是计算建议和与其对应的真实框之间的偏移量，仅针对那些基于 IoU 阈值分配了类别的建议。<br></p><p>我们随机抽样了一个尺寸为 64 的 balanced mini batch，其中我们有高达 25% 的前景建议（有类别）和 75% 的背景。<br></p><p>按照我们对 RPN 损失所做的相同处理方式，现在的分类损失是一个多类别的交叉熵损失，使用所有选定的建议和用于与真实框匹配的 25% 建议的 Smooth L1 loss。由于 R-CNN 边框回归的全连接网络的输出对于每个类都有一个预测，所以当我们得到这种损失时必须小心。在计算损失时，我们只需要考虑正确的类。<br><br>这里若假定类别可知，则每个类都有预测，若类别不可知，则仅有一个预测即可，代码如下：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e0p66uafj30nq03vt8o.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Image Caption 常用评价指标</title>
      <link href="/2019/03/24/Image-Caption-%E5%B8%B8%E7%94%A8%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
      <url>/2019/03/24/Image-Caption-%E5%B8%B8%E7%94%A8%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Graph R-CNN for Scene Graph Generation</title>
      <link href="/2019/03/24/Graph-R-CNN-for-Scene-Graph-Generation/"/>
      <url>/2019/03/24/Graph-R-CNN-for-Scene-Graph-Generation/</url>
      
        <content type="html"><![CDATA[<p>这是ECCV 2018 场景图生成 的一篇文章。<br>写在前面，本文使用的GCN网络与“Graph Attention Networks”一致，都是计算两个节点之间的attention来计算邻接矩阵中的元素值，更新节点特征的公式是AXW。</p><ul><li><strong>查看本文的原因，主要是想看，其是如何提取relation feature的，但是文中仅使用了union box feature 作为relation feature。较为朴素！</strong>—————–不好</li><li><strong>同时也将relation 作为node放入graph 中，但是是object feature 与 realtion feature之间的混合graph，与“Auto-Encoding Scene Graphs for Image Captioning”一样采用的是异构图</strong>  </li><li><strong>文中对W<sup>sr</sup>Z<sup>r</sup>α<sup>sr</sup>， 为该node<sub>i</sub>与所有的relation nodes之间计算的注意力系数，并不合理，因为object 可能仅有一两个relation，怎么可能与所有的relation有关系呢</strong>————————–不好（后又考虑了一下，可能没有关系的直接算0，就不再计算attention了）</li></ul><p>思索良久，终于发现是哪里不对了，一般的情况下，都是X’=AXW，这样的形式，以 X 为中心，更新X的特征，而Z<sub>i</sub><sup>r</sup>的更新公式中是以Z<sup>o</sup>为中心，因此是不是有些不对头呢？？？</p><p>本文的<strong>两个主要的贡献</strong>：</p><ol><li><strong>GCN</strong> with attention 用于scene graph generate 任务。Updating each object and relationship representation based on its neighbors</li><li>对于N个object ,若两两配对，则会产生N×N个relation，数量是N的二次，数量很多，但是很多又是没有必要的，以前的工作采用随机采样的方式，但是本文提出了<strong>RePN 网络来采样relation</strong>。</li><li>提出了一个新的用于scene graph generate 的评价指标，SGGen+（不是笔者关注的内容，因此此处忽略了对SGGen+的介绍）</li></ol><h2 id="场景图生成任务的主要步骤"><a href="#场景图生成任务的主要步骤" class="headerlink" title="场景图生成任务的主要步骤"></a>场景图生成任务的主要步骤</h2><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1dxanmhpfj312o08p75l.jpg">    <ol><li>P( V|I )  指：在给定image的情况下，去得到 <strong>object proposals</strong><br>使用pytorch 版本[1]的faster R-CNN来得到 bbox，类似于[2]，采用分段训练的方式，先对faster R-CNN进行预训练，然后，固定faster r-cnn参数，训练整个场景图生成网络。  </li><li>P( E|V, I ) 指：在给定image 和 bbox的情况下来得到 <strong>relation proposals</strong><br>如果假设每个object proposals 之间都会有一个relation，则有N×N个relation，或者是说，有N×N个object pairs。但是含有很多不合适的relation（本身这object pairs 之间不存在关系，但是却指定了某种关系），因此本文提出使用ReRN 网络来采样得到 relatedness relations。</li><li>P( R,O|V,E,I ) 指： 在给定image，object proposals以及relation proposal之后，得到object label 和 relation label。<br>一般的方法是采用iterative refinement process[2]，本文使用的是用GCN来迭代。</li></ol><ul><li>overview<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1dy02tb1zj31du0crajb.jpg"></li></ul><h2 id="Object-Proposal"><a href="#Object-Proposal" class="headerlink" title="Object Proposal"></a>Object Proposal</h2><p>使用faster r-cnn来提取<strong>object proposals</strong>，并得到相对应的一维特征向量（<strong>pooled feat</strong>），faster r-cnn 使用类别可知，则可以得到每个object 对应的<strong>label</strong><br>使用</p><h2 id="Relation-Proposal-Network"><a href="#Relation-Proposal-Network" class="headerlink" title="Relation Proposal Network"></a>Relation Proposal Network</h2><p>输入： <strong>labels</strong> of object pairs<br>输出：relatedness relations/ m 个object pairs<br>主要的步骤见下图：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e8ud0p66j32v311qjyg.jpg"></p><h2 id="Attention-GCN"><a href="#Attention-GCN" class="headerlink" title="Attention GCN"></a>Attention GCN</h2><h3 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h3><ul><li>与GAT的公式是一致的，具体可以参看论文GAT[ 3]，<strong>α<sub>i</sub></strong> 是注意力系数<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e8wl8yubj30f60360sr.jpg" style="zoom:60%"><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e8wl8m92j30gn04sjrp.jpg" style="zoom:60%"></li></ul><h3 id="aGCN-for-Scene-Graph-Generation"><a href="#aGCN-for-Scene-Graph-Generation" class="headerlink" title="aGCN for Scene Graph Generation"></a>aGCN for Scene Graph Generation</h3><ul><li>只构建一个graph，在这个graph中，object是node，relation也是node。<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e98pvy7aj30dh0cq0t3.jpg" style="zoom:60%">  </li><li>用skip代表object node之间的连接；构建的是有向边；捕捉了三中类型的连接：</li></ul><p><strong>object &lt;–&gt; relationship</strong>， <strong>relationship &lt;–&gt; subject</strong> and <strong>object &lt;–&gt; object</strong><br><strong>s</strong>=subjects, <strong>o</strong>=objects, and <strong>r</strong>=relationships<br>object and relationship features as  <strong>Z<sup>o</sup></strong> and <strong>Z<sup>r</sup></strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e9gaiumnj30vp0dg0v2.jpg" style="zoom:60%"><br>对上图的解读：<br>（1）虽说是一个图，但是进行了两个aGCN的计算，使用的object and relationship node representation是什么？文中说，visual aGCN 使用visual feature 来进行计算，semantic aGCN 使用pre-softmax outputs来进行计算。（没看懂）<br>（2）WZα公式是GCN的计算公式，</p><ul><li><p>以 <strong>W<sup>skip</sup> Z<sup>o</sup> α<sup>skip</sup></strong> 为例，<strong>W<sup>skip</sup></strong> 是可学习参数，<strong>Z<sup>o</sup></strong> 是object nodes feature 组成的矩阵（d,N），<strong>α<sup>skip</sup></strong> 是一个向量，为该 node<sub>i</sub>与所有的object nodes之间计算的注意力系数，维度为(1,N）（N个objects）</p></li><li><p>以 <strong>W<sup>sr</sup> Z<sup>r</sup> α<sup>sr</sup></strong> 为例，<strong>W<sup>sr</sup></strong> 是可学习参数，<strong>Z<sup>r</sup></strong> 是realtion nodes feature 组成的矩阵（d,m），<strong>α<sup>sr</sup></strong> 是一个向量，为该node<sub>i</sub>与所有的relation nodes之间计算的注意力系数，维度为(1,m）（m个realtion）</p></li><li><p>以 <strong>W<sup>rs</sup> Z<sup>o</sup> α<sup>rs</sup></strong> 为例，<strong>W<sup>rs</sup></strong> 是可学习参数，<strong>Z<sup>o</sup></strong> 是object nodes feature 组成的矩阵（d,N），<strong>α<sup>rs</sup></strong> 是一个向量，为该relation<sub>i</sub>与所有的object nodes之间计算的注意力系数，维度为(1,N）（N个objects）  </p><p>（3）需要注意的是，<strong>α<sub>ii</sub></strong>=1，这将使得，每一行想加不为1</p></li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1ea7p889dj313l0dzwj3.jpg"><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] A faster pytorch implementation of faster  r-cnn. <a href="https://github.com/jwyang/faster-rcnn.pytorch" target="_blank" rel="noopener">https://github.com/jwyang/faster-rcnn.pytorch</a><br>[2] Scene graph generation by iterative message passing<br>[3] Graph Attention Networks<br>[3] Graph Attention Networks</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Batch Normalization(BN层)</title>
      <link href="/2019/03/23/Batch-Normalization-BN%E5%B1%82/"/>
      <url>/2019/03/23/Batch-Normalization-BN%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<p>参看：<a href="https://blog.csdn.net/donkey_1993/article/details/81871132" target="_blank" rel="noopener">https://blog.csdn.net/donkey_1993/article/details/81871132</a><br>参看：<a href="https://zhuanlan.zhihu.com/p/34879333" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34879333</a></p><h2 id="BN层的原理"><a href="#BN层的原理" class="headerlink" title="BN层的原理"></a>BN层的原理</h2><ul><li><p>在训练阶段，输入到网络中的是mini batch</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1chneox3hj30dv0bddgx.jpg">解析：</li><li><p>Normalization操作我们虽然缓解了ICS问题，让每一层网络的输入数据分布都变得稳定，但却导致了数据表达能力的缺失。也就是我们通过变换操作改变了原有数据的信息表达（representation ability of the network），使得底层网络学习到的参数信息丢失。另一方面，通过让每一层的输入分布均值为0，方差为1，会使得输入在经过sigmoid或tanh激活函数时，容易陷入非线性激活函数的线性区域。  </p></li><li><p>因此，BN又引入了两个可学习（learnable）的参数  γ与β  。这两个参数的引入是为了恢复数据本身的表达能力，对规范化后的数据进行线性变换，即<img src="https://www.zhihu.com/equation?tex=%5Ctilde%7BZ_j%7D%3D%5Cgamma_j+%5Chat%7BZ%7D_j%2B%5Cbeta_j" alt="\tilde{Z_j}=\gamma_j \hat{Z}_j+\beta_j">。特别地，当  γ<sup>2</sup>=σ<sup>2</sup> ， β=μ 时，可以实现等价变换（identity transform）并且保留了原始输入特征的分布信息。</p></li></ul><h2 id="测试阶段如何使用Batch-Normalization？"><a href="#测试阶段如何使用Batch-Normalization？" class="headerlink" title="测试阶段如何使用Batch Normalization？"></a>测试阶段如何使用Batch Normalization？</h2><p>我们知道BN在每一层计算的  μ与σ<sup>2</sup>都是基于当前batch中的训练数据，但是这就带来了一个问题：我们在预测阶段，有可能只需要预测一个样本或很少的样本，没有像训练样本中那么多的数据，此时 μ与σ<sup>2</sup>的计算一定是有偏估计，这个时候我们该如何进行计算呢？</p><p>利用BN训练好模型后，我们保留了每组mini-batch训练数据在网络中每一层的<br> μ<sub>batch</sub>与σ<sup>2</sup><sub>batch</sub> 。此时我们使用整个样本的统计量来对Test数据进行归一化，具体来说使用均值与方差的无偏估计：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmu_%7Btest%7D%3D%5Cmathbb%7BE%7D+%28%5Cmu_%7Bbatch%7D%29" alt="\mu_{test}=\mathbb{E} (\mu_{batch})"></p><p><img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2_%7Btest%7D%3D%5Cfrac%7Bm%7D%7Bm-1%7D%5Cmathbb%7BE%7D%28%5Csigma%5E2_%7Bbatch%7D%29" alt="\sigma^2_{test}=\frac{m}{m-1}\mathbb{E}(\sigma^2_{batch})"></p><p>得到每个特征的均值与方差的无偏估计后，我们对test数据采用同样的normalization方法：</p><p><img src="https://www.zhihu.com/equation?tex=BN%28X_%7Btest%7D%29%3D%5Cgamma%5Ccdot+%5Cfrac%7BX_%7Btest%7D-%5Cmu_%7Btest%7D%7D%7B%5Csqrt%7B%5Csigma%5E2_%7Btest%7D%2B%5Cepsilon%7D%7D%2B%5Cbeta" alt="BN(X_{test})=\gamma\cdot \frac{X_{test}-\mu_{test}}{\sqrt{\sigma^2_{test}+\epsilon}}+\beta"></p><p>另外，除了采用整体样本的无偏估计外。吴恩达在Coursera上的Deep Learning课程指出可以对train阶段每个batch计算的mean/variance采用<a href="[https://zhuanlan.zhihu.com/p/29895933](https://zhuanlan.zhihu.com/p/29895933)"><font color="#0099ff" size="5" face="楷体"> 指数加权平均</font></a>来得到test阶段mean/variance的估计。</p><h2 id="Batch-Normalization的优势"><a href="#Batch-Normalization的优势" class="headerlink" title="Batch Normalization的优势"></a>Batch Normalization的优势</h2><p>Batch Normalization在实际工程中被证明了能够缓解神经网络难以训练的问题，BN具有的有事可以总结为以下三点：</p><p><strong>（1）BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度</strong></p><p>BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。</p><p><strong>（2）BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定</strong></p><p>在神经网络中，我们经常会谨慎地采用一些权重初始化方法（例如Xavier）或者合适的学习率来保证网络稳定训练。<br>当学习率设置太高时，会使得参数更新步伐过大，容易出现震荡和不收敛。但是使用BN的网络将不会受到参数数值大小的影响。<br>在使用Batch Normalization之后，抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强，此时我们可以设置较大的学习率而不用过于担心模型divergence的风险。</p><p><strong>（3）BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题</strong></p><p>在不使用BN层的时候，由于网络的深度与复杂性，很容易使得底层网络变化累积到上层网络中，导致模型的训练很容易进入到激活函数的梯度饱和区；通过normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；另外通过自适应学习 γ与β又让数据保留更多的原始信息。</p><p><strong>（4）BN具有一定的正则化效果</strong></p><p>在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。</p><p>另外，原作者通过也证明了网络加入BN后，可以丢弃Dropout，模型也同样具有很好的泛化效果。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>optical flow(光流)</title>
      <link href="/2019/03/23/optical-flow-%E5%85%89%E6%B5%81/"/>
      <url>/2019/03/23/optical-flow-%E5%85%89%E6%B5%81/</url>
      
        <content type="html"><![CDATA[<p>参考：<a href="https://blog.csdn.net/zouxy09/article/details/8683859" target="_blank" rel="noopener">https://blog.csdn.net/zouxy09/article/details/8683859</a></p><h2 id="光流的定义"><a href="#光流的定义" class="headerlink" title="光流的定义"></a>光流的定义</h2><p>在人的眼睛在观察物体时，物体的景象在人的视网膜上形成一系列连续变化的图像，这一系列连续变化的信息不断“流过”视网膜，好像一种光的流，故称之为光流。<br>一般，光流是由于场景中前景目标本身的移动、相机的移动，或者两者的共同运动所产生的。<br>定义：它是空间运动物体在观察成像平面上的像素运动的瞬时速度，是利用图像序列中<strong>像素在时间域上的变化以及相邻帧之间的相关性</strong>来找到上一帧跟当前帧之间存在的<strong>对应关系</strong>，从而计算出<strong>相邻帧</strong>之间物体的运动信息的一种方法。</p><h2 id="如何计算光流"><a href="#如何计算光流" class="headerlink" title="如何计算光流"></a>如何计算光流</h2><ul><li><p>那通俗的讲就是通过一个图片序列，把每张图像中每个像素的运动速度和运动方向找出来就是光流场。那怎么找呢？咱们直观理解肯定是：第t帧的时候A点的位置是(x1, y1)，那么我们在第t+1帧的时候再找到A点，假如它的位置是(x2,y2)，那么我们就可以确定A点的运动了：(ux, vy) = (x2, y2) - (x1,y1)。</p></li><li><p>那怎么知道第t+1帧的时候A点的位置呢？ 这就存在很多的光流计算方法了。</p></li><li><p>1981年，Horn和Schunck创造性地将二维速度场与灰度相联系，引入光流约束方程，得到光流计算的基本算法。人们基于不同的理论基础提出各种光流计算方法，算法性能各有不同。Barron等人对多种光流计算技术进行了总结，按照理论基础与数学方法的区别把它们分成四种：<strong>基于梯度的方法、基于匹配的方法、基于能量的方法、基于相位的方法</strong>。近年来神经动力学方法也颇受学者重视。</p></li><li><p>yaya: 即光流法计算的是：相邻两帧之间的对应像素点之间的<strong>速度矢量</strong>，但是如何得到相邻帧对应的像素点是一个问题。<br>光流法主要依赖于三个假设：</p><p>  [亮度恒定] 图像中目标的像素强度在连续帧之间不会发生变化。<br>  [时间规律] 相邻帧之间的时间足够短，以至于在考虑运行变化时可以忽略它们之间的差异。该假设用于导出下面的核心方程。<br>  [空间一致性] 相邻像素具有相似的运动。  </p></li></ul><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1cgxrjz19j30hi0h90uo.jpg">  <p>上式中，I<sub>x</sub>,  I<sub>y</sub>可以通过图像沿x方向和y方向的导数计算，I<sub>t</sub>可以通过I(x,y,t)−I(x,y,t−1)计算。未知数是(u,v)， 正是我们想要求解的每个像素在前后相邻两帧的位移。</p><p>这里只有一个方程，却有两个未知数（实际是NN个方程，2N2N个未知数，NN是图像中待估计的像素点的个数，但是我们通过矩阵表示，将它们写成了如上式所述的紧凑形式），所以是一个不定方程。我们需要找出其它的约束求解方程。</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1cgxrjo7dj30g00e6ta4.jpg"><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1cgxrhmkgj30gh05swej.jpg">  <p>如上图所示，H中的像素点(x,y)在I中的移动到了(x+u,y+v)的位置，偏移量为(u,v)。速度=位移在极短时间你内的位移量。  </p><p>参看：<a href="https://xmfbit.github.io/2017/05/03/cs131-opticalflow/" target="_blank" rel="noopener">https://xmfbit.github.io/2017/05/03/cs131-opticalflow/</a><br>参看：<a href="https://blog.csdn.net/carson2005/article/details/7581642" target="_blank" rel="noopener">https://blog.csdn.net/carson2005/article/details/7581642</a></p>]]></content>
      
      
      <categories>
          
          <category> 行为识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 行为识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从bagging到dropout</title>
      <link href="/2019/03/22/%E4%BB%8Ebagging%E5%88%B0dropout/"/>
      <url>/2019/03/22/%E4%BB%8Ebagging%E5%88%B0dropout/</url>
      
        <content type="html"><![CDATA[<ul><li>转载 from: <a href="https://blog.csdn.net/m0_37477175/article/details/77145459" target="_blank" rel="noopener">https://blog.csdn.net/m0_37477175/article/details/77145459</a></li></ul><p>dropout的思想继承自bagging方法，学习dropout先了解一下bagging方法。</p><h2 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h2><ul><li>bagging是一种集成方法（ensemble methods）,可以通过集成来减小泛化误差（generalization error）。 </li><li>bagging的<strong>最基本的思想</strong>是通过分别训练几个不同分类器，最后对测试的样本，每个分类器对其进行投票。在机器学习上这种策略叫model averaging。 </li><li>model averaging 之所以有效，是因为并非所有的分类器都会产生相同的误差，只要有不同的分类器产生的误差不同就会对减小泛化误差非常有效。 </li><li>对于bagging方法，允许采用相同的分类器，相同的训练算法，相同的目标函数。但是在<strong>数据集方面</strong>，<a href="https://www.baidu.com/s?wd=%E6%96%B0%E6%95%B0%E6%8D%AE&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd" target="_blank" rel="noopener">新数据</a>集与原始数据集的大小是相等的。每个数据集都是通过在原始数据集中随机选择一个样本进行替换而得到的。意味着，每个新数据集中会<strong>存在重复</strong>的样本。 </li><li>在数据集建好之后，用<strong>相同的学习算法</strong>分别作用于每个数据集就得到了几个分类器。 </li><li>下面这幅图片很好的解释了bagging的工作方式：我们想实现一个对数字8进行分类的分类器。此时构造了两个数据集，使用相同的学习算法，第一个分类器学习到的是8的上面那部分而第二个分类器学习的是8的下面那个部分。当我们把两个分类器集合起来的时候，此时的分类才是比较好的。 </li><li>Each of these individual classification ruls is brittle, but if we average there output then the detector is robust.<br><img src="https://img-blog.csdn.net/20170813153102572?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbTBfMzc0NzcxNzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></li></ul><h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><ul><li><p>我们可以把dropout类比成将许多大的神经网络进行集成的一种bagging方法。 </p></li><li><p>但是每一个神经网络的训练是非常耗时和占用很多内存的，训练很多的神经网络进行集合分类就显得太不实际了。 </p></li><li><p>但是，dropout可以训练所有子网络的集合，这些子网络通过去除整个网络中的一些<a href="https://www.baidu.com/s?wd=%E7%A5%9E%E7%BB%8F%E5%85%83&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd" target="_blank" rel="noopener">神经元</a>来获得。 </p></li><li><p>如下图所示：<br><img src="https://img-blog.csdn.net/20170813154717429?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbTBfMzc0NzcxNzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p></li><li><p>可能有些人会问上图的有些子网络，从输入到不了最终的输出，怎么办？其实对于比较宽的层（wider layers）从输入到输出都切断的概率是非常小的，多以影响不是很大。</p></li><li><p>如何移除一个神经元呢，我们通过仿射和非线性变换，试神经元的输出乘以0。</p></li><li><p>每次我们加载一个样本到minibatch，然后随机的采样一个不同的二进制掩膜作用在所有的输出，输入，隐藏节点上。每个节点的掩膜都是独立采样的。采样一个掩膜值为1的概率是固定的超参数。</p></li></ul><h2 id="bagging与dropout训练的对比"><a href="#bagging与dropout训练的对比" class="headerlink" title="bagging与dropout训练的对比"></a>bagging与dropout训练的对比</h2><ul><li>在bagging中，所有的分类器都是独立的，而在dropout中，所有的模型都是共享参数的。</li><li>在bagging中，所有的分类器都是在特定的数据集下训练至收敛，而在dropout中没有明确的模型训练过程。网络都是在一步中训练一次（输入一个样本，随机训练一个子网络）</li><li>（相同点）对于训练集来说，每一个子网络的训练数据是通过原始数据的替代采样得到的子集。<strong>？？？</strong>（自己的理解：每一个输入一个样本初始化某一个子网络）</li></ul><h2 id="dropout的优势"><a href="#dropout的优势" class="headerlink" title="dropout的优势"></a>dropout的优势</h2><ul><li>very computationally cheap在dropout训练阶段，每一个样本每一次更新只需要O(n)<br>，同时要生成n个二进制数字与每个状态相乘。除此之外，还需要O(n)的额外空间存储这些二进制数字，直到反向传播阶段。</li><li>没有很显著的限制模型的大小和训练的过程。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>梯度消失与梯度爆炸的原因以及解决方案</title>
      <link href="/2019/03/22/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E5%8E%9F%E5%9B%A0%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
      <url>/2019/03/22/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E5%8E%9F%E5%9B%A0%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
      
        <content type="html"><![CDATA[<p>转载from: <a href="https://blog.csdn.net/raojunyang/article/details/79962665" target="_blank" rel="noopener">https://blog.csdn.net/raojunyang/article/details/79962665</a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文主要深入介绍深度学习中的梯度消失和梯度爆炸的问题以及解决方案。本文分为三部分，第一部分主要直观的介绍深度学习中为什么使用梯度更新，第二部分主要介绍深度学习中梯度消失及爆炸的原因，第三部分对提出梯度消失及爆炸的解决方案。有基础的同鞋可以跳着阅读。<br>其中，梯度消失爆炸的解决方案主要包括以下几个部分。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">- </span>预训练加微调</span><br><span class="line"><span class="bullet">- </span>梯度剪切、权重正则（针对梯度爆炸）</span><br><span class="line"><span class="bullet">- </span>使用不同的激活函数</span><br><span class="line"><span class="bullet">- </span>使用batchnorm</span><br><span class="line"><span class="bullet">- </span>使用残差结构</span><br><span class="line"><span class="bullet">- </span>使用LSTM网络</span><br></pre></td></tr></table></figure><h1 id="第一部分：为什么要使用梯度更新规则"><a href="#第一部分：为什么要使用梯度更新规则" class="headerlink" title="第一部分：为什么要使用梯度更新规则"></a>第一部分：为什么要使用梯度更新规则</h1><hr><ul><li><p>在介绍梯度消失以及爆炸之前，先简单说一说梯度消失的根源—–深度神经网络和反向传播。目前深度学习方法中，深度神经网络的发展造就了我们可以构建更深层的网络完成更复杂的任务，深层网络比如深度卷积网络，LSTM等等，而且最终结果表明，在处理复杂任务上，深度网络比浅层的网络具有更好的效果。但是，目前优化神经网络的方法都是基于反向传播的思想，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。这样做是有一定原因的，首先，深层网络由许多非线性层堆叠而来，每一层非线性层都可以视为是一个非线性函数 (非线性来自于非线性激活函数），因此整个深度网络可以视为是一个复合的非线性多元函数 </p></li><li><p>我们最终的目的是希望这个多元函数可以很好的完成输入到输出之间的映射，那么，优化深度网络就是为了寻找到合适的权值，满足取得极小值点，比如最简单的损失函数 </p></li><li><p>假设损失函数的数据空间是下图这样的，我们最优的权值就是为了寻找下图中的最小值点，对于这种数学寻找最小值问题，采用梯度下降的方法再适合不过了。 </p></li></ul><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1baz0q65tj30e809edj1.jpg"><h1 id="第二部分：梯度消失、爆炸"><a href="#第二部分：梯度消失、爆炸" class="headerlink" title="第二部分：梯度消失、爆炸"></a>第二部分：梯度消失、爆炸</h1><p>梯度消失与梯度爆炸其实是一种情况，看接下来的文章就知道了。两种情况下梯度消失经常出现，一是在<strong>深层网络</strong>中，二是采用了<strong>不合适的损失函数</strong>，比如sigmoid。梯度爆炸一般出现在深层网络和<strong>权值初始化值太大</strong>的情况下，下面分别从这两个角度分析梯度消失和爆炸的原因。</p><h3 id="1-深层网络角度"><a href="#1-深层网络角度" class="headerlink" title="1.深层网络角度"></a>1.深层网络角度</h3><p>比较简单的深层网络如下：<br><img src="https://img-blog.csdn.net/20171219215626301?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>图中是一个四层的全连接网络，假设每一层网络激活后的输出为,其中为第层, 代表第层的输入，也就是第层的输出，是激活函数，那么，得出，简单记为。<br>BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整，参数的更新为，给定学习率，得出。如果要更新第二隐藏层的权值信息，根据链式求导法则，更新梯度信息：<br>，很容易看出来，即第二隐藏层的输入。<br>所以说，就是对激活函数进行求导，如果此部分大于1，那么层数增多的时候，最终的求出的梯度更新将以指数形式增加，即发生<strong>梯度爆炸</strong>，如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了<strong>梯度消失</strong>。如果说从数学上看不够直观的话，下面几个图可以很直观的说明深层网络的梯度问题（图片内容来自参考文献1）：</p><p>注：下图中的隐层标号和第一张全连接图隐层标号刚好相反。<br>图中的曲线表示权值更新的速度，对于下图两个隐层的网络来说，已经可以发现隐藏层2的权值更新速度要比隐藏层1更新的速度慢</p><p><img src="https://img-blog.csdn.net/20171220110058983?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那么对于四个隐层的网络来说，就更明显了，第四隐藏层比第一隐藏层的更新速度慢了两个数量级：</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/20171220110732927?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p><strong>总结：</strong>从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。因此，梯度消失、爆炸，其<strong>根本原因</strong>在于反向传播训练法则，属于<a href="https://www.baidu.com/s?wd=%E5%85%88%E5%A4%A9%E4%B8%8D%E8%B6%B3&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd" target="_blank" rel="noopener">先天不足</a>，另外多说一句，Hinton提出capsule的原因就是为了彻底抛弃反向传播，如果真能大范围普及，那真是一个革命。</p><h3 id="2-激活函数角度"><a href="#2-激活函数角度" class="headerlink" title="2.激活函数角度"></a>2.激活函数角度</h3><p>其实也注意到了，上文中提到计算权值更新信息的时候需要计算前层偏导信息，因此如果激活函数选择不合适，比如使用sigmoid，梯度消失就会很明显了，原因看下图，左图是sigmoid的损失函数图，右边是其倒数的图像，如果使用sigmoid作为损失函数，其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失，sigmoid函数数学表达式为：<br><img src="https://img-blog.csdn.net/20171220113129230?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="sigmoid函数"><img src="https://img-blog.csdn.net/20171220113422675?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="sigmoid函数导数"></p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">同理，<span class="built-in">tanh</span>作为损失函数，它的导数图如下，可以看出，<span class="built-in">tanh</span>比<span class="built-in">sigmoid</span>要好一些，但是它的倒数仍然是小于<span class="number">1</span>的。<span class="built-in">tanh</span>数学表达为：</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/20171220114016270?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><h1 id="第三部分：梯度消失、爆炸的解决方案"><a href="#第三部分：梯度消失、爆炸的解决方案" class="headerlink" title="第三部分：梯度消失、爆炸的解决方案"></a>第三部分：梯度消失、爆炸的解决方案</h1><hr><h3 id="2-1-方案1-预训练加微调"><a href="#2-1-方案1-预训练加微调" class="headerlink" title="2.1 方案1-预训练加微调"></a>2.1 方案1-预训练加微调</h3><p>此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。</p><h3 id="2-2-方案2-梯度剪切、正则"><a href="#2-2-方案2-梯度剪切、正则" class="headerlink" title="2.2 方案2-梯度剪切、正则"></a>2.2 方案2-梯度剪切、正则</h3><p><strong>梯度剪切</strong>这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注：在WGAN中也有梯度剪切限制操作，但是和这个是不一样的，WGAN限制梯度更新信息是为了保证lipchitz条件。</span><br></pre></td></tr></table></figure><p>另外一种解决梯度爆炸的手段是采用<strong>权重正则化</strong>（weithts regularization）比较常见的是正则，和正则，在各个深度框架中都有相应的API可以使用正则化，比如在中，若搭建网络的时候已经设置了正则化参数，则调用以下代码可以直接计算出正则损失：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regularization_loss = tf.add_n(tf<span class="selector-class">.losses</span><span class="selector-class">.get_regularization_losses</span>(scope=<span class="string">'my_resnet_50'</span>))</span><br></pre></td></tr></table></figure><p>如果没有设置初始化参数，也可以使用以下代码计算正则损失：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l2_loss = tf.add_n([tf<span class="selector-class">.nn</span><span class="selector-class">.l2_loss</span>(var) <span class="keyword">for</span> <span class="selector-tag">var</span> <span class="keyword">in</span> tf.trainable_variables() <span class="keyword">if</span> <span class="string">'weights'</span> <span class="keyword">in</span> <span class="selector-tag">var</span>.name])</span><br></pre></td></tr></table></figure><p>正则化是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式： </p><p>其中，是指正则项系数，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注：事实上，在深度神经网络中，往往是梯度消失出现的更多一些。</span><br></pre></td></tr></table></figure><h3 id="2-3-方案3-relu、leakrelu、elu等激活函数"><a href="#2-3-方案3-relu、leakrelu、elu等激活函数" class="headerlink" title="2.3 方案3-relu、leakrelu、elu等激活函数"></a>2.3 方案3-relu、leakrelu、elu等激活函数</h3><p><strong>Relu:</strong>思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。先看一下relu的数学表达式：</p><p><img src="https://img-blog.csdn.net/20171220115642365?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>其函数图像：</p><p><img src="https://img-blog.csdn.net/20171220115719332?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>从上图中，我们可以很容易看出，relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。</p><p><strong>relu</strong>的主要贡献在于：</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 解决了梯度消失、爆炸的问题</span></span><br><span class="line"><span class="comment">-- 计算方便，计算速度快</span></span><br><span class="line"><span class="comment">-- 加速了网络的训练</span></span><br></pre></td></tr></table></figure><p>同时也存在一些<strong>缺点</strong>：</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）</span></span><br><span class="line"> <span class="comment">-- 输出不是以0为中心的</span></span><br></pre></td></tr></table></figure><p>尽管relu也有缺点，但是仍然是目前使用最多的激活函数</p><p><strong>leakrelu</strong><br>leakrelu就是为了解决relu的0区间带来的影响，其数学表达为：其中k是leak系数，一般选择0.01或者0.02，或者通过学习而来</p><p><img src="https://img-blog.csdn.net/20170702211001517?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2FpY2FpYXRuYnU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>leakrelu解决了0区间带来的影响，而且包含了relu的所有优点<br><strong>elu</strong><br>elu激活函数也是为了解决relu的0区间带来的影响，其数学表达为：<img src="https://img-blog.csdn.net/20171220134603079?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>其函数及其导数数学形式为：</p><p><img src="https://img-blog.csdn.net/20171220134614121?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>但是elu相对于leakrelu来说，计算要更耗时间一些</p><h3 id="2-4-解决方案4-batchnorm"><a href="#2-4-解决方案4-batchnorm" class="headerlink" title="2.4 解决方案4-batchnorm"></a>2.4 解决方案4-batchnorm</h3><p><strong>Batchnorm</strong>是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。batchnorm全名是batch normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。<br>具体的batchnorm原理非常复杂，在这里不做详细展开，此部分大概讲一下batchnorm解决梯度的问题上。具体来说就是反向传播中，经过每一层的梯度会乘以该层的权重，举个简单例子：<br>正向传播中，那么反向传播中，，反向传播式子中有的存在，所以的大小影响了梯度的消失和爆炸，batchnorm就是通过对每一层的输出规范为均值和方差一致的方法，消除了带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。<br>有关batch norm详细的内容可以参考我的另一篇博客：<br><a href="http://blog.csdn.net/qq_25737169/article/details/79048516" target="_blank" rel="noopener">http://blog.csdn.net/qq_25737169/article/details/79048516</a></p><h3 id="2-5-解决方案5-残差结构"><a href="#2-5-解决方案5-残差结构" class="headerlink" title="2.5 解决方案5-残差结构"></a>2.5 解决方案5-残差结构</h3><p><strong>残差结构</strong>说起残差的话，不得不提这篇论文了：Deep Residual Learning for Image Recognition，关于这篇论文的解读，可以参考知乎链接：<a href="https://zhuanlan.zhihu.com/p/31852747" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31852747</a>这里只简单介绍残差如何解决梯度的问题。<br>事实上，就是残差网络的出现导致了image net比赛的终结，自从残差提出后，几乎所有的深度网络都离不开残差的身影，相比较之前的几层，几十层的深度网络，在残差网络面前都不值一提，残差可以很轻松的构建几百层，一千多层的网络而不用担心梯度消失过快的问题，原因就在于残差的捷径（shortcut）部分，其中残差单元如下图所示：<br><img src="https://img-blog.csdn.net/20171220144105760?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>相比较于以前网络的直来直去结构，残差中有很多这样的跨层连接结构，这样的结构在反向传播中具有很大的好处，见下式：<br><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%20loss%7D%7B%5Cpartial%20%7B%7Bx%7D_%7Bl%7D%7D%7D=%5Cfrac%7B%5Cpartial%20loss%7D%7B%5Cpartial%20%7B%7Bx%7D_%7BL%7D%7D%7D%5Ccdot%20%5Cfrac%7B%5Cpartial%20%7B%7Bx%7D_%7BL%7D%7D%7D%7B%5Cpartial%20%7B%7Bx%7D_%7Bl%7D%7D%7D=%5Cfrac%7B%5Cpartial%20loss%7D%7B%5Cpartial%20%7B%7Bx%7D_%7BL%7D%7D%7D%5Ccdot%20%5Cleft%28%201%2B%5Cfrac%7B%5Cpartial%20%7D%7B%5Cpartial%20%7B%7Bx%7D_%7BL%7D%7D%7D%5Csum%5Climits_%7Bi=l%7D%5E%7BL-1%7D%7BF%28%7B%7Bx%7D_%7Bi%7D%7D,%7B%7BW%7D_%7Bi%7D%7D%29%7D%20%5Cright%29" alt="这里写图片描述"><br>式子的第一个因子  表示的损失函数到达 L 的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注：上面的推导并不是严格的证明。</span><br></pre></td></tr></table></figure><h3 id="2-6-解决方案6-LSTM"><a href="#2-6-解决方案6-LSTM" class="headerlink" title="2.6 解决方案6-LSTM"></a>2.6 解决方案6-LSTM</h3><p><strong>LSTM</strong>全称是长短期记忆网络（long-short term memory networks），是不那么容易发生梯度消失的，主要原因在于LSTM内部复杂的“门”(gates)，如下图，LSTM通过它内部的“门”可以接下来更新的时候“记住”前几次训练的”残留记忆“，因此，经常用于生成文本中。目前也有基于CNN的LSTM，感兴趣的可以尝试一下。</p><p><img src="http://upload-images.jianshu.io/upload_images/42741-b9a16a53d58ca2b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="这里写图片描述"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料:"></a>参考资料:</h2><p>1.《Neural networks and deep learning》<br>2.<a href="https://www.baidu.com/s?wd=%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd" target="_blank" rel="noopener">《机器学习》</a>周志华 </p><ol start="3"><li><p><a href="https://www.cnblogs.com/willnote/p/6912798.html&gt;" target="_blank" rel="noopener">https://www.cnblogs.com/willnote/p/6912798.html&gt;</a> </p></li><li><p><a href="https://www.zhihu.com/question/38102762" target="_blank" rel="noopener">https://www.zhihu.com/question/38102762</a> </p><ol start="5"><li><a href="http://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="noopener">http://www.jianshu.com/p/9dc9f41f0b29</a></li></ol></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>python: list vs tuple</title>
      <link href="/2019/03/20/python-list-vs-tuple/"/>
      <url>/2019/03/20/python-list-vs-tuple/</url>
      
        <content type="html"><![CDATA[<ul><li><p><a href="https://shiyaya.github.io/2019/03/12/python%E5%9F%BA%E7%A1%80%EF%BC%9A-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-python-%E4%B8%AD%E7%9A%84%E8%B5%8B%E5%80%BC%E3%80%81%E5%BC%95%E7%94%A8%E3%80%81%E6%8B%B7%E8%B4%9D%E3%80%81%E4%BD%9C%E7%94%A8%E5%9F%9F/" target="_blank" rel="noopener">shiyaya.github.io-python基础： 深入理解 python 中的赋值、引用、拷贝、作用域</a></p></li><li><p><a href="https://data-flair.training/blogs/python-tuples-vs-lists/" target="_blank" rel="noopener">https://data-flair.training/blogs/python-tuples-vs-lists/</a>  </p></li></ul><table><thead><tr><th>list</th><th>tuple</th></tr></thead><tbody><tr><td>可变对象</td><td>不可变对象</td></tr><tr><td>参数传递是传递的是引用</td><td>参数传递是传递的是值</td></tr><tr><td></td><td></td></tr><tr><td>可以修改某个元素的值</td><td>不可以修改某个元素的值，即不可以按索引来修改元素值</td></tr><tr><td>a= [1,2,3]<br>b=a<br>b[0]=8<br>print(a) #a=[8,2,3]</td><td>略</td></tr><tr><td>可以修改slice<br>del a[0:2]</td><td>不可以修改slice<br>del a[0:2]#会提示错误</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习知识点</title>
      <link href="/2019/03/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
      <url>/2019/03/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<ul><li><p>讲一下正则化，L1和L2正则化各自的特点和适用场景。<br>答：L1用来获得稀疏化特征；L2用来防止过拟合。L1让一部分特征的系数缩小到0，从而间接实现特征选择，用于特征间有关联的场合；L2让所有的特征系数都减小，但不会减为0，会使优化求解稳定快速。</p></li><li><p>防止过拟合的方法：<br>（1）早停，使用验证集，当验证集的损失下降，但是训练集的损失仍在上升时，则停止训练<br>（2）加入正则化项，L1、L2</p></li><li><p>分类问题有哪些评价指标？每种的适用场景<br>Precision  精确率，在所有预测为正样本的样本(TP+FP)中预测正确(TP)的比例，也就是：</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1asfgshjuj307c01d3yb.jpg">适用于：检索出的信息有多少是用户感兴趣的Recall  召回率，在所有正样本(TP+FN)中，预测正确(TP)的比例，也就是：<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1asfv7ikpj306d019t8i.jpg">  适用于：用户感兴趣的信息有多少被检索出来了Accuracy  准确率，正确分类的样本占所有样本的比例，不适于数据极度不平衡的场景如广告点击率一般在千分之几。<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1asg2ggqgj30ab01eglg.jpg">适用于多分类问题F1-measure  F1分数，是综合考虑Precision和Recall得到的一个指标，一般在需要PR都要保证的场景使用，针对一个值的优化更加直观容易衡量<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1asgb0qo9j304701dwe9.jpg"></li><li><p>逻辑回归可以处理非线性问题吗<br>只用原始特征不能；对特征做非线性变换，比如kernel，当然可以。 但那就不是lr了 或者一个神经网络 最后一层看成是lr 前面看成是提特征<br>lr的应用场景主要是特征很多的情况下 比如特征是上亿维的一些场景</p></li><li><p>讲一下pooling的作用， 为什么max pooling要更常用？哪些情况下，average pooling比max pooling更合适？<br>（1）保证特征的位置与旋转不变性。对于图像处理这种特性是很好的，但是对于NLP来说特征出现的位置是很重要的。比如主语一般出现在句子头等等<br>maxpooling提供了一定position的invariance，当图像某些区域像素变化时，maxpooling得到的output并不会变<br>（2）减少模型参数数量，减少过拟合问题。2D或1D的数组转化为单一数值，对于后续的convolution层或者全连接隐层来说，减少了单个Filter参数或隐层神经元个数<br>（3）可以把变长的输入x整理成固定长度的输入。CNN往往最后连接全连接层，神经元个数需要固定好，但是cnn输入x长度不确定，通过pooling操作，每个filter固定取一个值。有多少个Filter，Pooling就有多少个神经元，这样就可以把全连接层神经元固定住<br>（4）yaya: pooling 一般是对缩小image size，从而可以减小后续步骤中的参数量<br>max-pooling还提供了非线性, 这是max-pooling效果更好的一个重要原因.</p></li></ul><p><strong>average pooling比max pooling更合适：</strong>有的时候在模型接近分类器的末端使用全局平均池化还可以代替Flatten操作，使输入数据变成一位向量。</p><ul><li><p>1x1的卷积核有什么作用？<br>1*1的卷积核在NIN、Googlenet中被广泛使用，作用：<br>（1）实现跨通道的交互和信息整合<br>（2）进行卷积核通道数的降维和升维<br>（3）对于单通道feature map 用单核卷积即为乘以一个参数，而一般情况都是多核卷积多通道，实现多个feature map的线性组合</p></li><li><p>梯度消失和梯度爆炸的原因是什么？ 有哪些解决方法？<br>转载：<a href="https://blog.csdn.net/raojunyang/article/details/79962665（解释的很清楚）" target="_blank" rel="noopener">https://blog.csdn.net/raojunyang/article/details/79962665（解释的很清楚）</a><br>梯度消失更容易发生，当网络较深或者使用了不合适的激活函数时，会发生梯度消失；当为深层网络且权值初始化值太大时，容易发生梯度爆炸<br>yaya: 由于深层网络，在底层的网络权重的更新，需要高层网络权重的连乘，因此，当高层网络权重较小时，使得发生梯度消失，相反，当权重较大时，则发生梯度爆炸。<br>那么什么时候高层网络权重小—当使用sigmoid/tanh这样的激活函数时，因为sigmoid的导数最大为1/2；什么时候高层网络权重大–当初始化的权重较大时<br>如何解决梯度消失与爆炸：（1）使用正确的非线性激活函数（2）对于梯度爆炸问题使用梯度剪切（3）使用batch normalization（4）使用残差结构</p></li><li><p>CNN和RNN的梯度消失是一样的吗？  </p></li><li><p>有哪些防止过拟合的方法？<br>早停；添加正则化项：L1、L2；使用dropout</p></li><li><p>讲一下激活函数sigmoid，tanh，relu. Leaky ReLU各自的优点和适用场景？<br>sigmoid，tanh 有梯度消失的问题<br>relu 部分解决梯度消失问题（x&gt;0）<br>leaky relu </p></li><li><p>relu的负半轴导数都是0，这部分产生的梯度消失怎么办？  </p></li><li><p>batch size对收敛速度的影响。  </p></li><li><p>讲一下batch normalization<br>对输入的数据进行mini batch 的归一化</p></li><li><p>讲一下你怎么理解dropout，分别从bagging和正则化的角度<br><a href="https://blog.csdn.net/m0_37477175/article/details/77145459" target="_blank" rel="noopener">https://blog.csdn.net/m0_37477175/article/details/77145459</a><br>bagging 都是使用集成学习的思想，但是</p></li><li><p>data augmentation有哪些技巧？  </p></li><li><p>讲一下你了解的优化方法，sgd, momentum, rmsprop, adam的区别和联系  </p></li><li><p>如果训练的神经网络不收敛，可能有哪些原因？<br>可以参见此博文，具体来说，可以简述为以下几点：<br>（1）<strong>没有对数据进行归一化</strong>，即对数据减均值，并除以方差。而大部分神经网络的输入输出都是在0附近的分布。因此无法收敛。<br>（2）<strong>学习率不正确</strong><br>（3）<strong>在输出层使用错误的激活函数</strong>：在最后一层使用激活函数时，无法产生所需全部范围的值。假使你使用Relu这类限制范围的函数，神经网络便只会训练得到正值<br>（4）<strong>没有正确初始化权重</strong></p></li></ul><p>（1）代码题（leetcode类型），主要考察数据结构和基础算法，以及代码基本功<br>虽然这部分跟机器学习，深度学习关系不大，但也是面试的重中之重。基本每家公司的面试都问了大量的算法题和代码题，即使是商汤、face++这样的深度学习公司，考察这部分的时间也占到了我很多轮面试的60%甚至70%以上。我去face++面试的时候，面试官是residual net，shuffle net的作者；但他们的面试中，写代码题依旧是主要的部分。<br>大部分题目都不难，基本是leetcode medium的难度。但是要求在现场白板编程，思路要流畅，能做到一次性Bug-free. 并且，一般都是要给出时间复杂度和空间复杂度最优的做法。对于少数难度很大的题，也不要慌张。一般也不会一点思路也没有，尽力给面试官展现自己的思考过程。面试官也会引导你，给一点小提示，沿着提示把题目慢慢做出来也是可以通过面试的。<br>以下是我所遇到的一些需要当场写出完整代码的题目：<br>&lt;1&gt; 二分查找。分别实现C++中的lower_bound和upper_bound.<br>&lt;2&gt; 排序。 手写快速排序，归并排序，堆排序都被问到过。<br>&lt;3&gt; 给你一个数组，求这个数组的最大子段积<br>时间复杂度可以到O(n)<br>&lt;4&gt; 给你一个数组，在这个数组中找出不重合的两段，让这两段的字段和的差的绝对值最小。<br>时间复杂度可以到O(n)<br>&lt;5&gt; 给你一个数组，求一个k值，使得前k个数的方差 + 后面n-k个数的方差最小<br>时间复杂度可以到O(n)<br>&lt;6&gt; 给你一个只由0和1组成的字符串，找一个最长的子串，要求这个子串里面0和1的数目相等。<br>时间复杂度可以到O(n)<br>&lt;7&gt; 给你一个数组以及一个数K， 从这个数组里面选择三个数，使得三个数的和小于等于K， 问有多少种选择的方法？<br>时间复杂度可以到O(n^2)<br>&lt;8&gt; 给你一个只由0和1组成的矩阵，找出一个最大的子矩阵，要求这个子矩阵是方阵，并且这个子矩阵的所有元素为1<br>时间复杂度可以到O(n^2)<br>&lt;9&gt; 求一个字符串的最长回文子串<br>时间复杂度可以到O(n) (Manacher算法)<br>&lt;10&gt; 在一个数轴上移动，初始在0点，现在要到给定的某一个x点， 每一步有三种选择，坐标加1，坐标减1，坐标乘以2，请问最少需要多少步从0点到x点。<br>&lt;11&gt; 给你一个集合，输出这个集合的所有子集。<br>&lt;12&gt; 给你一个长度为n的数组，以及一个k值（k &lt; n) 求出这个数组中每k个相邻元素里面的最大值。其实也就是一个一维的max pooling<br>时间复杂度可以到O(n)<br>&lt;13&gt; 写一个程序，在单位球面上随机取点，也就是说保证随机取到的点是均匀的。<br>&lt;14&gt; 给你一个长度为n的字符串s，以及m个短串（每个短串的长度小于10）， 每个字符串都是基因序列，也就是说只含有A,T,C,G这四个字母。在字符串中找出所有可以和任何一个短串模糊匹配的子串。模糊匹配的定义，两个字符串长度相等，并且至多有两个字符不一样，那么我们就可以说这两个字符串是模糊匹配的。<br>&lt;15&gt; 其它一些描述很复杂的题这里就不列了。</p><p>（2）数学题或者”智力”题。<br>不会涉及特别高深的数学知识，一般就是工科数学（微积分，概率论，线性代数）和一些组合数学的问题。<br>下面是我在面试中被问到过的问题：<br>&lt;1&gt; 如果一个女生说她集齐了十二个星座的前男友，她前男友数量的期望是多少？<br>ps：这道题在知乎上有广泛的讨论，作为知乎重度用户我也看到过。如果一个女生说，她集齐了十二个星座的前男友，我们应该如何估计她前男友的数量？<br>&lt;2&gt; 两个人玩游戏。有n堆石头，每堆分别有a1, a2, a3…. an个石头，每次一个游戏者可以从任意一堆石头里拿走至少一个石头，也可以整堆拿走，但不能从多堆石头里面拿。无法拿石头的游戏者输，请问这个游戏是否有先手必胜或者后手必胜的策略？ 如果有，请说出这个策略，并证明这个策略能保证必胜。<br>&lt;3&gt; 一个一维数轴，起始点在原点。每次向左或者向右走一步，概率都是0.5. 请问回到原点的步数期望是多少？<br>&lt;4&gt; 一条长度为1的线段，随机剪两刀，求有一根大于0.5的概率。<br>&lt;5&gt; 讲一下你理解的矩阵的秩。低秩矩阵有什么特点？ 在图像处理领域，这些特点有什么应用？<br>&lt;6&gt; 讲一下你理解的特征值和特征向量。<br>&lt;7&gt; 为什么负梯度方向是使函数值下降最快的方向？简单数学推导一下</p><p>（3）机器学习基础<br>这部分建议参考周志华老师的《机器学习》。<br>下面是我在面试中被问到过的问题：<br>&lt;1&gt; 逻辑回归和线性回归对比有什么优点？<br>&lt;2&gt; 逻辑回归可以处理非线性问题吗？<br>&lt;3&gt; 分类问题有哪些评价指标？每种的适用场景。<br>&lt;4&gt; 讲一下正则化，L1和L2正则化各自的特点和适用场景。<br>&lt;5&gt; 讲一下常用的损失函数以及各自的适用场景。<br>&lt;6&gt; 讲一下决策树和随机森林<br>&lt;7&gt; 讲一下GBDT的细节，写出GBDT的目标函数。 GBDT和Adaboost的区别与联系<br>&lt;8&gt; 手推softmax loss公式<br>&lt;9&gt; 讲一下SVM, SVM与LR有什么联系。<br>&lt;10&gt;讲一下PCA的步骤。PCA和SVD的区别和联系<br>&lt;11&gt; 讲一下ensemble<br>&lt;12&gt; 偏差和方差的区别。ensemble的方法中哪些是降低偏差，哪些是降低方差？<br>…… 这部分问得太琐碎了，我能记起来的问题就这么多了。我的感觉，这部分问题大多数不是问得很深，所以不至于被问得哑口无言，总有得扯；但是要想给出一个特别深刻的回答，还是需要对机器学习的基础算法了解比较透彻。</p><p>（4）深度学习基础<br>这部分的准备，我推荐花书（Bengio的Deep learning）和 @魏秀参 学长的《解析卷积神经网络-深度学习实践手册》<br>下面是我在面试中被问到过的问题：<br>&lt;1&gt; 手推BP<br>&lt;2&gt; 手推RNN和LSTM结构<br>&lt;3&gt; LSTM中每个gate的作用是什么，为什么跟RNN比起来，LSTM可以防止梯度消失<br>&lt;4&gt; 讲一下pooling的作用， 为什么max pooling要更常用？哪些情况下，average pooling比max pooling更合适？<br>&lt;5&gt; 梯度消失和梯度爆炸的原因是什么？ 有哪些解决方法？<br>&lt;6&gt; CNN和RNN的梯度消失是一样的吗？<br>&lt;6&gt; 有哪些防止过拟合的方法？<br>&lt;7&gt; 讲一下激活函数sigmoid，tanh，relu. 各自的优点和适用场景？<br>&lt;8&gt; relu的负半轴导数都是0，这部分产生的梯度消失怎么办？<br>&lt;9&gt; batch size对收敛速度的影响。<br>&lt;10&gt; 讲一下batch normalization<br>&lt;11&gt; CNN做卷积运算的复杂度。如果一个CNN网络的输入channel数目和卷积核数目都减半，总的计算量变为原来的多少？<br>&lt;12&gt; 讲一下AlexNet的具体结构，每层的作用<br>&lt;13&gt; 讲一下你怎么理解dropout，分别从bagging和正则化的角度<br>&lt;14&gt; data augmentation有哪些技巧？<br>&lt;15&gt; 讲一下你了解的优化方法，sgd, momentum, rmsprop, adam的区别和联系<br>&lt;16&gt; 如果训练的神经网络不收敛，可能有哪些原因？<br>&lt;17&gt; 说一下你理解的卷积核， 1x1的卷积核有什么作用？<br>……..<br>同上，这部分的很多问题也是每个人都或多或少能回答一点，但要答得很好还是需要功底的。</p><p>（5）科研上的开放性问题<br>这部分的问题没有固定答案，也没法很好地针对性准备。功在平时，多读paper多思考，注意培养自己的insight和intuition<br>下面是我在面试中被问到过的问题：<br>&lt;1&gt; 选一个计算机视觉、深度学习、机器学习的子领域，讲一下这个领域的发展脉络，重点讲出各种新方法提出时的motivation，以及谈谈这个领域以后会怎么发展。<br>&lt;2&gt; 讲一下你最近看的印象比较深的paper<br>&lt;3&gt; 讲一下经典的几种网络结构， AlexNet， VGG，GoogleNet， Residual Net等等，它们各自最重要的contribution<br>&lt;4&gt; 你看过最近很火的XXX paper吗? 你对这个有什么看法？<br>……<br>（6） 编程语言、操作系统等方面的一些问题。<br>C++， Python， 操作系统，Linux命令等等。这部分问得比较少，但还是有的，不具体列了<br>（7）针对简历里项目/论文 / 实习的一些问题。<br>这部分因人而异，我个人的对大家也没参考价值，也不列了。</p><p>作者：wendy_要努力努力再努力<br>链接：<a href="https://www.jianshu.com/p/d40fc51874c8" target="_blank" rel="noopener">https://www.jianshu.com/p/d40fc51874c8</a><br>来源：简书<br>简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unexpected key(s) in state_dict: “**module**.features.conv1.0.weight”</title>
      <link href="/2019/03/20/Unexpected-key-s-in-state-dict-%E2%80%9C-module-features-conv1-0-weight%E2%80%9D/"/>
      <url>/2019/03/20/Unexpected-key-s-in-state-dict-%E2%80%9C-module-features-conv1-0-weight%E2%80%9D/</url>
      
        <content type="html"><![CDATA[<ul><li><p>参考此处<a href="https://discuss.pytorch.org/t/when-loading-a-model-unexpected-key-s-in-state-dict-module-features-conv1-0-weight/20505" target="_blank" rel="noopener">[link]</a></p></li><li><p>问题描述：在使用pytorch 加载预训练的模型时:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decoder.load_state_dict(checkpoint[<span class="string">'dec'</span>])</span><br></pre></td></tr></table></figure></li></ul><p>出现错误：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">Missing</span> <span class="selector-tag">key</span>(<span class="selector-tag">s</span>) <span class="selector-tag">in</span> <span class="selector-tag">state_dict</span>: “<span class="selector-tag">features</span><span class="selector-class">.conv1</span><span class="selector-class">.0</span><span class="selector-class">.weight</span>”,</span><br><span class="line"><span class="selector-tag">Unexpected</span> <span class="selector-tag">key</span>(<span class="selector-tag">s</span>) <span class="selector-tag">in</span> <span class="selector-tag">state_dict</span>: “**<span class="selector-tag">module</span>**<span class="selector-class">.features</span><span class="selector-class">.conv1</span><span class="selector-class">.0</span><span class="selector-class">.weight</span>”,</span><br></pre></td></tr></table></figure><ul><li>原因：<br>在训练阶段，使用的是多GPU，采用了nn.DataParallel，因此在测试阶段，对应的模型也需要是多GPU的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>非极大值抑制(NMS)</title>
      <link href="/2019/03/20/%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6-NMS/"/>
      <url>/2019/03/20/%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6-NMS/</url>
      
        <content type="html"><![CDATA[<p>非极大值抑制（Non-maximum suppression，NMS）是一种去除非极大值的算法，常用于计算机视觉中的边缘检测、物体识别等。</p><h2 id="算法流程："><a href="#算法流程：" class="headerlink" title="算法流程："></a>算法流程：</h2><p>给出一张图片和上面许多物体检测的候选框（即每个框可能都代表某种物体），但是这些框很可能有互相重叠的部分，我们要做的就是只保留最优的框。假设有N个框，每个框被分类器计算得到的分数为Si, 1&lt;=i&lt;=N。</p><p>0、建造一个存放待处理候选框的集合H，初始化为包含全部N个框；</p><p>建造一个存放最优框的集合M，初始化为空集。</p><p>1、将所有集合 H 中的框进行排序，选出分数最高的框 m，从集合 H 移到集合 M；</p><p>2、遍历集合 H 中的框，分别与框 m 计算交并比（Interection-over-union，IoU），如果高于某个阈值（一般为0~0.5），则认为此框与 m 重叠，将此框从集合 H 中去除。</p><p>3、回到第1步进行迭代，直到集合 H 为空。集合 M 中的框为我们所需。</p><h2 id="需要优化的参数："><a href="#需要优化的参数：" class="headerlink" title="需要优化的参数："></a>需要优化的参数：</h2><p>IoU 的阈值是一个可优化的参数，一般范围为0~0.5，可以使用交叉验证来选择最优的参数。<br>比如人脸识别的一个例子：</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g19dbcocjfj30gw07kdmg.jpg"><p>已经识别出了 5 个候选框，但是我们只需要最后保留两个人脸。</p><p>首先选出分数最大的框（0.98），然后遍历剩余框，计算 IoU，会发现露丝脸上的两个绿框都和 0.98 的框重叠率很大，都要去除。</p><p>然后只剩下杰克脸上两个框，选出最大框（0.81），然后遍历剩余框（只剩下0.67这一个了），发现0.67这个框与 0.81 的 IoU 也很大，去除。</p><p>至此所有框处理完毕，算法结果：</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g19dbr3kk4j30gw07kdmi.jpg"><p>（图片来自<a href="https://blog.csdn.net/shuzfan/article/details/52711706）" target="_blank" rel="noopener">https://blog.csdn.net/shuzfan/article/details/52711706）</a></p><h2 id="添加-by-yaya"><a href="#添加-by-yaya" class="headerlink" title="添加 by yaya:"></a>添加 by yaya:</h2><ul><li>在faster r-cnn中，得到了pred_boxes以及cls_boxes 之后，分别对每个类的objects进行NMS。（这里多说一句：使用了class_agnostic=false，即对每个bbox都有N个类别的得分）</li><li>首先得到得分最高的一个object bbox，之后，进行IOU分析，若IoU大于阈值，则剔除，否则保留。</li><li>对这一个得分最高的bbox分析完之后，再分析下一个次高得分的，并剔除所有与它IoU值大于阈值的object。一直这样分析，直到剩下的object之间的IoU值两两之间均小于阈值。</li><li>即可得到该类对应的bbox，且不交叠。</li><li>下一循环分析下一个类<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> xrange(<span class="number">1</span>, imdb.num_classes):</span><br><span class="line">    inds = torch.nonzero(scores[:,j]&gt;thresh).view(<span class="number">-1</span>) </span><br><span class="line">    <span class="comment"># thresh = 0   inds.shape = torch.Size([300])</span></span><br><span class="line">    <span class="comment"># if there is det</span></span><br><span class="line">    <span class="keyword">if</span> inds.numel() &gt; <span class="number">0</span>:</span><br><span class="line">      cls_scores = scores[:,j][inds] <span class="comment"># 某个类在300个object上的得分</span></span><br><span class="line">      _, order = torch.sort(cls_scores, <span class="number">0</span>, <span class="literal">True</span>) <span class="comment"># 某个类在这300个object上的得分的高低排序</span></span><br><span class="line">      <span class="keyword">if</span> args.class_agnostic:</span><br><span class="line">        cls_boxes = pred_boxes[inds, :]</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        cls_boxes = pred_boxes[inds][:, j * <span class="number">4</span>:(j + <span class="number">1</span>) * <span class="number">4</span>]  <span class="comment"># 某个类对应的predict bbox</span></span><br><span class="line">      </span><br><span class="line">      cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(<span class="number">1</span>)), <span class="number">1</span>)  <span class="comment"># torch.Size([300, 5])</span></span><br><span class="line">      <span class="comment"># cls_dets = torch.cat((cls_boxes, cls_scores), 1)</span></span><br><span class="line">      cls_dets = cls_dets[order]  <span class="comment"># torch.Size([300, 5]) 排了序之后的cat</span></span><br><span class="line">      keep = nms(cls_dets, cfg.TEST.NMS)  <span class="comment"># torch.Size([91, 1])</span></span><br><span class="line">      cls_dets = cls_dets[keep.view(<span class="number">-1</span>).long()]  <span class="comment"># torch.Size([91, 5])</span></span><br><span class="line">      <span class="keyword">if</span> vis:</span><br><span class="line">        im2show = vis_detections(im2show, imdb.classes[j], cls_dets.cpu().numpy(), <span class="number">0.3</span>)</span><br><span class="line">      all_boxes[j][i] = cls_dets.cpu().numpy()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      all_boxes[j][i] = empty_array</span><br></pre></td></tr></table></figure></li></ul><p>作者：HappyRocking<br>来源：CSDN<br>原文：<a href="https://blog.csdn.net/HappyRocking/article/details/79970627" target="_blank" rel="noopener">https://blog.csdn.net/HappyRocking/article/details/79970627</a><br>版权声明：本文为博主原创文章，转载请附上博文链接！</p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机视觉学术会议</title>
      <link href="/2019/03/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/"/>
      <url>/2019/03/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/</url>
      
        <content type="html"><![CDATA[<h3 id="A类"><a href="#A类" class="headerlink" title="A类"></a>A类</h3><ul><li>(AAAI)   Conference on Artificial Intelligence  </li></ul><p>截稿日期：2019-8-30</p><ul><li>CVPR 2019: IEEE Conference on Computer Vision and Pattern Recognition    </li></ul><p><a href="http://cvpr2019.thecvf.com/" target="_blank" rel="noopener">http://cvpr2019.thecvf.com/</a> </p><p>截稿日期：2018-11-16<br>通知日期：2019-03-02<br>会议日期：2019-06-15</p><ul><li>IJCAI 2019: International Joint Conference on Artificial Intelligence</li></ul><p><a href="http://www.ijcai19.org" target="_blank" rel="noopener">http://www.ijcai19.org</a></p><p>截稿日期：2019-02-05<br>会议日期：Aug 10 - Aug 16, 2019</p><ul><li>ICCV2019: International Conference on Computer Vision</li></ul><p><a href="http://iccv2019.thecvf.com" target="_blank" rel="noopener">http://iccv2019.thecvf.com</a></p><p>截稿日期：2019-05-01<br>会议日期：Oct 27 - Nov 3, 2019</p><ul><li>ECCV</li></ul><p>截稿时间：3 月 14 日<br>会议时间：9 月 8-14 日</p><ul><li>ACM International Conference on Multimedia (ACM MM) </li></ul><p><a href="https://www.acmmm.org/2019/" target="_blank" rel="noopener">https://www.acmmm.org/2019/</a></p><p>截稿日期：2019.4.1</p><h3 id="B类"><a href="#B类" class="headerlink" title="B类"></a>B类</h3><ul><li>ICME 2019: International Conference on Multimedia and Expo</li></ul><p><a href="http://www.icme2019.org" target="_blank" rel="noopener">http://www.icme2019.org</a> </p><p>截稿日期：2018-12-03<br>通知日期：2019-03-11<br>会议日期：2019-07-08</p><h3 id="C类"><a href="#C类" class="headerlink" title="C类"></a>C类</h3><ul><li>BMVC</li></ul><p><a href="http://bmvc2018.org" target="_blank" rel="noopener">http://bmvc2018.org</a></p><p>截稿时间：4 月 30 日</p><ul><li>ICIP</li></ul><p>截稿时间：3 月 2 日</p>]]></content>
      
      
      <categories>
          
          <category> 杂类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CV vs PIL</title>
      <link href="/2019/03/20/CV-vs-PIL/"/>
      <url>/2019/03/20/CV-vs-PIL/</url>
      
        <content type="html"><![CDATA[<h2 id="PIL"><a href="#PIL" class="headerlink" title="PIL"></a>PIL</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line">pil_image = Image.open(<span class="string">'test.jpg'</span>) <span class="comment"># 图片是360x480 宽x高  </span></span><br><span class="line">print(type(pil_image)) <span class="comment"># out: PIL.JpegImagePlugin.JpegImageFile  </span></span><br><span class="line">print(pil_image.size)  <span class="comment"># out: (360,480) # w,h  </span></span><br><span class="line">print(pil_image.mode) <span class="comment"># out: 'RGB'  </span></span><br><span class="line">  </span><br><span class="line">pil_image = np.array(pil_image,dtype=np.float32) <span class="comment"># image = np.array(image)默认是uint8  </span></span><br><span class="line">print(pil_image.shape) <span class="comment"># out: (480, 360, 3)  </span></span><br><span class="line"><span class="comment"># 神奇的事情发生了，w和h换了，变成(h,w,c)了  </span></span><br><span class="line"><span class="comment"># 注意ndarray中是 行row x 列col x 维度dim 所以行数是高，列数是宽</span></span><br></pre></td></tr></table></figure><blockquote><pre><code>输出结果：&lt;class &apos;PIL.JpegImagePlugin.JpegImageFile&apos;&gt;(360, 480)RGB(480, 360, 3)</code></pre></blockquote><p>这里截图在pycharm下调试的信息（未转化成numpy array之前）</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g193y86ko4j30lt0ixmy8.jpg">  <blockquote><p>可以看到PIL.Image读出的image格式为（w,h,c）且image.mode = ‘RGB’<br>并且由代码的注释可以看到，当PIL.Image转化成numpy.array格式之后，image.size将转为（h,w,c）,c 仍为“RGB”</p></blockquote><h2 id="CV"><a href="#CV" class="headerlink" title="CV"></a>CV</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line">cv_image = cv2.imread(<span class="string">'test.jpg'</span>)  </span><br><span class="line">print(type(cv_image)) <span class="comment"># out: numpy.ndarray  </span></span><br><span class="line">print(cv_image.dtype) <span class="comment"># out: dtype('uint8')  </span></span><br><span class="line">print(cv_image.shape) <span class="comment"># out: (360,480, 3) (h,w,c) 和skimage类似  </span></span><br><span class="line"><span class="comment"># print(image) # BGR</span></span><br></pre></td></tr></table></figure><h2 id="为了比较PIL-和-CV"><a href="#为了比较PIL-和-CV" class="headerlink" title="为了比较PIL 和 CV"></a>为了比较PIL 和 CV</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(cv_image == pil_image)</span><br></pre></td></tr></table></figure><p>可以看到 分别是 False True False<br><strong>原因是PIL提取的是“RGB”，而CV提取的是“BGR”</strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g194ap1lw4j306d07zmx0.jpg"></p><p>综上，可以凝练为以下几点：</p><ol><li>PIL提取的是PIL.image类型的数据（w,h,c），通道是“RGB”。</li><li>将该数据转为numpy.array之后，得到的是（h,w,c），通道仍是“RGB”。</li><li>CV提取的是numpy.array类型的数据（h,w,c），注意通道是“BGR”。</li></ol><ul><li>再</li><li>在pytorch中输入的image模式是“RGB”</li><li>在caffe中输入的是“BGR”</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pytorch_normalze</span><span class="params">(img)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    https://github.com/pytorch/vision/issues/223</span></span><br><span class="line"><span class="string">    return appr -1~1 RGB</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    normalize = tvtsf.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                                std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    img = normalize(t.from_numpy(img))</span><br><span class="line">    <span class="keyword">return</span> img.numpy()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">caffe_normalize</span><span class="params">(img)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    return appr -125-125 BGR</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    img = img[[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>], :, :]  <span class="comment"># RGB-BGR</span></span><br><span class="line">    img = img * <span class="number">255</span></span><br><span class="line">    mean = np.array([<span class="number">122.7717</span>, <span class="number">115.9465</span>, <span class="number">102.9801</span>]).reshape(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    img = (img - mean).astype(np.float32, copy=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python基础知识-try与except处理异常语句</title>
      <link href="/2019/03/19/python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-try%E4%B8%8Eexcept%E5%A4%84%E7%90%86%E5%BC%82%E5%B8%B8%E8%AF%AD%E5%8F%A5/"/>
      <url>/2019/03/19/python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-try%E4%B8%8Eexcept%E5%A4%84%E7%90%86%E5%BC%82%E5%B8%B8%E8%AF%AD%E5%8F%A5/</url>
      
        <content type="html"><![CDATA[<h2 id="try-except介绍"><a href="#try-except介绍" class="headerlink" title="try/except介绍"></a>try/except介绍</h2><blockquote><p>与其他语言相同，在python中，try/except语句主要是用于处理程序正常执行过程中出现的一些异常情况，如语法错（python作为脚本语言没有编译的环节，在执行过程中对语法进行检测，出错后发出异常消息）、数据除零错误、从未定义的变量上取值等；而try/finally语句则主要用于在无论是否发生异常情况，都需要执行一些清理工作的场合，如在通信过程中，无论通信是否发生错误，都需要在通信完成或者发生错误时关闭网络连接。尽管<strong>try/except和**</strong>try/finally**的作用不同，但是在编程实践中通常可以把它们组合在一起使用try/except/else/finally的形式来实现稳定性和灵活性更好的设计。</p><p>默认情况下，在程序段的执行过程中，如果没有提供try/except的处理，脚本文件执行过程中所产生的异常消息会自动发送给程序调用端，如python shell，而python shell对异常消息的默认处理则是终止程序的执行并打印具体的出错信息。这也是在python shell中执行程序错误后所出现的出错打印信息的由来。</p></blockquote><h2 id="try-except格式"><a href="#try-except格式" class="headerlink" title="try/except格式"></a>try/except格式</h2><blockquote><p>python中try/except/else/finally语句的完整格式如下所示：</p><p>try:</p><p>​     Normal execution block</p><p>except A:</p><p>​     Exception A handle</p><p>except B:</p><p>​     Exception B handle</p><p>except:</p><p>​     Other exception handle</p><p>else:</p><p>​     if no exception,get here</p><p>finally:</p><p>​     print(“finally”)   </p></blockquote><blockquote><p>说明：</p><p>正常执行的程序在try下面的Normal execution block执行块中执行，在执行过程中如果发生了异常，则<strong>中断当前在Normal execution block中的执行</strong>，跳转到对应的异常处理块中开始执行；</p><p>python<strong>从第一个except X处开始查找</strong>，如果找到了对应的exception类型则进入其提供的exception handle中进行处理，如果没有找到则直接进入except块处进行处理。except块是可选项，如果没有提供，该exception将会被提交给python进行默认处理，处理方式则是<strong>终止应用程序并打印提示信息</strong>；</p><p>如果在Normal execution block执行块中执行过程中没有发生任何异常，则在执行完Normal execution block后会进入else执行块中（如果存在的话）执行。</p></blockquote><blockquote><p>无论是否发生了异常，只要提供了finally语句，以上try/except/else/finally代码块执行的最后一步总是执行finally所对应的代码块。</p><p>需要注意的是：</p><p>1.在上面所示的完整语句中try/except/else/finally所出现的顺序必须是try–&gt;except X–&gt;except–&gt;else–&gt;finally，即所有的<strong>except必须在else和finally之前</strong>，<strong>else（如果有的话）必须在finally之前</strong>，而<strong>except X必须在except之前</strong>。否则会出现语法错误。</p><p>2.对于上面所展示的try/except完整格式而言，else和finally都是可选的，而不是必须的，但是如果存在的话e<strong>lse必须在finally之前</strong>，<strong>finally</strong>（如果存在的话）<strong>必须在整个语句的最后位置</strong>。</p><p>3.在上面的完整语句中，else语句的存在必须以except X或者except语句为前提，<strong>如果在没有except语句的try block中使用else语句会引发语法错误</strong>。也就是说<strong>else不能与try/finally配合使用</strong>。</p></blockquote><p>4.except的使用要非常小心，慎用。</p><p>class AError(Exception):<br>     “””AError—exception”””<br>     print(‘AError’)</p><blockquote><p>try:</p><p>​     #raise AError</p><p>​     asdas(‘123’)</p><p>except AError:</p><p>​     print(“Get AError”)</p><p>except:</p><p>​     print(“exception”)     </p><p>else:</p><p>​     print(“else”)</p><p>finally:</p><p>​     print(“finally”)     </p><p>print(“hello wolrd”)</p><p>在上面的代码中，Normal execution block中出现了语法错误，但是由于使用了except语句，该语法错误就被掩盖掉了。因此在使用try/except是最好还是要非常清楚的知道Normal execution block中有可能出现的异常类型以进行针对性的处理。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(SGAE)Auto-Encoding Scene Graphs for Image Captioning</title>
      <link href="/2019/03/16/SAGE-Auto-Encoding-Scene-Graphs-for-Image-Captioning/"/>
      <url>/2019/03/16/SAGE-Auto-Encoding-Scene-Graphs-for-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<p>本文是CVPR2019 的关于图像描述的文章，主要让我关注的原因是用到了scene graph 和 GCN，这也是本文最大的创新点。但是本文利用的是saptial GCN（悄悄说，构建的graph节点数量少，而且是异质的，不如说是多方面融合信息已达到丰富信息的目的 :-) ）</p><ul><li>后记<br>这里讲一下图卷积中spatial  gcn①②③ 与 spectral gcn ④⑤⑥<br>①Learning task-dependent distributed representations by backpropagation through structure.<br>②A new model for  learning in graph domains<br>③The graph neural network model<br>④Spectral networks and locally connected networks on graphs.<br>⑤Deep convolutional networks on graph-structured data.<br>⑥Semi-supervised classification with graph convolutional networks</li></ul><p>关于这篇论文采用的graph  convolutional network：采用的数</p><ul><li>为什么这样说？<br>本文提到了两类graph，一类是sentence scene graph，另一类是image scene graph，而在这两类下，又进行细分为relationship、attribute、object graph。但是，每个graph 中节点是异质的，比如在relationship graph中，obejct<sub>a</sub>， obejct<sub>b</sub>，relationship<sub>ab</sub>构成了一个graph，目的是来更新relationship embedding。从我的角度来看，只是融合相关信息来更新某一目标的特征，与graph无关。  </li><li>写在最前面，我个人的理解，在得到graph 节点的embedding 之后，就要输入gcn layer 来更新特征，这里，gcn layer 的表达公式可以这样理解： 该graph中所有的节点v<sub>i</sub>，经过concatenate之后，再经过一个全连接层。<br>gcn(.) = fc( concatenate(v1, v2, … , vn) )**</li></ul><p>下面说正文：</p><h1 id="General-encoder-decoder-network-for-image-captioning"><a href="#General-encoder-decoder-network-for-image-captioning" class="headerlink" title="General encoder-decoder network for image captioning"></a>General encoder-decoder network for image captioning</h1><ul><li><p>目前一般的encoder-decoder network for image captioning 是<strong>CNN提取image的特征，然后RNN来生成句子</strong>，例如下图所示。</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14r211sidj30kb0fy0v3.jpg" style="zoom:80%"></li><li><p>进一步有<strong>加入attention</strong> [1]，下图是提取14×14×512 feature map of the fourth  convolutional layer，然后 flatten to 196 × 512 before feed into decoder。在输入到decoder时对这196个feature vector进行attention的加权求和。</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14rdh7wg7j33ak1ep7wj.jpg" style="zoom:50%"></li><li><p>也有<strong>提取images 中的object，以此来提取显著信息，对object feature 进行attention的加权求和并送入decoder</strong>。具体地，使用RPN 的ROI pooling来提取objects feature，然后对LSTM的每一个step ,对这所有的object features进行attention操作，再作为输入送入LSTM[2]。如下图</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14u74yahwj30q30modsg.jpg" style="zoom:70%"></li></ul><h1 id="本文的encoder-decoder-baseline"><a href="#本文的encoder-decoder-baseline" class="headerlink" title="本文的encoder-decoder baseline"></a>本文的encoder-decoder baseline</h1><h2 id="1-Encoder"><a href="#1-Encoder" class="headerlink" title="1. Encoder"></a><strong>1.</strong> <strong>Encoder</strong></h2><p>本文有两个encoder ： image-encoder；sentence-encoder<br>本文sentence-encoder  是用来预训练Dictionary，并共享给 image-encoder。但是在baseline中不用GCN/MGCN 和 Dictionary，因此image-encoder与sentence-encoder 之间是没有交集的。（我猜测 baseline中没有用到sentence-encoder）</p><p><strong>（1）对于image encoder 得到object embedding，relationship embedding , attribute embedding。</strong></p><p>如何得到object embedding，relationship embedding , attribute embedding？</p><ul><li>object detector : 采用与[1]一致的方式来训练faster r-cnn， 0.7的阈值 for proposal NMS， 0.3的阈值for object NMS。Faster R-CNN在visual genome上预训练，预训练之后，对proposals采取0.7的IoU阈值进行NMS，对objects 采取0.3的IoU阈值进行NMS，对每个image，采取10-100个object。使用RoI pooling 来提取object features，该object features 将作为后边relationship classifier 和attribute classifier的输入。</li><li>relationship classifier：使用在[5] 中提到的LSTM结构来作为关系分类器，来为两个object 分配一个relationship label。</li><li>attribute classifier : 为某个object 分配属性标签，将该object feature输入fc-relu-fc-softmax网络，则得到属性标签。</li></ul><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14vip74y0j311l09eq4y.jpg" style="zoom:60%">      <ul><li><p>以上三个输出的所有构成一个image scene graph  </p></li><li><p>需要说明以上三个是在<strong>Visual Genome数据集</strong>上预训练的，该数据集具有丰富的scene graph 标注，含有obejct’s categories，obejct’s attributes and pairwise relationships，因此可以用来训练目标检测器、属性分类器、关系分类器。但是由于这些标注含有很多噪声，因此采用一定的措施进行过滤：对于在数据集中出现超过2000次的objects，attributes，relationships保留下来，其余的去掉。经过这样的处理，则得到305个objects类，103个属性类，64个关系类  </p></li><li><p>经过分类器得到 label 之后，还需要得到相对应的embedding: <strong>u<sub>o</sub> , u<sub>r</sub> , u<sub>a</sub></strong> 。<br>其中 label 的维度472 = 305 + 103 + 64，即object/realation/attribute label的one hot vector 维度是三种节点的总类别数。</p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14w80gi8tj30w70ik0xu.jpg" style="zoom:70%">  </li></ul><p><strong>（2）对于sentence encoder 使用[6] 来得到 parse scene graph，进而得到object embedding，relationship embedding , attribute embedding</strong><br>注意这里的sentences使用的是<strong>MS COCO</strong>中的caption，而不是Visual Genome中的caption。同样对其进行过滤，过滤掉在all parsed scene graph中出现的objects、attributes、relationships次数少于10的，则剩下5364个objects类，1308个realtionships类，3430个attributes类。<br>sentence encoder 使用[6] 来得到 parse scene graph，但实际上，[6]又是使用[7]中的方法，所以读者最好看[7]<br>这里介绍 一下[6] SPICE 是用来评价image caption的一个性能指标，这里为什么使用它，它是用来做为一个评价指标的吗？首先回答第二个问题，不是用来作为评价指标的，而是利用了它的原理：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1dwj56im1j30iq0nmk00.jpg"></p><p>parse scene graph 的过程：sentence-&gt; syntax dependency tree-&gt; scene graph<br>给定一个句子，首先分析句法依赖树，再根据给定句法依赖树的情况下，输出scene graph，而scene graph 的输出是对sentence中的每一个次元，输出其是object，还是 relation，还是attribute（即，对每个word 输出一个one hot vector （我的猜测））。如下图所示：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1gbldqsamj30q50kntkm.jpg"></p><p>经过parse scene graph得到的是object <strong>label</strong>、relationship <strong>label</strong>、attributes <strong>label</strong>的one hot vector（注意one hot 的长度是 5346+1308+3430 = 10102，即 将三种node合在了一起）。得到label之后，再经过word embedding层即可得到对应的word embedding: <strong>e<sub>o</sub> , e<sub>r</sub> , e<sub>a</sub></strong>。<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14w6j9aouj31670aqdiw.jpg" style="zoom:50%"></p><h2 id="2-Decoder"><a href="#2-Decoder" class="headerlink" title="2.  Decoder"></a><strong>2.</strong>  <strong>Decoder</strong></h2><p>由两层LSTM组成（该部分与论文[1][4] 的decoder部分是完全一致的，只是输入的encoder output不一样而已），下图给出了我自己画的decoder 结构[1]给出的结构[4]中给出的decoder结构</p><ul><li>在[4]中encoder output是GCN输出的object features（两个graph生成的encoder output分别送入decoder）。</li><li>但是在本文中encoder output是 <strong>u<sub>o</sub> , u<sub>r</sub> , u<sub>a</sub></strong> 组合成的d×M 矩阵，或者是 <strong>e<sub>o</sub> , e<sub>r</sub> , e<sub>a</sub></strong> 组合成的d×M 矩阵 。M = num_objects + num_relationships + num_attributes  注意：在sentence-encoder中的输出，这里的M是该句子的parse scene graph实际生成的object、relation、attribute的数量（有可能baseline 中不使用 sentence-encoder）；在image-encoder中的输出，这里也是实际的object detector、relationship classifier、attribute classifier 输出的实际数量总和</li></ul><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14xg3jwvdj30mo0h2wfi.jpg" style="zoom:60%"><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g19d1x7kiij30im0bvaat.jpg" style="zoom:60%"><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14v8kb8kmj30sr0gzwgz.jpg" style="zoom:60%">具体地，这里也给出本文的表达方式如下表:这里的10369是对MS COCO中的captions 进行预处理之后，得到的 len of vocabulary<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14wrwfzplj30ze0hcjwe.jpg" style="zoom:60%"><h1 id="在baseline-上加东西"><a href="#在baseline-上加东西" class="headerlink" title="在baseline 上加东西"></a>在baseline 上加东西</h1><ul><li>一般的encoder-decoder如下图中的top所示，本文提出加入MGCN for image和GCN for sentence，同时加入Dictionary（共享参数 betwen sentence and image）<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14yn2vbtej30k40jzac6.jpg" style="zoom:50%">由以上可知，构建了image scene graph 和 sentence scene graph，下面将本文的主要创新点，加入GCN和Dictionary  </li></ul><h2 id="sentence-graph-gt-GCN-更新embedding"><a href="#sentence-graph-gt-GCN-更新embedding" class="headerlink" title="sentence graph -&gt; GCN (更新embedding )"></a>sentence graph -&gt; GCN (更新embedding )</h2><p>经上面的分析构建的sentence scene graph 在得到了object、relation、attribute label 对应的word embedding之后，将通过GCN来更新embedding。</p><ul><li>表中的（7）（8）（9）可以认为是三个relationship、attribute、object graph。<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14y1f5p02j31370e1jw1.jpg" style="zoom:60%"></li><li>解释这里的<strong>g<sub>r</sub> g<sub>a</sub> g<sub>o</sub> g<sub>s</sub></strong><br>以<strong>g<sub>r</sub></strong> 为例：g<sub>r</sub> (D<sub>in</sub>，D<sub>out</sub>）。输入维度为什么是3000？因为e<sub>oi</sub> , e<sub>rij</sub> , e<sub>oj</sub>的维度分别均是1000，论文中省略了将其concatenate的操作的说明，但是实际上是进行了concatenate操作，使得维度变为3000，并作为g<sub>r</sub>的输入。输出是1000维度。</li><li><strong>因此这里图卷积层，可以认为是 该graph中所有的节点v<sub>i</sub>，经过concatenate之后，再经过一个全连接层。<br>g(.) = fc( concatenate(v1, v2, … , vn) )</strong><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14y594pa8j30og0ksdi2.jpg" style="zoom:50%">  </li></ul><h2 id="image-graph-gt-Multi-modal-GCN-更新embedding"><a href="#image-graph-gt-Multi-modal-GCN-更新embedding" class="headerlink" title="image graph -&gt; Multi-modal GCN (更新embedding )"></a>image graph -&gt; Multi-modal GCN (更新embedding )</h2><p>与sentence graph 对应的GCN类似，这里的 multimodal 也没什么意思，就是特征融合时（9）（10）（11），既使用了label 对应的word embedding，又使用了feature。<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14yf14p1mj30sp0k0ten.jpg">  </p><ul><li>解释这里的<strong>f<sub>r</sub> f<sub>a</sub> f<sub>o</sub> f<sub>s</sub></strong><br>与sentence scene graph 对应的g一致，首先对输入进行了concatenate操作<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14yhc6gpzj314l0gktax.jpg"></li></ul><h2 id="Dictionary"><a href="#Dictionary" class="headerlink" title="Dictionary"></a>Dictionary</h2><ul><li><strong>该部分的作用</strong>是一个memory network。首先Dictionary在<strong>S-&gt;G-&gt;D-&gt;S</strong>上预训练，之后才被用于<strong>I-&gt;G-&gt;D-&gt;S</strong>  。即是参数共享的。而在<strong>S-&gt;G-&gt;D-&gt;S</strong>中，输入的sentence是由human generated。因此Dictionary中就preserve human’s inductive bias。进而与<strong>I-&gt;G-&gt;D-&gt;S</strong>  共享，使得由image 生成的predict caption也含有 human’s inductive bias</li><li>由上文分析可知，经过GCN/MGCN更新的embedding的维度均是1000，则Dictionary的输入维度均是1000，D是可学习参数矩阵1000*10000，输入的x经过Dictionary得到 x^<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14yrlt89sj310c07e40b.jpg">  </li></ul><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>经过encoder 结合GCN/MGCN之后，输出发生了改变，这里再讲一下decoder的输入。</p><ul><li>在<strong>S-&gt;G-&gt;D-&gt;S</strong> 这个序列过程中由D-&gt;S 即输入decoder LSTM的过程，输入的是D的输出。</li><li>而在<strong>I-&gt;G-&gt;D-&gt;S</strong> 这个序列过程中，输入decoder LSTM的过程，输入的是D的输出v’和G的输出v^，即concate[v’, v^]<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1507wpmlij30zf0h8dku.jpg"></li></ul><h1 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h1><ul><li>首先使用交叉熵损失函数训练 <strong>S-&gt;G-&gt;S</strong>  20 epoch。注意D不参与训练</li><li>使用交叉熵损失函数训练 <strong>S-&gt;G-&gt;D-&gt;S</strong>  20 epoch。这里的D参与训练</li><li>使用交叉熵损失函数训练 <strong>I-&gt;G-&gt;D-&gt;S</strong>  20 epoch。这里的D使用在上一步骤中预训练的参数，并fine-tune</li><li>使用RL-based reward 训练 <strong>I-&gt;G-&gt;D-&gt;S</strong>  40 epoch。 D参与训练  </li></ul><h1 id="推理过程"><a href="#推理过程" class="headerlink" title="推理过程"></a>推理过程</h1><ul><li>文章中没有写，但是我认为是<strong>I-&gt;G-&gt;D-&gt;S</strong>  </li></ul><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><ul><li>由下表可知，实验结果并不突出，尤其是与GCN-LSTM[7]对比可知，GCN-LSTM的结构更加简单的情况下，两个模型的结果却相差不多。</li><li>可以看到表中有三个GCN-LSTM， 最上边那个是本文作者的复现，由于GCN-LSTM的作者batch_size 太大，本文作者觉得对比不公平，因此重新复现了代码（没公开代码）并减小了bs进行实验得到的结果。第二个GCN-LSTM（sem graph）是原作者论文中的实验数据。第三个GCN-LSTM是融合了semantic graph 和saptial graph。</li><li>对于SGAE的融合应该是输入decoder的不仅是<strong>I-&gt;G-&gt;D-&gt;S</strong> 中D的输出v’，而且也输入G的输出 v^，即concate[v’,  v^]<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14zjng66yj30oc0f0djs.jpg">   </li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] Bottom-up and top-down attention for image captioning and visual question answering<br>[2] Show, Attend and Tell: Neural Image Caption  Generation with Visual Attention<br>[3] Image Captioning with Object Detection and Localization<br>[4] Exploring Visual Relationship  for Image Captioning<br><strong>[5] Neural motifs: Scene graph parsing with global context</strong><br><strong>[6] Spice:  Semantic propositional image caption evaluation</strong><br><strong>[7] Generating Semantically Precise Scene Graphs from Textual Descriptions for Improved Image Retrieval</strong></p>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
            <tag> 图卷积网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Video as Space-Time Region Graphs</title>
      <link href="/2019/03/15/Video-as-Space-Time-Region-Graphs/"/>
      <url>/2019/03/15/Video-as-Space-Time-Region-Graphs/</url>
      
        <content type="html"><![CDATA[<p>本篇文章主要是讲图卷积网络应用在行为识别任务中。<br>使用的两个数据集是：charades和something-something数据集</p><ul><li>从数据集中video丰富性方面：其中charades数据集含有丰富的室内场景，video中的object较为复杂，也不居中。而，something-something 数据集中的video只含有1~2个object，且位于画面中的中心位置。</li><li>从数据集中video时长：charades的一整个video近30s长，但是annotation是对clips of video进行的标注，clips的分割也不具备规律性，几秒到几十秒不等。something-something数据集的video 时长为3s-6s。均为较短的视频。  </li></ul><p>由以上对数据集的分析，与作者的实验结果，结合，由于something-something的video时长短，objects of video 也较小，因此gcn+i3d 相比于对i3d的提升不大，而相反，charades数据集的提升较大。</p><ul><li>这里给出自己的看法：由于在charades上的提升较为明显，因此使用该网络应用到其他的网络中时，最好可以使用charades数据集进行pre-train，而不要使用something-something数据集。</li></ul><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>在训练阶段，首先对video以6 fps的帧率进行截取帧，输入网络时，每个video选取30帧，这样相当于video中的5s。即在训练阶段，每个5s长的clips作为一个sample，赋给它相对应的label，进行训练。</p><h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>在测试阶段，charades对每个video 提取10个clips ，对每个clips的输出结果，以最大池化的方式进行聚合，对于something-something数据集，每个video提取2个clips，其他同理。</p><h2 id="Construct-Graph"><a href="#Construct-Graph" class="headerlink" title="Construct Graph"></a>Construct Graph</h2><p>对于charades dataset，每帧中提取50个object，对于something-something dataset ，每帧中提取10个object。</p><ul><li>Similarity Graph<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g13qsziabzj30fk0ahjrw.jpg" style="zoom:65%">Similarity graph 含有可学习参数</li><li>Spatial Graph</li><li>无可学习参数</li><li>We denote the IoU between object i in frame t and object  j in frame t + 1 as σ<sub>ij</sub><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g13qv0z3bxj30fq0b30t3.jpg" style="zoom:65%"></li></ul>]]></content>
      
      
      <categories>
          
          <category> 行为识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 行为识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux-物理CPU和逻辑CPU</title>
      <link href="/2019/03/13/linux-%E7%89%A9%E7%90%86CPU%E5%92%8C%E9%80%BB%E8%BE%91CPU/"/>
      <url>/2019/03/13/linux-%E7%89%A9%E7%90%86CPU%E5%92%8C%E9%80%BB%E8%BE%91CPU/</url>
      
        <content type="html"><![CDATA[<p>通过cat /proc/cpuinfo 来查看CPU的信息</p><p><img src="https:////upload-images.jianshu.io/upload_images/5262207-4e29a8e7da45169c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/837/format/webp" alt="img"></p><p>cpu.png</p><p>physical id 表示物理CPU的编号<br> CPU cores 表示每个物理CPU上的内核数<br> core id 表示每个内核的编号<br> processor 表示每个逻辑CPU的编号</p><p>逻辑CPU的总数=物理CPU的数量 * 每个物理CPU上的核数 * 超线程数<br> 如果 逻辑CPU的总数=物理CPU的数量 * 每个物理CPU上的核数 则表示超线程没开，否则表示超线程以开</p><p>作者：君子亮剑</p><p>链接：<a href="https://www.jianshu.com/p/ff8e8be262ac" target="_blank" rel="noopener">https://www.jianshu.com/p/ff8e8be262ac</a></p><p>来源：简书</p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux-查看cpu状态</title>
      <link href="/2019/03/13/linux-%E6%9F%A5%E7%9C%8Bcpu%E7%8A%B6%E6%80%81/"/>
      <url>/2019/03/13/linux-%E6%9F%A5%E7%9C%8Bcpu%E7%8A%B6%E6%80%81/</url>
      
        <content type="html"><![CDATA[<ul><li>转载 “<a href="https://www.tianmaying.com/tutorial/cpu-top&quot;" target="_blank" rel="noopener">https://www.tianmaying.com/tutorial/cpu-top&quot;</a><br><code>top</code>命令是<code>Linux</code>下常用的性能分析工具，但本质上它提供了实时的对系统处理器的状态监视</li></ul><p>在命令行中输入<code>top</code>将输出一下信息：</p><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">top - 23:16:12 up  7:40, <span class="number"> 1 </span>user,  load average: 0.97, 0.98, 1.01</span><br><span class="line">Tasks:<span class="number"> 440 </span>total,  <span class="number"> 2 </span>running,<span class="number"> 438 </span>sleeping,  <span class="number"> 0 </span>stopped,  <span class="number"> 0 </span>zombie</span><br><span class="line">%Cpu(s):  1.3 us,  1.4 sy,  0.0 ni, 96.9 id,  0.0 wa,  0.0 hi,  0.4 si,  0.0 st</span><br><span class="line">KiB Mem : 13183891+total, 12378241+free, <span class="number"> 3884532 </span>used, <span class="number"> 4171956 </span>buff/cache</span><br><span class="line">KiB Swap:       <span class="number"> 0 </span>total,       <span class="number"> 0 </span>free,       <span class="number"> 0 </span>used. 12719112+avail Mem </span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                               </span><br><span class="line">11746 jenkins  <span class="number"> 20 </span> <span class="number"> 0 </span><span class="number"> 159972 </span> <span class="number"> 4760 </span> <span class="number"> 1600 </span>R  99.7  0.0 362:41.65 root/2                                                                                                                </span><br><span class="line">  <span class="number"> 42 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   4.3  0.0  14:46.50 rcu_sched                                                                                                             </span><br><span class="line">  <span class="number"> 68 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   2.6  0.0   0:55.10 rcuos/25                                                                                                              </span><br><span class="line">11414 jenkins  <span class="number"> 20 </span> <span class="number"> 0 </span>42.134g 1.652g <span class="number"> 24516 </span>S   0.7  1.3   2:37.54 java                                                                                                                  </span><br><span class="line">  <span class="number"> 49 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.3  0.0   0:32.35 rcuos/6                                                                                                               </span><br><span class="line"><span class="number"> 6818 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.3  0.0   0:34.33 kworker/0:1                                                                                                           </span><br><span class="line">14702 root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.3  0.0   0:00.26 kworker/2:0                                                                                                           </span><br><span class="line">15491 txq      <span class="number"> 20 </span> <span class="number"> 0 </span><span class="number"> 158044 </span> <span class="number"> 2616 </span> <span class="number"> 1552 </span>R   0.3  0.0   0:00.13 top                                                                                                                   </span><br><span class="line">   <span class="number"> 1 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span> <span class="number"> 45892 </span> <span class="number"> 8580 </span> <span class="number"> 3908 </span>S   0.0  0.0   0:13.06 systemd                                                                                                               </span><br><span class="line">   <span class="number"> 2 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.06 kthreadd</span><br></pre></td></tr></table></figure><p>前五行是当前整个系统资源的统计信息。</p><p>第一行是任务队列，包括当前时间，系统运行的总时间，系统用户登陆的数量，以及1分钟，5分钟，15分钟系统的负载情况。</p><p>第二行是<code>Tasks</code>信息，显示当前系统总共的进程数为440，运行状态的进程有两个，438个处于休眠状态，0个停止，0个僵尸进程。</p><p>第三行是<code>CPU</code>信息，很多人可能会忽略这些信息，我之前就是，所以详细说一下。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%<span class="selector-tag">Cpu</span>(<span class="selector-tag">s</span>):  1<span class="selector-class">.3</span> <span class="selector-tag">us</span>,  1<span class="selector-class">.4</span> <span class="selector-tag">sy</span>,  0<span class="selector-class">.0</span> <span class="selector-tag">ni</span>, 96<span class="selector-class">.9</span> <span class="selector-tag">id</span>,  0<span class="selector-class">.0</span> <span class="selector-tag">wa</span>,  0<span class="selector-class">.0</span> <span class="selector-tag">hi</span>,  0<span class="selector-class">.4</span> <span class="selector-tag">si</span>,  0<span class="selector-class">.0</span> <span class="selector-tag">st</span></span><br></pre></td></tr></table></figure><p><code>us</code> user CPU time —-用户空间占用<code>CPU</code>百分比</p><p><code>sy</code> system CPU time—-内核空间占用<code>CPU</code>百分比</p><p><code>ni</code> nice CPU time—-用户进程空间内改变过优先级的进程占用<code>CPU</code>百分比</p><p><code>id</code> idle—-空闲<code>CPU</code>百分比</p><p><code>wa</code> iowait—- 等待输入输出的<code>CPU</code>时间百分比</p><p><code>hi</code> hardware irq—-硬件中断</p><p><code>si</code> software irq—-软件中断</p><p><code>st</code> steal time—-实时</p><p>具体对应到第三行的详细信息，大家自己对照一下就行，或者在你命令行中试一下。</p><p>第四行<code>Memory</code>的状态信息，总共13183891+内存，空闲12378241+，使用3884532，缓存为4171956</p><p>第五行<code>Swap</code>交换分区信息，总共0，空闲0，使用0，缓存交换区总量12719112+</p><p>第六行是各个进程监视的项目列</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PID   <span class="built_in"> USER </span>     PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+   COMMAND</span><br><span class="line">PID` — 进程`id</span><br></pre></td></tr></table></figure><p><code>USER</code> — 进程所有者</p><p><code>PR</code> — 进程优先级</p><p><code>NI</code> — nice值。负值表示高优先级，正值表示低优先级</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">VIRT` — 进程使用的虚拟内存总量。`VIRT=SWAP+RES</span><br><span class="line">RES` — 进程使用的、未被换出的物理内存大小。`RES=CODE+DATA</span><br></pre></td></tr></table></figure><p><code>SHR</code> — 共享内存大小</p><p><code>S</code>— 进程状态。<code>D</code>=不可中断的睡眠状态 <code>R</code>=运行 <code>S</code>=睡眠 <code>T</code>=跟踪/停止 <code>Z</code>=僵尸进程</p><p><code>%CPU</code> — 上次更新到现在的<code>CPU</code>时间占用百分比</p><p><code>%MEM</code> — 进程使用的物理内存百分比</p><p><code>TIME+</code> — 进程使用的<code>CPU</code>时间总计</p><p><code>COMMAND</code> — 进程名称（命令名/命令行）</p><p>对应的每个进程的信息，大家可以自己看一下。</p><p>如果你在命令行下再输入<code>1</code>，输出如下：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">top - <span class="number">23</span>:<span class="number">16</span>:<span class="number">12</span> <span class="keyword">up</span>  <span class="number">7</span>:<span class="number">40</span>,  <span class="number">1</span> user,  load average: <span class="number">0.97</span>, <span class="number">0.98</span>, <span class="number">1.01</span></span><br><span class="line">Task<span class="variable">s:</span> <span class="number">440</span> total,   <span class="number">2</span> running, <span class="number">438</span> sleeping,   <span class="number">0</span> stopped,   <span class="number">0</span> zombie</span><br><span class="line">%Cpu0  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu1  :  <span class="number">0.0</span> us,  <span class="number">0.3</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni, <span class="number">99.7</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu2  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu3  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu4  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu5  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu6  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu7  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu8  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu9  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu10 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu11 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu12 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu13 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu14 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu15 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu16 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu17 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu18 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu19 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu20 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu21 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu22 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu23 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu24 : <span class="number">44.2</span> us, <span class="number">43.9</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,  <span class="number">0.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>, <span class="number">12.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu25 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu26 :  <span class="number">0.3</span> us,  <span class="number">0.3</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni, <span class="number">99.3</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu27 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu28 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu29 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu30 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu31 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">KiB Mem : <span class="number">13183891</span>+total, <span class="number">12377862</span>+free,  <span class="number">3887628</span> used,  <span class="number">4172660</span> buff/cache</span><br><span class="line">KiB Swap:        <span class="number">0</span> total,        <span class="number">0</span> free,        <span class="number">0</span> used. <span class="number">12718814</span>+avail Mem </span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                               </span><br><span class="line"><span class="number">11746</span> jenkins   <span class="number">20</span>   <span class="number">0</span>  <span class="number">159972</span>   <span class="number">4760</span>   <span class="number">1600</span> R <span class="number">100.0</span>  <span class="number">0.0</span> <span class="number">393</span>:<span class="number">16.94</span> root/<span class="number">2</span>                                                                                                                </span><br><span class="line">   <span class="number">42</span> root      <span class="number">20</span>   <span class="number">0</span>       <span class="number">0</span>      <span class="number">0</span>      <span class="number">0</span> S   <span class="number">2.7</span>  <span class="number">0.0</span>  <span class="number">15</span>:<span class="number">59.27</span> rcu_sched                                                                                                             </span><br><span class="line">   <span class="number">67</span> root      <span class="number">20</span>   <span class="number">0</span>       <span class="number">0</span>      <span class="number">0</span>      <span class="number">0</span> S   <span class="number">1.3</span>  <span class="number">0.0</span>   <span class="number">1</span>:<span class="number">03.60</span> rcuos/<span class="number">24</span></span><br></pre></td></tr></table></figure><p>输入<code>1</code>可以查看每个逻辑<code>CPU</code>的情况，如上总共有32个逻辑<code>CPU</code>；</p><p>其他命令：</p><p>输入<code>b</code>，显示高亮，<code>shift+&gt;</code>和<code>shift+&lt;</code>可以左右切换</p><p>输入<code>x</code>也是显示高亮，但没有<code>b</code>那么明显，同理<code>shift+&gt;</code>和<code>shift+&lt;</code>可以左右切换</p><p>直接输入<code>top -c</code>，会显示完整命令，输出如下：</p><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">top - 23:56:31 up  8:20, <span class="number"> 1 </span>user,  load average: 0.95, 0.97, 1.00</span><br><span class="line">Tasks:<span class="number"> 439 </span>total,  <span class="number"> 2 </span>running,<span class="number"> 437 </span>sleeping,  <span class="number"> 0 </span>stopped,  <span class="number"> 0 </span>zombie</span><br><span class="line">%Cpu(s):  1.4 us,  1.5 sy,  0.0 ni, 96.8 id,  0.0 wa,  0.0 hi,  0.4 si,  0.0 st</span><br><span class="line">KiB Mem : 13183891+total, 12377344+free, <span class="number"> 3892304 </span>used, <span class="number"> 4173168 </span>buff/cache</span><br><span class="line">KiB Swap:       <span class="number"> 0 </span>total,       <span class="number"> 0 </span>free,       <span class="number"> 0 </span>used. 12718340+avail Mem </span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                               </span><br><span class="line">11746 jenkins  <span class="number"> 20 </span> <span class="number"> 0 </span><span class="number"> 159972 </span> <span class="number"> 4760 </span> <span class="number"> 1600 </span>R 100.0  0.0 402:57.42 root/2                                                                                                                </span><br><span class="line">  <span class="number"> 42 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   3.3  0.0  16:21.24 [rcu_sched]                                                                                                           </span><br><span class="line">  <span class="number"> 57 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.7  0.0   1:03.01 [rcuos/14]                                                                                                            </span><br><span class="line">  <span class="number"> 63 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.3  0.0   0:29.22 [rcuos/20]                                                                                                            </span><br><span class="line"><span class="number"> 7933 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.3  0.0   0:05.50 [kworker/20:0]                                                                                                        </span><br><span class="line">11414 jenkins  <span class="number"> 20 </span> <span class="number"> 0 </span>42.134g 1.661g <span class="number"> 24516 </span>S   0.3  1.3   2:47.29 /etc/alternatives/java -Dcom.sun.akuma.Daemon=daemonized -Djava.awt.headless=true -DJENKINS_HOME=/var/lib/jenkins -j+ </span><br><span class="line">14702 root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.3  0.0   0:01.48 [kworker/2:0]                                                                                                         </span><br><span class="line">15098 root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.3  0.0   0:01.37 [kworker/6:2]                                                                                                         </span><br><span class="line">18465 txq      <span class="number"> 20 </span> <span class="number"> 0 </span><span class="number"> 158088 </span> <span class="number"> 2720 </span> <span class="number"> 1640 </span>R   0.3  0.0   0:00.18 top -c                                                                                                                </span><br><span class="line">   <span class="number"> 1 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span> <span class="number"> 45892 </span> <span class="number"> 8580 </span> <span class="number"> 3908 </span>S   0.0  0.0   0:13.78 /usr/lib/systemd/systemd --switched-root --system --deserialize<span class="number"> 21 </span>                                                   </span><br><span class="line">   <span class="number"> 2 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.06 [kthreadd]                                                                                                            </span><br><span class="line">   <span class="number"> 3 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.34 [ksoftirqd/0]                                                                                                         </span><br><span class="line">   <span class="number"> 5 </span>root      <span class="number"> 0 </span>-20      <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.00 [kworker/0:0H]                                                                                                        </span><br><span class="line">   <span class="number"> 8 </span>root      rt  <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.35 [migration/0]                                                                                                         </span><br><span class="line">   <span class="number"> 9 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.00 [rcu_bh]                                                                                                              </span><br><span class="line">  <span class="number"> 10 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.00 [rcuob/0]                                                                                                             </span><br><span class="line">  <span class="number"> 11 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.00 [rcuob/1]</span><br></pre></td></tr></table></figure><p>输入<code>q</code>是退出，还有其他命令参数，用到的时候再说，今天先统计这几个。</p><p>版权声明</p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> cpu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NumPy 副本和视图</title>
      <link href="/2019/03/13/NumPy-%E5%89%AF%E6%9C%AC%E5%92%8C%E8%A7%86%E5%9B%BE/"/>
      <url>/2019/03/13/NumPy-%E5%89%AF%E6%9C%AC%E5%92%8C%E8%A7%86%E5%9B%BE/</url>
      
        <content type="html"><![CDATA[<h1 id="NumPy-副本和视图"><a href="#NumPy-副本和视图" class="headerlink" title="NumPy 副本和视图"></a>NumPy 副本和视图</h1><p><strong>副本（赋值）</strong>是一个数据的完整的拷贝，如果我们对副本进行修改，它不会影响到原始数据，物理内存不在同一位置。</p><p><strong>视图（引用）</strong>是数据的一个别称或引用，通过该别称或引用亦便可访问、操作原有数据，但原有数据不会产生拷贝。如果我们对视图进行修改，它会影响到原始数据，物理内存在同一位置。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><strong>视图（引用）一般发生在：</strong></p><ul><li>1、numpy 的切片操作返回原数据的视图。</li><li>2、调用 ndarray 的 view() 函数产生一个视图。</li></ul><p><strong>副本一般发生在：</strong></p><ul><li>Python 序列的切片操作，调用deepCopy()函数。</li><li>调用 ndarray 的 copy() 函数产生一个副本。</li></ul><h2 id="yaya-举例："><a href="#yaya-举例：" class="headerlink" title="yaya 举例："></a>yaya 举例：</h2><p><strong>视图（引用）一般发生在：</strong></p><ul><li>1、numpy 的切片操作返回原数据的视图。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">6</span>)  <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b = a[:]   <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b[<span class="number">0</span>] = <span class="number">10</span>  <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line">print(a)   <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line"><span class="comment"># 修改b数据值，将修改原数据a的值</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">6</span>)  <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b = a[:]   <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b.shape = <span class="number">2</span>,<span class="number">3</span>  <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line">print(b)  </span><br><span class="line"><span class="comment"># [[0 1]</span></span><br><span class="line"><span class="comment">#  [2 3]</span></span><br><span class="line"><span class="comment">#  [4 5]]</span></span><br><span class="line">print(a)   <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line"><span class="comment"># 修改b数据形状，不会修改原数据a的形状</span></span><br></pre></td></tr></table></figure><ul><li>2、调用 ndarray 的 view() 函数产生一个视图。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">6</span>)  <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b = a.view()   <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b[<span class="number">0</span>] = <span class="number">10</span>  <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line">print(a)   <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line"><span class="comment"># 修改b数据值，将修改原数据a的值</span></span><br></pre></td></tr></table></figure></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">6</span>)  <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b = a[:]   <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b.shape = <span class="number">2</span>,<span class="number">3</span>  <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line">print(b)  </span><br><span class="line"><span class="comment"># [[0 1]</span></span><br><span class="line"><span class="comment">#  [2 3]</span></span><br><span class="line"><span class="comment">#  [4 5]]</span></span><br><span class="line">print(a)   <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line"><span class="comment"># 修改b数据形状，不会修改原数据a的形状</span></span><br></pre></td></tr></table></figure><p><strong>副本一般发生在：</strong></p><ul><li>Python 序列的切片操作，调用deepCopy()函数。</li><li>调用 ndarray 的 copy() 函数产生一个副本。</li></ul><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><h3 id="无复制-指向同一地址"><a href="#无复制-指向同一地址" class="headerlink" title="无复制 (指向同一地址)"></a>无复制 (指向同一地址)</h3><p>简单的赋值不会创建数组对象的副本。 相反，它使用原始数组的相同id()来访问它。 id()返回 Python 对象的通用标识符，类似于 C 中的指针。</p><p>此外，一个数组的任何变化都反映在另一个数组上。 例如，一个数组的形状改变也会改变另一个数组的形状。</p><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np    </span><br><span class="line">a = np.arange(<span class="number">6</span>)   </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'我们的数组是：'</span>) </span><br><span class="line"><span class="keyword">print</span> (a) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'调用 id() 函数：'</span>) </span><br><span class="line"><span class="keyword">print</span> (id(a)) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'a 赋值给 b：'</span>) </span><br><span class="line">b = a  <span class="keyword">print</span> (b) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'b 拥有相同 id()：'</span>) </span><br><span class="line"><span class="keyword">print</span> (id(b)) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'修改 b 的形状：'</span>) </span><br><span class="line">b.shape =  <span class="number">3</span>,<span class="number">2</span>   </span><br><span class="line"><span class="keyword">print</span> (b) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'a 的形状也修改了：'</span>) </span><br><span class="line"><span class="keyword">print</span> (a)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">我们的数组是：</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span>]</span><br><span class="line">调用 id() 函数：</span><br><span class="line"><span class="number">4349302224</span></span><br><span class="line">a 赋值给 b：</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span>]</span><br><span class="line">b 拥有相同 id()：</span><br><span class="line"><span class="number">4349302224</span></span><br><span class="line">修改 b 的形状：</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span>]]</span><br><span class="line">a 的形状也修改了：</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span>]]</span><br></pre></td></tr></table></figure><h3 id="视图或浅拷贝"><a href="#视图或浅拷贝" class="headerlink" title="视图或浅拷贝"></a>视图或浅拷贝</h3><p>ndarray.view() 方会创建一个新的数组对象，该方法创建的新数组的维数更改不会更改原始数据的维数。但是修改新数组的数值将会更改原始数据的数值。</p><h2 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  numpy  <span class="keyword">as</span>  np  </span><br><span class="line">a = np.arange(<span class="number">6</span>)  </span><br><span class="line"><span class="keyword">print</span>  (<span class="string">'我们的数组是：'</span>)  </span><br><span class="line"><span class="keyword">print</span>  (a)  </span><br><span class="line"><span class="keyword">print</span>  (<span class="string">'调用 id() 函数：'</span>)  </span><br><span class="line"><span class="keyword">print</span>  (id(a))  </span><br><span class="line"><span class="keyword">print</span>  (<span class="string">'a 赋值给 b：'</span>)  </span><br><span class="line">b = a  <span class="keyword">print</span>  (b)  </span><br><span class="line"><span class="keyword">print</span>  (<span class="string">'b 拥有相同 id()：'</span>)  </span><br><span class="line"><span class="keyword">print</span>  (id(b))  </span><br><span class="line"><span class="keyword">print</span>  (<span class="string">'修改 b 的形状：'</span>)  </span><br><span class="line">b.shape = <span class="number">3</span>,<span class="number">2</span>  </span><br><span class="line"><span class="keyword">print</span>  (b)  </span><br><span class="line"><span class="keyword">print</span>  (<span class="string">'a 的形状也修改了：'</span>)  </span><br><span class="line"><span class="keyword">print</span>  (a)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">数组 a：</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span>]]</span><br><span class="line">创建 a 的视图：</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span>]]</span><br><span class="line">两个数组的 id() 不同：</span><br><span class="line">a 的 id()：</span><br><span class="line"><span class="number">4314786992</span></span><br><span class="line">b 的 id()：</span><br><span class="line"><span class="number">4315171296</span></span><br><span class="line">b 的形状：</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span> <span class="number">5</span>]]</span><br><span class="line">a 的形状：</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span>]]</span><br></pre></td></tr></table></figure><p>使用切片创建视图修改数据会影响到原始数组：</p><h2 id="实例-2"><a href="#实例-2" class="headerlink" title="实例"></a>实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np   </span><br><span class="line">arr = np.arange(<span class="number">12</span>) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'我们的数组：'</span>) </span><br><span class="line"><span class="keyword">print</span> (arr) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'创建切片：'</span>) </span><br><span class="line">a=arr[<span class="number">3</span>:] </span><br><span class="line">b=arr[<span class="number">3</span>:] </span><br><span class="line">a[<span class="number">1</span>]=<span class="number">123</span> </span><br><span class="line">b[<span class="number">2</span>]=<span class="number">234</span> </span><br><span class="line">print(arr) </span><br><span class="line">print(id(a),id(b),id(arr[<span class="number">3</span>:]))</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我们的数组：</span><br><span class="line">[ <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span>]</span><br><span class="line">创建切片：</span><br><span class="line">[  <span class="number">0</span>   <span class="number">1</span>   <span class="number">2</span>   <span class="number">3</span> <span class="number">123</span> <span class="number">234</span>   <span class="number">6</span>   <span class="number">7</span>   <span class="number">8</span>   <span class="number">9</span>  <span class="number">10</span>  <span class="number">11</span>]</span><br><span class="line"><span class="number">4545878416</span> <span class="number">4545878496</span> <span class="number">4545878576</span></span><br></pre></td></tr></table></figure><p>变量 a,b 都是 arr 的一部分视图，对视图的修改会直接反映到原数据中。但是我们观察 a,b 的 id，他们是不同的，也就是说，视图虽然指向原数据，但是他们和赋值引用还是有区别的。</p><h3 id="副本或深拷贝"><a href="#副本或深拷贝" class="headerlink" title="副本或深拷贝"></a>副本或深拷贝</h3><p>ndarray.copy() 函数创建一个副本。 对副本数据进行修改，不会影响到原始数据，它们物理内存不在同一位置。</p><h2 id="实例-3"><a href="#实例-3" class="headerlink" title="实例"></a>实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np    </span><br><span class="line">a = np.array([[<span class="number">10</span>,<span class="number">10</span>],  [<span class="number">2</span>,<span class="number">3</span>],  [<span class="number">4</span>,<span class="number">5</span>]])   </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'数组 a：'</span>) </span><br><span class="line"><span class="keyword">print</span> (a) <span class="keyword">print</span> (<span class="string">'创建 a 的深层副本：'</span>) </span><br><span class="line">b = a.copy()   </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'数组 b：'</span>) </span><br><span class="line"><span class="keyword">print</span> (b) <span class="comment"># b 与 a 不共享任何内容   </span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">'我们能够写入 b 来写入 a 吗？'</span>) </span><br><span class="line"><span class="keyword">print</span> (b <span class="keyword">is</span> a) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'修改 b 的内容：'</span>) </span><br><span class="line">b[<span class="number">0</span>,<span class="number">0</span>]  =  <span class="number">100</span>   </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'修改后的数组 b：'</span>) </span><br><span class="line"><span class="keyword">print</span> (b) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'a 保持不变：'</span>) </span><br><span class="line"><span class="keyword">print</span> (a)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">数组 a：</span><br><span class="line">[[<span class="number">10</span> <span class="number">10</span>]</span><br><span class="line"> [ <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line"> [ <span class="number">4</span>  <span class="number">5</span>]]</span><br><span class="line">创建 a 的深层副本：</span><br><span class="line">数组 b：</span><br><span class="line">[[<span class="number">10</span> <span class="number">10</span>]</span><br><span class="line"> [ <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line"> [ <span class="number">4</span>  <span class="number">5</span>]]</span><br><span class="line">我们能够写入 b 来写入 a 吗？</span><br><span class="line"><span class="literal">False</span></span><br><span class="line">修改 b 的内容：</span><br><span class="line">修改后的数组 b：</span><br><span class="line">[[<span class="number">100</span>  <span class="number">10</span>]</span><br><span class="line"> [  <span class="number">2</span>   <span class="number">3</span>]</span><br><span class="line"> [  <span class="number">4</span>   <span class="number">5</span>]]</span><br><span class="line">a 保持不变：</span><br><span class="line">[[<span class="number">10</span> <span class="number">10</span>]</span><br><span class="line"> [ <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line"> [ <span class="number">4</span>  <span class="number">5</span>]]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
          <category> numpy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python  面试</title>
      <link href="/2019/03/12/python-%E9%9D%A2%E8%AF%95/"/>
      <url>/2019/03/12/python-%E9%9D%A2%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<p><a href="https://juejin.im/post/5b6bc1d16fb9a04f9c43edc3" target="_blank" rel="noopener">https://juejin.im/post/5b6bc1d16fb9a04f9c43edc3</a></p><p><a href="https://juejin.im/post/5b8505b6e51d4538884d22bf" target="_blank" rel="noopener">https://juejin.im/post/5b8505b6e51d4538884d22bf</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python基础： 深入理解 python 中的赋值、引用、拷贝、作用域</title>
      <link href="/2019/03/12/python%E5%9F%BA%E7%A1%80%EF%BC%9A-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-python-%E4%B8%AD%E7%9A%84%E8%B5%8B%E5%80%BC%E3%80%81%E5%BC%95%E7%94%A8%E3%80%81%E6%8B%B7%E8%B4%9D%E3%80%81%E4%BD%9C%E7%94%A8%E5%9F%9F/"/>
      <url>/2019/03/12/python%E5%9F%BA%E7%A1%80%EF%BC%9A-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-python-%E4%B8%AD%E7%9A%84%E8%B5%8B%E5%80%BC%E3%80%81%E5%BC%95%E7%94%A8%E3%80%81%E6%8B%B7%E8%B4%9D%E3%80%81%E4%BD%9C%E7%94%A8%E5%9F%9F/</url>
      
        <content type="html"><![CDATA[<h3 id="浅复制："><a href="#浅复制：" class="headerlink" title="浅复制："></a>浅复制：</h3><ul><li>仅复制对象的引用，而不开辟内存，即，改变复制后的对象时，其实是在改变原对象内存中的内容。</li><li>b = a[ : ]<h3 id="深复制"><a href="#深复制" class="headerlink" title="深复制"></a>深复制</h3>将开辟新的内存，把原对象内存中的内容复制到新的内存中来，如果改变复制后的对象，将改变原对象的内容。即，这两个对象在完成复制之后，已经是两个独立的对象了</li></ul><p><strong>- 转载： <a href="https://draapho.github.io/2016/11/21/1618-python-variable/" target="_blank" rel="noopener">https://draapho.github.io/2016/11/21/1618-python-variable/</a></strong></p><h3 id="可变对象：list-dict-set-（引用传递）"><a href="#可变对象：list-dict-set-（引用传递）" class="headerlink" title="可变对象：list dict set  （引用传递）"></a>可变对象：list dict set  （引用传递）</h3><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">   <span class="meta"># list</span></span><br><span class="line">a= [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">b = a</span><br><span class="line">b[<span class="number">0</span>] = <span class="number">9</span></span><br><span class="line"><span class="keyword">print</span>(b) <span class="meta"># [9, 2, 3]</span></span><br><span class="line"><span class="keyword">print</span>(a) <span class="meta"># [9, 2, 3]</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># dict</span></span><br><span class="line">a = &#123;&#125;</span><br><span class="line">a['key1'] = <span class="number">1</span></span><br><span class="line">b = a</span><br><span class="line">b['key1'] = <span class="number">9</span></span><br><span class="line"><span class="keyword">print</span>(b) <span class="meta"># &#123;'key1': 9&#125;</span></span><br><span class="line"><span class="keyword">print</span>(a) <span class="meta"># &#123;'key1': 9&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">values = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]  </span><br><span class="line">values[<span class="number">1</span>] = values  </span><br><span class="line">values  </span><br><span class="line">[<span class="number">0</span>, [...], <span class="number">2</span>] # 实际结果, 为何要赋值无限次?  </span><br><span class="line">[<span class="number">0</span>, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], <span class="number">2</span>] # 预想结果</span><br></pre></td></tr></table></figure><p>   Python 没有赋值，只有引用。你这样相当于创建了一个引用自身的结构，所以导致了无限循环</p><h3 id="不可变对象：tuple-string-int-float-bool-（值传递）"><a href="#不可变对象：tuple-string-int-float-bool-（值传递）" class="headerlink" title="不可变对象：tuple string int float bool （值传递）"></a>不可变对象：<strong><em>tuple</em></strong> string int float bool （值传递）</h3><p>对于可变对象，对象的操作不会重建对象，而对于不可变对象，每一次操作就重建新的对象。</p><pre><code>def func_int(a):    a += 4def func_list(a_list):    a_list[0] = 4t = 0func_int(t)print t# output: 0t_list = [1, 2, 3]func_list(t_list)print t_list# output: [4, 2, 3]</code></pre><h3 id="Dictionary-与-List-与-Tuple的区别"><a href="#Dictionary-与-List-与-Tuple的区别" class="headerlink" title="Dictionary 与 List 与 Tuple的区别"></a>Dictionary 与 List 与 Tuple的区别</h3><p>元组和列表在结构上没有什么区别，唯一的差异在于元组是只读的，不能修改。</p><p><strong>Dictionary</strong> </p><ol><li>Dictionary 是 Python 的内置数据类型之一, 它定义了键和值之间一对一的关系。</li><li>每一个元素都是一个 key-value 对, 整个元素集合用大括号括起来</li><li>您可以通过 key 来引用其值, 但是不能通过值获取 key</li><li>在一个 dictionary 中不能有重复的 key。给一个存在的 key 赋值会覆盖原有的值。 <a href="file:///home/echo/.chmsee/bookshelf/4791a2294b7b1d4cf68b0e1df606181d/native_data_types/index.html#odbchelper.dict.2.2" target="_blank" rel="noopener"><img src="file:///home/echo/.chmsee/bookshelf/4791a2294b7b1d4cf68b0e1df606181d/images/callouts/2.png" alt="2"></a> 在任何时候都可以加入新的 key-value 对。这种语法同修改存在的值是一样的。</li><li>当使用 dictionary 时, 您需要知道: dictionary 的 key 是大小写敏感的</li><li>Dictionary 不只是用于存储字符串。Dictionary 的值可以是任意数据类型, 包括字符串, 整数, 对象, 甚至其它的 dictionary。在单个 dictionary 里, dictionary 的值并不需要全都是同一数据类型, 可以根据需要混用和匹配。 Dictionary 的 key 要严格多了, 但是它们可以是字符串, 整数和几种其它的类型 (后面还会谈到这一点) 。也可以在一个 dictionary 中混用和配匹 key 的数据类型</li><li><code>del</code> 允许您使用 key 从一个 dictionary 中删除独立的元素。</li><li><code>clear</code> 从一个 dictionary 中清除所有元素。注意空的大括号集合表示一个没有元素的 dictionary。</li></ol><hr><p><strong>List</strong> </p><ol><li>list是一个使用方括号括起来的有序元素集合。</li><li>List 可以作为以 0 下标开始的数组。任何一个非空 list 的第一个元素总是 <code>li[0]</code></li><li><code>负数索引从 list 的尾部开始向前计数来存取元素。任何一个非空的 list 最后一个元素总是</code> li[-1]。 <a href="file:///home/echo/.chmsee/bookshelf/4791a2294b7b1d4cf68b0e1df606181d/native_data_types/lists.html#odbchelper.list.2.2" target="_blank" rel="noopener"><img src="file:///home/echo/.chmsee/bookshelf/4791a2294b7b1d4cf68b0e1df606181d/images/callouts/2.png" alt="2"></a>如果负数索引使您感到糊涂, 可以这样理解: <code>li[-n] == li[len(li) - n]</code>。 所以在这个 list 里, <code>li[-3] == li[5 - 3] == li[2]</code>。</li><li><code>您可以通过指定 2 个索引得到 list 的子集, 叫做一个 “slice” 。返回值是一个新的 list, 它包含了 list 中按顺序从第一个 slice 索引 (这里为</code> li[1]) 开始, 直到但是不包括第二个 slice 索引 (这里为<code>li[3]</code>) 的所有元素。</li><li><code>如果将两个分片索引全部省略, 这将包括 list 的所有元素。但是与原始的名为</code> li 的 list 不同, 它是一个新 list, 恰好拥有与 <code>li</code> 一样的全部元素。<code>li[:]</code> 是生成一个 list 完全拷贝的一个简写。</li><li><code>``append</code> 向 list 的末尾追加单个元素。</li><li><code>insert</code> 将单个元素插入到 list 中。数值参数是插入点的索引。请注意, list 中的元素不必唯一, 有有两个独立的元素具有 <code>&#39;new&#39;</code> 这个相同的值<code>。</code></li><li><code>extend</code> 用来连接 list。请注意不要使用多个参数来调用 <code>extend</code>, 要使用一个 list 参数进行调用。</li><li>Lists 的两个方法 <code>extend</code> 和 <code>append</code> 看起来类似, 但实际上完全不同。 <code>extend</code> 接受一个参数, 这个参数总是一个 list, 并且添加这个 list 中的每个元素到原 list 中</li><li>另一方面, <code>append</code> 接受一个参数, 这个参数可以是任何数据类型, 并且简单地追加到 list 的尾部。 在这里使用一个含有 3 个元素的 list 参数调用 <code>append</code> 方法。</li><li><code>index</code> 在 list 中查找一个值的首次出现并返回索引值。</li><li>要测试一个值是否在 list 内, 使用 <code>in</code>, 如果值存在, 它返回 <code>True</code>, 否则返为 <code>False</code> 。</li><li><code>remove</code> 从 list 中删除一个值的首次出现。</li><li><code>pop</code> 是一个有趣的东西。它会做两件事: 删除 list 的最后一个元素, 然后返回删除元素的值。请注意, 这与 <code>li[-1]</code> 不同, 后者返回一个值但不改变 list 本身。也不同于 <code>li.remove(*value*)</code>, 后者改变 list 但并不返回值。</li><li>Lists 也可以用 <code>+</code> 运算符连接起来。 <code>*list* = *list* + *otherlist*</code> 相当于 <code>*list*.extend(*otherlist*)</code>。 但 <code>+</code>运算符把一个新 (连接后) 的 list 作为值返回, 而 <code>extend</code> 只修改存在的 list。 也就是说, 对于大型 list 来说, <code>extend</code> 的执行速度要快一些。</li><li>Python 支持 <code>+=</code> 运算符。 <code>li += [&#39;two&#39;]</code> 等同于 <code>li.extend([&#39;two&#39;])</code>。 <code>+=</code> 运算符可用于 list, 字符串和整数, 并且它也可以被重载用于用户自定义的类中。</li><li><code>*</code> 运算符可以作为一个重复器作用于 list。 <code>li = [1, 2] * 3</code> 等同于 <code>li = [1, 2] + [1, 2] + [1, 2]</code>, 即将三个 list 连接成一个。</li></ol><hr><p><strong>Tuple</strong></p><ol><li>​    Tuple是不可变的list.一是创建了一个tuple就不能以任何方式改变它.</li><li>​    定义tuple与定义list的方式相同,除了整个元素集是用小括号包围的而不是方括号.</li><li>　 Tuple的元素与list一样按定义的次序进行排序.Tuples的索引与list一样从0开始,所以一个非空的tuple的第一个元素总是t[0].</li><li>​    负数索引与 list 一样从 tuple 的尾部开始计数。</li><li>​    与 list 一样分片 (slice) 也可以使用。注意当分割一个 list 时, 会得到一个新的 list ；当分割一个 tuple 时, 会得到一个新的 tuple。</li><li>​    Tuple 没有方法：没有 <code>append</code> 或 <code>extend</code> 方法、没有 <code>remove</code> 或 <code>pop</code> 方法、没有 <code>index</code> 方法、可以使用 <code>in</code> 来查看一个元素是否存在于 tuple 中。</li></ol><hr>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(LSTM_TSA)Video Captioning with Transferred Semantic Attributes</title>
      <link href="/2019/03/03/LSTM-TSA-Video-Captioning-with-Transferred-Semantic-Attributes/"/>
      <url>/2019/03/03/LSTM-TSA-Video-Captioning-with-Transferred-Semantic-Attributes/</url>
      
        <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>在这篇文章中，我们提出了一个 Long Short-Term Memory with Transferred Semantic Attributes （LSTM-TSA）model，这是一个新颖的结构，可以将从images 和 videos 中学习到的transferred semantic attributes  结合到 encoder - decoder 结构中去。</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch-Multi-Gpus</title>
      <link href="/2019/03/02/pytorch-Multi-Gpus/"/>
      <url>/2019/03/02/pytorch-Multi-Gpus/</url>
      
        <content type="html"><![CDATA[<ul><li>源为pytorch的官方文档</li><li><a href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html" target="_blank" rel="noopener">website</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters and DataLoaders</span></span><br><span class="line">input_size = <span class="number">5</span></span><br><span class="line">output_size = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">30</span></span><br><span class="line">data_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># Our model</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, output_size)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(input_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        output = self.fc(input)</span><br><span class="line">        print(<span class="string">"\tIn Model: input size"</span>, input.size(),</span><br><span class="line">              <span class="string">"output size"</span>, output.size())</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, length)</span>:</span></span><br><span class="line">        self.len = length</span><br><span class="line">        self.data = torch.randn(length, size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[index]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.len</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size), batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">model = Model(input_size, output_size)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">    print(<span class="string">"Let's use"</span>, torch.cuda.device_count(), <span class="string">"GPUs!"</span>)</span><br><span class="line">    <span class="comment"># dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs</span></span><br><span class="line">    model = nn.DataParallel(model)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> rand_loader:</span><br><span class="line">    input = data.to(device)</span><br><span class="line">    output = model(input)</span><br><span class="line">    print(<span class="string">"Outside: input size"</span>, input.size(),</span><br><span class="line">          <span class="string">"output_size"</span>, output.size())</span><br></pre></td></tr></table></figure><h2 id="转载：-PyTorch-论文pytorch复现中遇到的BUG-https-www-cnblogs-com-kk17-p-10139884-html"><a href="#转载：-PyTorch-论文pytorch复现中遇到的BUG-https-www-cnblogs-com-kk17-p-10139884-html" class="headerlink" title="转载：[[PyTorch]论文pytorch复现中遇到的BUG]](https://www.cnblogs.com/kk17/p/10139884.html)"></a>转载：[[PyTorch]论文pytorch复现中遇到的BUG]](<a href="https://www.cnblogs.com/kk17/p/10139884.html" target="_blank" rel="noopener">https://www.cnblogs.com/kk17/p/10139884.html</a>)</h2><ul><li>我在Multi-GPUs时，也遇到了第一个问题</li></ul><p>目录</p><ul><li><a href="https://www.cnblogs.com/kk17/p/10139884.html#zip-argument-1-must-support-iteration" target="_blank" rel="noopener">1. zip argument #1 must support iteration</a></li><li><a href="https://www.cnblogs.com/kk17/p/10139884.html#torch.nn.dataparallel" target="_blank" rel="noopener">2. torch.nn.DataParallel</a></li><li><a href="https://www.cnblogs.com/kk17/p/10139884.html#model.state_dict" target="_blank" rel="noopener">3. model.state_dict()</a></li></ul><h1 id="1-zip-argument-1-must-support-iteration"><a href="#1-zip-argument-1-must-support-iteration" class="headerlink" title="1. zip argument #1 must support iteration"></a>1. zip argument #1 must support iteration</h1><p>在多gpu训练的时候，自动把你的batch_size分成n_gpu份，每个gpu跑一些数据， 最后再合起来。我之所以出现这个bug是因为返回的时候 返回了一个常量。。</p><h1 id="2-torch-nn-DataParallel"><a href="#2-torch-nn-DataParallel" class="headerlink" title="2. torch.nn.DataParallel"></a>2. torch.nn.DataParallel</h1><p>在使用torch.nn.DataParallel时候，要先把模型放在gpu上，再进行parallel。</p><h1 id="3-model-state-dict"><a href="#3-model-state-dict" class="headerlink" title="3. model.state_dict()"></a>3. model.state_dict()</h1><p>一般在现有的网络加载预训练模型通常是找到预训练模型在现有的model里面的参数，然后model进行更新，遇到一个bug， 发现加载预训练模型的时候， 效果很差，跟参数没有更新一样，找了一大顿bug，最后才发现，之前是单gpu进行的预训练，现在的模型使用的是多gpu， 打印现在模型的参数你会发现他所有的参数前面都加了一个module. 所以向以前一样更新，没有一个参数会被更新，因此写了一个万能模型参数加载函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">pretrained_dict = checkpoint[<span class="string">'state_dict'</span>]</span><br><span class="line">model_dict = self.model.state_dict()</span><br><span class="line"><span class="keyword">if</span> checkpoint[<span class="string">'config'</span>][<span class="string">'n_gpu'</span>] &gt; <span class="number">1</span> <span class="keyword">and</span> self.config[<span class="string">'n_gpu'</span>] == <span class="number">1</span>:</span><br><span class="line">    new_dict = OrderedDict()</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> pretrained_dict.items():</span><br><span class="line">        name = k[<span class="number">7</span>:]</span><br><span class="line">        new_dict[name] = v</span><br><span class="line">    pretrained_dict = new_dict</span><br><span class="line"><span class="keyword">elif</span> checkpoint[<span class="string">'config'</span>][<span class="string">'n_gpu'</span>] == <span class="number">1</span> <span class="keyword">and</span> self.config[<span class="string">'n_gpu'</span>] &gt; <span class="number">1</span>:</span><br><span class="line">    new_dict = OrderedDict()</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> pretrained_dict.items():</span><br><span class="line">        name = <span class="string">"module."</span>+k</span><br><span class="line">        new_dict[name] = v</span><br><span class="line">    pretrained_dict = new_dict</span><br><span class="line">print(<span class="string">"The pretrained model's para is following"</span>)</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> pretrained_dict.items():</span><br><span class="line">    print(k)</span><br><span class="line">pretrained_dict = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> pretrained_dict.items() <span class="keyword">if</span> k <span class="keyword">in</span> model_dict&#125;</span><br><span class="line">model_dict.update(pretrained_dict)</span><br><span class="line">self.model.load_state_dict(model_dict)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>coco-detection</title>
      <link href="/2019/03/02/coco-detection/"/>
      <url>/2019/03/02/coco-detection/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning</title>
      <link href="/2019/03/01/Spatio-Temporal-Dynamics-and-Semantic-Attribute-Enriched-Visual-Encoding-for-Video-Captioning/"/>
      <url>/2019/03/01/Spatio-Temporal-Dynamics-and-Semantic-Attribute-Enriched-Visual-Encoding-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>nltk-tokenize</title>
      <link href="/2019/02/28/nltk-tokenize/"/>
      <url>/2019/02/28/nltk-tokenize/</url>
      
        <content type="html"><![CDATA[<p>转载：<a href="https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/" target="_blank" rel="noopener">https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/</a></p><p>Next, you’re going to need NLTK 3. The easiest method to installing the NLTK module is going to be with pip.</p><p>For all users, that is done by opening up cmd.exe, bash, or whatever shell you use and typing:<br><code>pip install nltk</code></p><p>These are the words you will most commonly hear upon entering the Natural Language Processing (NLP) space, but there are many more that we will be covering in time. With that, let’s show an example of how one might actually tokenize something into tokens with the NLTK module.</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from nltk<span class="selector-class">.tokenize</span> import sent_tokenize, word_tokenize</span><br><span class="line"></span><br><span class="line">EXAMPLE_TEXT = <span class="string">"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard."</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(sent_tokenize(EXAMPLE_TEXT)</span></span>)</span><br></pre></td></tr></table></figure><p>At first, you may think tokenizing by things like words or sentences is a rather trivial enterprise. For many sentences it can be. The first step would be likely doing a simple .split(‘. ‘), or splitting by period followed by a space. Then maybe you would bring in some <a href="https://pythonprogramming.net/regular-expressions-regex-tutorial-python-3/" target="_blank" rel="noopener"><strong>regular expressions</strong></a> to split by period, space, and then a capital letter. The problem is that things like Mr. Smith would cause you trouble, and many other things. Splitting by word is also a challenge, especially when considering things like concatenations like we and are to we’re. NLTK is going to go ahead and just save you a ton of time with this seemingly simple, yet very complex, operation.</p><p>The above code will output the sentences, split up into a list of sentences, which you can do things like iterate through with a <a href="https://pythonprogramming.net/loop-python-3-basics-tutorial/" target="_blank" rel="noopener"><strong>for loop</strong></a>.<br><code>[&#39;Hello Mr. Smith, how are you doing today?&#39;, &#39;The weather is great, and Python is awesome.&#39;, &#39;The sky is pinkish-blue.&#39;, &quot;You shouldn&#39;t eat cardboard.&quot;]</code></p><p>So there, we have created tokens, which are sentences. Let’s tokenize by word instead this time:</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="name">word_tokenize</span>(<span class="name">EXAMPLE_TEXT</span>))</span><br></pre></td></tr></table></figure><p>Now our output is: <code>[&#39;Hello&#39;, &#39;Mr.&#39;, &#39;Smith&#39;, &#39;,&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;, &#39;doing&#39;, &#39;today&#39;, &#39;?&#39;, &#39;The&#39;, &#39;weather&#39;, &#39;is&#39;, &#39;great&#39;, &#39;,&#39;, &#39;and&#39;, &#39;Python&#39;, &#39;is&#39;, &#39;awesome&#39;, &#39;.&#39;, &#39;The&#39;, &#39;sky&#39;, &#39;is&#39;, &#39;pinkish-blue&#39;, &#39;.&#39;, &#39;You&#39;, &#39;should&#39;, &quot;n&#39;t&quot;, &#39;eat&#39;, &#39;cardboard&#39;, &#39;.&#39;]</code></p><p>There are a few things to note here. First, notice that punctuation is treated as a separate token. Also, notice the separation of the word “shouldn’t” into “should” and “n’t.” Finally, notice that “pinkish-blue” is indeed treated like the “one word” it was meant to be turned into. Pretty cool!</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Maxout Networks</title>
      <link href="/2019/02/27/Maxout-Networks/"/>
      <url>/2019/02/27/Maxout-Networks/</url>
      
        <content type="html"><![CDATA[<p>本文是蒙特利尔大学信息与信息技术学院的几位大牛2013年在ICML上发表的一篇论文，提出了一种叫maxout的新模型，到目前用的比较少，可能这个算法参数个数会成k倍增加(k是maxout的一个参数)。文中提到这样命名的原因：（1）它的输出是一组输入的最大值（2）它与dropout是天然的组合。</p><p>文章首先肯定了dropout的意义，从音频分类到超大规模物体识别都有很好的改进，同时提出不应该把dropout作为一个适用于任意模型的轻微性能增强，而是通过直接设计一个模型来提高dropout能力，作为模型平均技术，可以获得最好的性能。</p><p>Dropout：</p><ul><li>dropout可以训练集成模型，它们共享参数并近似的对这些模型的预测进行了平均。它可以被当作一种通用的方法用在任何一种MLP和CNN模型中，但是在论文中，由于dropout的模型平均过程没有被证明，因而一个模型最好的性能的获得，应该通过直接设计这个模型使之可以增强dropout的模型平均的能力。使用了dropout的训练过程和一般的SGD方法完全不同。dropout在更新时使用更大的步长最有效，因为这样可以在不同的训练子集上对不同的模型有明显的影响来使得目标函数有持续的波动性，理想情况下整个训练过程就类似于使用bagging来训练集成的模型（带有参数共享的约束）。而一般的SGD更新时会使用更小的步长，来使得目标函数平滑的下降。对于深度网络模型，dropout只能作为模型平均的一种近似，显式的设计模型来最小化这种近似误差也可以提高dropout的性能。</li><li>dropout训练的集成模型中，所有模型都只包括部分输入和部分隐层参数。对每一个训练样本，我们都会训练一个包括不同隐层参数的子模型。dropout与bagging的相同点是不同的模型使用不同数据子集，不同点是dropout的每个模型都只训练一次且所有模型共享参数。</li><li>对于预测时如何平均所有子模型的问题，bagging一般使用的是算数平均，而对dropout产生的指数多个子模型则并非显而易见。但是如果模型只有一层 <img src="https://www.zhihu.com/equation?tex=p%28y+%7C+v%3B%CE%B8%29%3Dsoftmax%28v%5E%7BT%7D%2Bb%29" alt="p(y | v;θ)=softmax(v^{T}+b)">作为输出（p(y | v;θ)的几何平均），则最终的预测分布就是简单的 <img src="https://www.zhihu.com/equation?tex=softmax%28v%5E%7BT%7DW%2F2%2Bb%29" alt="softmax(v^{T}W/2+b)">，即指数多个子模型的平均预测就是完整模型的预测仅仅将权重减半而已。这个结果只能用在单softmax层的模型中，如果是深层模型如MLP，那么权重减半的方法只是几何平均的一种近似。</li></ul><p>Maxout是深度学习网络中的一层网络，就像池化层、卷积层一样等，我们可以把maxout 看成是网络的激活函数层。我们假设网络某一层的输入特征向量为：X=（x1,x2,……xd），也就是我们输入是d个神经元。Maxout隐藏层每个神经元的计算公式如下：</p><p><img src="https://www.zhihu.com/equation?tex=h_%7Bi%7D%3D%5Cmax_%7Bj+%5Cin+%5B1%2Ck%5D%7D%7Bz_%7Bij%7D%7D" alt="h_{i}=\max_{j \in [1,k]}{z_{ij}}"></p><p>上面的公式就是maxout隐藏层神经元i的计算公式。其中，k就是maxout层所需要的参数了，由我们人为设定大小。就像dropout一样，也有自己的参数p(每个神经元dropout概率)，maxout的参数是k。公式中Z的计算公式为： <img src="https://www.zhihu.com/equation?tex=z_%7Bij%7D%3Dx%5E%7BT%7DW_%7B..ij%7D%2Bb_%7Bij%7D" alt="z_{ij}=x^{T}W_{..ij}+b_{ij}"> ，权重w是一个大小为(d,m,k)三维矩阵，b是一个大小为(m,k)的二维矩阵，这两个就是我们需要学习的参数。如果我们设定参数k=1，那么这个时候，网络就类似于以前我们所学普通的MLP网络。</p><p>我们可以这么理解，本来传统的MLP算法在第i层到第i+1层，参数只有一组，然而现在我们不怎么干了，我们在这一层同时训练n组参数，然后选择激活值最大的作为下一层神经元的激活值。下面还是用一个例子进行讲解，比较容易搞懂。</p><p>（1）以前MLP的方法。<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0ldm6b5ujj30ix08mdg0.jpg"></p><p>其中 f 就是我们所谓的激活函数，比如Sigmod、Relu、Tanh等。</p><p>(2)Maxout 的方法。如果我们设置maxout的参数k=5，maxout层就如下所示：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0ldmxgg2zj30n60e33zh.jpg"></p><p>相当于在每个输出神经元前面又多了一层。这一层有5个神经元。<br>所以这就是为什么采用maxout的时候，参数个数成k倍增加的原因。本来我们只需要一组参数就够了，采用maxout后，就需要有k组参数。</p><ul><li>对MLP而言，2个输入节点先构成5个“隐隐层”节点，然后在5个“隐隐层”节点中使用最大的构成了本层的一个节点，本层其他节点类似。<strong>实现技巧：</strong><code>maxout</code>和<code>relu</code>唯一的区别是，<code>relu</code>使用的max(x,0)是对隐层每一个单元执行的与0比较最大化操作，而<code>maxout</code>是对5个“隐隐层”单元的值执行最大化操作。如果将“隐隐层”单元在隐层展开，那么隐层就有20个“隐隐层”单元，<code>maxout</code>做的就是在这20个中每5个取一个最大值作为最后的隐层单元，最后的隐层单元仍然为4个。这里每5个取一个最大值也称为最大池化步长（max pooling stride）为5，最大池化步长默认和“隐隐层”个数相等，如果步长更小，则可以实现重叠最大池化。实现的时候，可以将隐层单元数设置为20个，权重维度（2，20）偏置维度（1，20），然后在20个中每5个取一个最大值得到4个隐层单元。</li><li>对于CNN而言，假设上一层有2个特征图，本层有4个特征图，那么就是将输入的2个特征图用5个滤波器卷积得到5张仿射特征图（affine feature maps），然后从这5张仿射特征图每个位置上选择最大值（跨通道池化，pool across channels）构成一张本层的特征图，本层其他特征图类似。<strong>实现技巧：</strong><code>relu</code>使用的max(x,0)是对每个通道的特征图的每一个单元执行的与0比较最大化操作，而<code>maxout</code>是对5个通道的特征图在通道的维度上执行最大化操作。而如果把5个特征图在本层展开，那么本层就有20个特征图，<code>maxout</code>做的就是在这20个中每5个取在通道维度上的最大值作为最后的特征图，最后本层特征图仍然为4个。同样最大池化步长默认为5。实现的时候，可以将本层特征图数设置为20个，权重维度（20，2，3，3）偏置维度（1，20，1，1），然后在20个中每5个取一个最大特征图得到4个特征图。<strong>注意：</strong> 对于CNN而言，在maxout输出后如果连接一个一般的降采样最大池化层，则可以将这个降采样最大池化融合进跨通道池化中，即在仿射特征图的每个池化窗口中选择最大值（相当于同时在通道间和空间取最大值）。这样就可以在maxout网络中省略显式的降采样最大池化层。</li></ul><h2 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h2><ol><li>MNIST</li></ol><p>排列不变限制的MNIST（MNIST with permutation invariant，即像素排列顺序可以改变，输入的数据是2维的），使用两个全连接maxout层再接上一个softmax层，结合dropout和权重衰减。验证集选取训练集中后10000个样本。在得到最小的验证集误差时记录下前50000个样本的训练集对数似然L，接着在整个60000样本的训练集上<strong>继续训练</strong>直到验证集的对数似然达到L。<em>0.94%</em></p><p>无排列不变限制的MNIST（MNIST without permutation invariant，即像素排列顺序不变，输入的数据是3维的），使用三个卷积maxout层，之后接上空间最大池化层，最后接上一个softmax层。还可以使用扩充数据集的方法进一步提高。<em>0.45%</em></p><p>\2. CIFAR-10</p><p>预处理：全局像素归一化和ZCA白化</p><p>过程与MNIST类似，只是将继续训练改为了<strong>重新训练</strong>，因为继续训练的学习率很低训练太久。</p><p>使用三个卷积maxout层，之后接上全连接maxout层，最后接上一个softmax层。<em>13.2%</em>（不使用验证集数据）<em>11.68%</em>（使用验证集数据）<em>9.35%</em>（使用平移、水平翻转的扩充数据集）</p><p>\3. CIFAR-100</p><p>超参数使用和CIFAR-10一样</p><p><em>41.48%</em>（不使用验证集数据）<em>38.57%</em>（使用验证集数据）</p><p>\4. SVHN</p><p>验证集为训练集每类选取400个样本和额外集每类选取200个样本，其他的为训练集。</p><p>预处理：局部像素归一化</p><p>使用三个卷积maxout层，之后接上全连接maxout层，最后接上一个softmax层（同CIFAR-10）。<em>2.47%</em></p><h2 id="maxout对比relu"><a href="#maxout对比relu" class="headerlink" title="maxout对比relu"></a>maxout对比relu</h2><ul><li>跨通道池化可以减少网络状态并减少模型所需要的参数。</li><li>对于maxout，性能与跨通道池化时滤波器数量有很大关系，但对relu，性能与输出单元的数量没有关系，也就是relu并不从跨通道池化中受益。</li><li>要让relu达到maxout的表现，需要使之具有和maxout相同数量的滤波器（即使用比原来k倍的滤波器，同样也要k倍的relu单元），但网络状态和所需要的参数也是原来的k倍，也是对应maxout的k倍。</li></ul><h2 id="模型平均"><a href="#模型平均" class="headerlink" title="模型平均"></a>模型平均</h2><ul><li>单层softmax有对模型进行平均的能力，但是通过观察，多层模型中使用dropout也存在这样的模型平均，只是有拟合精度的问题。</li><li>训练中使用dropout使得maxout单元有了更大的输入附近的线性区域，因为每个子模型都要预测输出，每个maxout单元就要学习输出相同的预测而不管哪些输入被丢弃。改变dropout mask将经常明显移动有效输入，从而决定了输入被映射到分段线性函数的哪一段。使用dropout训练的maxout具有一种特性，即当dropout mask改变时每个maxout单元的最大化滤波器相对很少变化。</li><li>maxout网络中的线性和最大化操作可以让dropout的拟合模型平均的精度很高。而一般的激活函数几乎处处都是弯曲的，因而dropout的拟合模型平均的精度不高。</li></ul><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><ul><li>训练中使用dropout时，maxout的优化性能比relu+max pooling好</li><li>dropout使用更大的步长最有效，使得目标函数有持续的波动性。而一般的SGD会使用更小的步长，来使得目标函数平滑的下降。dropout快速的探索着许多不同的方向然后拒绝那些损害性能的方向，而SGD缓慢而平稳的朝向最可能的方向移动。</li><li>实验中SGD使得relu饱和在0值的时间少于5%，而dropout则超过60%。由于relu激活函数中的0值是一个常数，这就会阻止梯度在这些单元上传播（无论正向还是反向），这也就使得这些单元很难再次激活，这会导致很多单元由激活转变为非激活。而maxout就不会存在这样的问题，梯度在maxout单元上总是能够传播，即使maxout出现了0值，但是这些0值是参数的函数可以被改变，从而maxout单元总是激活的。单元中较高比例的且不易改变的0值会损害优化性能。</li><li>dropout要求梯度随着dropout mask的改变而明显改变，而一旦梯度几乎不随着dropout mask的改变而改变时，dropout就简化成为了SGD。relu网络的低层部分会有梯度衰减的问题（梯度的方差在高层较大而反向传播到低层后较小）。maxout更好的将变化的信息反向传播到低层并帮助dropout以类似bagging的方式训练低层参数。relu则由于饱和使得梯度损失，导致dropout在低层的训练类似于一般的SGD。</li></ul><h2 id="总结文中的点"><a href="#总结文中的点" class="headerlink" title="总结文中的点"></a>总结文中的点</h2><ul><li>单个<code>maxout</code>激活函数可以理解成一种分段线性函数来近似任意凸函数（任意的凸函数都可由分段线性函数来拟合）。它在每处都是局部线性的（k个“隐隐层”节点都是线性的，取其最大值则为局部线性，分段的个数与k值有关），而一般的激活函数都有明显的曲率。</li><li>如同MLP一样，maxout网络也可以拟合任意连续函数。只要<code>maxout</code>单元含有任意多个“隐隐层”节点，那么只要两个隐层的maxout网络就可以实现任意连续函数的近似。</li><li>maxout网络不仅可以学习到隐层之间的关系，还可以学习到每个隐层单元的激活函数。</li><li>maxout放弃了传统激活函数的设计，它产生的表示不再是稀疏的，但是它的梯度是稀疏的，且dropout可以将它稀疏化。</li><li>maxout没有上下界，所以让它在某一端饱和是零概率事件。</li><li>如果训练时使用dropout，则dropout操作在矩阵相乘之前，而并不对<code>max</code>操作的输入执行dropout。</li><li>使用maxout会默认一个先验：样本集是凸集可分的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python:理解 yield 关键字</title>
      <link href="/2019/02/25/Python-%E7%90%86%E8%A7%A3-yield-%E5%85%B3%E9%94%AE%E5%AD%97/"/>
      <url>/2019/02/25/Python-%E7%90%86%E8%A7%A3-yield-%E5%85%B3%E9%94%AE%E5%AD%97/</url>
      
        <content type="html"><![CDATA[<pre><code>转载：https://liam.page/2017/06/30/understanding-yield-in-python/</code></pre><h1 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h1><p>为了理解<a href="https://github.com/sususushi/reconstruction-network-for-video-captioning" target="_blank" rel="noopener">reconstruction-network</a> 代码中，如下代码是如何实现的，查看了此篇博客，并转载。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cycle</span><span class="params">(iterable)</span>:</span>  </span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:  </span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> iterable:  </span><br><span class="line">            <span class="keyword">yield</span> x</span><br><span class="line"></span><br><span class="line">train_data_loader = iter(cycle(MSVD.train_data_loader))</span><br><span class="line"><span class="keyword">for</span> iteration, batch <span class="keyword">in</span> enumerate(train_data_loader, <span class="number">1</span>):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> iteration == C.train_n_iteration:  </span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>解释：<strong>iter()</strong> 是python的一个函数，用来生成迭代器。而cycle中一整个for循环是对整个数据集调用一遍，而外部又有一个while true，该判断是一直成立的，则，会一直调用数据。因此此处不适用n_epoch 来停止加载数据，而是使用train_n_iteration。</p><hr><p>Python 是非常灵活的语言，其中 <code>yield</code> 关键字是普遍容易困惑的概念。</p><p>此篇将介绍 <code>yield</code> 关键字，及其相关的概念。</p><h2 id="迭代、可迭代、迭代器"><a href="#迭代、可迭代、迭代器" class="headerlink" title="迭代、可迭代、迭代器"></a>迭代、可迭代、迭代器</h2><h3 id="迭代（iteration）与可迭代（iterable）"><a href="#迭代（iteration）与可迭代（iterable）" class="headerlink" title="迭代（iteration）与可迭代（iterable）"></a>迭代（iteration）与可迭代（iterable）</h3><blockquote><p>迭代是一种操作；可迭代是对象的一种特性。</p></blockquote><p>很多数据都是「容器」；它们包含了很多其他类型的元素。实际使用容器时，我们常常需要逐个获取其中的元素。<strong>逐个获取元素的过程，就是「迭代」</strong>。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iteration</span></span><br><span class="line">a_list = [1, 2, 3]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a_list:</span><br><span class="line">    <span class="builtin-name">print</span>(i)</span><br></pre></td></tr></table></figure><p>如果我们可以从一个对象中，逐个地获取元素，那么我们就说这个对象是「可迭代的」。</p><p>Python 中的顺序类型，都是可迭代的（<code>list</code>, <code>tuple</code>, <code>string</code>）。其余包括 <code>dict</code>, <code>set</code>, <code>file</code> 也是可迭代的。对于用户自己实现的类型，如果提供了 <code>__iter__()</code> 或者 <code>__getitem__()</code> 方法，那么该类的对象也是可迭代的。</p><h3 id="迭代器（iterator）"><a href="#迭代器（iterator）" class="headerlink" title="迭代器（iterator）"></a>迭代器（iterator）</h3><blockquote><p>迭代器是一种对象。</p></blockquote><p>迭代器抽象的是一个「数据流」，是只允许迭代一次的对象。对迭代器不断调用 <code>next()</code> 方法，则可以依次获取下一个元素；当迭代器中没有元素时，调用 <code>next()</code> 方法会抛出 <code>StopIteration</code> 异常。迭代器的 <code>__iter__()</code> 方法返回迭代器自身；因此迭代器也是可迭代的。</p><h3 id="迭代器协议（iterator-protocol）"><a href="#迭代器协议（iterator-protocol）" class="headerlink" title="迭代器协议（iterator protocol）"></a>迭代器协议（iterator protocol）</h3><blockquote><p>迭代器协议指的是容器类需要包含一个特殊方法。</p></blockquote><p>如果一个容器类提供了 <code>__iter__()</code> 方法，并且该方法能返回一个能够逐个访问容器内所有元素的迭代器，则我们说该容器类实现了迭代器协议。</p><p>Python 中的迭代器协议和 Python 中的 <code>for</code> 循环是紧密相连的。</p><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">iterator</span> protocol <span class="keyword">and</span> <span class="keyword">for</span> <span class="keyword">loop</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> something:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><p>Python 处理 <code>for</code> 循环时，首先会调用内建函数 <code>iter(something)</code>，它实际上会调用 <code>something.__iter__()</code>，返回 <code>something</code> 对应的迭代器。而后，<code>for</code> 循环会调用内建函数 <code>next()</code>，作用在迭代器上，获取迭代器的下一个元素，并赋值给 <code>x</code>。此后，Python 才开始执行循环体。</p><h2 id="生成器、yield-表达式"><a href="#生成器、yield-表达式" class="headerlink" title="生成器、yield 表达式"></a>生成器、<code>yield</code> 表达式</h2><h3 id="生成器函数（generator-function）和生成器（generator）"><a href="#生成器函数（generator-function）和生成器（generator）" class="headerlink" title="生成器函数（generator function）和生成器（generator）"></a>生成器函数（generator function）和生成器（generator）</h3><blockquote><p>生成器函数是一种特殊的函数；生成器则是特殊的迭代器。</p></blockquote><p>如果一个函数包含 <code>yield</code> 表达式，那么它是一个生成器函数；调用它会返回一个特殊的迭代器，称为生成器。</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def <span class="built_in">func</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">def <span class="built_in">gen</span>():</span><br><span class="line">    yield <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span>(<span class="built_in">type</span>(func))   <span class="meta"># &lt;class 'function'&gt;</span></span><br><span class="line"><span class="keyword">print</span>(<span class="built_in">type</span>(gen))    <span class="meta"># &lt;class 'function'&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span>(<span class="built_in">type</span>(<span class="built_in">func</span>())) <span class="meta"># &lt;class 'int'&gt;</span></span><br><span class="line"><span class="keyword">print</span>(<span class="built_in">type</span>(<span class="built_in">gen</span>()))  <span class="meta"># &lt;class 'generator'&gt;</span></span><br></pre></td></tr></table></figure><p>如上，生成器 <code>gen</code> 看起来和普通的函数没有太大区别。仅只是将 <code>return</code> 换成了 <code>yield</code>。用 <code>type()</code> 函数打印二者的类型也能发现，<code>func</code> 和 <code>gen</code> 都是函数。然而，二者的返回值的类型就不同了。<code>func()</code> 是一个 <code>int</code> 类型的对象；而 <code>gen()</code> 则是一个迭代器对象。</p><h3 id="yield-表达式"><a href="#yield-表达式" class="headerlink" title="yield 表达式"></a><code>yield</code> 表达式</h3><p>如前所述，如果一个函数定义中包含 <code>yield</code> 表达式，那么该函数是一个生成器函数（而非普通函数）。实际上，<code>yield</code> 仅能用于定义生成器函数。</p><p>与普通函数不同，生成器函数被调用后，其函数体内的代码并不会立即执行，而是返回一个生成器（generator-iterator）。当返回的生成器调用成员方法时，相应的生成器函数中的代码才会执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        <span class="keyword">yield</span> x ** <span class="number">2</span></span><br><span class="line">square_gen = square()</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> square_gen:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><p>前面说到，<code>for</code> 循环会调用 <code>iter()</code> 函数，获取一个生成器；而后调用 <code>next()</code> 函数，将生成器中的下一个值赋值给 <code>x</code>；再执行循环体。因此，上述 <code>for</code> 循环基本等价于：</p><figure class="highlight monkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">genitor = square_gen.__iter__()</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    x = geniter.<span class="keyword">next</span>()<span class="meta"> # Python 3 是 __next__()</span></span><br><span class="line">    <span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p>注意到，<code>square</code> 是一个生成器函数；作为它的返回值，<code>square_gen</code> 已经是一个迭代器；迭代器的 <code>__iter__()</code> 返回它自己。因此 <code>geniter</code> 对应的生成器函数，即是 <code>square</code>。</p><p>每次执行到 <code>x = geniter.next()</code> 时，<code>square</code> 函数会从上一次暂停的位置开始，一直执行到下一个 <code>yield</code> 表达式，将 <code>yield</code> 关键字后的表达式列表返回给调用者，并再次暂停。注意，<strong>每次从暂停恢复时，生成器函数的内部变量、指令指针、内部求值栈等内容和暂停时完全一致</strong>。</p><h3 id="生成器的方法"><a href="#生成器的方法" class="headerlink" title="生成器的方法"></a>生成器的方法</h3><p>生成器有一些方法。调用这些方法可以控制对应的生成器函数；不过，若是生成器函数已在执行过程中，调用这些方法则会抛出 <code>ValueError</code> 异常。</p><ul><li><code>generator.next()</code>：从上一次在 <code>yield</code> 表达式暂停的状态恢复，继续执行到下一次遇见 <code>yield</code> 表达式。当该方法被调用时，当前 <code>yield</code> 表达式的值为 <code>None</code>，下一个 <code>yield</code> 表达式中的表达式列表会被返回给该方法的调用者。若没有遇到 <code>yield</code> 表达式，生成器函数就已经退出，那么该方法会抛出 <code>StopIterator</code> 异常。</li><li><code>generator.send(value)</code>：和 <code>generator.next()</code> 类似，差别仅在与它会将当前 <code>yield</code> 表达式的值设置为 <code>value</code>。</li><li><code>generator.throw(type[, value[, traceback]])</code>：向生成器函数抛出一个类型为 <code>type</code> 值为 <code>value</code> 调用栈为 <code>traceback</code> 的异常，而后让生成器函数继续执行到下一个 <code>yield</code> 表达式。其余行为与 <code>generator.next()</code> 类似。</li><li><code>generator.close()</code>：告诉生成器函数，当前生成器作废不再使用。</li></ul><h3 id="举例和说明"><a href="#举例和说明" class="headerlink" title="举例和说明"></a>举例和说明</h3><h4 id="如果你看不懂生成器函数"><a href="#如果你看不懂生成器函数" class="headerlink" title="如果你看不懂生成器函数"></a>如果你看不懂生成器函数</h4><p>如果你还是不太能理解生成器函数，那么大致上你可以这样去理解。</p><ul><li>在函数开始处，加入 <code>result = list()</code>；</li><li>将每个 <code>yield</code> 表达式 <code>yield expr</code> 替换为 <code>result.append(expr)</code>；</li><li>在函数末尾处，加入 <code>return result</code>。</li></ul><h4 id="关于「下一个」yield-表达式"><a href="#关于「下一个」yield-表达式" class="headerlink" title="关于「下一个」yield 表达式"></a>关于「下一个」<code>yield</code> 表达式</h4><p>介绍「生成器的方法」时，我们说当调用 <code>generator.next()</code> 时，生成器函数会从当前位置开始执行到下一个 <code>yield</code> 表达式。这里的「下一个」指的是执行逻辑的下一个。因此</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f123</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="number">2</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> f123(): <span class="comment"># 1, 2, and 3, will be printed</span></span><br><span class="line">    print(item)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f13</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">False</span>:</span><br><span class="line">        <span class="keyword">yield</span> <span class="number">2</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> f13(): <span class="comment"># 1 and 3, will be printed</span></span><br><span class="line">    print(item)</span><br></pre></td></tr></table></figure><h4 id="使用-send-方法与生成器函数通信"><a href="#使用-send-方法与生成器函数通信" class="headerlink" title="使用 send() 方法与生成器函数通信"></a>使用 <code>send()</code> 方法与生成器函数通信</h4><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def <span class="function"><span class="keyword">func</span><span class="params">()</span>:</span></span><br><span class="line">    x = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        y = (yield x)</span><br><span class="line">        x += y</span><br><span class="line"></span><br><span class="line">geniter = <span class="function"><span class="keyword">func</span><span class="params">()</span></span></span><br><span class="line">geniter.<span class="keyword">next</span>()  <span class="meta"># 1</span></span><br><span class="line">geniter.<span class="built_in">send</span>(<span class="number">3</span>) <span class="meta"># 4</span></span><br><span class="line">geniter.<span class="built_in">send</span>(<span class="number">10</span>)<span class="meta"># 14</span></span><br></pre></td></tr></table></figure><p>此处，生成器函数 <code>func</code> 用 <code>yield</code> 表达式，将处理好的 <code>x</code> 发送给生成器的调用者；与此同时，生成器的调用者通过 <code>send</code> 函数，将外部信息作为生成器函数内部的 <code>yield</code> 表达式的值，保存在 <code>y</code> 当中，并参与后续的处理。</p><p>这一特性是使用 <code>yield</code> 在 Python 中使用协程的基础。</p><h2 id="yield-的好处"><a href="#yield-的好处" class="headerlink" title="yield 的好处"></a><code>yield</code> 的好处</h2><p>Python 的老用户应该会熟悉 Python 2 中的一个特性：内建函数 <code>range</code> 和 <code>xrange</code>。其中，<code>range</code> 函数返回的是一个列表，而 <code>xrange</code> 返回的是一个迭代器。</p><blockquote><p>在 Python 3 中，<code>range</code> 相当于 Python 2 中的 <code>xrange</code>；而 Python 2 中的 <code>range</code> 可以用 <code>list(range())</code> 来实现。</p></blockquote><p>Python 之所以要提供这样的解决方案，是因为在很多时候，我们只是需要逐个顺序访问容器内的元素。大多数时候，我们不需要「一口气获取容器内所有的元素」。比方说，顺序访问容器内的前 5 个元素，可以有两种做法：</p><ul><li>获取容器内的所有元素，然后取出前 5 个；</li><li>从头开始，逐个迭代容器内的元素，迭代 5 个元素之后停止。</li></ul><p>显而易见，如果容器内的元素数量非常多（比如有 <code>10 ** 8</code> 个），或者容器内的元素体积非常大，那么后一种方案能节省巨大的时间、空间开销。</p><p>现在假设，我们有一个函数，其产出（返回值）是一个列表。而若我们知道，调用者对该函数的返回值，只有逐个迭代这一种方式。那么，如果函数生产列表中的每一个元素都需要耗费非常多的时间，或者生成所有元素需要等待很长时间，则使用 <code>yield</code> 把函数变成一个生成器函数，每次只产生一个元素，就能节省很多开销了。</p><p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Video captioning summary</title>
      <link href="/2019/02/23/Video-captioning-summary/"/>
      <url>/2019/02/23/Video-captioning-summary/</url>
      
        <content type="html"><![CDATA[<ul><li>总结一下各论文使用的encoder的model分别是什么，是否采用了C3D，若采用了C3D,每16帧输出一个特征向量，这样的话，n_frames/16 个特征向量，那么论文中又是如何聚合特征来得到video 特征的？<br>## </li></ul><h2 id="训练和测试的一般过程"><a href="#训练和测试的一般过程" class="headerlink" title="训练和测试的一般过程"></a>训练和测试的一般过程</h2><p>The training process predicts the next word given the previous words from groundtruth, while the generation process conditions the prediction on the ones previously generated by itself.  </p><h2 id="训练损失"><a href="#训练损失" class="headerlink" title="训练损失"></a>训练损失</h2><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0iyn2xsz9j30r00higpg.jpg" style="zoom:70%"><h2 id="Code-总结"><a href="#Code-总结" class="headerlink" title="Code 总结"></a>Code 总结</h2><table><thead><tr><th>model</th><th>batch_size</th><th>epoch</th><th>learning_rate</th><th>MSVD<br>train-dataset</th><th>MSR-train-dataset</th></tr></thead><tbody><tr><td>video-caption.pytorch</td><td>128</td><td>6001（MSR）</td><td>0.0004 (每200epoch下降0.8)</td><td>✘</td><td>6513 pairs<br>(每一次随机的从captions中选择一个作为label)</td></tr><tr><td>SA-tensorflow</td><td>100</td><td>200</td><td>0.0001（不变）</td><td>1200×41个pairs</td><td>✘</td></tr><tr><td>reconstruction-network</td><td>100</td><td>iter=100000  （epoch=100000×100/(1200*41）=203</td><td>0.00001（不变）</td><td>1200×41个pairs</td><td>✘</td></tr><tr><td>saliency-based</td><td>100</td><td>100</td><td>0.0003（不变）</td><td>略</td><td>✘</td></tr><tr><td>HRNE</td><td>200</td><td>128</td><td>0.0002</td><td>略</td><td>✘</td></tr></tbody></table><table><thead><tr><th>model</th><th>construct vocab use which dataset</th></tr></thead><tbody><tr><td>video-caption.pytorch</td><td>MSR: all</td></tr><tr><td>SA-tensorflow</td><td>MSVD: train</td></tr><tr><td>reconstruction-network</td><td>MSVD: all</td></tr><tr><td>saliency-based</td><td></td></tr><tr><td>HRNE</td><td></td></tr></tbody></table><table><thead><tr><th>model</th><th>loss function</th><th>input of decoder</th></tr></thead><tbody><tr><td>video-caption.pytorch</td><td>output of decoder（bs, hidden size）,经过一个全连接层得到one-hot形式(bs, n_vocab），在经过F.log_softmax。则损失函数 nn.NLLLoss（）</td><td>rnn1的输入是video feature;<br>rnn2的输入是rnn1的输出cancatenate 上一步ground truth的word embedding<br> output1, state1 = self.rnn1(vid_feats, state1)<br> input2 = torch.cat((output1, padding_words), dim=2)<br>          output2, state2 = self.rnn2(input2, state2)</td></tr><tr><td>SA-tensorflow</td><td><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0izmumz8sj30ry01l74i.jpg">LSTM的output/hidden state、经过attention加权求和得到的videofeature、上一步step的groundtruth word embedding进行concatenate，在经过全连接层、非线性层、全连接层、输入交叉熵损失函数：tf.nn.softmax_cross_entropy_with_logits</td><td>LSTM的输入是concatenate(video_feature, 上一步ground truth的word embdedding）</td></tr><tr><td>reconstruction-network</td><td>output of decoder（bs, hidden size）,经过一个全连接层得到one-hot形式(bs, n_vocab），在经过dropout。  则损失函数 nn.CrossEntropyLoss()</td><td>LSTM的输入是concatenate(video_feature, 上一步ground truth的word embdedding）</td></tr></tbody></table><hr><h2 id="Paper-总结"><a href="#Paper-总结" class="headerlink" title="Paper 总结"></a>Paper 总结</h2><table><thead><tr><th>model</th><th>dataset</th><th>n_frames</th></tr></thead><tbody><tr><td>S2VT</td><td>MSVD</td><td>每10帧取1帧</td></tr><tr><td>SA</td><td>MSVD</td><td>前240帧等间隔取26帧</td></tr><tr><td>h-RNN</td><td>MSVD</td><td>没讲( ˇˍˇ )</td></tr><tr><td>HRNE</td><td>MSVD</td><td>fixed 160帧</td></tr><tr><td>LSTM-TSA</td><td>MSVD</td><td>等间隔采取25帧</td></tr><tr><td>LSTM-E</td><td>MSVD</td><td>all frames</td></tr><tr><td>Reconstruction</td><td>MSVD  MSR-VTT</td><td>等间隔28帧</td></tr><tr><td>M3</td><td>MSVD  MSR-VTT</td><td>28帧for MSVD; 40帧for MSR-VTT</td></tr></tbody></table><table><thead><tr><th>model</th><th>词频</th><th>MSVD  vocabulary</th><th>MSR-VTT  vocabulary</th></tr></thead><tbody><tr><td>Hierarchical Boundary-Aware Neural Encoder for Video Captioning</td><td>大于等于5</td><td>4215</td><td></td></tr><tr><td>Multimodal Memory Modelling for Video Captioning</td><td></td><td>13,000</td><td>29,000</td></tr><tr><td>Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning</td><td></td><td>9450</td><td>23500</td></tr><tr><td>Describing Videos by Exploiting Temporal Structure</td><td></td><td>16,000</td><td></td></tr><tr><td>Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks</td><td></td><td>12, 766（1, 297 and 670 videos ）</td><td></td></tr></tbody></table><table><thead><tr><th>model</th><th>feature</th><th>METER</th></tr></thead><tbody><tr><td>Mean Pool + LSTM</td><td>在COCO上预训练的Alex net</td><td>29.1</td></tr><tr><td>S2VT</td><td>RGB frames on VGG Net<br>optical flows on AlexNet</td><td>29.8</td></tr><tr><td>SA</td><td>GoogLeNet and 3D-CNN</td><td>29.6</td></tr><tr><td>LSTM-E</td><td>VGGNet and C3D</td><td>31.0</td></tr><tr><td>h-RNN</td><td>VGGNet and C3D</td><td>32.6</td></tr><tr><td>HRNE</td><td>GooLeNet</td><td>33.1</td></tr><tr><td>Reconstruction</td><td>Inception-V4<br> last pooling layer</td><td>34.1</td></tr></tbody></table><h2 id="提取frames-features-之后，获取video-feature的几种方法："><a href="#提取frames-features-之后，获取video-feature的几种方法：" class="headerlink" title="提取frames features 之后，获取video feature的几种方法："></a>提取frames features 之后，获取video feature的几种方法：</h2><p><strong>1. Mean pooling</strong></p><ul><li>Translating videos to natural language using deep recurrent neural networks. NACACL, 2015</li><li>Jointly modeling embedding and translation to bridge video and language. CoRR,  2015  </li></ul><p><strong>2. Weighted mean Pooling with an attention model</strong>    </p><ul><li>Describing videos by exploiting temporal structure. ICCV, 2015  </li><li>Exploring Visual Relationship for Image Captioning</li><li>2层LSTM，第一层LSTM的输入是对object/frames features进行平均池化，第二层LSTM的输入是给定第一层的hidden state 来得到attention 系数，从而对object/frames features进行加权求和。 即第一层用平均池化的特征来表征 global feture，第二层用加权求和的特征来表征 global feature</li></ul><p><strong>3. Taking the last output from an RNN encoder which summarizes the feature sequence</strong>    </p><ul><li>Long-term recurrent convolutional networks for visual recognition and description. CVPR, 2015</li><li>Sequence to sequence - video to tex. ICCV, 2015</li><li>A multi-scale multiple instance video description network. CoRR, 2015  </li></ul><h2 id="video-captioning-的模型中，含有extract-object-proposal的论文"><a href="#video-captioning-的模型中，含有extract-object-proposal的论文" class="headerlink" title="video captioning 的模型中，含有extract object proposal的论文"></a>video captioning 的模型中，含有extract object proposal的论文</h2><ul><li>Video paragraph captioning using hierarchical recurrent neural networks.  CVPR, 2016.  </li><li>object-aware aggregation with bidirectional temporal graph for video capioning. CVPR, 2019</li></ul><h2 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h2><ul><li>一般的情况下是对decoder的部分计算loss, 并反向传播，encoder采用预训练好的model，并不在进行finetune。</li></ul><h2 id="使用objects-feature-的文章"><a href="#使用objects-feature-的文章" class="headerlink" title="使用objects feature 的文章"></a>使用objects feature 的文章</h2><ul><li></li><li>video as graph : charades 每帧提取50个objects(当objects 的数量将为25的时候，score只降了0.2), something2 :每帧提取10个objects</li><li>我的msr-vtt: 提取5个效果比较好，不会包含太多的噪声</li><li>==尽量让一个video中的objects 不同，去聚类帧之间的objects==  </li></ul><table><thead><tr><th>论文</th><th>charades( 30s)</th><th></th><th>something-something( 3-6s )</th><th>activity</th><th>MSVD（10-25s）</th></tr></thead><tbody><tr><td>video as graph</td><td>16帧 *50</td><td></td><td>16帧* 10</td><td>10帧*100</td><td></td></tr><tr><td>HTM （video captioning)</td><td>80帧 *30</td><td></td><td></td><td></td><td>28帧*30</td></tr></tbody></table><table><thead><tr><th>论文</th><th>object detector</th><th>return</th></tr></thead><tbody><tr><td>(ACM 2019)Hierarchical Global-Local Temporal Modeling for VideoCaptioning</td><td>Faster rcnn 去掉rcnn的分类层，</td><td>提取_head_to_tail之后的特征 2048维</td></tr><tr><td>(CVPR 2019)Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning</td><td>YoLo</td><td>没说</td></tr><tr><td>(CVPR 2019)Grounded Video Description</td><td>a Faster RCNN model [24] with a ResNeXt-101 FPN backbone (在VG上预训练，类别会比coco 多，同时训练目标检测和属性分类)</td><td>返回的是fc6,我认为是_head_to_tail</td></tr><tr><td>( ECCV 2018)Videos as Space-Time Region Graph</td><td>the RPN with ResNet-50 backbone and FPN ==(需要注意，这里具体的：先由I3D得到THWd的特征，然后对32帧，每2帧取1帧，去得到这16帧的bbox，得到了bbox不是直接去得到pooled_feats，而是通过I3D的空间特征，bbox, Roi Align来得到bbox 的 region feat) ==</td><td>返回的是roi_pooling的7*7的，然后再进行平均池化</td></tr><tr><td>(CVPR 2019)Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning</td><td>MASK RCNN，在COCO上预训练，</td><td>将得到的regions 裁剪成图像，再送入ResNet-200中，得到res-layer5c的局部特征</td></tr><tr><td>（CVPR 2019）Auto-Encoding Scene Graphs for Image Captioning</td><td>faster rcnn , 使用 r-cnn 输出的 rois， 然后作用到 base feat上，使用 roi pooling 的到 pooled feats</td><td>返回 7*7的pooled feats</td></tr></tbody></table><ul><li>根据faster r-cnn 的网络结构，rpn部分输出bbox的预测，rcnn部分也输出bbox的预测，在目标检测任务中，采用rcnn的输出作为最后的结果。</li><li>但是在视频帧提取 object 的任务中，一般采用的是rpn部分输出的bbox,  why？ 这是因为，想要得到的不是bbox的坐标，而是bbox feats， 因此，直接取pooled_feats更加简洁方便。</li><li>所以在利用mmdetection时，设置 在rpn部分的max_region_per</li><li></li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(h-RNN)Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks</title>
      <link href="/2019/02/23/h-RNN-Video-Paragraph-Captioning-Using-Hierarchical-Recurrent-Neural-Networks/"/>
      <url>/2019/02/23/h-RNN-Video-Paragraph-Captioning-Using-Hierarchical-Recurrent-Neural-Networks/</url>
      
        <content type="html"><![CDATA[<h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li>这篇文章主要针对于一个video 划分为多个interval，并分别对其进行caption这样的数据集。其中段落生成器的作用：可以捕捉句子之间的相互依赖关系，同时段落生成器的输出作为句子生成器的输入，可以使得<strong>下一个句子的生成是建立在当前句子的语境下生成的</strong>。</li><li>另外对于MSVD这种一个video直接由一个sentence来描述的数据集，段落生成器不起作用，只是在<strong>decoder的结构相较于其他的model有不同之处</strong>：video feature 不输入decoder 的 RNN，而是与RNN的hidden state 级联后输入Multimodal层，Multimodal( concatenate( hidden state，video feature ) )。</li></ul><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>提出了一个方法：<strong>利用分层RNN开解决视频描述</strong>。我们的分层框架包含一个句子生成器和一个段落生成器。<strong><em>句子生成器</em></strong>产生一个简短的句子，这个句子可以描述一个特定的短视频间隔。它利用时间和空间的注意力机制，有选择地将注意力集中在视觉元素上。<strong><em>段落生成器</em></strong>通过将句子生成器产生的句子嵌入与段落历史结合起来作为输入来捕获句子间的依赖关系，并段落生成器的parahraph state 将作为输出语句生成器的新初始状态，然后句子生成器再生成下一个句子，~ 循环</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li><strong>对当前方法的总结：</strong><br>当给出了从视频帧中提取的深度卷积特征序列(例如vggnet 和c3d)，则视频的特征可以用以下几种方法获取：<br>（1）平均池化[1, 2]<br>（2）加权平均池化（attention 机制）[3]<br>（3）RNN encoder 的最后一个输出[4, 5, 6]<br>当前基于序列学习的视频描述方法，只专注于为一个简短的视频剪辑生成一个句子。到目前为止，深层次学习方法还没有尝试为长视频生成多个句子或段落的问题。使用平均池化得到 video feature 的方法，只适用于 short video clips where there is only one major event，随后有了 recurrent encoder 和 attention model。<br>我们的方法也采用了attention 机制。但是我们的框架和他们的框架之间存在两个不同之处，1. 解释<strong>空间注意力</strong>：即对每个frames 提取object proposals 然后基于注意力机制对proposal features of one frames 进行加权求和来得到frames features。这对于数据集中 object 非常小且难定位的情况有很大的帮助。另外，也解释一下<strong>时域注意力</strong>：是指对features of frames 进行加权求和，从而得到 video feature。 <strong>本文的注意力机制</strong>：提取M帧，每帧K个object, 则对这M*K个 object 进行基于attention 系数的加权求和。2. 在加权视频特征和注意权重之后，我们不会在加权特征的基础上限制递归层的隐藏状态。 </li><li><strong>Motivation</strong><br>大多数视频描述的不仅仅是一个事件。只用一个简短的句子来描述一个语义丰富的视频通常会产生信息不多甚至无聊的结果。例如，一个video 应该描述成<strong>那个人把土豆切成片，把洋葱切成块，把洋葱和土豆放进锅里</strong>，但是只产生one sentence的方法可能会说<strong>这个人在做饭</strong>。</li><li><strong>Idea</strong><br>我们想要利用句子之间的时域依赖性，这样，在生成段落时，句子就不会独立地生成。相反，一个句子的生成可能会受到前几句所提供的语义上下文的影响。<br>我们的分层RNN结构包括两个生成器，一个句子生成器和一个段落生成器，这两个生成器都使用RNN layers<br>据我们所知，这是分层RNN在视频字幕任务中的首次应用。  </li></ul><h2 id="Hierarchical-RNN-for-Video-Captioning"><a href="#Hierarchical-RNN-for-Video-Captioning" class="headerlink" title="Hierarchical RNN for Video Captioning"></a>Hierarchical RNN for Video Captioning</h2><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0gmi1zayjj314o0h2n1f.jpg">***designed by yaya:***<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0k7r1p5ygj316f0jyq47.jpg">我们的方法：在句子生成器之上堆叠了一个段落生成器**句子生成器**：1) RNN 用来语言建模 2) 多模态层对多源信息进行聚合 3) 注意力模型<ul><li>RNN1：word embedding 作为RNN的输入，并更新 hidden state </li><li>Attention layer: RNN 的hidden state 作为attention layer 的输入，来计算weight:   <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0gohii8woj30xx02saab.jpg" style="zoom:30%"><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0gom9zybsj30kg04mwes.jpg" style="zoom:45%">假设视频中有M帧，每帧有K个objects，则features：  <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0go2kemkqj30bq01pt8m.jpg" style="zoom:50%">若计算出了一组权重:  <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0go382u2qj309m0250so.jpg" style="zoom:50%">则 video feature：  <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0go5n8g7dj30if036mx8.jpg" style="zoom:30%">得到的video feature是一个特征通道，完整的模型是两个特征通道，一个由 object appearance 生成，另一个由action 生成</li><li>Multimodal<br>输入：RNN 的hidden state <strong>concate</strong> 2个Attention 的输出（两个特征通道Ua  C3D，action feature；Uo aggregate object appearance）。既有语言，又有视觉，因此成为多模态。<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0gpaasqgmj30sb022dg0.jpg" style="zoom:50%"></li><li>Hidden layer<br>-输出维度512 与 word embdeding 的维度一致</li><li>Softmax layer<br>输出维度与vocabulary size 一致</li><li>Maxid layer<br>Maxid layer 在softmax layer 的输出中挑选了最大值所在的索引，该索引将会被作为predicted word 的id(对应到vocabulary 的 索引)</li><li>预测的单词将会作为句子生成器的下一个输入（test）；下一个输入单词总是由带注释的句子（ground truth/ reference）提供。</li></ul><p><strong>段落生成器</strong> : 另外一个RNN，来建模句子之间的相互依赖。输入：1.句子生成器的输出， 2. paragraph history  输出：该输出作为句子生成器的初始状态<br>使用的RNN为GRU</p><ul><li>Word Embedding<br>1) 对sentences中的所有单词的embedding 取平均，得到一个压缩embedding vector<br>2) 同时也接受RNN1 的最后一个hidden state 作为 压缩表达<br>将上面两个压缩表达concatenated </li><li>Sentences Embedding<br>将上面concatenated 的特征输入该层，得到512维度的输出</li><li>RNN2</li><li>Paragraph State layer<br>输入：结合RNN2的hidden state 和 sentence embedding<br>输出：作为RNN1下一个句子的初始状态，为句子生成器提供了段落历史是有必要的，以便在上下文语境中中生成下一句。<br>它实质上为句子生成器提供了段落历史，以便在上下文中生成下一句。</li></ul><h2 id="Training-and-Generation"><a href="#Training-and-Generation" class="headerlink" title="Training and Generation"></a>Training and Generation</h2><p><strong>整个网络的循环过程</strong></p><ul><li>当RNN1在每一时间步骤中不断更新其hidden state，RNN2只在处理完整句子时才更新其hidden state。</li><li>RNN1 由beam search 得到 J 个sequence cost 最低的句子，挑选出1个最低的，然后送入RNN2。RNN2又输出隐层状态，最为RNN1下一个句子的初始隐层状态。如此循环，直至， when the sentence received by the paragraph generator is the EOP (end-of-paragraph) which consists of only the BOS and the EOS。</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>数据集</strong><br>two benchmark datasets: YouTubeClips and TACoS-MultiLevel<br>YouTubeClips： 虽然每个video 有多个sentences进行描述，但是sentences是对这个video的整体描述，而不是分别对video进行分段描述。因此这个数据集作为我们方法的特例，paragraph length N=1.<br><strong>Encoder</strong></p><ul><li>由于YouTubeClips数据集中的object 十分显著，因此不进行提取object的操作，只对frame 提取特征，这样attention 只包括temporal attention ，而不包括 spatial attention。</li><li>对于TACoS-MultiLevel 数据集，首先使用光流大体的提取boundinig box，然后沿着bounding box 的边，提取220*220的image patches，保证相邻两个box 重合度为50%。使用VGG模型对每个patch提取特征，并使用attention的权重，对这些patches进行加权求和。此时，attention同时包括temporal 和 spatial。</li><li>C3D 提取 action/motion feature of video<br>C3D 模型：输入frames of video ，每16帧输出一个固定长度的特征向量。然后采用attention机制对C3D特征进行polling（加权求和）<br><strong>实验结果对比分析</strong></li><li>YouTubeClips  <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0hdv8rde9j30i7099jtg.jpg">   相比于**LSTM-E[2]**（同样同时利用了VGG和C3D特征），我们的模型效果更好。相比于**SA[3]**（同样利用了temporal attention）我们的方法更好，原因：RNN的输入不包括视频特征，换句话说， hidden state的更新不建立在video feature的基础上。video feature直接的输入到multimodal layer。</li><li>TACoS-MultiLevel<br>这里不做分析（可以自行参考论文）</li></ul><h2 id="Discussions-and-Limitations"><a href="#Discussions-and-Limitations" class="headerlink" title="Discussions and Limitations"></a>Discussions and Limitations</h2><ol><li>目前我们使用的目标检测方法很难处理small object，造成在生成句子时，极容易混淆，比如应该是orange ，却生成了mango</li><li>句子信息通过段落循环层单向流动，从段落开始到结尾，但也不是以相反的方式。如果第一个句子中含有错误信息，则会导致错误信息依次传递，目前使用双向RNN来生成句子，仍然是一个开放性的问题（yaya: sorry , i don’t kow what’s mean，可能是目前还不知道使用BiRNN来生成句子的效果是否好于单向RNN）。</li><li>与其他大多数图像/视频字幕方法一样，我们的方法存在一个已知的问题，即训练所使用的目标函数与生成方法所使用的目标函数之间存在差异。训练过程给定来自groundtruth的先前单词来预测下一个单词，而生成过程则对先前由其自身生成的单词进行预测。这个问题在我们的分层框架中更加放大，因为在训练时，段落生成器输入的是groundtruth，但是在测试阶段，输入的是句子生成器生成的句子。潜在的解决办法：</li></ol><ul><li>Scheduled Sampling<br>在训练过程中增加Scheduled Sampling，即随机的选择words of groundtruth或者由model生成的单词。</li><li>在训练的过程中直接优化metric(BLEU， CIDER, etc)</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <strong><em>Translating videos to natural language using deep recurrent neural networks</em></strong>. NACACL, 2015<br>[2] <strong><em>Jointly modeling embedding and translation to bridge video and language</em></strong>. CoRR,  2015<br>[3] <strong><em>Describing videos by exploiting temporal structure</em></strong>. ICCV, 2015<br>[4] <strong><em>Long-term recurrent convolutional networks for visual recognition and description</em></strong>. CVPR, 2015<br>[5] <strong><em>Sequence to sequence - video to text</em></strong>. ICCV, 2015<br>[6]  <strong><em>A multi-scale multiple instance video description network</em></strong>. CoRR, 2015</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hierarchical Boundary-Aware Neural Encoder for Video Captioning</title>
      <link href="/2019/02/22/Hierarchical-Boundary-Aware-Neural-Encoder-for-Video-Captioning/"/>
      <url>/2019/02/22/Hierarchical-Boundary-Aware-Neural-Encoder-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0g5c5cwmxj30l00mh45o.jpg">  In this paper, we focus on the video encoding stage. we propose a recurrent network which can learn to adapt its temporal structure to input data.Our network is the first proposal which exploits temporal segments invideo captioning。在这篇文章中，给出了一个循环视频编码方案，该方案可以发现和利用视频的分层结构。不同于经典的编码解码方法（视频由一个循环层来连续的编码），我们提出了一个新颖的LSTM单元， 其可以识别帧/段之间非连续的点，相应地修改编码层的时间连接。Encoder Model---------<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0g57reyepj316c0m7k74.jpg">   - **traditional lstm network**使用LSTM来获得video feature，将每个frames 按每time step 依次送入LSTM，最后一个LSTM cell 的hidden state 用来得到video feature。  - **Time Boundary-aware LSTM network** ***(ours)***figure1 与 figure2 结合来看，存在BD（boundary detection ）来检测该帧是否为一个边界（an appearance or action change），若BD检测到存在一个边界，则**保存当前LSTM的输出**，并开始一个新的LSTM（即，hidden state and the cell memory 被重新初始化）。这就确保了在边界之后的输入数据，不受边界之前数据的影响。经过对all frames of video 进行这样的操作，于是得到可变长度的输出 (s1; s2; ...; sm), m是检测到segments的数量。这组输出又经过另外一个LSTM层（称为第二LSTM层），第二LSTM层的hidden state 作为整个视频的特征（参考figure1）。Decoder model-------------A Gated Recurrent Unit (GRU) layer]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>video-caption-dataset</title>
      <link href="/2019/02/22/video-caption-dataset/"/>
      <url>/2019/02/22/video-caption-dataset/</url>
      
        <content type="html"><![CDATA[<h2 id="Youtube2Text-（MSVD）-1"><a href="#Youtube2Text-（MSVD）-1" class="headerlink" title="Youtube2Text （MSVD）[1]"></a>Youtube2Text （MSVD）[1]</h2><ul><li>这个数据集包含 1967个短视频，10-25s，平均时长为9s，视频包含不同的人，动物，动作，场景等。</li><li>每个视频由不同的人标注了多个句子，大约41 annotated sentences per clip，共有 80839 个sentences，平均每个句子有8个words，这些所有的句子中共包含近16000个 unique words。</li><li>caption中包括多国的语言进行描述，部分论文中采取只选用laguage = english 的caption 进行训练和测试[3][4]</li><li>采用的split根据 [2] ： 1,200 videos for training, 100 for validation and 670 for testing.<br><a href="https://github.com/ShiYaya/video_captioning/tree/master/MSVD" target="_blank" rel="noopener">我的整理</a></li></ul><ul><li>数据的下载：</li><li><a href="https://www.microsoft.com/en-us/download/details.aspx?spm=a2c4e.11153940.blogcont209612.6.42ba7e9eAA1K2o&id=52422&from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F38cf15fd-b8df-477e-a4e4-a4680caa75af%2Fdefault.aspx" target="_blank" rel="noopener">[website]</a></li><li>原数据：只给出了video_id,以及strart and end time , 若需要video数据，则需要自己通过url下载</li><li>某篇对于视频分析的总结，给出了<a href="https://github.com/sinyeratlantis/sinyeratlantis.github.io/blob/master/content/dl/%E8%A7%86%E9%A2%91%E5%88%86%E6%9E%90%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94.md" target="_blank" rel="noopener">MSVD的下载链接</a>，可用，推荐(下载速度快，且video命名相对较好)☀☀<br><br><a href="http://www.cs.utexas.edu/users/ml/clamp/videoDescription/YouTubeClips.tar" target="_blank" rel="noopener">直接的下载链接</a><strong>[推荐]</strong></li><li>某篇github 含有MSVD(Youtube2Text)的<a href="https://github.com/yaoli/arctic-capgen-vid" target="_blank" rel="noopener">[preproceed dataset]</a><br><br><a href="http://lisaweb.iro.umontreal.ca/transfert/lisa/users/yaoli/youtube2text_iccv15.zip" target="_blank" rel="noopener">直接的下载链接</a>(下载速度慢)</li><li>该篇github上含有<a href="https://github.com/ShiYaya/Video-Description-with-Spatial-Temporal-Attention#video-datas-and-pre-extracted-features-on-msvd-dataset" target="_blank" rel="noopener">MSVD数据集下载链接</a><br><br><a href="https://www.multcloud.com/share/050e69cd-cab9-4ba3-a671-ed459341ab41" target="_blank" rel="noopener">直接的下载链接</a></li><li>对caption 常用的预处理: 1) verting all text to lower case, 2) tokenizing the sentences, 3) removing punctuation.</li></ul><h2 id="MSR-VTT-dataset"><a href="#MSR-VTT-dataset" class="headerlink" title="MSR-VTT dataset"></a>MSR-VTT dataset</h2><p> 共10000个video, 每个video有20个sentences, 共20万 video/sentence pair，10-30s居多</p><ul><li>split:  train:6513, val:497, test：2990</li><li>MSR-VTT dataset v2 , just video url: <a href="http://ms-multimedia-challenge.com/2017/dataset" target="_blank" rel="noopener">http://ms-multimedia-challenge.com/2017/dataset</a></li><li>author split train test val by himself and provied video data :<a href="https://github.com/xiadingZ/video-caption.pytorch" target="_blank" rel="noopener">https://github.com/xiadingZ/video-caption.pytorch</a>  <br></li></ul><p><strong>下载这个数据集即可使用，但是还需要再找split!</strong><br></p><ul><li>MSR VTT 采用的split 是2016年提供的，<strong>目前科研广泛使用的都是2016年的</strong>。</li><li><a href="https://github.com/adi-dhal/In_Depth_Video_Analysis/tree/master/msr-vtt/2016" target="_blank" rel="noopener">[split]</a><br>MSR-VTT. Test video doesn’t have captions, so I spilit train-viedo to train/val/test. Extract and put them in <code>./data/</code> directory</li></ul><p>train-video: <a href="https://drive.google.com/file/d/1Qi6Gn_l93SzrvmKQQu-drI90L-x8B0ly/view?usp=sharing" target="_blank" rel="noopener">download link</a> <br><br>test-video: <a href="https://drive.google.com/file/d/10fPbEhD-ENVQihrRvKFvxcMzkDlhvf4Q/view?usp=sharing" target="_blank" rel="noopener">download link</a> <br><br>json info of train-video: <a href="https://drive.google.com/file/d/1LcTtsAvfnHhUfHMiI4YkDgN7lF1-_-m7/view?usp=sharing" target="_blank" rel="noopener">download link</a> <br><br>json info of test-video: <a href="https://drive.google.com/file/d/1Kgra0uMKDQssclNZXRLfbj9UQgBv-1YE/view?usp=sharing" target="_blank" rel="noopener">download link</a> <br></p><ul><li><p>download.py 可以下载MSR-VTT数据集(step by video)：<a href="https://github.com/OSUPCVLab/VideoToTextDNN" target="_blank" rel="noopener">[链接]</a></p></li><li><p>msr-vtt 2017 vs 2016<br>In the 2nd MSR Video to Language Challenge, we have combined the training set, validation set, and testing data in the 1st MSR Video to Language Challenge as the new training data. An additional test set of around 3K video clips will be released on June 1st as the final evaluation set. As such, we have 10K video clips for training and 3K video clips for testing this year. Each video is annotated with 20 natural sentences.<br><br>总结：就仅仅是将2016的train val and test 综合到一起，组成了2017： 一个大的含10000个video的train 数据集，并另外提供了2000个test video。<br><br>科研上普遍使用2016的分割方案，</p></li></ul><ul><li>In MSR-VTT dataset, we provide the category information for each video clip and the video clip contains audio information as well.</li></ul><h2 id="VATEX数据集"><a href="#VATEX数据集" class="headerlink" title="VATEX数据集"></a>VATEX数据集</h2><ul><li><p>一个新的数据集，41269个video， 时长大约10s, 每个video有10个中文，10个英文，同时这10个之中，中英文之间有5个是两两配对的</p></li><li><p>提出了两个新的任务：（1）一个encoder-decoder模型，在两种语言之间共享参数，即希望一个模型，可以得到两种语言的描述。（2）提出了一种新的机器翻译任务，即当进行中英文的机器翻译任务时，可以添加视频的视觉特征作为辅助信息</p></li><li><p>数据集的来源：来自于kinetics的validation dataset, 然后它们找人进行了caption的标注。它们将这41269个video 分成了4部分，train, validation, public test, secret test(不公开，用于比赛)</p></li><li><p>具体我的介绍见这篇博文</p></li></ul><h2 id="三个数据集的caption-length-的长度的统计情况"><a href="#三个数据集的caption-length-的长度的统计情况" class="headerlink" title="三个数据集的caption length 的长度的统计情况"></a>三个数据集的caption length 的长度的统计情况</h2><ul><li>eg, 句长为10 的captions 在当前这个数据集中所占比例   </li><li>msvd : 主要是len=6 为中心的居多<br><img src="https://i.loli.net/2019/09/07/jJ7ztsQb9MUR15X.png" alt="msvd_cap_length_.png"></li><li>msr-vtt：以len=9 为中心的居多<br><img src="https://i.loli.net/2019/09/07/SNYoIqHxPLWmU9D.png" alt="msr-vtt_cap_length_.png"></li><li>vatex：以len=15为中心的居多<br><img src="https://i.loli.net/2019/09/07/he7KYqMt8xj5pUs.png" alt="vatex_cap_length_.png"></li></ul><p>[1] Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition. In ICCV 2013</p><p>[2] Translating videos to natural language using deep recurrent neural networks. NAACL, 2015.<br>[3] (ICCV 2015)Sequence to Sequence – Video to Text<br>[4] Jointly Modeling Embedding and Translation to Bridge Video and Language</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown用法</title>
      <link href="/2019/02/22/Markdown%E7%94%A8%E6%B3%95/"/>
      <url>/2019/02/22/Markdown%E7%94%A8%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<ul><li><p>参考该网址：<br><a href="https://www.kancloud.cn/wizardforcel/markdown-simple-world/97375" target="_blank" rel="noopener">https://www.kancloud.cn/wizardforcel/markdown-simple-world/97375</a></p></li><li><p>在线编辑器<br><a href="https://stackedit.io/editor" target="_blank" rel="noopener">https://stackedit.io/editor</a></p></li><li><p>首行缩进两个字符：<br>切换至中文全角，shift+space 按一次，然后再摁space两次，即可出现首行缩进两个字符</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 杂类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(LSTM-E)Jointly Modeling Embedding and Translation to Bridge Video and Language</title>
      <link href="/2019/02/22/LSTM-E-Jointly-Modeling-Embedding-and-Translation-to-Bridge-Video-and-Language/"/>
      <url>/2019/02/22/LSTM-E-Jointly-Modeling-Embedding-and-Translation-to-Bridge-Video-and-Language/</url>
      
        <content type="html"><![CDATA[<h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><p>本文的主要贡献：</p><ol><li>同时使用了action feature of C3D and frames features。但是由于对C3D的特征也采用了mean pooling的方法，有缺陷，对action 特征的一种破坏。</li><li>提出了relevance loss ， 来加强整个句子的语义与视觉特征之间的关系。  </li></ol><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>目前存在的方法，依据给定的先前的单词以及视觉信息，来生成words，但是并没有利用句子语义与视觉内容之间的关系，导致生成的句子可能上下文是正确的，但是语义是错误的。<br>如 figure1，LSTM model 生成的句子是a man is riding a horse，逻辑上是没有错误的，但是语义却错了，图中出现的是woman 而不是man。<br>　　<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0f0qr6d2ij30lq0biq8s.jpg" width="500" hegiht="313" align="center"></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>  简单介绍LSTM-E：LSTM-E可以同时利用LSTM学习和视觉-语义embedding。LSTM 学习是为了在给定先前的单词以及视觉特征的基础上，最大化生成下一个单词的概率，后者是为了生成视觉-语义embedding，来加强整个句子的语义与视觉特征之间的关系。<br>  <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0f87gorc9j316d0kr11m.jpg" title="overview">  </p><ul><li><strong>Overview</strong><br>2D/3D 卷积神经网络被用来提取视频frames/clips的特征，平均池化来得到视频的特征。<br>基于视频特征<strong><em>v</em></strong>和句子语义<strong><em>s</em></strong>，生成sentences的<strong>LSTM model</strong> 和<strong>视觉-语义embeddeing model</strong> 联合学习。</li><li><strong>The sprit of LSTM-E</strong><br>在coherence和relevance之间的相互增强下来生成sentences。<strong><em>coherence:</em></strong>表达了生成words与视频内容之间的相关关系，由LSTM优化完成。<strong><em>relevance:</em></strong>整个句子的语义与视频内容之间的关系，由视觉-语义embeddeing model来度量。通过联合学习coherence和relevance，期望生成的句子在语境和语义上是正确的。</li><li><strong>说人话</strong><br>由两个model组成，一个是sequence learning 都有的coherence loss ，来最大化生成next word的似然概率；另一个是本文添加的relevance loss，通过优化视频特征与生成句子之间的差距，使得生成的句子语义上能对应video的内容。即同时考虑了句子单词之间的上下文关系，也考虑了句子语义与视频内容之间的关系。</li><li><strong>contribution</strong><br>提出了relevance loss !  </li></ul><h2 id="Video-Description-with-Relevance-and-Coherence"><a href="#Video-Description-with-Relevance-and-Coherence" class="headerlink" title="Video Description with Relevance and Coherence"></a>Video Description with Relevance and Coherence</h2><ul><li><strong>Visual-Semantic Embedding: Relevance</strong><br><strong><em>v</em></strong> 和 <strong><em>s</em></strong> 分别是视频的特征和sentences的特征（即，都是已知的），Ts和Tv用来降维到相同的维度，为了度量视频内容与句子语义之间的相关性，一个自然地方法是计算embedding之间的距离，因此定义relevance loss:<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0f8zoni8tj30bc01pmx2.jpg" 　style="zoom:45%"></li><li><strong>Translation by Sequence Learning: Coherence</strong><br>coherence loss：即为在给定视频特征的条件下，生成sentences的最大似然概率。  <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0f9l8bbvoj30ph02pglq.jpg" style="zoom:50%">  由overview的图可知，在实际的情况下LSTM的输入是：第一个LSTM输入是视频特征，其余的是前一个time step 生成的单词（在train时：是caption中给定的第t个单词，在test时：是前一个time step 生成的单词）。因此似然函数可以具体的表示为：  　　<img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0f9lgy6nfj317806fgm5.jpg" style="zoom:40%"></li></ul><h2 id="Joint-Modeling-Embedding-and-Translation"><a href="#Joint-Modeling-Embedding-and-Translation" class="headerlink" title="Joint Modeling Embedding and Translation"></a>Joint Modeling Embedding and Translation</h2><ul><li><p><strong>simultaneously minimizing the relevance loss and coherence loss.</strong>  </p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0fa1y3nkgj30nu05pq3p.jpg" style="zoom:70%"></li><li><p>*<em>LSTM结构 *</em><br>这里有多种方式来结合visual content 和 word of last time step。法一：each time step 都输入视频特征；法二：只在第一步输入视频特征。但是在[ 1 ]中指出，由于网络可以显式地利用噪声和更容易覆盖，所以每次输入图像都会产生劣质的效果。 因此，采用第二种方法，在给定视频特征v 和相对应的 sentence W ≡ [w0, w1, …, wNs]，LSTM的更新步骤如下：  </p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0fcc8tiqej30z708l0t3.jpg">  <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0faic2fuzj30n508njs2.jpg" style="width: 50%; height: 50%">  在初始的第一步，视频特征作为LSTM的输入；在第二步，<#start#>开始的标志作为LSTM的输入，同时接受上一步的hidden state, cell state，以后每一步，都将上一步生成的word 作为输入，直至生成<#end#>。从第二步开始，使用LSTM cell 的hidden state 来预测 word( 对于LSTM output 与 hidden[0] 是一样的，参考：[https://mp.csdn.net/postedit/87516958](https://mp.csdn.net/postedit/87516958))<blockquote><p><code>output, hidden = self.rnn(input, hidden)</code></p></blockquote></#end#></#start#></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul><li><strong>dataset</strong><br>MSVD:<br>Microsoft Research Video Description Corpus (YouTube2Text) , which contains 1,970 YouTube snippets. There are roughly 40 available English descriptions per video. In our experiments, we follow the setting used in prior works, taking 1,200 videos for training, 100 for validation and 670 for testing.</li><li><strong>result</strong><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0fcs7kt8cj31ap0bhgow.jpg"></li><li><strong>The effect of hidden layer size</strong>  <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0fcvqbzvsj30qz09x760.jpg" style="width: 70%; height: 70%">  </li></ul><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>a visual-semantic embedding space is additionally incorporated into LSTM learning. In this way, <strong>a global relationship between the video content and sentence semantics</strong> is simultaneously measured in addition to <strong>the local contextual relationship between the word at each step and the previous ones</strong> in LSTM learning. On the popular YouTube2Text dataset, the results of our experiments demonstrate the success of our approach, outperforming the current state-ofthe-art models </p><h2 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h2><p>视频作为一个时域序列，未来将会探究使用RNN来获得更好的特征；另外，如果有更大的数据集，更多的video sentences pairs ，那么可以使用更深的RNN，来得到更好的视频描述</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR, 2015.</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GitHub+Hexo 搭建个人网站详细教程</title>
      <link href="/2019/02/21/GitHub-Hexo-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B/"/>
      <url>/2019/02/21/GitHub-Hexo-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26625249</a></p><p><a href="https://blog.csdn.net/xuezhisdc/article/details/53130328" target="_blank" rel="noopener">https://blog.csdn.net/xuezhisdc/article/details/53130328</a></p><h3 id="增加本地搜索功能"><a href="#增加本地搜索功能" class="headerlink" title="增加本地搜索功能"></a>增加本地搜索功能</h3><h4 id="自定义站点内容搜索"><a href="#自定义站点内容搜索" class="headerlink" title="自定义站点内容搜索"></a>自定义站点内容搜索</h4><ol><li><p>安装 hexo-generator-searchdb，在站点的根目录下执行以下命令：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm <span class="keyword">install</span> hexo-generator-searchdb <span class="comment">--save</span></span><br></pre></td></tr></table></figure></li><li><p>编辑博客配置文件，新增以下内容到任意位置：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">search:</span></span><br><span class="line"><span class="symbol">      path:</span> search.xml</span><br><span class="line"><span class="symbol">      field:</span> post</span><br><span class="line"><span class="symbol">      format:</span> html</span><br><span class="line"><span class="symbol">      limit:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure></li><li><p>编辑主题配置文件，启用本地搜索功能：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Local search</span></span><br><span class="line"><span class="attr">local_search:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="加密"><a href="#加密" class="headerlink" title="加密"></a>加密</h3><p><a href="https://www.jianshu.com/p/44e211829447" target="_blank" rel="noopener">https://www.jianshu.com/p/44e211829447</a></p><p><a href="http://npm.taobao.org/package/hexo-blog-encrypt" target="_blank" rel="noopener">http://npm.taobao.org/package/hexo-blog-encrypt</a></p><p><a href="https://github.com/MikeCoder/hexo-blog-encrypt/" target="_blank" rel="noopener">https://github.com/MikeCoder/hexo-blog-encrypt/</a></p>]]></content>
      
      
      <categories>
          
          <category> 杂类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
