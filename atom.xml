<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShiYaya</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-03-21T04:37:01.145Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ShiYaya</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>How Many Data Points is a PromptWorth?</title>
    <link href="http://yoursite.com/2021/03/20/How-Many-Data-Points-is-a-PromptWorth/"/>
    <id>http://yoursite.com/2021/03/20/How-Many-Data-Points-is-a-PromptWorth/</id>
    <published>2021-03-20T03:24:45.000Z</published>
    <updated>2021-03-21T04:37:01.145Z</updated>
    
    <content type="html"><![CDATA[<p>转载：<a href="https://mp.weixin.qq.com/s/vX6o4lJKP4ajVfGSw8-tqg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/vX6o4lJKP4ajVfGSw8-tqg</a></p><blockquote><p>今天介绍的是一篇 NAACL’21 新鲜出炉的工作！NAACL 上周四出的结果，这篇工作本周一上传 arxiv，周二被王苏小哥哥发现，周三拜读了一下，今天就来和大家分享啦！！</p></blockquote><p>给大家提个问题：如果训练样本只有几百条，这时候我们该怎么办呢？</p><p>传统的 RNN 在这个样本大小下很难被训练好，自然地，我们会想到使用预训练模型，在其基础上进行 finetune。具体来讲，就是将预训练模型作为模型的底层，在上面添加与当前任务特点相关的网络结构。这样就引入了预训练的知识，对当前任务能产生很大的帮助。</p><p><img src="https://i.loli.net/2021/03/21/QodMJ9GWgv6fcUy.png" alt="微信截图_20210321123106" style="zoom:33%;"></p><p>除了预训练的知识，是不是还有其他的信息我们没有用上呢？近年来，越来越多的人在使用另一种 finetune 方法，即<strong>结合具体场景，设计新的 finetune 任务形式，从而将与当前任务相关的提示信息（prompt）引入模型</strong>。我们大名鼎鼎的 GPT 系列就是这么干的。比如我们拿 GPT3 做 QA 的 finetune，直接喂给他一串“<em>Question：问题内容 Answer：</em>”，剩下的答案部分就让 GPT3 自己填完。</p><p><img src="https://i.loli.net/2021/03/21/s8DHwgmNJY7Ryvk.png" alt="image-20210321123155512" style="zoom: 33%;"></p><p>这类 finetune 技巧虽然陆续被使用，但并没有人论证：<strong>这种做法相比于传统的 finetune 方法，真的能带来提升吗</strong>？如果答案是肯定的，<strong>那么究竟能提升多少呢（能否量化这种提升）？</strong></p><p>今天这篇来自 Huggingface 的文章就填补了上述两个问题的答案。他们通过大量实验证明：<strong>引入提示信息和多标注几百条数据带来的性能提升是相当的</strong>！所以，下次老板只给少量样本，就要你 finetune 模型——不要慌！我们今天又多学了一个 trick！</p><p><strong>论文题目</strong>:<br><strong><em>How Many Data Points is a Prompt Worth?</em></strong></p><p><strong>论文链接</strong>:<br><em><a href="https://arxiv.org/abs/2103.08493" target="_blank" rel="noopener">https://arxiv.org/abs/2103.08493</a></em></p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>前文提到，这一类 finetune 是将任务对应的输入改写成新的完形填空格式，让模型预测 <mask> 部分的词，作为抽取任务的答案或者生成任务的结果。这种方法不需要改变模型结构、没有额外的参数，简直不要太方便！</mask></p><h3 id="引入描述集合"><a href="#引入描述集合" class="headerlink" title="引入描述集合"></a>引入描述集合</h3><p>本文对这类方法进行了进一步简化：不要求 <mask> 处生成任意的文本，而是只需要完成类似于有选项的完形填空任务。这里的选项是固定的几个词，我们称之为描述集合（verbalizer），不同任务会有不同的描述集合。</mask></p><p>比如，对于判断题的阅读理解任务，就可以将阅读文本、问题和 <mask> 拼接，让预训练模型直接预测 <mask> 属于描述集合 {yes, no} 中的哪一种描述：</mask></mask></p><blockquote><p>小明天天码代码码到天明 [SEP] <strong>小明有女朋友吗？</strong> <mask></mask></p></blockquote><p>其中前半部分是阅读文本，后面<strong>加粗</strong>的部分是问题。模型只需要判断 <mask> 属于描述集合 {yes, no} 中的哪一种。</mask></p><p>可能读到这里，大家会疑惑：直接拼起来搞一个 True / False 的二分类不就好了嘛，何必让模型填空呢？嘿嘿，这恰好是作者的用意：通过让模型填空，<strong>模型可以习得描述集合中标签文本的语义信息</strong>。</p><h3 id="引入提示信息"><a href="#引入提示信息" class="headerlink" title="引入提示信息"></a>引入提示信息</h3><p>直接拼接是最朴素的，但这能让模型知道自己在做什么任务嘛？为此，作者引入了<strong>提示信息</strong>（prompt）。</p><p>还是判断题的阅读理解任务，对文章 和问题 ，作者将他们与一些固定的词进行整合，以此输入模型，让模型预测 <mask> 。作者提出了三种整合方式：</mask></p><p><img src="https://i.loli.net/2021/03/21/QEPwMG7bfI2UzNH.png" alt="image-20210321123632246" style="zoom: 33%;"></p><p>没错，就是这么简单！这些固定的词作为提示信息，让模型了解当前在做的任务；同时，提示词文本的含义也对于模型的理解产生了一定的帮助。</p><p>除了单选阅读理解，这篇文章还关注了文本蕴含、多选阅读理解、指代销歧等共六个任务。对于不同的任务，有不同的提示信息与输入格式：</p><p>对于文本蕴含任务，可以将前提 (premise, ) 与假设 (hyphothesis, ) 通过提示信息整合，作者提出了两种整合方式：</p><p><img src="https://i.loli.net/2021/03/21/k1ul7icFX69IKnL.png" alt="image-20210321123241288" style="zoom: 33%;"></p><p>这样就只需要让模型预测 <mask> 属于描述集合 {yes, no, maybe} 中的哪一种，以此判断前提能否支撑假设。</mask></p><p>对于指代销歧任务，可以将句子 、带标记的介词 与名词 通过提示信息整合：</p><p><img src="https://i.loli.net/2021/03/21/sRZIGzn2Pg76mpd.png" alt="image-20210321123253166" style="zoom: 33%;"></p><p>这样就只需要让模型预测 <mask> ，以此判断介词是否指代名词。这里的描述集合是不受限制的，即让模型在 <mask> 处预测指代的名词 。</mask></mask></p><p>其他任务也采用类似的整合方式，感兴趣可以参考原文～</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者发现，这种使用提示信息的整合方式，在低资源的情况下对模型性能有非常大的提升！</p><p>比如在阅读理解任务的 BoolQ 数据集上，作者将使用提示信息整合的 finetune 方法与增加一层分类层的 finetune 方法进行了对比。下图是在使用不同数量的样本训练时，模型准确率的对比。</p><p><img src="https://i.loli.net/2021/03/21/31cqsF5Q7VoSeLX.png" alt="image-20210321123431996" style="zoom:50%;"></p><p>可以发现，在数据量比较小的时候，使用提示信息整合的 finetune 方法（黄色）比增加一层分类层的 finetune 方法（紫色）有更好的表现。</p><p>在某些任务上，这种表现的提升是惊人的：</p><p><img src="https://i.loli.net/2021/03/21/WCo9uUakm1rOH8N.png" alt="image-20210321123504144" style="zoom:50%;"></p><p>这是在指代销歧任务的 WSC 数据集上的实验结果。在水平方向看，<strong>仅使用 25 个样本，就达到传统 fintune 方法使用 300 个样本才能达到的效果！</strong></p><p>此外，作者还进行了一系列的消融实验，得到一些有意思的结论：</p><ol><li>模型通过预测 <mask> 属于描述集合中的哪种，以此完成任务。如果将这里改为不带语义的单纯的分类，性能也会有所下降。</mask></li><li>作者为每个任务都提供了多种整合提示信息的方式，但是发现，不同方式的区别对性能影响甚微。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇文章对基于提示信息的 finetune 方法在进行了大量实验，证明了这类方法在低资源的情况下性能大幅优于传统方法。这种 finetune 的思路应该是可以应用于各类 NLP 下游任务的。尤其是低资源场景下，应该会非常有帮助。如果老板真的只给几百条数据让训练模型，这样的方法说不定就有奇效！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载：&lt;a href=&quot;https://mp.weixin.qq.com/s/vX6o4lJKP4ajVfGSw8-tqg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://mp.weixin.qq.com/s/vX6o4lJKP4ajVfGS
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>[It’s Not Just Size That Matters] Small Language Models Are Also Few-Shot Learners</title>
    <link href="http://yoursite.com/2021/03/20/It%E2%80%99s-Not-Just-Size-That-Matters-Small-Language-Models-Are-Also-Few-Shot-Learners/"/>
    <id>http://yoursite.com/2021/03/20/It’s-Not-Just-Size-That-Matters-Small-Language-Models-Are-Also-Few-Shot-Learners/</id>
    <published>2021-03-20T02:51:52.000Z</published>
    <updated>2021-03-21T04:29:05.819Z</updated>
    
    <content type="html"><![CDATA[<p>转载：<a href="https://www.sohu.com/a/422484297_500659" target="_blank" rel="noopener">https://www.sohu.com/a/422484297_500659</a></p><p>显然，这标题对标的就是 GPT-3，于是笔者饶有兴趣地点进去看看是谁这么有勇气挑战 GPT-3，又是怎样的小模型能挑战 GPT-3？经过阅读，原来作者提出通过适当的构造， <strong>用 BERT 的 MLM 模型</strong>也可以做小样本学习，看完之后颇有一种“原来还可以这样做”的恍然大悟感。在此与大家分享一下。</p><h2 id="冉冉升起的MLM"><a href="#冉冉升起的MLM" class="headerlink" title="冉冉升起的MLM"></a><strong>冉冉升起的MLM</strong></h2><p>MLM，全称“Masked Language Model”，可以翻译为“掩码语言模型”，实际上就是一个完形填空任务，随机 Mask 掉文本中的某些字词，然后要模型去预测被 Mask 的字词，示意图如下：</p><p><img src="https://i.loli.net/2021/03/20/izGapk4S6ZRqgFI.png" alt="img" style="zoom:50%;"></p><p>▲ BERT的MLM模型简单示意图</p><p>其中被 Mask 掉的部分，可以是直接随机选择的 Token，也可以是随机选择连续的能组成一整个词的 Token，后者称为 WWM（Whole Word Masking）。</p><p>开始，MLM 仅被视为 BERT 的一个预训练任务，训练完了就可以扔掉的那种，因此有一些开源的模型干脆没保留 MLM 部分的权重，比如 brightmart 版 [3] 和 clue 版 [4] 的 RoBERTa，而哈工大开源的 RoBERTa-wwm-ext-large [5]则不知道出于什么原因随机初始化了 MLM 部分的权重，因此如果要复现本文后面的结果，这些版本是不可取的。</p><p>然而，随着研究的深入，研究人员发现不止 BERT 的 Encoder 很有用，预训练用的 MLM 本身也很有用。</p><p>比如论文 <strong>BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model</strong>[6]指出 MLM 可以作为一般的生成模型用，论文 <strong>Spelling Error Correction with Soft-Masked BERT</strong>[7] 则将 MLM 用于文本纠错。</p><p>笔者之前在 的实验也表明 MLM 的预训练权重也可以当作 UniLM 来用做 Seq2Seq 任务，还有一文将 MLM 的思想用于无监督分词和句法分析了。可以说 MLM 已经是大放异彩了。</p><h2 id="将任务转成完形填空"><a href="#将任务转成完形填空" class="headerlink" title="将任务转成完形填空"></a><strong>将任务转成完形填空</strong></h2><p>在本文里，我们再学习 MLM 的一个精彩应用：用于小样本学习或半监督学习，某些场景下甚至能做到零样本学习。</p><p>怎么将我们要做的任务跟 MLM 结合起来呢？很简单， <strong>给任务一个文本描述，然后转换为完形填空问题</strong>即可。举个例子，假如给定句子“这趟北京之旅我感觉很不错。”，那么我们补充个描述，构建如下的完形填空：</p><blockquote><p> <strong>__</strong>满意。这趟北京之旅我感觉很不错。</p></blockquote><p>进一步地，我们限制空位处只能填一个“很”或“不”，问题就很清晰了，就是要我们根据上下文一致性判断是否满意，如果“很”的概率大于“不”的概率，说明是正面情感倾向，否则就是负面的，这样我们就将<strong>情感分类问题</strong>转换为一个完形填空问题了，它可以用 MLM 模型给出预测结果，而 MLM 模型的训练可以不需要监督数据，因此理论上这能够实现零样本学习了。</p><p><strong style="color:blue;">多分类问题</strong>也可以做类似转换，比如<strong>新闻主题分类</strong>，输入句子为“八个月了，终于又能在赛场上看到女排姑娘们了。”，那么就可以构建：</p><blockquote><p> 下面播报一则<strong>__</strong>新闻。八个月了，终于又能在赛场上看到女排姑娘们了。</p></blockquote><p>这样我们就将新闻主题分类也转换为完形填空问题了，一个好的 MLM 模型应当能预测出“体育”二字来。</p><p>还有一些<strong style="color:blue;">简单的推理任务</strong>也可以做这样的转换，常见的是给定两个句子<strong>，判断这两个句子是否相容</strong>，比如“我去了北京”跟“我去了上海”就是矛盾的，“我去了北京”跟“我在天安门广场”是相容的，常见的做法就是将两个句子拼接起来输入到模型做，作为一个二分类任务。如果要转换为完形填空，那该怎么构造呢？一种比较自然的构建方式是：</p><blockquote><p>我去了北京？<strong>__</strong>，我去了上海。</p><p>我去了北京？<strong>__</strong>，我在天安门广场。</p><p>其中空位之处的候选词为 是 的 不 是 。</p></blockquote><h2 id="Pattern-Exploiting-Training"><a href="#Pattern-Exploiting-Training" class="headerlink" title="Pattern-Exploiting Training"></a><strong>Pattern-Exploiting Training</strong></h2><p>读到这里，读者应该不难发现其中的规律了，就是给输入的文本增加一个前缀或者后缀描述，并且 Mask 掉某些 Token，转换为完形填空问题，这样的转换在原论文中称为 <strong>Pattern</strong>，这个转换要尽可能与原来的句子组成一句自然的话，不能过于生硬，因为预训练的 MLM 模型就是在自然语言上进行的。</p><p>显然同一个问题可以有很多不同的 Pattern，比如情感分类的例子，描述可以放最后，变成“这趟北京之旅我感觉很不错。<strong><strong>满意。”；也可以多加几个字，比如“觉得如何？</strong></strong>满意。这趟北京之旅我感觉很不错。”。</p><p>然后，我们需要构建预测 Token 的候选空间，并且建立 Token 到实际类别的映射，这在原论文中称为 <strong>Verbalizer</strong>，比如情感分类的例子，我们的候选空间是 很 不 ，映射关系是 很 正 面 不 负 面 ，候选空间与实际类别之间不一定是一一映射，比如我们还可以加入“挺”、“太”、“难”字，并且认为 很 挺 太 正 面 以 及 不 难 负 面 ，等等。</p><p>不难理解，不少 NLP 任务都有可能进行这种转换，但显然这种转换一般只适用于 <strong>候选空间有限</strong>的任务，说白了就是只用来做 <strong>选择题</strong>，常见任务的就是 <strong>文本分类</strong>。</p><p>刚才说了，同一个任务可以有多种不同的 Pattern，原论文是这样处理的：</p><ol><li><p>对于每种 Pattern，单独用训练集 Finetune一个 MLM 模型出来；</p></li><li><p>然后将不同 Pattern对应的模型进行集成，得到融合模型；</p></li><li><p>用融合模型预测未标注数据的伪标签；</p></li><li><p>用伪标签数据 Finetune 一个常规的（非 MLM 的）模型。</p></li></ol><p>具体的集成方式大家自己看论文就行，这不是重点。这种训练模式被称为 <strong>Pattern-Exploiting Training（PET）</strong>，它首先出现在论文 <strong>Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</strong>。</p><p><strong style="color:blue;">yaya: 与 这篇论文思想很类似：How Many Data Points is a PromptWorth? （arXiv: 2103.08493v1）</strong></p><p>本文要介绍的这篇论文则进一步肯定和完善了 Pattern-Exploiting Training 的价值和结果，并整合了多任务学习，使得它在 SuperGLUE 榜单上的小样本学习效果超过了 GPT3。两篇论文的作者是相同的，是一脉相承的作品。</p><p><img src="https://i.loli.net/2021/03/20/dOMZNLsKykIDioQ.png" alt="img"></p><p>▲ PET在SuperGLUE上的小样本学习的结果</p><p>不过要吐槽一个点是，上图中 PET 的 223M 参数，所用的模型是 ALBERT-xxlarge-v2，事实上称 ALBERT 为“小模型”是一种很耍流氓的行为，因为它前向计算的速度并没有得到任何提升。ALBERT-xxlarge 共有 12 层，层与层之间参数是共享的，就前向计算而言，它应该等价于约 2700M（12 倍）参数的 GPT 才对。</p><h2 id="中文实践，检验效果"><a href="#中文实践，检验效果" class="headerlink" title="中文实践，检验效果"></a><strong>中文实践，检验效果</strong></h2><p>要真正确认一个方法或模型的价值，看论文的实验表格是不够的，论文给出的实验结果谁都不好说能否复现，其次就算英文上能复现也不代表中文上有价值，因此最实际的还是亲自动手做实验验证。下面是笔者的实验代码，供读者参考：</p><p>Github 地址：</p><p><a href="https://github.com/bojone/Pattern-Exploiting-Training" target="_blank" rel="noopener">https://github.com/bojone/Pattern-Exploiting-Training</a></p><p>我们将从以下几个角度来探讨 PET 的可行性：</p><p>\1. 直接利用现成的 MLM 模型效果如何？ <strong>（零样本学习1）</strong></p><p>\2. 用“大量无标签数据”微调现成的 MLM 模型效果如何？ <strong>（零样本学习2）</strong></p><p>\3. 用“小量标签数据”微调现成的 MLM 模型效果如何？ <strong>（小样本学习）</strong></p><p>\4. 用“小量标签数据+大量无标签数据”微调现成的MLM模型效果如何？ <strong>（半监督学习）</strong></p><p>下面主要给出 <strong>情感二分类</strong>的实验结果。另外还有一个新闻主题的多分类，代码也放到 Github 了，其结果是类似的，就不重复陈述了。</p><h3 id="4-1-零样本学习1"><a href="#4-1-零样本学习1" class="headerlink" title="4.1 零样本学习1"></a><strong>4.1 零样本学习1</strong></h3><p>这里主要探索的是给输入文本补上对应的 Pattern 后，直接基于现成的 MLM 模型进行预测，预测的准确率。由于构建模型的整个过程都不涉及到标签数据监督训练，因此这算是一种“零样本学习”。我们需要比较的是不同 Pattern、不同 MLM 模型上的效果：</p><p>下面是实验的几个 Pattern，其中空位处候选词语都为“很”和“不”：</p><p>P1：____满意。这趟北京之旅我感觉很不错。</p><p>P2：这趟北京之旅我感觉很不错。____满意。</p><p>P3：____好。这趟北京之旅我感觉很不错。</p><p>P4：____理想。这趟北京之旅我感觉很不错。</p><p>P5：感觉如何？____满意。这趟北京之旅我感觉很不错。</p><p>至于 MLM 模型，则是下面几个：</p><p>M1：Google 开源的中文版 BERT Base：</p><p><a href="https://github.com/google-research/bert" target="_blank" rel="noopener">https://github.com/google-research/bert</a></p><p>M2：哈工大开源的 RoBERTa-wwm-ext Base：</p><p><a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">https://github.com/ymcui/Chinese-BERT-wwm</a></p><p>M3：腾讯 UER 开源的 BERT Base：</p><p><a href="https://share.weiyun.com/5QOzPqq" target="_blank" rel="noopener">https://share.weiyun.com/5QOzPqq</a></p><p>M4：腾讯 UER 开源的 BERT Large：</p><p><a href="https://share.weiyun.com/5G90sMJ" target="_blank" rel="noopener">https://share.weiyun.com/5G90sMJ</a></p><p>实验结果如下表（验证集/测试集）：</p><p><img src="https://i.loli.net/2021/03/20/8Yg2qH5CXoFPkcx.png" alt="img"></p><p>可以观察到，不同的 Pattern、不同的预训练模型之间还是有一定的差异的，整体而言 Large 版本的效果要明显好于 Base 版本的模型，说明像 GPT 到 GPT2 再到 GPT3 一样，还是把模型做得更大会更好。</p><p>此外，这还有可能说明实际上 MLM 还没有被充分训练好，或许是因为 BERT 这种 Mask 掉一部分的训练方式过于低效了，可能用 修改 Transformer 结构，设计一个更快更好的 MLM 模型 一文提到的改进版 MLM 会更好。</p><h3 id="4-2-零样本学习2"><a href="#4-2-零样本学习2" class="headerlink" title="4.2 零样本学习2"></a><strong>4.2 零样本学习2</strong></h3><p>看完上述结果，读者可能会想到：如果我用领域内的数据继续预训练 MLM 模型，那么能不能提升效果呢？答案是：能！下面是我们的实验结果，算力有限，我们只在 RoBERTa-wwm-ext（上述的 M2，继续预训练后的模型我们称为 M2+ 无监督）的基础上做了比较：</p><p><img src="https://i.loli.net/2021/03/20/Y8CdHbW1cjqByrD.png" alt="img"></p><p>要注意的是，这里我们只是用领域内的数据继续做 MLM 训练，这个过程是无监督的，也不需要标注信号，因此也算是“零样本学习”。同时，从到目前为止的结果我们可以看出，给输入本文加入“前缀”的效果比“后缀”更有优势一些。</p><h3 id="4-3-小样本学习"><a href="#4-3-小样本学习" class="headerlink" title="4.3 小样本学习"></a><strong>4.3 小样本学习</strong></h3><p>刚才我们讨论了无标签数据继续预训练 MLM 的提升，如果回到 PET 的目标场景，直接用小量的标签数据配合特定的 Pattern 训练 MLM 又如何呢？</p><p>这也就是真正的“小样本学习”训练了，这里我们保留约 200 个标注样本，构造样本的时候，我们先给每个句子补上 Pattern，除了 Pattern 自带的 Mask 位置之外，我们还随机 Mask 其他一部分，以增强对模型的正则。最终实验结果如下：</p><p><img src="https://i.loli.net/2021/03/20/qVFgDbyLQrXpZUt.png" alt="img"></p><p>结论就是除了“后缀式”的 P2 之外，其它结果都差不多，这进一步说明了“前缀式”的 Pattern 会比“后缀式”更有竞争力一些。在效果上，直接用同样的数据用常规的方法去微调一个 BERT 模型，大概的结果是 88.93 左右，所以基于 “MLP+Pattern” 的小样本学习方法可能带来轻微的性能提升。</p><h3 id="4-4-半监督学习"><a href="#4-4-半监督学习" class="headerlink" title="4.4 半监督学习"></a><strong>4.4 半监督学习</strong></h3><p>无监督的零样本学习和有监督的小样本学习都说完了，自然就轮到把标注数据和非标注数据都结合起来的“半监督学习”了。还是同样的任务，标注数据和非标注数据的比例大约是 1:99，标注数据带 Pattern，非标注数据不带 Pattern，大家都 Mask 掉一部分 Token 进行 MLM 预训练，最终测出来的效果如下：</p><p><img src="https://i.loli.net/2021/03/20/4QKcUYDtgAmeGqM.png" alt="img"></p><p>还是同样的，“后缀”明显比“前缀”差，“前缀”的效果差不多。具体效果上，则是肯定了额外的无标注数据也是有作用的。</p><p>直觉上来看，“前缀”比“后缀”要好，大体上是因为“前缀”的 Mask 位置比较固定，微弱的监督信号得以叠加增强？但这也不能解释为什么零样本学习的情况下也是“前缀”更好，估计还跟模型的学习难度有关系，可能句子前面部分的规律更加明显，相对来说更加容易学一些，所以前面部分就学习得更加充分？这一切都还只是猜测。</p><h3 id="4-5-汇总与结论"><a href="#4-5-汇总与结论" class="headerlink" title="4.5 汇总与结论"></a><strong>4.5 汇总与结论</strong></h3><p>将上述结果汇总如下：</p><p><img src="https://i.loli.net/2021/03/20/cpGqLZ2twR5z4di.png" alt="img"></p><p>读者还可以对比我们之前在文章 泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练 中用虚拟对抗训练（VAT）做半监督学习的结果，可以看到不管是零样本学习、小样本学习还是半监督学习，基于 MLM 模型的方式都能媲美基于 VAT 的半监督学习的结果。</p><p>我们在做短新闻多分类实验时的结果也是相似的。因此，这说明了 MLM 模型确实也可以作为一个优秀的零样本/小样本/半监督学习器来使用。</p><p>当然，基于 MLM 模型的缺点还是有的，比如 MLM 所使用的独立假设限制了它对更长文本的预测能力（说白了空位处的文字不能太长），以及无法预测不定长的答案也约束了它的场景（所以当前只能用于做选择题）。我们期待有更强的 MLM 模型出现，那时候就有可能在所有任务上都能与 GPT3 一较高下了。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文介绍了 BERT 的 MLM 模型的一个新颖应用：配合特定的描述将任务转化为完形填空，利用 MLM 模型做零样本学习、小样本学习和半监督学习。</p><p>在原论文的 SuperGLUE 实验里边，它能达到媲美 GPT3 的效果，而笔者也在中文任务上做了一些实验，进一步肯定了该思路的有效性。整个思路颇为别致，给人一种“原来还可以这样做”的恍然大悟感，推荐大家学习一下。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] <a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">https://arxiv.org/abs/2005.14165</a></p><p>[2] <a href="https://arxiv.org/abs/2009.07118" target="_blank" rel="noopener">https://arxiv.org/abs/2009.07118</a></p><p>[3] <a href="https://github.com/brightmart/roberta_zh" target="_blank" rel="noopener">https://github.com/brightmart/roberta_zh</a></p><p>[4] <a href="https://github.com/CLUEbenchmark/CLUEPretrainedModels" target="_blank" rel="noopener">https://github.com/CLUEbenchmark/CLUEPretrainedModels</a></p><p>[5] <a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">https://github.com/ymcui/Chinese-BERT-wwm</a></p><p>[6] <a href="https://arxiv.org/abs/1902.04094" target="_blank" rel="noopener">https://arxiv.org/abs/1902.04094</a></p><p>[7] <a href="https://kexue.fm/archives/7661" target="_blank" rel="noopener">https://kexue.fm/archives/7661</a></p><p>[8] <a href="https://arxiv.org/abs/2001.07676" target="_blank" rel="noopener">https://arxiv.org/abs/2001.07676</a></p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><p>不懂的问题：</p><ul><li><p>mask 一个 Span, 多个空位然后逐词预测？？</p></li><li><p>在 [MASK] 位置 预测空间是多大？整个vocabulary ??</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载：&lt;a href=&quot;https://www.sohu.com/a/422484297_500659&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.sohu.com/a/422484297_500659&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;显然
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>[All NLP Tasks Are Generation Tasks] A General Pretraining Framework</title>
    <link href="http://yoursite.com/2021/03/20/All-NLP-Tasks-Are-Generation-Tasks-A-General-Pretraining-Framework/"/>
    <id>http://yoursite.com/2021/03/20/All-NLP-Tasks-Are-Generation-Tasks-A-General-Pretraining-Framework/</id>
    <published>2021-03-20T01:53:44.000Z</published>
    <updated>2021-03-20T02:47:38.568Z</updated>
    
    <content type="html"><![CDATA[<h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p>目前基于预训练 的语言模型大致分为三类：</p><ul><li>autoregressive models (e.g.,GPT) 擅长长文本生成</li><li>autoencoding models (e.g., BERT) 擅长理解型任务，分类任务</li><li>encoder-decoder models (e.g., T5) 擅长基于条件的文本生成任务，比如 text summarize</li></ul><p>但是目前还未存在一个预训练框架可以在这三种任务上同时表现出优异的性能。这给模型的开发和选择带来了不便。</p><p>下表总结了不同的预训练框架可以处理的任务：</p><p><img src="https://i.loli.net/2021/03/20/ldKAem71a86xBcq.png" alt="image-20210320100920079" style="zoom: 25%;"></p><p>先前的工作试图通过多任务学习将各自的 objective 结合起来，从而统一不同的框架。但是，自回归和自编码的 objective 在本质上是不同的，简单的结合不能够充分的揭示所有框架的优势。</p><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>本文提出了一个新颖的预训练框架GLM（General Language Model）来解决这个问题。</p><ul><li><p>本文的GLM基于autoregressive blank-filling，<strong style="color:red;">遵循自动编码(auto-encoding)的思想，我们从输入文本中随机消除了令牌的连续跨度。并遵循自回归预训练(auto-regressive)的思想训练模型以重建跨度。</strong></p></li><li><p>为了在一个框架中同时学习双向和单向的注意力机制，本文将文本分成两部分，未掩码的部分可以互相关注。掩码的部分不可以关注后续的掩码的token。</p></li><li>本文还提出了一个 2D位置编码技术，来指示inter- and intra- span position information。</li></ul><p>因此，本文的框架 GLM在预训练过程中，可以同时学习上下文表达和自回归生成。</p><p><img src="https://i.loli.net/2021/03/20/VrzfCnOFQi27NRU.png" alt="image-20210320103954828" style="zoom: 33%;"></p><h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><p>本文的结构有三个主要的优势：</p><ul><li>在一个预训练模型上，可以在三种任务上都表现的很好。</li><li>由于 <strong style="color:blue;">pretrain-finetune consistency</strong>，在分类任务上，本文提出的模型相比 BERT-like models 性能更加优异。</li><li>可以自然的处理 <strong style="color:red;">variable-length blank filling</strong>，这对很多下游任务是很重要的。</li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;存在的问题&quot;&gt;&lt;a href=&quot;#存在的问题&quot; class=&quot;headerlink&quot; title=&quot;存在的问题&quot;&gt;&lt;/a&gt;存在的问题&lt;/h2&gt;&lt;p&gt;目前基于预训练 的语言模型大致分为三类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;autoregressive models (e
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Transformer</title>
    <link href="http://yoursite.com/2021/03/19/Transformer/"/>
    <id>http://yoursite.com/2021/03/19/Transformer/</id>
    <published>2021-03-19T10:58:13.000Z</published>
    <updated>2021-03-19T11:32:07.770Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/03/19/DrhRUENTcwXPo89.png" alt="image-20210319185913348" style="zoom:50%;"></p><p>Transformer模型中采用了 encoer-decoder 架构。论文中encoder层由6个encoder堆叠在一起，decoder层也一样。</p><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>对于encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。</p><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>decoder也包含encoder提到的两层网络，但是在这两层中间还有一层attention层，帮助当前节点获取到当前需要关注的重点内容。</p><p>Decoder 和 Encoder的结构差不多，但是多了一个attention的sub-layer，这里先明确一下decoder的输入输出和解码过程：</p><ul><li>输出：对应 $i$ 位置的输出词的概率分布</li><li>输入：encoder的输出 与 对应  $i-1$ 位置decoder的输出。所以中间的attention不是self-attention，它的<strong style="color:blue;">K，V来自encoder</strong>，<strong style="color:blue;">Q来自上一位置decoder的输出</strong></li><li><p>解码：这里要注意一下，训练和预测是不一样的。在训练时，解码是一次全部decode出来，用上一步的ground truth来预测（mask矩阵也会改动，让解码时看不到未来的token）；而预测时，因为没有ground truth了，需要一个个预测。</p><p>为了确保按照生成顺序：从左到右，使用sequence mask。</p><p>sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。</p><p>那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。</p></li></ul><p><img src="https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156846899939997439.gif" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/19/DrhRUENTcwXPo89.png&quot; alt=&quot;image-20210319185913348&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Transformer模型中采用了 en
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>On Semantic Similarity in Video Retrieval</title>
    <link href="http://yoursite.com/2021/03/19/On-Semantic-Similarity-in-Video-Retrieval/"/>
    <id>http://yoursite.com/2021/03/19/On-Semantic-Similarity-in-Video-Retrieval/</id>
    <published>2021-03-19T08:50:22.000Z</published>
    <updated>2021-03-19T08:50:22.671Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Non-Autoregressive Coarse-to-Fine Video Captioning</title>
    <link href="http://yoursite.com/2021/03/19/Non-Autoregressive-Coarse-to-Fine-Video-Captioning/"/>
    <id>http://yoursite.com/2021/03/19/Non-Autoregressive-Coarse-to-Fine-Video-Captioning/</id>
    <published>2021-03-19T08:49:05.000Z</published>
    <updated>2021-03-19T08:49:05.794Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[Less is More] CLIPBERT for Video-and-Language Learning via Sparse Sampling</title>
    <link href="http://yoursite.com/2021/03/18/Less-is-More-CLIPBERT-for-Video-and-Language-Learning-via-Sparse-Sampling/"/>
    <id>http://yoursite.com/2021/03/18/Less-is-More-CLIPBERT-for-Video-and-Language-Learning-via-Sparse-Sampling/</id>
    <published>2021-03-18T07:33:09.000Z</published>
    <updated>2021-03-18T07:33:45.556Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="end-to-end" scheme="http://yoursite.com/categories/cross-modal/end-to-end/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
      <category term="end-to-end" scheme="http://yoursite.com/tags/end-to-end/"/>
    
  </entry>
  
  <entry>
    <title>[LightningDOT] Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval</title>
    <link href="http://yoursite.com/2021/03/18/LightningDOT-Pre-training-Visual-Semantic-Embeddings-for-Real-Time-Image-Text-Retrieval/"/>
    <id>http://yoursite.com/2021/03/18/LightningDOT-Pre-training-Visual-Semantic-Embeddings-for-Real-Time-Image-Text-Retrieval/</id>
    <published>2021-03-18T06:02:17.000Z</published>
    <updated>2021-03-18T12:03:51.495Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-任务"><a href="#1-任务" class="headerlink" title="1. 任务"></a>1. 任务</h2><p>本文发表在 NAACL 2021，本文要研究的内容是如何提高 <strong>Image-text retrieval 任务的计算效率。</strong></p><h2 id="2-存在的问题"><a href="#2-存在的问题" class="headerlink" title="2. 存在的问题"></a>2. 存在的问题</h2><p>基于预训练的跨模态模型取得了很好的进展，但是在测试阶段存在<strong>推理速度慢</strong>的问题。 主要是由于Transformer 结构中的cross-modal attention 造成的巨大的计算消耗。 这种延迟以及计算消耗使其很难在实际中应用。</p><p>下图可视化了近年来 ITR task 的研究进展，（a） 早期，使用CNN和RNN分别提取视觉和语言特征，然后使用dot-product 来计算similarity。 （b）后来有人提出使用faster-RCNN 和 RNN 分别提取两个模态的特征，使用使用cross-attention，最后再计算相似性。（c）随着BERT的发展，有人使用BERT扩展出 V+L BERT 模型。(d) 由于cross-modal attention 是耗时的，因此，本文中提出去掉cross-modal 这个模块。</p><p><img src="https://i.loli.net/2021/03/18/fdU3lxwpkZo8yuG.png" alt="image-20210318143100485" style="zoom:50%;"></p><h2 id="3-本文的点"><a href="#3-本文的点" class="headerlink" title="3. 本文的点"></a>3. 本文的点</h2><ul><li>本文希望可以重新回归到 <strong>dot-product</strong> 这个简单的操作。本文中使用dot product 来做多模态融合，而不是使用计算量的self-attention。同时，为了利用有效的多模态嵌入学习，本文在两个encoder上都使用 [CLS] token。</li><li><p>通过消除模态之间耗时的交叉注意力，该模型可以在推理过程中学习视觉语义嵌入而无需在每个图像-文本对之间进行广泛匹配。此外，通过消除对图像-文本对的实时计算的依赖，我们可以一次<strong>离线地独立地</strong>计算所有图像和文本嵌入，并将这些嵌入重新用作新查询的<strong>缓存索引</strong>。</p></li><li><p>LightningDOT通过预先训练<strong>三个新颖的学习目标</strong>：Visual-embedding fused MLM (namely VMLM), Semantic-embedding  fused MRM (namely SMRM) and a cross-modal retrieval objective (namely CMR).</p><p>前两个预训练任务（VMLM 和 SMRM）是为了确保跨模态信息可以被获取到。CMR是为了鼓励模型在预训练阶段获得多模态融合。</p></li><li><p>重新排名（re-ranking）机制</p></li></ul><p>Note: 本文不是从模型压缩的角度来解决问题。</p><h2 id="4-贡献"><a href="#4-贡献" class="headerlink" title="4. 贡献"></a>4. 贡献</h2><p>提出了一个简单有效的方法，在不牺牲accuracy 的情况下，LightningDOT 可以数千倍的加速推理时间。</p><p>我们的工作是在基于预训练视觉语义嵌入，实现低延迟的实时跨模式检索的第一个已知工作。</p><h2 id="5-Method"><a href="#5-Method" class="headerlink" title="5. Method"></a>5. Method</h2><p><img src="https://i.loli.net/2021/03/18/kASuJa8xc6hYFne.png" alt="image-20210318175408610" style="zoom: 67%;"></p><p>在本节中，我们介绍LightningDOT框架，该框架由两个深层的Transformer作为图像和语言编码器。我们首先介绍三个预训练任务，然后介绍从<strong>离线特征提取</strong>到<strong>在线即时检索</strong>的 <strong>推理流程（inference pipline）</strong>。</p><p>图像编码器得到region features: $f_{\theta_{V}}(\mathbf{v})=\mathbf{h}=\left\{\mathbf{h}_{0}, \ldots, \mathbf{h}_{N}\right\}\left(\mathbf{h}_{j} \in \mathbb{R}^{d}\right)$</p><p>语言编码器得到token representations: $f_{\theta_{L}}(\mathbf{w})=\mathbf{z}=\left\{\mathbf{z}_{0}, \ldots, \mathbf{z}_{T}\right\}\left(\mathbf{z}_{j} \in \mathbb{R}^{d}\right)$</p><p>regard the output [CLS] embedding <strong><strong style="color:red;">$h_0$</strong> as global image representation</strong>, and <strong><strong style="color:red;">$z_0$</strong>as global text representation</strong></p><h3 id="5-1-Model-Pre-training"><a href="#5-1-Model-Pre-training" class="headerlink" title="5.1 Model Pre-training"></a>5.1 Model Pre-training</h3><h4 id="Visual-embedding-Fused-Masked-Language-Modeling-VMLM"><a href="#Visual-embedding-Fused-Masked-Language-Modeling-VMLM" class="headerlink" title="Visual-embedding Fused Masked Language Modeling (VMLM)"></a>Visual-embedding Fused Masked Language Modeling (VMLM)</h4><p>设有M个 masked tokens</p><p>对于 sentence $t$ and image $i$ ， The loss function of VMLM can be formulated as:</p><p>$\mathcal{L}_{\mathrm{VMLM}}(t, i)=-\log P_{\theta}\left(\mathbf{w}_{\mathbf{m}} \mid \mathbf{w}_{\backslash \mathbf{m}}, i\right)$<br>$=-\frac{1}{M} \sum_{k=1}^{M} \log P_{\theta_{\mathrm{mlm}}}\left(\mathbf{w}_{\mathbf{m}_{k}} \mid \mathbf{z}_{\mathbf{m}_{k}}+\mathbf{h}_{0}\right)$</p><p>其中 $z$ 是 hidden state。</p><p>Note： 这里的 +$h_0$ 是显式的加和，而不是使用cross-modal attention.</p><h4 id="Semantic-embedding-Fused-Masked-Region-Modeling-SMRM"><a href="#Semantic-embedding-Fused-Masked-Region-Modeling-SMRM" class="headerlink" title="Semantic-embedding Fused Masked Region Modeling (SMRM)"></a>Semantic-embedding Fused Masked Region Modeling (SMRM)</h4><p>$\mathcal{L}_{\mathrm{SMRM}}(i, t)=\mathcal{D}_{\theta_{\mathrm{mrm}}}\left(\mathbf{v}_{\mathbf{m}}, f_{\theta_{V}}\left(\mathbf{v}_{\backslash \mathbf{m}}\right), t\right)$<br>$=\frac{1}{M} \sum_{k=1}^{M} \mathcal{D}_{\theta_{\mathrm{mrm}}}\left(\mathbf{v}_{\mathbf{m}_{k}}, \mathbf{h}_{\mathbf{m}_{k}}+\mathbf{z}_{0}\right)$</p><p>这里的 $\mathcal{D}_{\theta_{\mathrm{mrm}}}$ 代表两个损失，一个是使用L2 distance 的 掩码区域特征回归，另外一个是用KL散度的掩码区域分类。</p><h4 id="Cross-modal-Retrieval-Objective-CMR"><a href="#Cross-modal-Retrieval-Objective-CMR" class="headerlink" title="Cross-modal Retrieval Objective (CMR)"></a>Cross-modal Retrieval Objective (CMR)</h4><p>The similarity score between query t and image i is defined as:</p><p>$S(t, i)=\left\langle\mathbf{z}_{0}, \mathbf{h}_{0}\right\rangle$</p><p>损失函数：</p><p>$\mathcal{L}_{\mathrm{IR}}^{(t)}=-\log \frac{e^{S\left(t, i_{1}\right)}}{\sum_{k=1}^{n} e^{S\left(t, i_{k}\right)}}$</p><p>$\mathcal{L}_{\mathrm{TR}}^{(i)}=-\log \frac{e^{S\left(i, t_{1}\right)}}{\sum_{k=1}^{n} e^{S\left(i, t_{k}\right)}}$</p><p>$\mathcal{L}_{\mathrm{CMR}}(B)=\frac{1}{2 n} \sum_{k=1}^{n} \mathcal{L}_{\mathrm{TR}}^{\left(i_{k}\right)}+\mathcal{L}_{\mathrm{IR}}^{\left(t_{k}\right)}$</p><h3 id="5-2-Real-time-Inference"><a href="#5-2-Real-time-Inference" class="headerlink" title="5.2 Real-time Inference"></a>5.2 Real-time Inference</h3><p>以text-to-image retrieval 作为样例来介绍 real-time inference pipline：</p><p>（1）离线图片特征提取与编码；（2）text query 在线检索；（3）使用top-retrieval images 做在线重拍</p><h4 id="Offline-Feature-Extraction"><a href="#Offline-Feature-Extraction" class="headerlink" title="Offline Feature Extraction"></a>Offline Feature Extraction</h4><p>首先使用 image encoder 来处理数据集中的所有图片，并存储其 global image representation 进入索引的内存中供以后使用。</p><p>整个image-to-index 过程，包括 faster rcnn 提取特征 以及 image transformer encoder 都是离线处理的。</p><h4 id="Online-Retrieval"><a href="#Online-Retrieval" class="headerlink" title="Online Retrieval"></a>Online Retrieval</h4><p>对于 text query, 使用language encoder 提取特征，然后依次计算与每个图片的相似度。图片将会被排序。实际中，人们感兴趣的是前top-k 检索结果。</p><p>使用FAISS来优化检索。</p><p>类似地，对于文本检索，可以通过简单地为所有句子预先计算嵌入并使用图像作为查询来应用相同的体系结构</p><h4 id="Re-ranking"><a href="#Re-ranking" class="headerlink" title="Re-ranking"></a>Re-ranking</h4><p>为了进一步提高检索结果，本文通过采用可选的<strong>重新排名模型</strong>提出了一种两阶段方法。</p><p>第一阶段，使用LightingDOT来检索 top-M images(or texts)。</p><p>第二阶段，使用一个性能更好的检索模型（通常比较慢）来重新排序从第一阶段检索到的 top-M pairs.</p><p>实验证明，可以同时从性能和效率两方面受益。</p><h2 id="6-Experiments"><a href="#6-Experiments" class="headerlink" title="6. Experiments"></a>6. Experiments</h2><h3 id="6-1-Results-on-Flickr30K-and-COCO"><a href="#6-1-Results-on-Flickr30K-and-COCO" class="headerlink" title="6.1 Results on Flickr30K and COCO"></a>6.1 Results on Flickr30K and COCO</h3><p><img src="https://i.loli.net/2021/03/18/TpcoUY7Zjel39dM.png" alt="image-20210318175824380" style="zoom: 50%;"></p><ul><li><p>在仅使用一阶段排序的情况下：</p><ul><li>相比于不使用预训练的模型，性能上有显著提升 CAAN (SOTA method with cross-attention）</li><li>与使用预训练的模型相比，UNITER，性能上仅下降了一点，但是速度上有600/1900倍的提升(Flickr30K/COCO)</li></ul></li><li><p>使用两阶段排序：</p><ul><li>性能上相比于一阶段有提升，同时比单纯的UNITER模型有 46-95倍速度的提升，</li></ul></li></ul><h3 id="6-2-Speed-amp-Space-Improvement"><a href="#6-2-Speed-amp-Space-Improvement" class="headerlink" title="6.2 Speed &amp; Space Improvement"></a>6.2 Speed &amp; Space Improvement</h3><ul><li><p>检索图像，比较推理速度差异</p><p>以 UNITER_base 作为比较对象。</p><p>SCAN，是一个不使用预训练的模型，但是采用了cross-modal attention.</p><p><img src="https://i.loli.net/2021/03/18/umlFqkgLfyKQ6Cx.png" alt="image-20210318180604083" style="zoom: 33%;"></p></li><li><p>扩大搜索池，性能仍然很好</p><p><img src="https://i.loli.net/2021/03/18/jelLQRdcFwWpaEU.png" alt="image-20210318180918374"></p></li></ul><h3 id="6-3-Ablation-Studies"><a href="#6-3-Ablation-Studies" class="headerlink" title="6.3 Ablation Studies"></a>6.3 Ablation Studies</h3><ul><li><p>观察各个模块的作用</p><p>(1) 【R-CNN only】不使用 image encoder, 直接使用 faster rcnn 提取的特征</p><p>(2)【 “+Image Encoder”】</p><p>(3)【+PT】 MLM+MRM+CMR 上预训练， 注意本文采用的预训练方案是 VMLM+SMRM+CMR</p><p><img src="https://i.loli.net/2021/03/18/ewAYK8M6idSBvEb.png" alt="image-20210318181810013" style="zoom: 33%;"></p><p><strong style="color:blue;">yaya: 其实，本文提出的预训练任务带来的提升并不明显。</strong></p></li><li><p>观察各个预训练任务的作用</p><p><img src="https://i.loli.net/2021/03/18/OkTcr9IVbxYBPgu.png" alt="image-20210318182135676" style="zoom: 50%;"></p><p>预训练任务对于本文提出的模型是有提升的，但是，提升的显著性似乎没有那么大。</p><p><strong style="color:blue;">yaya: 奇怪，为什么 这个 PT(ALL) 与 上个表Table 4 中的LightingDOT结果 不一致呢都？都是在Flickr30k validation上的结果</strong></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-任务&quot;&gt;&lt;a href=&quot;#1-任务&quot; class=&quot;headerlink&quot; title=&quot;1. 任务&quot;&gt;&lt;/a&gt;1. 任务&lt;/h2&gt;&lt;p&gt;本文发表在 NAACL 2021，本文要研究的内容是如何提高 &lt;strong&gt;Image-text retrieval 
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="Image-Text Retrieval" scheme="http://yoursite.com/categories/cross-modal/Image-Text-Retrieval/"/>
    
      <category term="real time" scheme="http://yoursite.com/categories/cross-modal/Image-Text-Retrieval/real-time/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
      <category term="Image-Text Retrieval" scheme="http://yoursite.com/tags/Image-Text-Retrieval/"/>
    
      <category term="real time" scheme="http://yoursite.com/tags/real-time/"/>
    
  </entry>
  
  <entry>
    <title>Improving Translation Robustness with Visual Cues and Error Correction</title>
    <link href="http://yoursite.com/2021/03/17/Improving-Translation-Robustness-with-Visual-Cues-and-Error-Correction/"/>
    <id>http://yoursite.com/2021/03/17/Improving-Translation-Robustness-with-Visual-Cues-and-Error-Correction/</id>
    <published>2021-03-17T07:55:13.000Z</published>
    <updated>2021-03-17T08:33:45.221Z</updated>
    
    <content type="html"><![CDATA[<h2 id="本文研究的任务"><a href="#本文研究的任务" class="headerlink" title="本文研究的任务"></a>本文研究的任务</h2><p>多模态机器翻译任务中对噪声样本的鲁棒性。</p><p>神经机器翻译模型对输入噪声很脆弱。当前的鲁棒性技术大多使模型<strong>适应</strong>现有的嘈杂文本，但是这些模型通常在<strong>遇到看不见的噪声</strong>时会失效，并且在clean  text 上的性能会下降（即相比于那些普通的模型，使用噪声样本来扩充数据的模型，其在clean text 上的性能会下降）。</p><h2 id="本文提出的点"><a href="#本文提出的点" class="headerlink" title="本文提出的点"></a>本文提出的点</h2><p>（1） 模型上：引入了<strong><em>视觉上下文</em></strong>的概念，以提高针对嘈杂文本的翻译鲁棒性。</p><p>（2）多任务：通过<strong>将纠错作为辅助任务</strong>来提出一种新的<strong><em>纠错训练</em>方案</strong>，以进一步提高鲁棒性。</p><p>实验证明，在 English-French and English-German 翻译任务上，（1）对于训练中遇到的噪声以及未遇到的噪声都有很好的鲁棒性。（2）同时保持了在 clean text 上的翻译质量。</p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>不是重点来做 MMT model 的，略过</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;本文研究的任务&quot;&gt;&lt;a href=&quot;#本文研究的任务&quot; class=&quot;headerlink&quot; title=&quot;本文研究的任务&quot;&gt;&lt;/a&gt;本文研究的任务&lt;/h2&gt;&lt;p&gt;多模态机器翻译任务中对噪声样本的鲁棒性。&lt;/p&gt;
&lt;p&gt;神经机器翻译模型对输入噪声很脆弱。当前的鲁棒
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="image-guided MT" scheme="http://yoursite.com/categories/cross-modal/image-guided-MT/"/>
    
    
      <category term="cross-modal,image-guided MT" scheme="http://yoursite.com/tags/cross-modal-image-guided-MT/"/>
    
  </entry>
  
  <entry>
    <title>[VisualSparta] Sparse Transformer Fragment-level Matching for Large-scale Text-to-Image Search</title>
    <link href="http://yoursite.com/2021/03/16/VisualSparta-Sparse-Transformer-Fragment-level-Matching-for-Large-scale-Text-to-Image-Search/"/>
    <id>http://yoursite.com/2021/03/16/VisualSparta-Sparse-Transformer-Fragment-level-Matching-for-Large-scale-Text-to-Image-Search/</id>
    <published>2021-03-16T03:38:41.000Z</published>
    <updated>2021-03-20T03:24:21.590Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-任务"><a href="#1-任务" class="headerlink" title="1. 任务"></a>1. 任务</h2><p>本文是做跨模态检索问题。文本到图像的检索是多模态信息检索中的一项基本任务，即在给定文本查询的情况下从大型且未标记的图像数据集中检索相关图像。</p><h2 id="2-存在的问题"><a href="#2-存在的问题" class="headerlink" title="2. 存在的问题"></a>2. 存在的问题</h2><p>图文检索问题上存在两个核心挑战：<strong style="color:red;">准确率以及速度</strong>。</p><h2 id="3-本文提出的点"><a href="#3-本文提出的点" class="headerlink" title="3. 本文提出的点"></a>3. 本文提出的点</h2><p>在本文中，提出了基于transformer 的 VisualSparta 模型，这是一种新颖的文本到图像检索模型，该模型在准确性和效率上都比现有模型显著提高。</p><p>本文提出的模型关注点在于两点：</p><p>（1）准确率，学习query tokens 与 image regions之间的细粒度关系，以丰富跨模态理解。</p><p>（2）有效性，独立的学习query 和 answer（image）的特征表示，从而使得模型可以<strong><strong style="color:red;">离线的</strong>索引所有的candidate images</strong>。整个VisualSparta 模型可以作为一个经典的反向索引（Inverted index）搜索引擎，以实现高效搜索。</p><h2 id="4-本文的贡献"><a href="#4-本文的贡献" class="headerlink" title="4. 本文的贡献"></a>4. 本文的贡献</h2><p>1) 性能优势：提出了一个新的基于片段交互（fragment-level interaction）的图文检索模型，并取得了SOTA的性能；</p><p>2) 速度优势：相比于标准的向量搜索，VisualSparta 有391x 速度提升。且实验证明，由于VisualSparta 可以有效的进行<strong>反向索引</strong> ，因此对于更大的数据集，速度优势会更加的明显，</p><p>3) 第一：VisualSparta 是<strong>第一个</strong>可以在大规模数据集上实现<strong>实时搜索</strong>的，基于transformer的 text-to-image retrieval model，并且实现了显著的性能提升。本文的方法证明了large pretrained model 也可以占用<strong>较少的内存和较少的计算时间</strong>。</p><p>4) 对当前存在的 text-to-image retrieval models 进行了 accuracy-latency comparisons。</p><h2 id="5-Method"><a href="#5-Method" class="headerlink" title="5. Method"></a>5. Method</h2><p>本文提出了 <strong>VisualSparta retriever</strong>, a fragment-level transformer-based model for efficient text-image matching.</p><p><img src="https://i.loli.net/2021/03/16/ZTKckhILpCEBv19.png" alt="image-20210316192344412"></p><h3 id="5-1-Query-representation"><a href="#5-1-Query-representation" class="headerlink" title="5.1 Query representation"></a>5.1 Query representation</h3><p>在检索中，<strong style="color:red;">query 的处理是一个在线操作</strong>。需要很好的考虑query 编码的效率。以前的方法，使用bi-RNN来处理 query sentence，为每个token获得上下文特征表示。</p><p>本文中，不采用序列处理的方式。丢掉query中的顺序信息，仅仅使用预训练的word embedding 来表征每个token。<strong>这种方法可以使得每个token的特征表达损失独立的，与上下文无关的</strong>。同时这种方式对于高效的indexing and inference 是必要的。</p><p>a query is represented as $\hat{w}=\left\{\hat{w}_{1}, \ldots, \hat{w}_{m}\right\}$</p><h3 id="5-2-Visual-Representation"><a href="#5-2-Visual-Representation" class="headerlink" title="5.2 Visual Representation"></a>5.2 Visual Representation</h3><p>相比于 query 需要实时在线处理，answer candidates 可以在 query 到来之前离线编制索引 (indexed offline)。因此，answer candidates 的处理可以更加丰富和复杂。因此，本文 follow OSCAR的工作，对于answer candidates 本文提取其上下文特征。</p><p>具体的看上图.</p><p>$H_{\text {image }} \in \mathbb{R}^{(n+k) \times d_{H}}$ is the final contextualized representation for one answer.</p><h3 id="5-3-Scoring-Function"><a href="#5-3-Scoring-Function" class="headerlink" title="5.3 Scoring Function"></a>5.3 Scoring Function</h3><p>第一个等式：学习 image element 和 每个query token 之间的fragment-level 交互。</p><p>$y_{i} =\max _{j \in[1, n+k]}\left(\hat{w}_{i}^{T} h_{j}\right) $              <strong>（equation 10）</strong></p><p>第二个等式：经过一个 ReLu 和 可训练的bias来得到sparse embedding。</p><p>$ \phi\left(y_{i}\right) =\operatorname{ReLU}\left(y_{i}+b\right) $              <strong>（equation 11）</strong></p><p>第三个等式：对于所有的分数求和，并为了抑制过大的分数，使用log operation</p><p>$ f(q, v) =\sum_{i=0}^{m} \log \left(\phi\left(y_{i}\right)+1\right) $             <strong>（equation 12）</strong></p><h3 id="5-4-Retriever-Training"><a href="#5-4-Retriever-Training" class="headerlink" title="5.4 Retriever Training"></a>5.4 Retriever Training</h3><p>最小化如下目标：</p><p>$J=f\left(q, v^{+}\right)-\log \sum_{k \in K^{-}} e^{\left.f\left(q, v_{k}\right)\right)}$</p><p><strong style="color:blue;"><strong>yaya: 这个损失函数其实与正常的NCE损失不同</strong></strong></p><p>负样本的选择：从相同batch 中的其他image samples作为负样本。</p><p><strong>而且本文发现，相比于一些复杂的负样本选择策略（比如，使用有相近标签的相似图像作为负样本），这种负样本的选择策略是简单有效地，效果相当。</strong></p><p><strong style="color:blue;">yaya: 为什么这种选择策略比复杂的策略是有效的？？是不是在不同的场合，应该使用不同的策略呢？？</strong></p><h3 id="5-5-Efficient-Indexing-and-Inference"><a href="#5-5-Efficient-Indexing-and-Inference" class="headerlink" title="5.5 Efficient Indexing and Inference"></a>5.5 Efficient Indexing and Inference</h3><p><strong style="color:red;">real-time inference</strong></p><p>定义 testing query 为 $q=\left[w_{0}, \ldots w_{m}\right]$</p><p>the <strong>ranking score</strong> between $q$ and an image is （利用5.3 中第二个等式得到的 sparse embedding）:</p><p>​    $\operatorname{CACHE}(w, v)=\log ($ sparse embedding $) \quad w \in W $             <strong>（equation 14）</strong></p><p>​    $f(q, v)=\sum_{i=1}^{m} \operatorname{CACHE}\left(w_{i}, v\right)$             <strong>（equation 15）</strong></p><p>由于query term embedding 不是基于上下文得到。因此，可以预先计算 vocabulary $W$ 中每个<strong>term</strong> $w$  与 每个 image candidates 之间的 ranking feature $\phi(w, v)$，<strong style="color:red;">生成的分数 is cached during indexing</strong>，如等式14 所示。得到了一个一个  <strong style="color:red;">$N_{vocab}*M_{images} $的矩阵</strong></p><p><strong>during inference time，最终的分数可以经过 O(1)的查询和一个简单的求和运算得到，如 公式15所示。</strong></p><p><strong style="color:red;">Inverted Index</strong></p><p>更加重要的是，以上的计算可以经由一个 Inverted Index 来高效的实施。 Inverted Index 是现代搜索引擎的基础数据结构，如图1所示。</p><blockquote><p><a href="https://zh.wikipedia.org/wiki/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95</a></p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><ul><li><p>使用图像描述数据集作为本文text-to-image model 的数据来源。<strong>benchmark: MSCOCO; Flickr 30K</strong></p></li><li><p>for large-scale efficiency experiments: 由于目前不存在大规模的图像描述数据集。</p><p>因此，we <strong>manually design 113K and 1M datasets</strong> for testing the inference speed of different models in the large-scale setting.   对于这两个数据集，我们只关注于speed comparison。在数据上的模型性能忽略不比较。</p><p>The 113K dataset refers to the MSCOCO training set。</p><p>The 1M dataset we design consists of 1 million images randomly sampled from the MSCOCO training set.</p><p>所有的 <strong>efficiency test  experiments</strong> 都是在MSCOCO 1K and 5k test splits 再加上这113k 和 1M 数据上进行的。</p></li></ul><h3 id="Recall-Performance"><a href="#Recall-Performance" class="headerlink" title="Recall Performance"></a>Recall Performance</h3><p><img src="https://i.loli.net/2021/03/17/8dVeNz7bngQEwKl.png" alt="image-20210317120703074" style="zoom:50%;"></p><h3 id="Speed-Performance"><a href="#Speed-Performance" class="headerlink" title="Speed Performance"></a>Speed Performance</h3><p>三个模型使用相同的Faster-rcnn image region features。下表中没有考虑这部分时间。</p><p><img src="https://i.loli.net/2021/03/17/GfApECsXomRWHPq.png" alt="image-20210317120800787" style="zoom:50%;"></p><p>（1）在不同size的数据集下，本文提出的模型的速度远高于另外两个模型（一个使用dual encoding, 另一个使用transformer model）</p><p>（2）Table 2 also reveals that as the number of images increases, <strong>the performance drop is much slower</strong> when comparing VisualSparta with other two methods.</p><h3 id="Speed-Accuracy-Flexibility"><a href="#Speed-Accuracy-Flexibility" class="headerlink" title="Speed-Accuracy Flexibility"></a>Speed-Accuracy Flexibility</h3><p>在 Efficient Indexing and Inference 这一节，得到了一个  <strong style="color:red;">$N_{vocab}*M_{images} $的矩阵</strong>， 对于每个image, 与 N个words 计算出了weights, 可以挑选出 top-K， 这样更新为一个  <strong style="color:red;">$K_{words}*M_{images} $的矩阵</strong>，K 越小，检索效率越高。</p><p><img src="https://i.loli.net/2021/03/17/nkhj8yC6dPxugfw.png" alt="image-20210317132243925" style="zoom: 67%;"></p><p><img src="https://i.loli.net/2021/03/17/FmkRlfDp4WqTzOV.png" alt="image-20210317132318637" style="zoom: 50%;"></p><h3 id="Implementation-details"><a href="#Implementation-details" class="headerlink" title="Implementation details"></a>Implementation details</h3><ul><li>image encoder 的初始权重 从 Oscar-base model （12 layers and 768 hidden dimensions）中获得。</li><li>the query embedding， 使用Oscar-base word embedding的参数作为初始权重</li></ul><h2 id="可以查看的其他文献"><a href="#可以查看的其他文献" class="headerlink" title="可以查看的其他文献"></a>可以查看的其他文献</h2><p>本文受到此篇论文的启发: <strong>Sparta: Efficient open-domain question answering via sparse transformer matching retrieval.</strong></p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a><strong>yaya</strong></h2><ul><li><p>对于输入的消融实现，如果不提供 object label ？</p><p><strong>本文没有做这个实验。</strong></p></li><li><p>实验结果与 transformer-based retrieval model 的对比， eg: Oscar, Unicoder-VL 等</p><p><strong>本文没有做对比，只是与不基于pre-trained models 进行了对比。</strong></p><p><strong>但是实际上，本文的实验效果在准确率上，是不如那些基于预训练模型的。</strong></p></li><li><p>使用了 transformer 结构，那么本文的学习率是如何设计的？先warm up吗？？</p><p><strong>本文学习率为 1e-5， bs=20, 没有对学习率的变化进行说明。</strong></p></li><li><p>本文发现，相比于一些复杂的负样本选择策略（比如，使用有相近标签的相似图像作为负样本），这种负样本的选择策略是简单有效地，效果相当。</p><p><strong>对于这部分，论文中并没有相关的解释与实验数据说明</strong></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-任务&quot;&gt;&lt;a href=&quot;#1-任务&quot; class=&quot;headerlink&quot; title=&quot;1. 任务&quot;&gt;&lt;/a&gt;1. 任务&lt;/h2&gt;&lt;p&gt;本文是做跨模态检索问题。文本到图像的检索是多模态信息检索中的一项基本任务，即在给定文本查询的情况下从大型且未标记的图像数
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="real time" scheme="http://yoursite.com/categories/cross-modal/real-time/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
      <category term="real time" scheme="http://yoursite.com/tags/real-time/"/>
    
  </entry>
  
  <entry>
    <title>Slot Filling</title>
    <link href="http://yoursite.com/2021/03/15/Slot-Filling/"/>
    <id>http://yoursite.com/2021/03/15/Slot-Filling/</id>
    <published>2021-03-15T11:51:07.000Z</published>
    <updated>2021-03-16T03:22:01.122Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Bridging the Gap between Training and Inference for Neural Machine Translation</title>
    <link href="http://yoursite.com/2021/03/15/Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation/"/>
    <id>http://yoursite.com/2021/03/15/Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation/</id>
    <published>2021-03-15T08:15:51.000Z</published>
    <updated>2021-03-15T09:31:10.180Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>本文是ACL 2019 的 最佳长文奖。</p><p><strong style="color:red;">论文主要解决神经网络在翻译领域，训练和测试时所用的上文信息不同造成的偏差问题。</strong></p><p><strong>论文提出了新的训练方法，而非新的模型。读完之后，发现这种方法适用于许多领域的训练-测试不匹配的问题，如：阅读理解、语言模型。</strong></p><h2 id="2-存在的问题"><a href="#2-存在的问题" class="headerlink" title="2. 存在的问题"></a>2. 存在的问题</h2><p>传统的神经机器翻译有两个问题：</p><ul><li><strong>exposure bias</strong> （训练和测试时所用的上文信息不同的问题）</li><li><strong>overcorrection</strong>（过度矫正）</li></ul><h3 id="2-1-exposure-bias"><a href="#2-1-exposure-bias" class="headerlink" title="2.1 exposure bias"></a>2.1 exposure bias</h3><p>那么，什么叫<code>训练和测试时所用的上文信息不同的问题</code>呢？</p><p>训练时, 无论上一步模型的预测输出是什么，在当前步decoder模型的输入都是ground truth word的，即：模型的输入都是正确的，如：<strong>are</strong> 。</p><p><img src="https://i.loli.net/2021/03/14/tMrJqWR1huDFHQ4.png" alt="image-20210314192737439" style="zoom:33%;"></p><p>在测试时，由于没有正确答案，所以用模型预测的上一个字的结果作为输入，如：is、 you 。</p><p>这就导致了在测试时，<strong>如果在某个地方预测错，那么之后模型的输入都是错误的</strong>，这就造成了错误会一直累积；或许模型在某个地方所预测的是另一种翻译的词，但是在训练时没有碰到过这种情况，所以模型无法进行处理。</p><p>这种偏差叫做<code>exposure bias</code>。</p><h3 id="2-2-overcorrection"><a href="#2-2-overcorrection" class="headerlink" title="2.2 overcorrection"></a>2.2 overcorrection</h3><p>训练翻译模型时，还会碰到另一个问题：<strong>overcorrection</strong>（过度矫正）</p><p>什么意思呢？</p><p><img src="https://i.loli.net/2021/03/15/k8USC1drMGB6Jwh.png" alt="img"></p><p>当模型在第三个位置预测出‘abide’时，为了让这句话的loss最小，模型之后会预测 with the rule，但是 abide with the rule 是错误的；正确的应该是 abide by the rule。</p><p>注解: abide 与 by 搭配，而不是与with 搭配。</p><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h2><p><img src="https://i.loli.net/2021/03/14/QLSZEk4dqNln6jV.png" alt="image-20210314193828727" style="zoom:50%;"></p><p>为了消除或减轻train阶段和infer阶段的差别, 论文提出从真实的词 $y_{t-1}^{*}$ 和预测的词 $y_{t-1}^{\text {oracle }}$ 中抽样, decoder根据抽样的词来预测下一个词 $y_{t}$ 。使用论文提出的方法, 在时间步 $t$ 预测 $y_{t}$ 分为三步:</p><p>[1] 设真实输出中上一个词为 $y_{j-1}^{*}$ 。从预测的词中选择oracle word $y_{j-1}^{\text {oracle }},$ 论文提出了两种方法来选择oracle word，分别是词级别的方法和句子级别的方法。</p><p>[2] 接着从 $\left\{y_{j-1}^{\text {oracle }}, y_{j-1}^{<em>}\right\}.$ 中抽样一个词，抽中 $y_{j-1}^{</em>}$ 的概率为 $p$, 抽中 $y_{j-1}^{\text {oracle }}$ 的概率为 $1-p_{\circ}$ </p><p>[3] 最后, decoder根据抽样的这个词来预测 $y_{j}$。</p><h3 id="3-1-Oracle-Word-Selection"><a href="#3-1-Oracle-Word-Selection" class="headerlink" title="3.1 Oracle Word Selection"></a>3.1 Oracle Word Selection</h3><p>传统的方法中， decoder会根据上一个时间步真实的 $y_{t-1}^{*}$ 来预测 $y_{t}$ 。</p><p>为了消除train阶段的infer阶段的 差别，可以从预测的词中选择oracle word $y_{t-1}^{\text {oracle }}$ 来代替 $y_{t-1^{\circ}}^{*}$ </p><p>一种方法是每个时间步采用词级别的 greedy search来生成oracle word, 称为word-level oracle(WO)。另一种方法是采用beam-search, 扩大搜索空间, 用句子级的衡量指标(如：BLEU)对beam-search的结果进行排序，称为sentence-level oracle(SO).</p><h4 id="3-1-1-Word-Level-Oracle"><a href="#3-1-1-Word-Level-Oracle" class="headerlink" title="3.1.1 Word Level Oracle"></a>3.1.1 <strong>Word Level Oracle</strong></h4><p><img src="https://i.loli.net/2021/03/14/oJlrVILE6FgkcyM.png" alt="image-20210314200245535" style="zoom: 33%;"></p><p><img src="https://i.loli.net/2021/03/14/wc7Ql4js39KBCXg.png" alt="image-20210314200310765" style="zoom:33%;"></p><p>选择 $y_{t-1}^{\text {oracle }}$ 最简单直观的方法是, 在时间步$t$-1 , 选择公式 $P_{t-1}$ 中概率最高的词作为 $y_{t-1}^{\text {oracle }},$ 如Fig.2所 示。 为了获得更健壮的 $y_{t-1}^{\text {oracle }}$, 更好地选择是使用<a href="https://www.cnblogs.com/initial-h/p/9468974.html" target="_blank" rel="noopener">gumbel max技术</a>来冲离散分布中进行抽样, 如 Fig.3所示。<br>具体地讲, 将gumbel noise <strong style="color:blue;">$\eta$</strong> 作为正则化项加到decoder的预测概率分布上，进而再做softmax操作。</p><p>$\eta=-\log (-\log u)$<br>$\tilde{o}_{j-1}=\left(o_{j-1}+\eta\right) / \tau$<br>$\tilde{P}_{j-1}=\operatorname{softmax}\left(\tilde{o}_{j-1}\right)$</p><p>其中变量 $u \sim U(0,1)$ 服从均匀分布。 $\tau$ 为温度系数, 当 $\tau \rightarrow 0$ 时， 公式(8)的softmax()逐渐相当于<br>$\operatorname{argmax}()$ 函数 $;$ 当 $\tau \rightarrow \infty$ 时, $\operatorname{softmax}()$ 函数逐渐相当于均匀分布。</p><p>则 $y_{t-1}^{\text {oracle }}$ 为：$y_{j-1}^{\text {oracle }}=\operatorname{argmax}\left(\tilde{P}_{j-1}\right)$</p><p>需要注意的是gumbel noise $\eta$ 只用来选择oracle word，而不会影响train阶段的目标函数。</p><h4 id="3-1-2-Sentence-Level-Oracle"><a href="#3-1-2-Sentence-Level-Oracle" class="headerlink" title="3.1.2 Sentence Level Oracle"></a>3.1.2 <strong>Sentence Level Oracle</strong></h4><p>在每一次训练前，模型先用 beam search找到最好的 k 个候选翻译，然后将这 k 句话与正确答案计算 BLEU得分，取得分最高的当作备选句子。</p><p>有了备选句子后怎么办？比如，模型现在要预测第四个词，那么模型的输入是第三个词，这第三个词可以是正确译文的第三个词（传统做法）、可以是模型所预测的第三个词（Word Level）、也可以是这句备选句子的第三个词（Sentence-Level）。</p><p>现在有一个问题： 如果备选句子的长度与答案的长度不一样怎么办，这样备选句子与ground truth不是一一对应的了，那么这样的替换就没有意义了，因为我们希望这个词和对应答案的词是意思相近的或者是近义词。</p><p>作者给出了办法：</p><p>beam search在生成句子时，直到模型预测出结尾符<eos>才结束。</eos></p><p>假设ground truth的长度是 n ：</p><p>1、若模型在 n 之前就预测出<eos>结尾符，那么，我们选择概率第二的作为预测词。</eos></p><p>2、若模型在 n 时没有预测出<eos>结尾符，那么，我们选择<eos>结尾符，并使用它的概率。</eos></eos></p><p>作者的思路就是这样，然后就是最小化每一个字与<strong>ground truth</strong>对应字的负似然对数。</p><p>是与<strong>原始的ground truth</strong>的词计算loss！！！ 而不是与替换了的词，这个替换只发生在模型的输入。</p><h3 id="3-2-Sampling-with-Decay"><a href="#3-2-Sampling-with-Decay" class="headerlink" title="3.2 Sampling with Decay"></a>3.2 Sampling with Decay</h3><p>在train阶段刚开始时，抽中真实的词 $y_{j-1}^{*}$ 的概率比较大，随着模型逐渐收敛，抽中预测的词 $y_{j-1}^{\text {oracle }}$ 的概率变大，让模型有能力处理”过度纠正的问题”。</p><p>在训练的初始阶段, 如果过多地选择 $y_{t-1}^{\text {oracle }},$ 会导致模型收敘速度慢; 在训练的后期阶段，如果过多地选择 $y_{t-1}^{*},$ 会导致模型在train阶段没有学习到如何处理infer阶段的差别。 </p><p>因此，好的选择是：在训练的初始阶段， 更大概率地选择 $y_{t-1}^{*}$ 来加快模型收敛，当模型逐渐收敛后, 以更大概率选择 $y_{t-1}^{\text {oracle }},$ 来让模型学习到如何处理infer阶段的差别以及让模型有能力处理”过度纠正的问题”。从数学表示上，概率 $p$ 先大后逐渐衰减，$p$ 随着训练轮数 $e$ 的增大而逐渐变小。</p><p>$p=\frac{\mu}{\mu+\exp \left(\frac{e}{\mu}\right)}$</p><p><img src="https://i.loli.net/2021/03/15/H6U7Dc5wP8KAsNG.png" alt="image-20210315173055286" style="zoom: 67%;"></p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><p>本文主要的两点贡献：</p><p>(1) word level oracle selection</p><p>(2) sampling with decay</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://zhuanlan.zhihu.com/p/76227765" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76227765</a></p><p><a href="https://spring-quan.github.io/2019/08/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ABridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation%E3%80%8B/" target="_blank" rel="noopener">https://spring-quan.github.io/2019/08/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ABridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation%E3%80%8B/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1. 简介&quot;&gt;&lt;/a&gt;1. 简介&lt;/h2&gt;&lt;p&gt;本文是ACL 2019 的 最佳长文奖。&lt;/p&gt;
&lt;p&gt;&lt;strong style=&quot;color:red;&quot;&gt;论文主要解
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Beam Search</title>
    <link href="http://yoursite.com/2021/03/15/Beam-Search/"/>
    <id>http://yoursite.com/2021/03/15/Beam-Search/</id>
    <published>2021-03-15T07:44:10.000Z</published>
    <updated>2021-03-15T08:24:12.121Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. <strong>前言</strong></h2><p>自然语言处理任务中，如机器翻译、对话、文本摘要等，都涉及到序列生成。文本序列生成解码过程中有用到 greedy search、维特比算法、beam search等。</p><h2 id="2-Beam-Search-介绍"><a href="#2-Beam-Search-介绍" class="headerlink" title="2. Beam Search 介绍"></a>2. Beam Search 介绍</h2><p>beam search尝试在广度优先基础上进行进行搜索空间的优化（类似于剪枝）达到减少内存消耗的目的。</p><ul><li><strong>算法过程</strong></li></ul><p>定义词表大小是V，beam size是 B，序列长度是L。</p><p>假设V=100，B=3：</p><ol><li><p>生成第1个词时，选择概率最大的3个词（假设是a，b，c），即从100个中选了前3个；</p></li><li><p>生成第2个词时，将当前序列a/b/c分别与词表中的 100个词组合，得到 3*100个序列，从中选 3个概率最大的，作为当前序列（假设现在是am，bq，as）；</p></li><li><p>持续上述过程，直到结束。最终输出3个得分最高的。</p></li></ol><ul><li><strong>算法复杂度</strong> $O(B<em>V</em>L)$</li></ul><p>在第2步，要计算 $B<em>V$ 次。序列长度是L，生成长度为L的序列，计算  $B</em>V*L$ 次。</p><h2 id="3-算法评价"><a href="#3-算法评价" class="headerlink" title="3. 算法评价"></a>3. 算法评价</h2><ul><li><strong>优点</strong></li></ul><p>(1) 减少计算开销。相对于广度优先搜索，广搜每次都要保留所有可能的结果，复杂度是  $O(V^L)$指数级。</p><ul><li><strong>缺点（第3部分详细讲）</strong></li></ul><p>(1) 数据下溢</p><p>(2) 倾向于生成短的序列</p><p>(3) 单一性问题</p><ul><li><strong>Beam size 设置</strong></li></ul><p>(1) B越大</p><p>优点：可考虑的选择越多，能找到的句子越好</p><p>缺点：计算代价更大，速度越慢，内存消耗越大</p><p>(2) B越小</p><p>优点：计算代价小，速度快，内存占用越小</p><p>缺点：可考虑的选择变少，结果没那么好</p><h2 id="4-问题解决"><a href="#4-问题解决" class="headerlink" title="4. 问题解决"></a>4. 问题解决</h2><h3 id="4-1-数据下溢"><a href="#4-1-数据下溢" class="headerlink" title="4.1 数据下溢"></a>4.1 数据下溢</h3><p>求序列概率的时候，序列概率是多个条件概率的乘积$P\left(y^{<1>} y^{<2>} \ldots y^{T_{y}}\right)=P\left(y^{<1>} \mid x\right) P\left(y^{<2>} \mid x, y^{<1>}\right) \ldots P\left(y^{T_{y}} \mid x, y^{<1>} \ldots, y^{T_{y}-1}\right)$.</1></1></2></1></2></1></p><p>每个概率都小于1甚至远远小于1，很多概率相乘起来，会得到很小很小的数字，会造成数据下溢，即数值太小，计算机的浮点表示不能精确储存。</p><p><strong>解决</strong>：<strong>将最大化的乘积式取对数</strong>，由 $\log M^{*} N=\log M+\log N$ 公式可得，上述需要最大化的王积式可以转化为: $\arg \max _{y} \sum_{y=1}^{T_{y}} \log P\left(y^{<t>} \mid x, y^{<1>}, \ldots, y^{<t-1>}\right)$</t-1></1></t></p><p>即乘积的log变成了log的求和，最大化这个log的求和值能够得到同样的结果，并且不会出现 数值下溢和四舍五入。</p><h3 id="4-2-倾向于生成短的序列"><a href="#4-2-倾向于生成短的序列" class="headerlink" title="4.2 倾向于生成短的序列"></a>4.2 倾向于生成短的序列</h3><p>生成的句子序列越长，对数概率相加的结果就越小（越为负值）, 所以倾向于生成短序列。 对序列长度进行惩罚，降低生成短序列的倾向。</p><p><strong>解决方法：</strong> 对数概率相加的结果, 除以序列长度 $L$ 。<br>实践中，通常采用更柔和的方法, 在 $L$ 上加上指数 $a \in(0,1),$ 即 $L^{a},$ 例如 $a=0.7$ 。如果 $a=1, \quad L^{a}=L$ 就相当于完全用长度来归一化; 如果 $a=0, \quad L^{a}=1$ 就相当于完全没有 归一化, $a \in(0,1)$ 就是在完全归一化和没有归一化之间。</p><p>或者更加复杂一点：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">norm</span> = self.opt.beam_search_norm</span><br><span class="line"><span class="attr">candidate_logprob</span> = (beam_logprobs_sum[q] * t ** norm + local_logprob) / ((t+<span class="number">1</span>) ** norm)</span><br></pre></td></tr></table></figure><h3 id="4-3-单一性问题"><a href="#4-3-单一性问题" class="headerlink" title="4.3 单一性问题"></a><strong>4.3 单一性问题</strong></h3><p>beam search 有一个大问题是输出的 $B$ 个句子的差异性很小，无法体现语言的多样性（比如文本摘要、机器翻译的生成文本，往往有不止一种表述方式）。</p><p><strong>解决方法：</strong> 分组 加入相似性惩罚。diverse beam search 来自<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.02424.pdf" target="_blank" rel="noopener">论文</a></p><p>具体如下：选择 Beam size 为 $B$，然后将其分为 $G$组，每一组就有 $B/G$个beam。每个单独的组内跟 beam search很像，不断延展序列。同时通过引入一个dissimilarity 项来保证组与组之间有差异。</p><p><img src="https://i.loli.net/2021/03/15/VQDxijZcGtzlEph.png" alt="image-20210315161115581" style="zoom: 50%;"></p><p>如上图所示，B = 6, G=3，每一组的beam width为2。</p><p>组内与 beam search 很像：从t-1到 t 时刻，不断的减少搜索空间（如同beam search一样）。</p><p>组间差异：对于t=4时刻，我们先对第一组输出y（t=4），然后我们开始对第二组输出y（t=4），但是第二组y（t=4）的score不仅取决于第二组之前的y（t=3），也取决于其与第一组的相似程度。以此类推，在t=4时刻对于第三组的输出，我们从上图可以看到其score的打分标准。这儿对于其 dissimilarity 项的计算采用的办法是 hamming diversity，这个理解起来很简单，比如这个时刻可能输出的词在上面的组出现过，我们就对这个词的分数-1，如果这个时刻可能输出的词在上面组没有出现过，我们就对这个词的分数不惩罚。</p><ul><li><strong>DBS算法：</strong></li></ul><p><img src="https://i.loli.net/2021/03/15/DdS1XPZvoEptVgw.png" alt="image-20210315161139107" style="zoom: 50%;"></p><p>DBS算法</p><ul><li><strong>附：</strong>很多论文里有对 beam search的改进，主要是针对生成序列的<strong>多样性</strong>的。多样性问题，在对话里很常见。</li></ul><h2 id="5-其他相关问题："><a href="#5-其他相关问题：" class="headerlink" title="5. 其他相关问题："></a><strong>5. 其他相关问题</strong>：</h2><h3 id="5-1-训练的时候需要-Beam-Search-吗？"><a href="#5-1-训练的时候需要-Beam-Search-吗？" class="headerlink" title="5.1 训练的时候需要 Beam Search 吗？"></a><strong>5.1 训练的时候需要 Beam Search 吗？</strong></h3><p>不需要。因为训练的时候知道每一步的正确答案，没必要进行这样的搜索。</p><p>5.2 为什么不用贪心搜索？**</p><p>贪心搜索相当于 Beam Search 中 B=1的情况，每次只选择概率最大的词，容易陷入局部最优，但我们真正需要的是一个序列，我们希望整个序列的概率最大。</p><h3 id="5-3-维特比算法"><a href="#5-3-维特比算法" class="headerlink" title="5.3 维特比算法"></a>5.3 维特比算法</h3><p>维特比算法是用动态规划的思想。简单来说就是：从开始状态之后每走一步，就记录下<strong>到达该状态的所有路径的概率最大值</strong>，然后以此最大值为基准继续向后推进。显然，如果这个最大值都不能使该状态成为最大似然状态路径上的结点的话，那些小于它的概率值（以及对应的路径）就更没有可能了。</p><p>Beam Search与Viterbi算法虽然都是解空间的剪枝算法，但它们的思路是不同的。Beam Search是对状态迁移的路径进行剪枝，而 Viterbi 算法是合并不同路径到达同一状态的概率值，用最大值作为对该状态的充分估计值，从而在后续计算中，忽略历史信息（这种以偏概全也就是所谓的Markov性），以达到剪枝的目的。<br>从状态转移图的角度来说，Beam Search是空间剪枝，而Viterbi算法是时间剪枝。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1. 前言&quot;&gt;&lt;/a&gt;1. &lt;strong&gt;前言&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;自然语言处理任务中，如机器翻译、对话、文本摘要等，都涉及到序列生成。文本序列生成解码过
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</title>
    <link href="http://yoursite.com/2021/03/15/Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering/"/>
    <id>http://yoursite.com/2021/03/15/Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering/</id>
    <published>2021-03-15T07:06:11.000Z</published>
    <updated>2021-03-15T08:24:37.179Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>人类视觉系统存在两种attention机制。Top-down attention由当前任务所决定，我们会根据当前任务（即VQA中的问题），聚焦于与任务紧密相关的部分。Bottom-up attention指的是我们会被显著的、突出的、新奇的事物给吸引。</p><p>以前的方法用到的visual attention mechanisms大都属于top-down类型，即取问题作为输入，建模attention分布，然后作用于CNN提取的图像特征（image features）。然而，这种方法的attention作用的图像对应于下图的左图，没有考虑图片的内容。对于人类来说，注意力会更加集中于图片的目标或其他显著区域，所以作者引进Bottom-up attention机制，如下图的右图所示，attention作用于object proposal。</p><p><img src="https://i.loli.net/2021/03/15/p8WFURqkGw6iIfC.png" alt="image-20210315150105258" style="zoom:50%;"></p><h2 id="Basic-idea"><a href="#Basic-idea" class="headerlink" title="Basic idea"></a>Basic idea</h2><p>Bottom-Up注意力机制: 即基于目标（objects）或显著区域（salient image regions）来计算attention。具体来说，bottom-up机制基于Faster R-CNN，得到图片中每个目标或显著区域的特征向量（feature vector）表示。</p><p>Top-Down机制: 取question作为输入，建模特征权重（feature weightings）或者说attention分布。</p><h2 id="概括："><a href="#概括：" class="headerlink" title="概括："></a>概括：</h2><p><strong>（1）Bottom-Up</strong><br>使用Faster R-CNN 中的R-CNN来得到object feature。<br><strong>（2）Top-Down Attention</strong><br>得到了该层的隐层状态，并与object features  中的每一个<strong>v<sub>i</sub></strong>来计算一个attention 系数。<br><strong>（3）对object features 进行attention 权重求和</strong><br>得到image feature<br><strong>（4）Decoder：language LSTM</strong><br>输出预测单词</p><p><img src="https://i.loli.net/2021/03/15/IcK4HYAsGwjzPqv.jpg" alt="img" style="zoom: 67%;"><br><img src="https://i.loli.net/2021/03/15/lqZx5tngfd9r2Gs.jpg" alt="img" style="zoom:50%;"></p><h2 id="Bottom-Up"><a href="#Bottom-Up" class="headerlink" title="Bottom-Up"></a>Bottom-Up</h2><ul><li><strong>主要介绍一下Faster R-CNN 的训练过程</strong><br>（1）首先Resnet-101 是在ImageNet上预训练的<br>（2）Faster R-CNN在MS COCO上进行预训练<br>rpn 的score classification loss，bbox regression loss<br>r-cnn 的score classification loss，bbox regression loss<br>（3）Faster R-CNN在Visual Genome上再进行预训练<br>为了得到更好的特征表达，增加一个预测属性的输出： </li><li><strong>具体的网络：</strong><br>To predict attributes for region i, we concatenate the mean  pooled convolutional feature vi with a learned embedding  of the ground-truth object class, and feed this into an additional output layer defining a softmax distribution over each  attribute class plus a ‘no attributes’ class.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;人类视觉系统存在两种attention机制。Top-down attenti
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Gumbel-Softmax Trick和Gumbel分布</title>
    <link href="http://yoursite.com/2021/03/15/Gumbel-Softmax-Trick%E5%92%8CGumbel%E5%88%86%E5%B8%83/"/>
    <id>http://yoursite.com/2021/03/15/Gumbel-Softmax-Trick和Gumbel分布/</id>
    <published>2021-03-15T02:02:30.000Z</published>
    <updated>2021-03-18T12:04:15.131Z</updated>
    
    <content type="html"><![CDATA[<h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li><p>由于最近看到的几篇论文中都有提及到gumble softmax的操作，因此想要具体了解一下。</p></li><li><p>用到gumbel softmax 的论文包括以下几篇</p><ul><li><p>解决不可微分问题</p><p>【ICCV 2019】Learning to Assemble Neural Module Tree Networks for Visual Grounding</p><p>【arXiv: 2101.12059v1】VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs</p><p>【arXiv: 2103.08862】Gumbel-Attention for Multi-modal Machine Translation</p><blockquote><p><strong>Categorical reparameterization with gumbel-softmax.</strong>  ICLR 2017</p></blockquote></li><li><p>在概率分布上添加gumble noise，再从新的概率分布上以概率检索样本</p><p> Bridging the Gap between Training and Inference for Neural Machine Translation</p><blockquote><p><strong>A* sampling.</strong> NIPS 2017</p></blockquote></li></ul></li></ul><p>来源:  <a href="https://www.cnblogs.com/initial-h/p/9468974.html" target="_blank" rel="noopener">https://www.cnblogs.com/initial-h/p/9468974.html</a></p><p>之前看MADDPG论文的时候，作者提到在离散的信息交流环境中，使用了Gumbel-Softmax estimator。于是去搜了一下，发现该技巧应用甚广，如深度学习中的各种GAN、强化学习中的A2C和MADDPG算法等等。只要涉及在离散分布上运用重参数技巧时(re-parameterization)，都可以试试Gumbel-Softmax Trick。</p><p>  这篇文章是学习以下链接之后的个人理解，内容也基本出于此，需要深入理解的可以自取。</p><ul><li><a href="http://amid.fish/humble-gumbel" target="_blank" rel="noopener">The Humble Gumbel Distribution</a></li><li><a href="https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/" target="_blank" rel="noopener">The Gumbel-Max Trick for Discrete Distributions</a></li><li><a href="https://casmls.github.io/general/2017/02/01/GumbelSoftmax.html" target="_blank" rel="noopener">The Gumbel-Softmax Trick for Inference of Discrete Variables</a></li><li><a href="https://www.zhihu.com/question/62631725/answer/201338234" target="_blank" rel="noopener">如何理解Gumbel-Max trick？</a></li></ul><p>  这篇文章从直观感觉讲起，先讲Gumbel-Softmax Trick用在哪里及如何运用，再编程感受Gumbel分布的效果，最后讨论数学证明。</p><h2 id="一、Gumbel-Softmax-Trick用在哪里"><a href="#一、Gumbel-Softmax-Trick用在哪里" class="headerlink" title="一、Gumbel-Softmax Trick用在哪里"></a>一、Gumbel-Softmax Trick用在哪里</h2><h3 id="问题来源"><a href="#问题来源" class="headerlink" title="问题来源"></a>问题来源</h3><p>  通常在强化学习中，如果动作空间是离散的，比如上、下、左、右四个动作，通常的做法是网络输出一个四维的one-hot向量(不考虑空动作)，分别代表四个动作。比如 [1,0,0,0] 代表上，[0,1,0,0] 代表下等等。而具体取哪个动作呢，就根据输出的每个维度的大小，选择值最大的作为输出动作, 即argmax(v)。</p><p>  例如网络输出的四维向量为v=[−20,10,9.6,6.2]，第二个维度取到最大值10，那么输出的动作就是[0,1,0,0]，也就是下，这和多类别的分类任务是一个道理。但是这种取法有个问题是不能计算梯度，也就不能更新网络。通常的做法是加softmax函数，把向量归一化，这样既能计算梯度，同时值的大小还能表示概率的含义。softmax函数定义：$\sigma\left(z_{i}\right)=\frac{e^{z_{i}}}{\sum_{j=1}^{K} e^{z_{j}}}$</p><p>  那么将v=[−20,10,9.6,6.2]通过softmax函数后有σ(v)=[0,0.591,0.396,0.013]，这样做不会改变动作或者说类别的选取，同时softmax倾向于让最大值的概率显著大于其他值，比如这里10和9.6经过softmax放缩之后变成了0.591和0.396，6.2对应的概率更是变成了0.013，这有利于把网络训成一个one-hot输出的形式，这种方式在分类问题中是常用方法。</p><p>  但是这么做还有一个问题，这个表示概率的向量σ(v)=[0,0.591,0.396,0.013]并没有真正显示出概率的含义，因为一旦某个值最大，就选择相应的动作或者分类。比如σ(v)=[0,0.591,0.396,0.013]和σ(v)=[0,0.9,0.1,0]在类别选取的结果看来没有任何差别，都是选择第二个类别，但是从概率意义上讲差别是巨大的。所以需要一种方法不仅选出动作，而且遵从概率的含义。</p><p>  很直接的方法是依概率分布采样就完事了，比如直接用<code>np.random.choice</code>函数依照概率生成样本值，这样概率就有意义了。这样做确实可以，但是又有一个问题冒了出来：这种方式怎么计算梯度？不能计算梯度怎么用BP的方式更新网络？</p><p>  这时重参数(re-parameterization)技巧解决了这个问题，<a href="https://casmls.github.io/general/2017/02/01/GumbelSoftmax.html" target="_blank" rel="noopener">这里</a>有详尽的解释，不过比较晦涩。简单来说重参数技巧的一个用处是把采样的步骤移出计算图，这样整个图就可以计算梯度BP更新了。之前我一直在想分类任务直接softmax之后BP更新不就完事了吗，为什么非得采样。后来看了VAE和GAN之后明白，还有很多需要采样训练的任务。这里举简单的VAE(变分自编码器)的例子说明需要采样训练的任务以及重参数技巧，详细内容来自<a href="https://www.bilibili.com/video/av20165127" target="_blank" rel="noopener">视频</a>和<a href="http://kvfrans.com/variational-autoencoders-explained/" target="_blank" rel="noopener">博客</a>。</p><h3 id="Re-parameterization-Trick"><a href="#Re-parameterization-Trick" class="headerlink" title="Re-parameterization Trick"></a>Re-parameterization Trick</h3><p>  最原始的自编码器通常长这样：</p><p><img src="C:\Users\shiyaya\Desktop\1428973-20180813165000500-1207992534.jpg" alt="img" style="zoom: 67%;"></p><p>  左右两边是端到端的出入输出网络，中间的绿色是提取的特征向量，这是一种直接从图片提取特征的方式。<br>  而VAE长这样:</p><p><img src="https://i.loli.net/2021/03/18/yO5L9kCMPhrwJub.png" alt="image-20210318135149420"></p><p>  VAE的想法是不直接用网络去提取特征向量，而是提取这张图像的分布特征，也就把绿色的特征向量替换为分布的参数向量，比如说均值和标准差。然后需要decode图像的时候，就从encode出来的分布中采样得到特征向量样本，用这个样本去重建图像，这时怎么计算梯度的问题就出现了。<br>  重参数技巧可以解决这个问题，它长下面这样:</p><p><img src="https://i.loli.net/2021/03/18/27e1EIhVFfWyJKi.png" alt="img" style="zoom:67%;"></p><p>假设图中的 $x$ 和 $\phi$ 表示VAE中的均值和标准差向量, 它们是确定性的节点。而需要输出的样本 $z$ 是带有随机性的节点， 重参数就是把带有随机性的 $z$ 变成确定性的节点, 同时随机性用另一个输入节点 $\epsilon$ 代替。<br>例如，这里用正态分布采样, 原本从均值为 $x$ 和标准差为 $\phi$ 的正态分布 $N\left(x, \phi^{2}\right)$ 中采样得到 $z_{\circ}$ 现在将其转化成从标准正态分布 $N(0,1)$ 中采样得到 $\epsilon$ , 再计算得到 $z=x+\epsilon \cdot \phi_{\circ}$ 这样一来, 采样的过程移出了计算图, 整张计算图就可以计算梯度进行更新了，而新加的 $\epsilon$ 的输入分支不 做更新，只当成一个没有权重变化的输入。</p><p>到这里，需要采样训练的任务实例以及重参数技巧基本有个概念了。</p><h3 id="Gumbel-Softmax-Trick"><a href="#Gumbel-Softmax-Trick" class="headerlink" title="Gumbel-Softmax Trick"></a>Gumbel-Softmax Trick</h3><p>VAE的例子是一个连续分布(正态分布)的重参数，离散分布的情况也一样，首先需要可以采样，使得离散的概率分布有意义而不是只取概率最大的值，其次需要可以计算梯度。那么怎么做到的，具体操作如下：</p><p>对于n维概率向量$\pi$, 对$\pi$对应的离散随机变量 $x_{\pi}$ 添加Gumbel噪声，再取样$x_{\pi}=\arg \max \left(\log \left(\pi_{i}\right)+G_{i}\right)$<br>其中, $G_{i}$ 是独立同分布的标准Gumbel分布的随机变量，标准Gumbel分布的CDF为$F(x)=e^{-e^{-x}}$</p><p>这就是<strong style="color:red;">Gumbel-Max trick</strong>。可以看到由于这中间有一个argmax操作，这是不可导的。所以用softmax函数代替之，也就是<strong style="color:red;">Gumbel-Softmax Trick</strong>，而$G_{i}$ 可以通过Gumbel分布求逆从均匀分布生成，即 $G_{i}=-\log \left(-\log \left(U_{i}\right)\right), U_{i} \sim U(0,1)$</p><p>算法流程如下：</p><ul><li><p>对于网络输出的一个 $n$ 维向量 $v$ （predict logits）, 生成 $n$ 个服从均匀分布 $U(0,1)$ 的独立样本 $\epsilon_{1}, \ldots, \epsilon_{n}$</p></li><li><p>通过 $G_{i}=-\log \left(-\log \left(\epsilon_{i}\right)\right)$ 计算得到 $G_{i}$</p></li><li><p>对应相加得到新的值向量 $v^{\prime}=\left[v_{1}+G_{1}, v_{2}+G_{2}, \ldots, v_{n}+G_{n}\right]$</p></li><li><p>通过Softmax函数</p></li></ul><script type="math/tex; mode=display">\sigma_{\tau}\left(v_{i}^{\prime}\right)=\frac{e^{v_{i}^{\prime} / \tau}}{\sum_{j=1}^{n} e^{v_{j}^{\prime} / \tau}}</script><p>计算概率大小得到最终的类别。其中 $\tau$ 是温度参数。</p><p>temperature控制着softmax的soft程度，温度越高，生成的分布越平滑（接近这里的均匀分布）；温度越低，生成的分布越接近离散的one-hot分布（argmax）。因此，<strong>训练时可以逐渐降低温度，以逐步逼近真实的离散分布。</strong></p><p><strong>yaya: 其实这里的 gumbel softmax 不是针对网络输出$v$ 进行一个离散采样，而是对 $v$ 添加噪声之后，再取softmax，而后得到新的概率分布，而这个新的概率分布也不是去argmax, 而是对于每个 $v_i$ 重新分配一个概率权重。其优点在于可以调控 temperature， 当温度低时，近似做了argmax 采样</strong></p><p>直观上来说，Gumbel-Softmax就是在原来的输出上加入了一个噪声，对于强化学习来说，在选择动作之前加一个Gumbel扰动，相当于增加了探索度，感觉上是合理的，而同时他又能保证采样是对原分布的逼近。对于深度学习的任务来说，添加随机性去模拟分布的样本生成，也是合情合理的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;yaya&quot;&gt;&lt;a href=&quot;#yaya&quot; class=&quot;headerlink&quot; title=&quot;yaya&quot;&gt;&lt;/a&gt;yaya&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;由于最近看到的几篇论文中都有提及到gumble softmax的操作，因此想要具体了解一下。&lt;/p&gt;
&lt;/
      
    
    </summary>
    
      <category term="杂类" scheme="http://yoursite.com/categories/%E6%9D%82%E7%B1%BB/"/>
    
    
      <category term="杂类" scheme="http://yoursite.com/tags/%E6%9D%82%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Object Relational Graph with Teacher-Recommended Learning for Video Captioning</title>
    <link href="http://yoursite.com/2021/03/13/Object-Relational-Graph-with-Teacher-Recommended-Learning-for-Video-Captioning/"/>
    <id>http://yoursite.com/2021/03/13/Object-Relational-Graph-with-Teacher-Recommended-Learning-for-Video-Captioning/</id>
    <published>2021-03-13T11:13:46.000Z</published>
    <updated>2021-03-15T01:32:56.252Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="video captioning" scheme="http://yoursite.com/categories/cross-modal/video-captioning/"/>
    
    
      <category term="cross-modal,video captioning" scheme="http://yoursite.com/tags/cross-modal-video-captioning/"/>
    
  </entry>
  
  <entry>
    <title>[VATEX Captioning Challenge 2019] Multi-modal Information Fusion and Multi-stage Training Strategy for Video Captioning</title>
    <link href="http://yoursite.com/2021/03/13/VATEX-Captioning-Challenge-2019-Multi-modal-Information-Fusion-and-Multi-stage-Training-Strategy-for-Video-Captioning/"/>
    <id>http://yoursite.com/2021/03/13/VATEX-Captioning-Challenge-2019-Multi-modal-Information-Fusion-and-Multi-stage-Training-Strategy-for-Video-Captioning/</id>
    <published>2021-03-13T07:39:13.000Z</published>
    <updated>2021-03-15T08:22:45.689Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="video captioning" scheme="http://yoursite.com/categories/cross-modal/video-captioning/"/>
    
    
      <category term="cross-modal,video captioning" scheme="http://yoursite.com/tags/cross-modal-video-captioning/"/>
    
  </entry>
  
  <entry>
    <title>多模态人工智能</title>
    <link href="http://yoursite.com/2021/03/12/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    <id>http://yoursite.com/2021/03/12/多模态交互人工智能/</id>
    <published>2021-03-12T10:11:52.000Z</published>
    <updated>2021-03-12T12:22:43.491Z</updated>
    
    <content type="html"><![CDATA[<p>多模态：视觉，语音，自然语言</p><p><img src="https://i.loli.net/2021/03/12/8ObjKzRVd4UXgS3.png" alt="image-20210312202150477"></p><p><img src="https://i.loli.net/2021/03/12/IqFHGuL6ZYOKwXb.png" alt="image-20210312202209438"></p><p><img src="https://i.loli.net/2021/03/12/CsF8TR3m9UlS54N.png" alt="image-20210312202229192"></p><p><img src="https://i.loli.net/2021/03/12/HbPD8yaB3GU7IAk.png" alt="image-20210312202108780"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;多模态：视觉，语音，自然语言&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/12/8ObjKzRVd4UXgS3.png&quot; alt=&quot;image-20210312202150477&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https:
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering</title>
    <link href="http://yoursite.com/2021/03/12/Learning-to-Contrast-the-Counterfactual-Samples-for-Robust-Visual-Question-Answering/"/>
    <id>http://yoursite.com/2021/03/12/Learning-to-Contrast-the-Counterfactual-Samples-for-Robust-Visual-Question-Answering/</id>
    <published>2021-03-12T09:43:10.000Z</published>
    <updated>2021-03-12T10:05:32.371Z</updated>
    
    <content type="html"><![CDATA[<p>转自：<a href="https://blog.csdn.net/weixin_45347379/article/details/112182143" target="_blank" rel="noopener">https://blog.csdn.net/weixin_45347379/article/details/112182143</a></p><p>学习对比反事实样本，以实现稳健的视觉问答<br>Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering<br>在阅读本文之前，一定要阅读论文：Counterfactual Samples Synthesizing for Robust Visual Question Answering（简称CSS）</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="https://i.loli.net/2021/03/12/gODk8QFCNWaUsVP.png" alt="image-20210312175656070"></p><p>文章的方法主要包括三个部分：（1）一个基本的VQA模型。（2）一个事实和反事实样本合成（CSS）模块。（3）一个对比学习（CL）目标。</p><h4 id="第一部分和第二部分"><a href="#第一部分和第二部分" class="headerlink" title="第一部分和第二部分"></a><strong>第一部分和第二部分</strong></h4><p>属于CSS已经实现的，主要作用在于：</p><p>（1）并通过多分类的方法预测答案，并产生图中右上方基本VQAloss。</p><p><img src="https://i.loli.net/2021/03/12/z3Q52bActwqMhov.png" alt="在这里插入图片描述"></p><p>（2）得到（I, I+, I-）和（Q, Q+, Q-），</p><p><img src="https://i.loli.net/2021/03/12/din9DctNLV1EORo.png" alt="在这里插入图片描述"></p><h4 id="第三部分"><a href="#第三部分" class="headerlink" title="第三部分"></a><strong>第三部分</strong></h4><p>以（I, I+, I-）为例，将（I, I+, I-）和Q喂给VQA模型，分别产生原始样本的嵌入mm（V, Q）作为anchor（a），事实样本的嵌入mm(V+, Q)作为positive（p），反事实样本嵌入mm(V-, Q)作为negati（n）<br>利用余弦相似度作为评分函数，对正样本输出高值，对负样本输出低值，公式如下：</p><p><img src="https://i.loli.net/2021/03/12/XIjKWu7szqi3A4w.png" alt="在这里插入图片描述"></p><p>同样的方法得到anchor和negative之间的评分s(a, n), 这就相当于图中展示的，拉近原始图像与事实区域图像的关系，推远原始图像与反事实区域的距离。<br>对比损失定义为：（这就是图片下方得到的Contrastive loss）</p><p><img src="https://i.loli.net/2021/03/12/CY4OJ7PZmD9Eaud.png" alt="在这里插入图片描述"></p><p>最后，这种对比损失与基础分类损失的加权总和弥补了整体损失：</p><p><img src="https://i.loli.net/2021/03/12/8GL3BxlSEpAg951.png" alt="在这里插入图片描述"></p><p>虽然文章说，<strong style="color:red;">这种方法能够使模型学习他们之间的关系，并从更有因果关系的方面预测正确答案。</strong>但是，个人感觉如果仅仅使以上方法，并不能从理论上提高模型的能力。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p><img src="https://i.loli.net/2021/03/12/wAmbOSxRU97dN12.png" alt="在这里插入图片描述"></p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>看了本文博客之后，没有看原文，个人任务这种方法有限，</li><li>可能模型的设计上，是有新意的，使用对比学习来增强VQA模型的性能，但是往往自己做的时候会收效甚微</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转自：&lt;a href=&quot;https://blog.csdn.net/weixin_45347379/article/details/112182143&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/weixin_
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="VQA" scheme="http://yoursite.com/categories/cross-modal/VQA/"/>
    
    
      <category term="cross-modal,VQA" scheme="http://yoursite.com/tags/cross-modal-VQA/"/>
    
  </entry>
  
  <entry>
    <title>Semantic Equivalent Adversarial Data Augmentation for Visual Question Answering</title>
    <link href="http://yoursite.com/2021/03/12/Semantic-Equivalent-Adversarial-Data-Augmentation-for-Visual-Question-Answering/"/>
    <id>http://yoursite.com/2021/03/12/Semantic-Equivalent-Adversarial-Data-Augmentation-for-Visual-Question-Answering/</id>
    <published>2021-03-12T03:27:29.000Z</published>
    <updated>2021-03-12T09:31:48.943Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>受到深度学习的快速发展，VQA 近年来取得了非常成功的进展。数据增强是深度学习中的一个有用的技巧，但是，目前很少有工作关注于VQA任务的数据增强。</p><p>对于image side: 一些简单的数据增强操作不能直接应用到VQA这一场景下，比如，rotation and flipping 等操作，都可能导致<image, question, answer> 这一结构的正确性遭到破坏。</image,></p><p>对于text side (eg: questions) , it is challenging to come up with <strong>generalized rules for language transformation.</strong> 另外，有一类任务是Visual Question Generation，根据image和 answer来生成问题，但是生成的问题常常是有语法错误的，而且，他们在同一个目标数据集上进行学习，生成的数据与原始数据的分布是一致的，因此，<strong>若使用这种方案来做数据扩充，难以解决过拟合问题</strong>(通常训练数据和测试数据不是同一个分布)。</p><p>在本文中，不直接对image或者是question进行操作，而是对images 和 questions生成对抗样本作为数据增强。增强的样本不会改变image的原始语义，也不会改变questions中的semantic meaning。对抗性示例是经过<strong>策略</strong>修改的样本，可以成功地欺骗深层模型以做出不正确的预测。这种修改是难以察觉的，<strong>它在使对抗性示例的基础分布远离原始数据的同时保持了数据的语义。</strong>本文是第一个同时对image 和 text进行数据扩充的方法（已有的方法只是单独对一方面进行数据扩充）。</p><p>进而，使用本文方法产生的<strong>数据增强样本</strong>和<strong>对抗训练</strong>来训练经典的VQA model (BUTD) 。</p><p>实验结果证明，不仅可以提高 VQAv2的整体性能，而且相比于baseline还可以有效抵抗对抗攻击。</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="https://i.loli.net/2021/03/12/rmyqUFXQewT2PBK.png" alt="image-20210312165449352" style="zoom:50%;"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="何时加入对抗样本"><a href="#何时加入对抗样本" class="headerlink" title="何时加入对抗样本"></a>何时加入对抗样本</h4><ul><li><p>本文发现，将干净样本和对抗样本进行混合，然后<strong>从头到尾</strong>的训练，这种方案不会在干净样本上收敛。因此本文只在特定的训练时期对样本进行混合，最后使用干净样本进行微调。</p><p>本文实验中max-epoch=25.</p><p><img src="https://i.loli.net/2021/03/12/YOblV6WT4rtMkSf.png" alt="image-20210312171853940" style="zoom: 50%;"></p><p>本文的解释：与干净样本相比，对抗样本与其有不同的分布。如果把提升模型在VQA任务上的性能作为我们的主要目标，那么模型在干净样本上的拟合能力需要<strong>在结束</strong>的时候to be retrieved。而<strong>在开始</strong>时，模型需要warm up，此时不适合加入对抗样本。因此在中间阶段加入融合对抗样本的训练。</p></li><li><p>实验证明本文提出的方法不仅可以提高在干净样本上的VQA任务的性能，还能提高<strong>在对抗样本上的鲁棒性</strong>。</p></li></ul><h4 id="相比于baseline还可以有效抵抗对抗攻击"><a href="#相比于baseline还可以有效抵抗对抗攻击" class="headerlink" title="相比于baseline还可以有效抵抗对抗攻击"></a>相比于baseline还可以有效抵抗对抗攻击</h4><p><img src="https://i.loli.net/2021/03/12/np4eUgXwkz2rK5M.png" alt="image-20210312172242341" style="zoom:50%;"></p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li><strong>何时加入对抗样本</strong> 这个实验告诉我们：一般情况，我们提出一种数据增强方案，通常会从头到尾的使用，但是未必是好的。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h3&gt;&lt;p&gt;受到深度学习的快速发展，VQA 近年来取得了非常成功的进展。数据增强是深度学
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="VQA" scheme="http://yoursite.com/categories/cross-modal/VQA/"/>
    
    
      <category term="cross-modal,VQA" scheme="http://yoursite.com/tags/cross-modal-VQA/"/>
    
  </entry>
  
</feed>
