<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShiYaya</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-07-19T06:54:35.519Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ShiYaya</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Bridging the Gap between Vision and Language Domains for Improved Image Captioning</title>
    <link href="http://yoursite.com/2021/07/18/Bridging-the-Gap-between-Vision-and-Language-Domains-for-Improved-Image-Captioning/"/>
    <id>http://yoursite.com/2021/07/18/Bridging-the-Gap-between-Vision-and-Language-Domains-for-Improved-Image-Captioning/</id>
    <published>2021-07-18T11:34:19.000Z</published>
    <updated>2021-07-19T06:54:35.519Z</updated>
    
    <content type="html"><![CDATA[<p>本文认为视频描述模型分两步：</p><p> 1) first link the image modality to the text modality.</p><p>2) generate a semantically and grammatically correct descriptive sentence based on such textual guided visual information.</p><p>但是 目前的captioning models将这两步都融合在一个模块中，即decoder。由于视觉和语言领域之间的巨大差异，这类模型中的解码器必须将其大部分能力用于执行步骤1），从而分散注意力，无法高质量地完成步骤2）。</p><p>因此，本文conduct the step 1) in the encoder by enriching image features with textual concepts. Specifically, we provide complete semantics information for the image features. In implementations, our approach consists of a Textual Distilling Module (TDM) and a Textual Association Module (TAM).</p><p>The TDM focuses on distilling textual concepts to the corresponding visual objects. Since the textual concepts are single words and only represent a fraction of a semantics, we further introduce the TAM to group related concepts to form a complete semantics. For instance, we group three separated concepts “woman”, “tennis” and “playing” into a phrase “woman playing tennis”. </p><blockquote><p>yaya: 所以<font color="blue"><strong>其实TAM的提出完全是因为TDM存在不足</strong></font>。这恰恰也是本文的缺陷。因为即便TAM把相关的概念组合到一起，也存在失误的问题，毕竟没有显式的监督。</p></blockquote><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://i.loli.net/2021/07/19/MygTjfGBRxeCt9W.png" alt="image-20210719123527662"></p><h4 id="Image-Encoder-and-Concept-Extractor"><a href="#Image-Encoder-and-Concept-Extractor" class="headerlink" title="Image Encoder and Concept Extractor"></a>Image Encoder and Concept Extractor</h4><p>对于图像特征，使用在VIsual-Genome上预训练的 Region-CNN model （Bottom-Up and Top-Down）.</p><p>对于 textual concepts, 使用 弱监督多示例学习方法： From Captions to Visual Concepts and Back。具体而言，是在MS-COCO caption 数据集上的1000个视觉概念上训练的。提取的视觉概念包括 objects, attributes, 和 relationships。对于每张图片，提取20个文本概念， 集合 $T=\left\{w_{1}, w_{2}, \ldots, w_{m}\right\} \in \mathbb{R}^{m \times e}$ where $w_{i} \in \mathbb{R}^{e}$ refers to the embedding of the $i^{t h}$ concept.</p><h3 id="Textual-Distilling-Module"><a href="#Textual-Distilling-Module" class="headerlink" title="Textual Distilling Module"></a>Textual Distilling Module</h3><p>使用注意力机制来选择与图像最相关的 top-K 文本概念。</p><blockquote><p>yaya: 其实没有必要，毕竟上一步提取到的textual concepts 就是与该图像最相关的内容。而且说是叫蒸馏，也没有任何蒸馏的操作!!!</p></blockquote><h3 id><a href="#" class="headerlink" title=" "></a> </h3><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><p>整体比较水，但是写作水平比较好。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文认为视频描述模型分两步：&lt;/p&gt;
&lt;p&gt; 1) first link the image modality to the text modality.&lt;/p&gt;
&lt;p&gt;2) generate a semantically and grammatically correct
      
    
    </summary>
    
      <category term="视频描述" scheme="http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="视频描述" scheme="http://yoursite.com/tags/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions</title>
    <link href="http://yoursite.com/2021/07/16/Show-Control-and-Tell-A-Framework-for-Generating-Controllable-and-Grounded-Captions/"/>
    <id>http://yoursite.com/2021/07/16/Show-Control-and-Tell-A-Framework-for-Generating-Controllable-and-Grounded-Captions/</id>
    <published>2021-07-16T02:07:16.000Z</published>
    <updated>2021-07-16T04:18:57.475Z</updated>
    
    <content type="html"><![CDATA[<p>来源：<a href="https://zhuanlan.zhihu.com/p/150667499" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/150667499</a></p><p>这是一篇收录于CVPR2019的图像描述生成文章，该文章提出了一种可以通过外部信号（图像区域）进行控制的图像描述生成模型，并且在可控描述质量与多样性两方面达到了Flickr30k Entities与COCO Entities数据集上的SOTA。作者声称，这是他们所知道的第一个可以基于图像区域进行控制的图像描述生成模型。</p><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>在对一张图像进行描述时，<strong>由于描述者的目标不同、关注的区域不同，进行描述的方式与角度也不尽相同</strong>。在对复杂的场景进行描述时，常常需要对模型进行控制，使模型更加关注于使用者感兴趣的区域，而现有的模型在生成图像描述时通常处于黑箱的状态，缺乏可控性与可解释性。</p><p>这篇文章提出了一种可控的图片描述生成模型，可以通过外界的控制信号操控模型生成多样化的描述。具体地，<strong>模型将图像中的一组区域作为控制信号</strong>，并生成基于这些区域的图像描述。如下图所示：</p><p><img src="https://i.loli.net/2021/07/16/aJeySqRXlhZCBNr.png" alt="image-20210716101209297" style="zoom:80%;"></p><p>上图中的（a）表示原始的图像描述生成模型，该模型将整张图像的特征作为输入，并得到基于图像整体的描述；（b）表示Up-Down模型生成描述的过程，该模型通过Faster R-CNN检测出一些目标区域，并综合这些目标区域的特征生成描述；（c）表示本文模型生成描述的过程，<strong>给定一个区域序列（或集合），模型会基于这些<font color="red">区域的特征</font>与<font color="red">顺序</font>生成描述，而忽略其他区域的特征，从而实现可控的图像描述</strong>。</p><p><img src="https://i.loli.net/2021/07/16/ANkfqLd3vpGQTRX.png" alt="image-20210716101331736" style="zoom: 67%;"></p><p>上图是将无序的区域集合作为控制信号时，模型的描述效果，图中的方框表示作为控制信号的区域。可以看到，控制信号中的物体都准确地出现在了描述中，而不在控制信号中的物体则没有出现。</p><p><img src="https://i.loli.net/2021/07/16/63CENAQTO5w9vhg.png" alt="image-20210716101547116"></p><p>上图是将有序的集合序列作为控制信号时模型的效果，可以看到描述中的名词块顺序与控制信号中的顺序一致。</p><h2 id="二、方法"><a href="#二、方法" class="headerlink" title="二、方法"></a>二、方法</h2><p><img src="https://i.loli.net/2021/07/16/4a6jA7cxRDlqo8k.png" alt="image-20210716101714728" style="zoom:67%;"></p><p>我们可以将图像描述中的单词分为两种：第一种是视觉词（visual words），表示对应于图中某个视觉实体的单词，如图中的boy、cap、shirt等；第二种是文本词（textual words），表示在图中没有对应实体的单词，如图中的a、with、on等。</p><p>进一步地，句中的每个名词可以与它的修饰语组成一个“名词块”（noun chunk），比如上文中的“a young boy”、“gray sweat jacket”等。这些名词块类似视觉词，通常与图像中的特定区域相对应，某些情况下，一个名词块也可能会与多个图像区域相对应。</p><p>每条描述由一系列的名词块连接而成，因此也对应着图像中的一组区域。只要改变生成描述时所考虑的图像区域或描述区域的顺序，便可以生成不同的图像描述。</p><p>基于上述假设，本文将图像中的一组区域作为控制信号输入模型中，使模型根据控制信号逐个生成名词块，并将名词块拼接成完整的句子，从而生成可控的、多样的图像描述。</p><p><img src="https://i.loli.net/2021/07/16/fCqmM3vZonAdbOu.png" alt="image-20210716101935615"></p><p>具体模型如上图所示，该模型的主要结构有：</p><p>1.Language model，由两层LSTM组成的语言模型，在每个时间点基于当前区域的特征与当前状态生成下一时刻的单词；</p><p>2.Chunk shifting gate，用于决定何时切换到下一个图像区域；</p><p>3.Adaptive attention，用于决定将要生成的单词是视觉词（基于视觉信息生成）还是文本词（基于当前的语句信息生成）；</p><p>4.Sorting network，在输入的控制信号为区域集合（无序）时，使用sorting network对控制信号进行排序。</p><p><strong>Language Model</strong></p><p><img src="https://i.loli.net/2021/07/16/vGzUDmCHS4qZt2f.png" alt="image-20210716102126047" style="zoom:50%;"></p><p>本文使用了双层LSTM结合attention作为语言模型。</p><p>第一层LSTM主要用于计算attention，给定上一时刻输出的单词向量 $y_{t-1}$ 、图像的总体特 征 $I$ 、上一时刻第二层LSTM的隐藏状态 $h_{t-1}^{2}$ 作为输入，第一层LSTM的输出结果用于计 算Adaptive Attention以及Chunk Shifting Gate, 决定下一步生成视觉词还是文本词, 以 及决定是否切换到下一个图像区域;<br>第二层LSTM主要作为language model，接受第一层的隐藏状态 $h_{t}^{1}$ 以及经过Adaptive Attention的上下文特征 $c_{t}$ 作为输入，预测下一个单词。<br>注意，本文通过对控制信号中所有区域的特征向量进行mean-pooling，将得到的结果作为 图像的总体特征 $I$ 。</p><p><strong>Chunk Shifting Gate</strong></p><p>Chunk Shifting Gate是一个布尔类型的门控变量，使用它更新当前区域的规则如下：</p><p>$\boldsymbol{r}_{t+1} \leftarrow \boldsymbol{R}[i], \quad$ where $i=\min \left(\sum_{k=1}^{t} g_{k}, N\right), g_{k} \in\{0,1\}$</p><p>上式中R表示由图像区域组成的有序序列，需要注意的是，由于每个名词块可能是基于多个 图像区域产生的，因此R中的每个元素都是一个区域集合，集合中通常只有一个区域，在集 合中包含多个区域的情况下，名词块便会综合这些区域的信息而产生。<br>上式中的 $r_{t+1}$ 表示在t+1时刻所考虑的区域集合, $\mathrm{N}$ 表示R中的区域集合数量。对于每个区 域, 作者使用Resnet-101提取区域中的特征, 并使用特征向量表示这个区域，因此控制信号 也可以视作由一组区域的特征向量组成。<br>可以看到，在每个时刻，如果该时刻的门控变量 $g_{t}$ 为 1, 则将 $r_{t+1}$ 更新为R中的下一个区 域集合，否则不更新。<br>门控变量 $g_{t}$ 的具体计算方法如下。在每个时间点，基于第一层LSTM的状态设立一个chunk sentinel, 记作 $s_{c}^{t}$, chunk sentinel可以表示在每个chunk结尾处LSTM的状态。</p><script type="math/tex; mode=display">\begin{aligned}&l_{t}^{c}=\sigma\left(\boldsymbol{W}_{i g} \boldsymbol{x}_{t}+\boldsymbol{W}_{h g} \boldsymbol{h}_{t-1}\right) \\&\boldsymbol{s}_{t}^{c}=l_{t}^{c} \odot \tanh \left(\boldsymbol{m}_{t}\right)\end{aligned}</script><p>其中 $m_{t}$ 表示LSTM的cel|状态, $h_{t-1}$ 表示上一时刻 $\mathrm{STM}$ 的隐藏状态。随后, 模型基于 $s_{c}^{t}$ 与区域特征 $r_{t} ，$ 计算 $g_{t}$ 的概率分布:</p><script type="math/tex; mode=display">\begin{gathered}z_{t}^{c}=\boldsymbol{w}_{h}^{T} \tanh \left(\boldsymbol{W}_{s g} \boldsymbol{s}_{t}^{c}+\boldsymbol{W}_{g} \boldsymbol{h}_{t}\right) \\\boldsymbol{z}_{t}^{r}=\boldsymbol{w}_{h}^{T} \tanh \left(\boldsymbol{W}_{s r} \boldsymbol{r}_{t}+\left(\boldsymbol{W}_{g} \boldsymbol{h}_{t}\right) \mathbb{1}^{T}\right)\end{gathered}</script><script type="math/tex; mode=display">p\left(g_{t}=1 \mid \boldsymbol{R}\right)=\frac{\exp z_{t}^{c}}{\exp z_{t}^{c}+\sum_{i=1}^{n} \exp z_{t i}^{r}}</script><p>计算过程类似于计算 $h_{t}$ 与 [ $\left.s_{t}^{c} ; r_{t}\right]$ 的attention，当模型更加注意 $s_{t}^{c}$ 时, $g_{t}=1$ 的概率 更大， 因此模型有更大的概率切换到下一个区域; 当模型更加注意 $r_{t}$ 时，模型则更有可能 停留在当前的区域。借助Chunk Shifting Gate，模型便可以决定何时切换关注的区域, 并 按照顺序生成这些区域对应的名词块。</p><p><strong>Adaptive Attention</strong></p><p>在生成单词时，为了判断当前单词究竟是基于视觉信息的视觉词还是基于当前语句信息的文本词，本文引入了Adaptive Attention机制。Adaptive Attention的计算方式如下，首先设立一个visual sentinel（类似上文的chunk sentinel）：</p><script type="math/tex; mode=display">\begin{aligned}l_{t}^{v} &=\sigma\left(\boldsymbol{W}_{i s} \boldsymbol{x}_{t}+\boldsymbol{W}_{h s} \boldsymbol{h}_{t-1}\right) \\\boldsymbol{s}_{t}^{v} &=\boldsymbol{l}_{t}^{v} \odot \tanh \left(\boldsymbol{m}_{t}\right)\end{aligned}</script><p>随后, 计算 $h_{t}$ 对 $\left[r_{t} ; s_{t}^{v}\right.$ ]的attention:</p><script type="math/tex; mode=display">\begin{gathered}\boldsymbol{z}_{t}^{r}=\boldsymbol{w}_{h}^{T} \tanh \left(\boldsymbol{W}_{s r} \boldsymbol{r}_{t}+\left(\boldsymbol{W}_{g} \boldsymbol{h}_{t}\right) \mathbb{1}^{T}\right) \\\boldsymbol{\alpha}_{t}=\operatorname{softmax}\left(\left[\boldsymbol{z}_{t}^{r} ; \boldsymbol{w}_{h}^{T} \tanh \left(\boldsymbol{W}_{s s} \boldsymbol{s}_{t}^{v}+\boldsymbol{W}_{g} \boldsymbol{h}_{t}\right)\right]\right)\end{gathered}</script><p>基于attention的结果, 可以计算出当前时刻模型正在关注的上下文特征 $c_{t}:$</p><script type="math/tex; mode=display">\boldsymbol{c}_{t}=\sum_{i=1}^{n+1} \boldsymbol{\alpha}_{t i}\left[\boldsymbol{r}_{t} ; \boldsymbol{s}_{t}^{v}\right]</script><p>在大多数时候, 模型只会关注 $s_{t}^{v}$ 或 $r_{t}$ 所包含的区域中的一个，如果模型更加关注visual sentinel, 便会生成textual word; 反之, 便会生成基于 $r_{t}$ 中某个区域的的visual word.</p><p><strong>Sorting Network</strong></p><p>上述讨论的前提是控制信号R是一组有序的图像区域序列，模型可以按照区域在R中的顺序生成相应的文本。如果给定的控制信号是无序的，如何使模型按照合适的顺序生成文本？本文使用了一种sorting network对无序的控制信号进行排序，实现方法如下。</p><p>假设无序的集合R中包含N个区域集，首先使用全连接层将每个区域集的特征映射为N维向量，随后将所有特征向量拼接，得到N*N的特征矩阵；随后使用Sinkhorn算子迭代地处理特征矩阵，经过该算子可以得到一个类似置换矩阵的“软”置换矩阵，它与置换矩阵的主要区别在于置换矩阵是离散的，而它是连续的。</p><p>在训练时，作者通过最小化该置换与真实结果之间的均方误差来训练网络；在测试时，则使用匈牙利算法进行匹配，将“软”置换矩阵转化为最终的置换，以此来对R进行排序。</p><h2 id="三、训练"><a href="#三、训练" class="headerlink" title="三、训练"></a>三、训练</h2><p>由于模型需要同时预测两个分布, $-$ 个是 $p\left(y_{t} \mid R\right) ，$ 表示寺刻生成单词的概率分布; 另一 个是 $p\left(g_{t} \mid R\right) ，$ 表示t时刻chunk shifting gate的概率分布。为了同时训练两个分布, 模型 训练时的loss function如下:</p><script type="math/tex; mode=display">\begin{aligned}L(\theta) &=-\sum_{t=1}^{T}(\log \overbrace{p\left(y_{t}^{*} \mid \boldsymbol{r}_{1: t}^{*}, \boldsymbol{y}_{1: t-1}^{*}\right)}^{\text {Word-level probability }}+\\&+g_{t}^{*} \log p\left(g_{t}=1 \mid \boldsymbol{r}_{1: t}^{*}, \boldsymbol{y}_{1: t-1}^{*}\right)+\\&+\left(1-g_{t}^{*}\right)(1-\log \underbrace{p\left(g_{t}=1 \mid \boldsymbol{r}_{1: t}^{*}, \boldsymbol{y}_{1: t-1}^{*}\right)}_{\text {Chunk-level probability }})\end{aligned}</script><p>其中 $y_{1: t}^{<em>}, g_{1: t}^{</em>} \quad r_{1: t}^{*}$ 为ground truth。除了使用cross entropy作为损失函数，本文 还使用了强化学习对模型进行优化，并使用如下表达式计算损失函数的梯度：</p><script type="math/tex; mode=display">\nabla_{\theta} L(\theta)=-\left(r\left(\boldsymbol{w}^{s}\right)-b\right)\left(\nabla_{\theta} \log p\left(\boldsymbol{w}^{s}\right)+\nabla_{\theta} \log p\left(\boldsymbol{g}^{s}\right)\right)</script><p>其中 $w^{s}$ 与 $g^{s}$ 表示模型预测得到的句子与chunk shifting gate序列, $r\left(w^{s}\right)$ 表示对于 $w^{s}$ 的奖励, 而b表示奖励的baseline, 只有超过baseline才能获得奖励。此处b $=r\left(w^{\prime}\right)$, $w^{\prime}$ 表示模型使用贪心策略生成的句子。<br>除了提高描述的质量，本文模型还需要保证描述的可控性，因此除了衡量描述质量的指标 CIDEr, 本文还提出了另一种指标NW，用于衡量模型生成的语句与控制信号的对齐程度，并 将CIDEr与NW同时作为强化学习时的奖励。NW的计算方式如下。<br>对于模型生成的句子y与数据集中与之对应的句子 $\mathrm{y}^{\star}$, 首先将所有名词从两个句子中提取出 来，随后将这些名词对齐。 对于相匹配的两个名词，将它们的embedding向量夹角的cos值 作为匹配分数; 如果存在未匹配的名词，则将匹配分数记作-1。将所有匹配分数加和后，使 用两个句子中较多的名次数对分数做归一化, 公式如下:</p><script type="math/tex; mode=display">\mathrm{NW}\left(\boldsymbol{y}, \boldsymbol{y}^{*}\right)=\frac{a l\left(\boldsymbol{y}, \boldsymbol{y}^{*}\right)}{\max \left(\# \boldsymbol{y}, \# \boldsymbol{y}^{*}\right)}</script><p>在所有的名词对齐中，挑选最高的NW分数作为最终的NW分数。</p><h2 id="四、评估"><a href="#四、评估" class="headerlink" title="四、评估"></a>四、评估</h2><h3 id="4-1-数据集"><a href="#4-1-数据集" class="headerlink" title="4.1 数据集"></a>4.1 <strong>数据集</strong></h3><p>本文在两个数据集上进行评估，分别是Flickr30k Entities与COCO Entities。其中，Flickr30k是图像描述生成工作中常用的数据集，Flickr30k Entities是它的扩展数据集。对于每条描述，Flickr30k Entities挑选出其中的所有名词块，并使用bounding box标注出图像中的对应物体。如下图所示：</p><p><img src="https://i.loli.net/2021/07/16/6KAp8l9vDuWnMba.jpg" alt="img"></p><p>MSCOCO是图像描述生成工作的常用数据集，COCO Entities是MSCOCO数据集的扩展，由本文作者提出。对于MSCOCO中的每条描述，作者将其中的每个名词与5个相似度最高的物体类别关联到一起（使用词向量的相似度）；随后使用目标检测器检测图片中的目标，如果检测到的目标类别与名词类别匹配，便将名词与目标区域相关联；最后，人工检查每个关联是否正确。下图是COCO Entities数据集中的一个例子：</p><p><img src="https://i.loli.net/2021/07/16/HukQ4vXIm291sWS.jpg" alt="img"></p><h3 id="4-2-评估指标"><a href="#4-2-评估指标" class="headerlink" title="4.2 评估指标"></a>4.2 <strong>评估指标</strong></h3><p>文章的评估方法与常规的image captioning评估有所区别。在评估时，除了将图像作为输入外，作者还将图像中的一系列区域作为控制信号输入；相应地，在评估生成描述的质量时，在数据集中只有控制信号与其相同的句子会作为衡量标准。</p><p>关于评估指标的选取，除了常用的BLUE-4、METEOR、ROUGE、CIDEr、SPICE外，在使用区域序列作为控制信号时，作者还使用了上文提到的NW分数来评估生成描述中的名词块与控制信号的对齐效果；而在使用区域集合作为控制信号时，为了评估生成的描述对控制信号的覆盖效果，作者还提出了一种类似交并比(IoU)的评测指标：</p><script type="math/tex; mode=display">\operatorname{IoU}\left(\boldsymbol{y}, \boldsymbol{y}^{*}\right)=\frac{\mathrm{I}\left(\boldsymbol{y}, \boldsymbol{y}^{*}\right)}{\# \boldsymbol{y}+\# \boldsymbol{y}^{*}-\mathrm{I}\left(\boldsymbol{y}, \boldsymbol{y}^{*}\right)}</script><p>其中, $I\left(y, y^{*}\right)$ 表示将两个句子中的名词对齐后，相应名词的词向量cos相似度之和。 $10 \mathrm{U}$ 越高, 两个句子中名词的重合率便越高。</p><h3 id="4-3-Baselines"><a href="#4-3-Baselines" class="headerlink" title="4.3 Baselines"></a>4.3 <strong>Baselines</strong></h3><p><strong>1.Controllable LSTM</strong></p><p>在Controllable LSTM中，作者使用一个没有attention的、单层的LSTM作为语言模型。随后，作者将控制信号序列R中的特征输入到另一个LSTM中，并提取这个LSTM最终的隐藏状态，将该隐藏状态与图像特征进行拼接，输入到LSTM语言模型中，以此实现控制效果。</p><p><strong>2.Controllable Up-Down</strong></p><p>Up-Down模型是一种比较具有代表性的图像描述生成模型，它通过目标检测器检测出图像中的物体，并基于这些物体所在的图像区域生成描述。作者实现Controllable Up-Down的方式比较简单，将控制信号R中的区域输入到Up-Down模型中，并忽略图像中的其他区域即可。当然，由于Up-Down模型中对于不同区域的关注是无序的，因此Controllable Up-Down并不能按照给定的区域顺序生成描述。</p><p><strong>3.Ours without visual sentinel</strong></p><p>为了验证visual sentinel的重要性，作者提出了没有visual sentinel的模型，此时模型缺乏识别visual word与textual word的能力。</p><p><strong>4.Ours with single sentinel</strong></p><p>本文中visual sentinel与chunk sentinel的计算方式比较相似，为了验证它们的重要性，作者提出了将visual sentinel与chunk sentinel融合的模型，此时将有一个单独的sentinel同时充当二者的功能。</p><p><strong>5.Others</strong></p><p>本文还使用了几种不可控的模型作为对比，分别是FC-2K、Up-Down、Neural Baby Talk。需要注意的是，由于这几种模型无法接收控制信号所带来的额外信息，因此将本文模型与它们直接做对比是不公平的。</p><h3 id="4-4-定量评估"><a href="#4-4-定量评估" class="headerlink" title="4.4 定量评估"></a>4.4 <strong>定量评估</strong></h3><p>作者从三个角度评估模型的效果，分别是在有序控制信号下的效果、无序控制信号下的效果以及描述生成的多样性。</p><p><img src="https://i.loli.net/2021/07/16/FWhJRBAcynQiafC.png" alt="image-20210716105025286"></p><center>在COCO Entities上的评估结果（有序控制信号）</center><p><img src="https://i.loli.net/2021/07/16/kONCgj7Z9iMpwJE.png" alt="image-20210716105151176" style="zoom:50%;"></p><center>在Flickr30k Entities上的评测结果（有序控制信号）</center><p>作者首先使用COCO Entities与Flickr30k Entities评估模型在有序控制信号下的效果。可以看到，本文的模型在所有指标上都达到了最好的效果，不过由于FC-2K、Up-Down、Neural Baby Talk模型本身是不可控的，而Controllable Up-Down模型虽然可控，但无法按照控制信号的顺序生成描述，因此<font color="red">本文模型与它们的比较并不公平</font>&gt;</p><p><img src="https://i.loli.net/2021/07/16/TgZWAOdFJzwexGc.png" alt="image-20210716105307222" style="zoom:50%;"></p><center>在COCO Entities上的评估结果（无序控制信号）</center><p><img src="https://i.loli.net/2021/07/16/tajQBzJMN1GViLW.png" alt="image-20210716105318174" style="zoom:50%;"></p><center>Flickr30k Entities上的评测结果（无序控制信号）</center><p>随后，作者又在两个数据集上评估了模型在无序控制信号下的效果，此时本文模型与Controllable Up-Down模型的对比是有意义的。可以发现，本文模型在绝大多数指标上都优于Controllable Up-Down。</p><p><img src="https://i.loli.net/2021/07/16/x5qd7ConpzR6haJ.png" alt="image-20210716105441980" style="zoom: 50%;"></p><center>在COCO Entities数据集上的多样性评估结果</center><p>作者在COCO Entities数据集上评测了模型生成描述的多样性，并与另外两个专注于提高描述多样性的模型作对比。在评估多样性时，作者将所有可行的区域都作为控制信号输入到模型中，以此保证生成多样的描述；在计算评估指标时，作者首先使用每个模型各自生成20条描述，对于数据集中的每条描述，将选择模型生成的描述中得分最高的一条进行评估，并对每个评估指标都进行如上操作。</p><p>经过上述操作，可以得到每个模型的20条描述对于每个指标的最高分，并将其作为最终的评估分数。模型能得到的最高分越高，说明模型有能力生成效果更好的描述，这也意味着模型的多样性更好。从评估结果中可以看到，在大多数指标中，本文的模型要明显好于其他两种模型。</p><h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p>这篇文章提出了一种可控的图像描述生成模型。这种可控性体现在两方面：第一，给定图像中的一些区域，模型会基于这些区域生成描述，忽视其他的区域；第二，给定这些区域的顺序，模型会按照顺序生成描述。</p><p>在COCO Entities与Flickr30k Entities数据集上，作者评估了模型的可控描述生成质量，并在与其他模型的对比中得到了最佳效果（<font color="red"><strong>个人认为只有在无序控制信号时，与Controllable Up-Down模型的对比比较有意义</strong></font>）；此外，作者还评估了模型的多样性，并在与其他模型的对比中得到了较好的效果，说明该模型有能力生成更高质量的描述。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;来源：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/150667499&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/150667499&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这是一篇
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Multimodal Few-Shot Learning with Frozen Language Models</title>
    <link href="http://yoursite.com/2021/07/09/Multimodal-Few-Shot-Learning-with-Frozen-Language-Models/"/>
    <id>http://yoursite.com/2021/07/09/Multimodal-Few-Shot-Learning-with-Frozen-Language-Models/</id>
    <published>2021-07-09T04:06:20.000Z</published>
    <updated>2021-07-09T09:16:19.430Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>GTP-3, T5 给出的一些启发，在大规模数据上，以自回归的方式训练的语言模型和 prefix tunning 展示出了在<strong>小样本</strong>上学习<strong>新任务</strong>的能力。</p><p><strong><font color="red">本文中，我们展示出一个简单且有效的方法来迁移这种小样本学习能力到多模态的设置中（vision and language）。</font></strong> 本文中使用 image 替换纯语言模型中的前缀 prefix。</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在这里，我们提出了Frozen，一种让预先训练好的语言模型获得视觉信息的方法，这种方法可以将它的小样本学习能力扩展到多模态环境中，而不改变其权重。 Frozen由一个经过训练的 <font color="red"><strong>神经网络 $\phi$​</strong> </font> 组成，将图像编码到一个<strong><font color="red">大型预训练语言模型 $\theta$ </font></strong>的单词嵌入空间中，这样语言模型就能为这些图像生成caption。 <strong>语言模型的权重保持不变</strong>，但梯度通过它进行反向传播, 从头开始训练图像编码器。</p><p><img src="https://i.loli.net/2021/07/09/oq9fuFkBjVCgm34.png" alt="image-20210709144031286" style="zoom:50%;"></p><font color="green">yaya: 由于预训练语言模型的预训练任务是自回归的生成式任务，因此，这里将其迁移到多模态任务时，也采用了image captioning 这种生成式任务。</font><p><img src="https://i.loli.net/2021/07/09/nDqWZCSiNExyjlk.png" alt="image-20210709144211177" style="zoom:50%;"></p><p>尽管Frozen是在单一的图像-文字对上训练的，但一旦训练好了，它就能对多个图像和文字的有序集合作出有效反应。 这使用户能够，例如 在评估其表现之前，用几个新的多模态任务的例子来 “提示 “它，或者在立即询问一个新的视觉类别之前 “教 “它这个类别的名称。</p><p>我们开发Frozen的目的不是为了在任何特定的任务上实现最大的性能，而且在许多情况下，它远远不是最先进的。 尽管如此，在没有看到这些基准所提供的少数训练实例的情况下，它在广泛的任务中的表现远远高于琐碎的基线。</p><p>总的来说，我们的贡献如下。 1. 我们提出了Frozen，一种模块化、可扩展和高效的方法，用于训练大型语言模型的视觉前端。 由此产生的组合模型保留了大型语言模型的所有能力，但也能以任意的顺序处理文本和图像输入。 2. 我们表明，这种模型将其快速适应任务的能力、百科全书式的知识和快速的概念结合从单纯的语言环境转移到了多模态环境中，并验证了用视觉和语言信息提示他们，严格来说比单纯用语言信息提示更有效。 3. 我们在一系列现有的和新的基准上对这些能力进行量化，为今后分析这些能力铺平道路。</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>使用 Conceptual Captions dataset 来训练。仅仅微调$\phi$, 不微调 $\theta$。因为我们可用的图文对相比于预训练 $\theta$ 时用到的数据是相当少的。</p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><p>没怎么看完，实验上，好像没有做image captioning 任务，比较奇怪。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h2&gt;&lt;p&gt;GTP-3, T5 给出的一些启发，在大规模数据上，以自回归的方式训练的语言
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>LM-MLC 一种基于完型填空的多标签分类算法</title>
    <link href="http://yoursite.com/2021/07/08/LM-MLC-%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E5%AE%8C%E5%9E%8B%E5%A1%AB%E7%A9%BA%E7%9A%84%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2021/07/08/LM-MLC-一种基于完型填空的多标签分类算法/</id>
    <published>2021-07-08T08:07:41.000Z</published>
    <updated>2021-07-08T08:15:13.768Z</updated>
    
    <content type="html"><![CDATA[<p>转载：<a href="https://mp.weixin.qq.com/s/1VnR1zQ4LWnUDl6USdxVhQ#" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/1VnR1zQ4LWnUDl6USdxVhQ#</a></p><p>同样一篇基于 PET， 模板的方式来做NLP任务的论文</p><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a><strong>1 前言</strong></h2><p>本文主要介绍本人在全球人工智能技术创新大赛【赛道一】设计的一种基于完型填空(模板)的多标签分类算法：LM-MLC，该算法拟合能力很强能感知标签关联性，在多个数据集上测试表明该算法与主流算法无显著性差异，在该比赛数据集上的dev效果很好，但是由于比赛期间事情多，没有好好在test集做测试。</p><p>个人认为该算法根正苗红，理论上可以获得更好的效果，因此做个开源，抛砖引玉，希望有人能提出更为有效的改进。本次开源的代码可读性较强，也有较高的扩展性，本人把LM-MLC可做的修改均写成超参形式，方便各位做测试。</p><p>Github：<a href="https://github.com/DunZhang/LM-MLC" target="_blank" rel="noopener">https://github.com/DunZhang/LM-MLC</a></p><h2 id="2-多标签分类任务"><a href="#2-多标签分类任务" class="headerlink" title="2 多标签分类任务"></a><strong>2 多标签分类任务</strong></h2><p>NLP里的多标签分类任务，输入多为一段文本，输出该文本的的标签。比如在新闻类型分类中，一篇新闻可以同时有”军事”、”政治”和”历史”三个标签，再举个例子，疾病分类中，一位患者可以既感冒又咳嗽。</p><p>多标签分类任务依据数据特点又可以划分为多种类型。</p><h3 id="2-1-文本长度"><a href="#2-1-文本长度" class="headerlink" title="2.1 文本长度"></a>2.1 文本长度</h3><p>文本长度会直接影响到算法的选择，长度过长对算法语义理解要求会变高，如果长度超过512个字符，就不好直接使用BERT，需要分段编码或使用其他算法(LSTM、XLNET等)。文本过长也使得训练时间变长，着实影响到了穷逼的炼丹进度。</p><h3 id="2-2-内容是否加密"><a href="#2-2-内容是否加密" class="headerlink" title="2.2 内容是否加密"></a>2.2 内容是否加密</h3><p>大部分数据集是未加密的，直接可以看到原文。少部分数据集做了脱敏处理，原字词会被替换为数字或其他符号。虽说保护了隐私，但是对算法要求变高了，因为无法使用预训练模型，也没有办法做错误分析。为了达到更好的效果，需要对此类数据集继续做预训练，然后基于该预训练模型微调多标签分类任务。上文中我说的比赛就是做了加密处理。</p><h3 id="2-3-标签数量"><a href="#2-3-标签数量" class="headerlink" title="2.3 标签数量"></a>2.3 标签数量</h3><p>多标签分类数据集标签数量有多有少，少则几个多达上千(比如知乎看山杯数据集)，标签数量多少也会影响到算法的选择，标签数量过多时，多标签分类任务也可以考虑转化为搜索任务，此外标签数量过多时，往往会有严重的类不平衡问题，这在设计算法时也是需要考虑的。</p><h3 id="2-4-标签关联性"><a href="#2-4-标签关联性" class="headerlink" title="2.4 标签关联性"></a>2.4 标签关联性</h3><p>有些数据集的标签之间会存在相关性，比如新闻分类中，关于军事的新闻可能也会和政治有关系，疾病分类中，如果得了高血压，就有可能影响到视网膜。所以对于有关联的标签，算法如能考虑到标签的关联性，那么理论上效果是可以提升的，LM-MLC算法里就认真探索了标签关联性。</p><h2 id="3-自然语言处理中的完型填空"><a href="#3-自然语言处理中的完型填空" class="headerlink" title="3 自然语言处理中的完型填空"></a><strong>3 自然语言处理中的完型填空</strong></h2><p>先说一说完型填空，即一段文本，挖掉几个词，让模型去猜挖掉的词是什么，其实这就是遮挡语言模型。我们可以借助完型填空完成一些自然处理任务，关于这块的介绍，苏建林的两篇博客：博客1，博客2介绍的细致、精彩，因此我就不再过多叙述。</p><p>为了便于理解，我举个完型填空做新闻分类的例子，待分类文本是：美国攻打伊拉克，是因为萨达姆偷了布什家的高压锅 ，我们在该段文本后(或前面)加上如下一段话：这是关于[MASK]的新闻，这样完整的进入BERT的输入就是：[CLS]美国攻打伊拉克，是因为萨达姆偷了布什家的高压锅，这是关于[MASK]的新闻[SEP], 我们只要让模型判断掩掉的词是什么即可，如预测词是军事，那么分类结果就是军事，通常情况下候选词是全体标签。</p><p><img src="https://i.loli.net/2021/07/08/Eybwgr72T4D6tQz.png" alt="640" style="zoom:50%;"></p><h2 id="4-LM-MLC-把完型填空用在多标签分类数据集上"><a href="#4-LM-MLC-把完型填空用在多标签分类数据集上" class="headerlink" title="4 LM-MLC:把完型填空用在多标签分类数据集上"></a><strong>4 LM-MLC:把完型填空用在多标签分类数据集上</strong></h2><p>说了那么多背景知识，下面开始正式介绍LM-MLC算法。</p><h3 id="4-1-模板构建"><a href="#4-1-模板构建" class="headerlink" title="4.1 模板构建"></a><strong>4.1 模板构建</strong></h3><p>上文举得例子是关于分类的，那么对于多标签分类任务要如何构建模板呢，很自然的可以加入如下模板：”有标签1：[MASK],有标签2：[MASK],有标签3：[MASK]”, 该[MASK]预测的词就是：YES或NO。一图胜千言，我们假设是在新闻多标签分类任务，共有三个标签分别是”军事”,”政治”和”历史”，假设文本是：美国攻打伊拉克，那么输入就是：</p><p><img src="https://i.loli.net/2021/07/08/BXYNRLdHau9jC7l.png" alt="640" style="zoom:50%;"></p><p>人工构建模板是一个困难的事情，模板选取很关键，模板是很不稳定的，因此本算法使用了P-tuning的做法，把模板变为[unused*]或者自己在bert的vocab中新建一些词汇，总之就是让模型自己去寻找最佳模板，所以上图输入可以进一步修改为如下形式：</p><p><img src="https://i.loli.net/2021/07/08/abroHAk3t7B12sY.png" alt="640"></p><p>至于[MASK]前后放多少未使用字符，模板位置，不同标签是否使用不同[MASK]等就是各种微小改动，具体可以看开源代码，均以超参形式存于代码中，可以一次试个够。</p><h3 id="4-2-模型架构"><a href="#4-2-模型架构" class="headerlink" title="4.2 模型架构"></a><strong>4.2 模型架构</strong></h3><p>本次博客主要还是抛砖引玉，想把方法公开了和各位讨论，本次所用模型都是极为简单的，没有使用任何比赛的trick，主要提供思想。</p><h4 id="4-2-1-Baseline模型架构"><a href="#4-2-1-Baseline模型架构" class="headerlink" title="4.2.1 Baseline模型架构"></a><strong>4.2.1 Baseline模型架构</strong></h4><p>本模型的Baseline模型就是基于BERT的，模型架构极为简单，CLS向量后接全连接层，然后过Sigmoid层作为每个tag的分数。损失函数可以选用最基础的BCELoss。多提一句，也可以当成分类任务做，用交叉熵优化，但其实看公式，其实是差不多的，本人就懒得折腾了。</p><h4 id="4-2-2-LM-MLC模型架构"><a href="#4-2-2-LM-MLC模型架构" class="headerlink" title="4.2.2 LM-MLC模型架构"></a><strong>4.2.2 LM-MLC模型架构</strong></h4><p>模型架构图前面已经有了，再用文字描述下：基础部分还是BERT，获取TokenEmbedding后使用gather方法提取[MASK]的embedding，然后通过Sigmoid获取每个标签的分数，同样使用BCELoss损失函数。</p><h3 id="4-3-如何训练"><a href="#4-3-如何训练" class="headerlink" title="4.3 如何训练"></a><strong>4.3 如何训练</strong></h3><p>最简单的训练方式就是一次掩盖掉所有的标签然后全部预测，此类方法适合标签没有关联性的数据集。如果标签之间存在关联性，肯定要通过一部分标签值来预测余下的标签值，这也是LM-MLC算法的核心，很多方法都是围绕这个点设计的。</p><p>如何判断标签之间有无关联性呢，方法很简单，取训练集的标签值购置01向量，然后计算统计相关系数即可，根据系数值和下表判断相关性：</p><p><img src="https://i.loli.net/2021/07/08/gbGJ4LmNfHtYKch.png" alt="图片"></p><p>在训练过程中，要把一部分[MASK]改为YES或NO，这种方式让模型在对[MASK]标签做预测时不仅能感知到哪些标签值是0哪些标签值是1以及哪些标签是待预测的。在本算法中使用了[MASK]的真实值，相当于teacher-force-learning，同时为了提升模型的鲁棒性，会以1%的概率故意给错标签，实测这个trick还是挺关键的。</p><p>想要完整实现该思想时，要考虑好多细节，本人想了3种实现策略，但是也没找到最优解，我把思路和逻辑一一罗列出来供大家参考讨论。</p><p>思路一，全随机 在训练时随机掩盖一部分标签，让模型进行预测并计算损失损失。</p><p>思路二，固定掩盖顺序 假设有四个标签，掩盖顺序为1-&gt;2-&gt;3-&gt;4，那么可能的掩盖顺序是：1，12，123，1234，这种方法在预测时也要使用相同的方式去预测，掩盖顺序目前没有发现最优解。</p><p>思路三，UniLM 把标签作为生成任务，通过修改AttentionMask的方式来实现，即以UniLM的形式去训练，这个我没有尝试，因为这种方式已经不再是完型填空的范畴了，欢迎大家尝试。</p><p>本人比较推荐思路一，在实验中思路一的效果也是不错的</p><h3 id="4-4-如何预测"><a href="#4-4-如何预测" class="headerlink" title="4.4 如何预测"></a><strong>4.4 如何预测</strong></h3><p>预测时的基本思想是先预测一个标签，然后在该标签预测结果的基础上继续预测其他标签。那么最重要的问题就是如何确定预测顺序，有如下几种预测方法：</p><p>方法1：随机，即随机确定一个顺序，不足在于不同顺序会影响性能上下浮动约2个百分点</p><p>方法2：固定顺序，即按照固定顺序预测，难点在于顺序难以确定 </p><p>方法3：Top-P,每一次选取模型置信度最高的标签作为首先预测的标签，效果尚可 </p><p>方法4：搜索算法，使用遗传算法等搜索算法选取一个在dev上效果最号的预测顺序作为最终顺序，也可以不用搜索算法，直接random几百次找个最好的也行</p><p>方法3效果还行，方法4可提升逼格发论文。</p><h3 id="4-5-如何进一步提升效果"><a href="#4-5-如何进一步提升效果" class="headerlink" title="4.5 如何进一步提升效果"></a><strong>4.5 如何进一步提升效果</strong></h3><p>在当前预训练+微调的框架下，有一个简单有效的方法那就是不要停止预训练，即把预训练模型在微调数据集上继续做预训练，然后再做微调，该方法以获得ACL2020最佳论文荣誉提名，具体参见Don’t Stop Pretraining:Adapt Language Models to Domains and Tasks。</p><p>为什么我说LM-MLC是根正苗红呢，因为完形填空他完美契合mlm预训练任务，都是预测[MASK], 我总结两个使用该思想的方法：</p><p>常规做法：先继续预训练，然后微调</p><p>联合训练：同时做Word Mask(mlm 任务)和Label Mask(完形填空任务)，然后把loss加一起，可以适当提高Label Mask的权重</p><p>本人是使用第二种，因为这两个任务实在是太契合了，通常我是微调25轮，其中前15轮联合训练，后10轮task-specific的训练，不能所有轮数都联合训练，那不然预测和训练的数据又会不一致。</p><h3 id="4-6-小结"><a href="#4-6-小结" class="headerlink" title="4.6 小结"></a><strong>4.6 小结</strong></h3><p>LM-MLC算法最大的缺点是不支持标签数量过多，假设有100个标签，模板长度为2，再加上自身MASK，那么光标签模板就占了300的长度，而BERT的输入长度限制为512，所以数据集标签多了是无法使用该方法的。</p><p>此外，由于时间精力有限，几乎没有找到合适的存在标签关联的数据集，所以对于标签关联性的一些构想还是缺少验证的，这种数据集怕是要手工构建了，绝大多数数据集都是标签无关，因此直接全部掩盖掉，全部预测就行了。</p><h2 id="5-简单实验"><a href="#5-简单实验" class="headerlink" title="5 简单实验"></a><strong>5 简单实验</strong></h2><p><strong>数据集介绍</strong></p><p>全球人工智能技术创新大赛【赛道一】比赛数据集，是关于医疗影像描述文本的，输出为哪些部位有异常，初赛是17分类，复赛在17分类的基础上又多了12标签分类，本人把复赛初赛复赛数据集合并到一起，当成17多标签分类任务来做。数据集不太方便提供下载。</p><p><strong>AAPD数据集</strong>，这是开源的数据集，我分析AAPD数据集并没有较强的标签关联性，搞不懂为啥SGM多标签分类算法要用这个训练集。。。</p><p><strong>Stackoverflow数据集</strong>，Stackoverflow的帖子都是带有tag的，截图如下，但是该数据集不能直接获得，需要去该网站（<a href="https://archive.org/details/stackexchange）下载，可能需要梯子，然后手工清洗后作为训练集，清洗代码可以见我另外一个开源库DomainSpecificThesaurus。或者先用我清洗的10W数据集，下载链接请往后看。" target="_blank" rel="noopener">https://archive.org/details/stackexchange）下载，可能需要梯子，然后手工清洗后作为训练集，清洗代码可以见我另外一个开源库DomainSpecificThesaurus。或者先用我清洗的10W数据集，下载链接请往后看。</a></p><p><strong>RCV1-V2数据集</strong>，也是开源数据集，标签间也没啥关联性，而且看着文本总感觉很奇怪。</p><p>我提供了AAPD数据集、清洗后的Stackoverflow数据集和RCv1-v2数据集，下载地址：</p><p><a href="https://drive.google.com/file/d/1qNOfb4WX7TpTSPuhGIAAvzAbOc1MbHCO/view" target="_blank" rel="noopener">https://drive.google.com/file/d/1qNOfb4WX7TpTSPuhGIAAvzAbOc1MbHCO/view</a></p><p><strong>实验结果</strong></p><p>因为硬件资源有限，本人工作也较忙，没有做太多的实验，这里把有记录的比赛数据集结果和AAPD数据集测试结果贴出来。</p><p>全球人工智能技术创新大赛【赛道一】的测试结果：</p><p><img src="https://i.loli.net/2021/07/08/mqOFiescWHYvM3J.png" alt="图片"></p><p>AAPD数据集的测试结果：</p><p><img src="https://i.loli.net/2021/07/08/DmXbY1r7us8UFoc.png" alt="图片"></p><p>简单解释下四个方法的含义：</p><p>Baseline: BERT+FC, 详情见上文或源码</p><p>Baseline+mlm: 与mlm联合训练，即mlm_loss+bce_loss</p><p>LM-MLC: 基于完形填空的多标签分类算法，就本人所设计算法</p><p>LM-MLC: 与mlm联合预训练，详情见上文或源码</p><p>前前后后做了很多实验，客观来说，实验结果不太符合预期，效果在其他数据集上没有显著性提升，还是挺郁闷的，抛砖引玉，希望读者能提出的改进意见。不过该方法也没有明显差于其他方法，在以后比赛中作为一种融合模型还是可以滴。</p><h2 id="6-代码介绍"><a href="#6-代码介绍" class="headerlink" title="6 代码介绍"></a><strong>6 代码介绍</strong></h2><p>Github开源地址：<a href="https://github.com/DunZhang/LM-MLC代码做了好多修改，力求简洁易用，同时具有较强的可读性和可扩展性，文中提到的好多点都做成了超参形式，欢迎试水" target="_blank" rel="noopener">https://github.com/DunZhang/LM-MLC代码做了好多修改，力求简洁易用，同时具有较强的可读性和可扩展性，文中提到的好多点都做成了超参形式，欢迎试水</a></p><p><img src="https://i.loli.net/2021/07/08/HEIkqPGivCL5r7y.png" alt="图片"></p><p>目录结构及文件名含义如下：</p><p><img src="https://i.loli.net/2021/07/08/UT5K6DJNX12fIeb.png" alt="图片"></p><h2 id="7-TODOList"><a href="#7-TODOList" class="headerlink" title="7 TODOList"></a><strong>7 TODOList</strong></h2><p>数据集，多标签分类数据集实在是太少了，需要多搞点数据集尤其是中文数据集和标签相关的数据集</p><p>UnilM，可以考虑试一试，文本部分全部交互，标签逐个生成</p><p>考虑标签本身的语义信息，比如经济标签，经济二字本身就是有语义信息的</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载：&lt;a href=&quot;https://mp.weixin.qq.com/s/1VnR1zQ4LWnUDl6USdxVhQ#&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://mp.weixin.qq.com/s/1VnR1zQ4LWnUDl6
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>MDETR - Modulated Detection for End-to-End Multi-Modal Understanding</title>
    <link href="http://yoursite.com/2021/07/07/MDETR-Modulated-Detection-for-End-to-End-Multi-Modal-Understanding/"/>
    <id>http://yoursite.com/2021/07/07/MDETR-Modulated-Detection-for-End-to-End-Multi-Modal-Understanding/</id>
    <published>2021-07-07T08:43:47.000Z</published>
    <updated>2021-07-07T09:06:32.871Z</updated>
    
    <content type="html"><![CDATA[<h2 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h2><p>将目标检测与下游任务结合起来，端到端的训练</p><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p><code>物体检测</code>构成了大多数最先进的<code>多模式理解系统</code>的一个组成部分，通常作为一个黑箱来检测图像中的<code>固定概念词汇</code>。 这种流行的使用目标检测的方法没有考虑到下游的多模态理解任务，而且往往是性能的瓶颈。</p><p>此外，目标检测模型通常被冻结。这不仅阻碍了模型感知能力的进一步完善，而且还限制了它只能接触到检测到的物体，而不是整个图像。 这种 “流水线 “式的方法限制了与其他作为上下文的模态的共同训练。 在视觉语言设置中，它意味着 resulting system 的词汇限制在检测器的类别和属性上。 因此，这样的系统不能识别可以用自由格式文本表达的新概念组合。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;研究内容&quot;&gt;&lt;a href=&quot;#研究内容&quot; class=&quot;headerlink&quot; title=&quot;研究内容&quot;&gt;&lt;/a&gt;研究内容&lt;/h2&gt;&lt;p&gt;将目标检测与下游任务结合起来，端到端的训练&lt;/p&gt;
&lt;h2 id=&quot;存在的问题&quot;&gt;&lt;a href=&quot;#存在的问题&quot; clas
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>Seq2Seq中Exposure Bias现象的浅析与对策</title>
    <link href="http://yoursite.com/2021/07/05/Seq2Seq%E4%B8%ADExposure-Bias%E7%8E%B0%E8%B1%A1%E7%9A%84%E6%B5%85%E6%9E%90%E4%B8%8E%E5%AF%B9%E7%AD%96/"/>
    <id>http://yoursite.com/2021/07/05/Seq2Seq中Exposure-Bias现象的浅析与对策/</id>
    <published>2021-07-05T03:53:43.000Z</published>
    <updated>2021-07-05T04:25:42.759Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Seq2Seq中Exposure-Bias现象的浅析与对策"><a href="#Seq2Seq中Exposure-Bias现象的浅析与对策" class="headerlink" title="Seq2Seq中Exposure Bias现象的浅析与对策"></a>Seq2Seq中Exposure Bias现象的浅析与对策</h2><p>前些天笔者写了<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247503481&amp;idx=1&amp;sn=f998484d31148762630e2fdc16af01aa&amp;chksm=96ea11f9a19d98ef4a4c29e7fd86c01a278888374bc5a5ae9bd208de41cdfe6e53b67a09e967&amp;token=748748465&amp;lang=zh_CN&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">CRF用过了，不妨再了解下更快的MEMM？</a>，里边提到了 MEMM 的局部归一化和 CRF 的全局归一化的优劣。</p><p>同时，笔者联想到了 Seq2Seq 模型，因为 Seq2Seq 模型的典型训练方案 Teacher Forcing 就是一个局部归一化模型，所以它也存在着局部归一化所带来的毛病——也就是我们经常说的“Exposure Bias”。</p><p>带着这个想法，笔者继续思考了一翻，将最后的思考结果记录在此文。</p><p><img src="https://i.loli.net/2021/07/05/3zdSt9gLwn41GZa.png" alt="image-20210705121610582" style="zoom:50%;"></p><p>▲ 经典的 Seq2Seq 模型图示</p><p>本文算是一篇进阶文章，适合对Seq2Seq模型已经有一定的了解、希望进一步提升模型的理解或表现的读者。关于Seq2Seq的入门文章，可以阅读旧作<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247491314&amp;idx=1&amp;sn=3e22d4a6d732b0877fdc567d2bce1076&amp;chksm=96e9c172a19e48646005da05e143751aa9012c141dd1cf9846a2b418cbf854c7d343013105a1&amp;token=748748465&amp;lang=zh_CN&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">玩转Keras之seq2seq自动生成标题</a>和<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247499793&amp;idx=1&amp;sn=685c54d27186a89dcf32d91ce0927274&amp;chksm=96ea1f91a19d9687af5dbe751accc9c1ddd7392f6cf4294dc8a024751a64053cbcb8c60ef8f8&amp;token=748748465&amp;lang=zh_CN&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">从语言模型到Seq2Seq：Transformer如戏，全靠Mask。</a></p><p>本文的内容大致为：</p><ol><li>Exposure Bias 的成因分析及例子；</li><li>简单可行的缓解 Exposure Bias 问题的策略。</li></ol><h2 id="1-Softmax"><a href="#1-Softmax" class="headerlink" title="1. Softmax"></a>1. Softmax</h2><p>首先，我们来回顾 Softmax 相关内容。大家都知道，对于向量 $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$, 它的 Softmax 为:<br>$<br>\left(p_{1}, p_{2}, \ldots, p_{n}\right)=\frac{1}{\sum_{i=1}^{n} e^{x_{i}}}\left(e^{x_{1}}, e^{x_{2}}, \ldots, e^{x_{n}}\right)<br>$<br>由于 $e^{t}$ 是关于 $t$ 的严格单调递增函数，所以如果 $x_{k}$ 是 $x_{1}, x_{2}, \ldots, x_{n}$ 中的最大者， 那么 $p_{k}$也是 $p_{1}, p_{2}, \ldots, p_{n}$ 中的最大者。<br>对于分类问题，我们所用的 Ioss 一般是交叉嫡，也就是：<br>$<br>-\log p_{t}=\log \left(\sum_{i=1}^{n} e^{x_{i}}\right)-x_{t}(2)<br>$<br>其中 $t$ 是目标类。如文章 《寻求一个光滑的最大值函数》[1] 所述，上式第一项实际上是$\max \left(x_{1}, x_{2}, \ldots, x_{n}\right)$ 的光滑近似，所以为了形象理解交叉嫡, 我们可以写出:<br>$<br>-\log p_{t} \approx \max \left(x_{1}, x_{2}, \ldots, x_{n}\right)-x_{t}<br>$<br>也就是说，交叉商实际上在缩小目标类得分 $x_{t}$ 与全局最大值的差距，显然这个差距最小只能为 0 ，并且此时目标类得分就是最大值者。所以， Softmax 加交叉嫡的效果就是“希望目标类的得分成为最大值”。</p><h2 id="2-Teacher-Forcing"><a href="#2-Teacher-Forcing" class="headerlink" title="2. Teacher Forcing"></a>2. Teacher Forcing</h2><p>现在，我们来看 Seq2Seq，它通过条件分解来建模联合概率分布：</p><p>$\begin{aligned} p(\boldsymbol{y} \mid \boldsymbol{x}) &amp;=p\left(y_{1}, y_{2}, \ldots, y_{n} \mid \boldsymbol{x}\right) \\ &amp;=p\left(y_{1} \mid \boldsymbol{x}\right) p\left(y_{2} \mid \boldsymbol{x}, y_{1}\right) \ldots p\left(y_{n} \mid \boldsymbol{x}, y_{1}, \ldots, y_{n-1}\right) \end{aligned}$</p><p>每一项自然也就用 Softmax 来建模的，即：</p><p>$p\left(y_{1} \mid \boldsymbol{x}\right)=\frac{e^{f\left(y_{1} ; \boldsymbol{x}\right)}}{\sum_{y_{1}} e^{f\left(y_{1} ; \boldsymbol{x}\right)}}$,<br>$p\left(y_{2} \mid \boldsymbol{x}, y_{1}\right)=\frac{e^{f\left(y_{1}, y_{2} ; \boldsymbol{x}\right)}}{\sum_{y_{2}} e^{f\left(y_{1}, y_{2} ; \boldsymbol{x}\right)}}$,<br>$\ldots$,<br>$p\left(y_{n} \mid \boldsymbol{x}, y_{1}, \ldots, y_{n-1}\right)=\frac{e^{f\left(y_{1}, y_{2}, \ldots, y_{n} ; \boldsymbol{x}\right)}}{\sum_{y_{n}} e^{f\left(y_{1}, y_{2}, \ldots, y_{n} ; \boldsymbol{x}\right)}}$</p><p>乘起来就是：<br>$<br>p(\boldsymbol{y} \mid \boldsymbol{x})=\frac{e^{f\left(y_{1} ; \boldsymbol{x}\right)}}{\left(\sum_{y_{1}} e^{f\left(y_{1} ; \boldsymbol{x}\right)}\right)\left(\sum_{y} e^{f\left(y_{1}, y_{2} ; \boldsymbol{x}\right)}\right) \ldots\left(\sum_{y_{n}} e^{f\left(y_{1}, y_{2}, \ldots, y_{n} ; \boldsymbol{x}\right)}\right)}<br>$<br>而训练目标就是：<br>$-\log p(\boldsymbol{y} \mid \boldsymbol{x})=-\log p\left(y_{1} \mid \boldsymbol{x}\right)-\log p\left(y_{2} \mid \boldsymbol{x}, y_{1}\right)-\cdots-\log p\left(y_{n} \mid \boldsymbol{x}, y_{1}, \ldots, y_{n-1}\right)$这个直接的训练目标就叫做 Teacher Forcing, 因为在算 $-\log p\left(y_{2} \mid \boldsymbol{x}, y_{1}\right)$ 的时候我们要知道真实的 $y_{1}$, 在算 $-\log p\left(y_{3} \mid \boldsymbol{x}, y_{1}, y_{2}\right)$ 我们需要知道真实的 $y_{1}, y_{2}$, 依此类推， 这就好像有一个经验丰富的老师预先给我们铺好了大部分的路，让我们只需要求下一步即可。</p><p>这个直接的训练目标就叫做 Teacher Forcing，因为在算 的时候我们要知道真实的 ，在算 我们需要知道真实的 ，依此类推，这就好像有一个经验丰富的老师预先给我们铺好了大部分的路，让我们只需要求下一步即可。</p><p>这种方法训练起来简单，而且结合 CNN 或 Transformer 那样的模型就可以实现并行的训练，但它可能会带来 Exposure Bias 问题。</p><h2 id="3-Exposure-Bias"><a href="#3-Exposure-Bias" class="headerlink" title="3. Exposure Bias"></a>3. Exposure Bias</h2><p>其实 Teacher Forcing 这个名称本身就意味着它本身会存在 Exposure Bias 问题。回想一下老师教学生解题的过程，一般的步骤为：</p><ul><li>第一步应该怎么思考；</li><li>第一步想出来后，第二步我们有哪些选择；</li><li>确定了第二步后，第三步我们可以怎么做；</li><li>…</li><li>有了这 n-1 步后，最后一步就不难想到了。</li></ul><p>这个过程其实跟 Seq2Seq 的 Teacher Forcing 方案的假设是一样的。有过教学经验的读者就知道，通常来说学生们都能听得频频点头，感觉全都懂了，然后让学生课后自己做题，多数还是一脸懵比。</p><p>为什么会这样呢？其中一个原因就是 Exposure Bias。说白了，问题就在于，老师总是假设学生能想到前面若干步后，然后教学生下一步，但如果前面有一步想错了或者想不出来呢？这时候这个过程就无法进行下去了，也就是没法得到正确答案了，这就是 Exposure Bias 问题。</p><h2 id="4-Beam-Search"><a href="#4-Beam-Search" class="headerlink" title="4. Beam Search"></a>4. Beam Search</h2><p>事实上，我们真正做题的时候并不总是这样子，假如我们卡在某步无法确定时，我们就遍历几种选择，然后继续推下去，看后面的结果反过来辅助我们确定前面无法确定的那步。对应到 Seq2Seq 来说，这其实就相当于基于 Beam Search 的解码过程。</p><p>对于 Beam Search，我们应该能发现，beam size 并不是越大越好，有些情况甚至是 beam size 等于 1 时最好，这看起来有点不合理，因为 beam size 越大，理论上找到的序列就越接近最优序列，所以应该越有可能正确才对。事实上这也算是 Exposure Bias 的现象之一。</p><p>从式（6）我们可以看出，Seq2Seq 对目标序列 的打分函数为：</p><p>$f\left(y_{1} ; \boldsymbol{x}\right)+f\left(y_{1}, y_{2} ; \boldsymbol{x}\right)+\cdots+f\left(y_{1}, y_{2}, \ldots, y_{n} ; \boldsymbol{x}\right)$</p><p>正常来说，我们希望目标序列是所有候选序列之中分数最高的，根据本文开头介绍的 Softmax 方法，我们建立的概率分布应该是：</p><p>$p(\boldsymbol{y} \mid \boldsymbol{x})=\frac{e^{f\left(y_{1} ; \boldsymbol{x}\right)+f\left(y_{1}, y_{2} ; \boldsymbol{x}\right)+\cdots+f\left(y_{1}, y_{2}, \ldots, y_{n} ; \boldsymbol{x}\right)}}{\sum_{y_{1}, y_{2}, \ldots, y_{n}} e^{f\left(y_{1} ; \boldsymbol{x}\right)+f\left(y_{1}, y_{2} ; \boldsymbol{x}\right)+\cdots+f\left(y_{1}, y_{2}, \ldots, y_{n} ; \boldsymbol{x}\right)}}$</p><p>但上式的分母需要遍历所有路径求和，难以实现，而式（6）就作为一种折衷的选择得到了广泛应用。但式（6）跟式（9）并不等价，因此哪怕模型已经成功优化，也可能出现“最优序列并不是目标序列”的现象。</p><h2 id="5-简单例子"><a href="#5-简单例子" class="headerlink" title="5. 简单例子"></a>5. 简单例子</h2><p>我们来举一个简单例子。设序列长度只有 2，候选序列是 $(a, b)$ 和 $(c, d)$, 而目标序列是<br>$(a, b)$, 训练完成后，模型的概率分布情况为：</p><p><img src="https://i.loli.net/2021/07/05/l5dg34prCf8vzbB.png" alt="image-20210705122243485" style="zoom:50%;"></p><p>如果 beam size 为 1, 那么因为 $p(a)&gt;p(c)$, 所以第一步只能输出 $a$, 接着因为$p(b \mid a)&gt;p(d \mid a)$, 所以第二步只能输出 $b$, 成功输出了正确序列 $(a, b)$ 。但如果 beam size 为 2，那么第一步输出 $(a, 0.6),(c, 0.4)$, 而第二步遍历所有组合，我们得到：</p><p><img src="https://i.loli.net/2021/07/05/9SyY3m6nARxGofN.png" alt="image-20210705122317192" style="zoom:50%;"></p><p>所以输出了错误的序列 $(c,d)$。</p><p>那是因为模型没训练好吗？并不是，前面说过 Softmax 加交叉商的目的就是让目标的得分最大，对于第一步我们有 $p(a)&gt;p(c)$, 所以第一步的训练目标已经达到了，而第二步在 $a$ 已经预先知道的前提下我们有 $p(b \mid a)&gt;p(d \mid a)$ ，这说明第二步的训练目标也达到了。</p><p>因此，模型已经算是训练好了，只不过可能因为模型表达能力限制等原因，得分并没有特别高，但“让目标的得分最大”这个目标已经完成了。</p><h2 id="6-思考对策"><a href="#6-思考对策" class="headerlink" title="6. 思考对策"></a>6. 思考对策</h2><p>从上述例子中读者或许可以看出问题所在了：主要是 $p(d \mid c)$ 太高了，而 $p(d \mid c)$ 是没有经过训练的，没有任何显式的机制去抑制 $p(d \mid c)$ 变大，因此就出现了“最优序列并不是目标序列”的现象。</p><p>看到这里，读者可能就能想到一个朴素的对策了：添加额外的优化目标，降低那些Beam Search出来的非目标序列不就行了？</p><p>事实上，这的确是一个有效的解决方法，相关结果发表在 2016 年的论文 <strong><em>Sequence-to-Sequence Learning as Beam-Search Optimization</em></strong> [2]。但这样一来几乎要求每步训练前的每个样本都要进行一次 Beam Search，计算成本太大。</p><p>还有一些更新的结果，比如 ACL 2019 的最佳长论文 <strong><em>Bridging the Gap between Training and Inference for Neural Machine Translation\</em></strong> [3] 就是聚焦于解决 Exposure Bias 问题。此外，通过强化学习直接优化 BLEU 等方法，也能一定程度上缓解 Exposure Bias。</p><p>然而，据笔者所了解，这些致力于解决 Exposure Bias 的方法，大部分都是大刀阔斧地改动了训练过程，甚至会牺牲原来模型的训练并行性（需要递归地采样负样本，如果模型本身是 RNN 那倒无妨，但如果本身是 CNN 或 Transformer，那伤害就很大了），成本的提升幅度比效果的提升幅度大得多。</p><h2 id="7-构建负样本"><a href="#7-构建负样本" class="headerlink" title="7. 构建负样本"></a>7. 构建负样本</h2><p>纵观大部分解决 Exposure Bias 的论文，以及结合我们前面的例子和体会，不难想到，其主要思想就是构造有代表性的负样本，然后在训练过程中降低这些负样本的概率，所以问题就是如何构造“有代表性”的负样本了。</p><p>这里给出笔者构思的一种简单策略，实验证明它能一定程度上缓解 Exposure Bias，提升文本生成的表现，重要的是，这种策略比较简单，基本能做到即插即用，几乎不损失训练性能。</p><p>方法很简单，就是随机替换一下 Decoder 的输入词（Decoder 的输入词有个专门的名字，叫做 oracle words），如下图所示：</p><p><img src="https://i.loli.net/2021/07/05/KblSQv4UzCkoVW5.png" alt="image-20210705121402164" style="zoom:50%;"></p><p>▲ 一种缓解Exposure Bias的简单策略：直接将Decoder的部分输入词随机替换为别的词</p><p>其中紫色的 [R] 代表被随机替换的词。其实不少 Exposure Bias 的论文也是这个思路，只不过随机选词的方案不一样。笔者提出的方案很简单：</p><ol><li><p>50% 的概率不做改变；</p></li><li><p>50% 的概率把输入序列中 30% 的词替换掉，替换对象为原目标序列的任意一个词。</p></li></ol><p>也就是说，随机替换发生概率是 50%&gt;&gt;随机替换的比例是 30%，随机抽取空间就是目标序列的词集。</p><p>这个策略的灵感在于：尽管 Seq2Seq 不一定能完全生成目标序列，但它通常能生成大部分目标序列的词（但顺序可能不对，或者重复出现同一些词），因此这样替换后的输入序列通常可以作为有代表性的负样本。对了，说明一下，50% 和 30% 这两个比例纯粹是拍脑袋的，没仔细调参，因为生成模型调一次实在是太累了。</p><p>效果如何呢？笔者做了两个标题（摘要）生成的实验（就是 CLGE [4] 的前两个），其中 baseline 是 task_seq2seq_autotitle_csl.py [5]，代码开源于：</p><p><a href="https://github.com/bojone/exposure_bias" target="_blank" rel="noopener">https://github.com/bojone/exposure_bias</a></p><p>结果如下表：</p><p><img src="https://i.loli.net/2021/07/05/ae1MA82g9zBV6Lr.png" alt="image-20210705121333189" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/07/05/WlX9DUvHn1iYA62.png" alt="image-20210705121322078" style="zoom:50%;"></p><p>可以发现，在 CSL 任务中，基于随机替换的策略稳定提升了文本生成的所有指标，而 LCSTS 任务的各个指标则各有优劣，考虑到 LCSTS 本身比较难，各项指标本来就低，所以应该说 CSL 的结果更有说服力一些。</p><p>这表明，笔者提出的上述策略确实是一种值得尝试的方案（注：所有实验都重复了两次然后取平均，所以实验结果应该是比较可靠的了）。</p><h2 id="8-对抗训练"><a href="#8-对抗训练" class="headerlink" title="8. 对抗训练"></a>8. 对抗训练</h2><p>思考到这里，我们不妨再“天马行空”一下：既然解决 Exposure Bias 的思路之一就是要构造有代表性的负样本输入，说白了就是让模型在扰动下依然能预测正确，而前些天我们不是才讨论了一种生成扰动样本的方法吗？</p><p>不错，那就是<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247504686&amp;idx=2&amp;sn=087dc7e98ede7960b3baacb20d55ce40&amp;chksm=96ea0caea19d85b8dccc52f07763b82f4aaffda6dd4cbdd784e682f13de6a11c6ced5b8df96f&amp;token=748748465&amp;lang=zh_CN&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">对抗训练</a>。如果直接往 baseline 模型里边加入对抗训练，能不能提升模型的性能呢？简单起见，笔者做了往 baseline 模型里边梯度惩罚（也算是对抗训练的一种）的实验，结果对比如下：</p><p><img src="https://i.loli.net/2021/07/05/ZOhswW8mGaSxIYc.png" alt="image-20210705121226535" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/07/05/J1pAQX28GScMPIB.png" alt="image-20210705121209230" style="zoom:50%;"></p><p>可以看到，对抗训练（梯度惩罚）进一步提升了 CSL 生成的所有指标，而 LCSTS 上则同样比较“随缘”。因此，对抗训练也可以列入“提升文本生成模型的潜力技巧”名单之中。</p><h2 id="9-本文小结"><a href="#9-本文小结" class="headerlink" title="9. 本文小结"></a>9. 本文小结</h2><p>本文讨论了 Seq2Seq 中的 Exposure Bias 现象，尝试从直观上和理论上分析 Exposure Bias 的原因，并给出了简单可行的缓解 Exposure Bias 问题的对策。</p><p>其中包括笔者构思的一种随机替换策略，以及基于对抗训练的策略，这两种策略的好处是它们几乎是即插即用的，并且实验表明它们能一定程度上提升文本生成的各个指标。</p><p><strong>相关链接</strong></p><p>[1] <a href="https://kexue.fm/archives/3290" target="_blank" rel="noopener">https://kexue.fm/archives/3290</a></p><p>[2] <a href="https://arxiv.org/abs/1606.02960" target="_blank" rel="noopener">https://arxiv.org/abs/1606.02960</a></p><p>[3] <a href="https://arxiv.org/abs/1906.02448" target="_blank" rel="noopener">https://arxiv.org/abs/1906.02448</a></p><p>[4] <a href="https://github.com/CLUEbenchmark/CLGE" target="_blank" rel="noopener">https://github.com/CLUEbenchmark/CLGE</a></p><p>[5] <a href="https://github.com/bojone/bert4keras/blob/master/examples/task_seq2seq_autotitle_csl.py" target="_blank" rel="noopener">https://github.com/bojone/bert4keras/blob/master/examples/task_seq2seq_autotitle_csl.py</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Seq2Seq中Exposure-Bias现象的浅析与对策&quot;&gt;&lt;a href=&quot;#Seq2Seq中Exposure-Bias现象的浅析与对策&quot; class=&quot;headerlink&quot; title=&quot;Seq2Seq中Exposure Bias现象的浅析与对策&quot;&gt;&lt;/a
      
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>R-Drop: Regularized Dropout for Neural Networks</title>
    <link href="http://yoursite.com/2021/07/05/R-Drop-Regularized-Dropout-for-Neural-Networks/"/>
    <id>http://yoursite.com/2021/07/05/R-Drop-Regularized-Dropout-for-Neural-Networks/</id>
    <published>2021-07-05T03:14:14.000Z</published>
    <updated>2021-07-07T02:51:35.584Z</updated>
    
    <content type="html"><![CDATA[<p>来源：<a href="https://mp.weixin.qq.com/s/IvhGbFEMotpKJIUPExUklg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/IvhGbFEMotpKJIUPExUklg</a></p><p>关注 NLP 新进展的读者，想必对四月份发布的 SimCSE [1] 印象颇深，它通过简单的“Dropout 两次”来构造正样本进行对比学习，达到了无监督语义相似度任务的全面 SOTA。无独有偶，最近的论文《R-Drop: Regularized Dropout for Neural Networks》提出了 R-Drop，它将“Dropout两次”的思想用到了有监督任务中，每个实验结果几乎都取得了明显的提升。此外，笔者在自己的实验还发现，它在半监督任务上也能有不俗的表现。</p><p>小小的“Dropout两次”，居然跑出了“五项全能”的感觉，不得不令人惊讶。本文来介绍一下 R-Drop，并分享一下笔者对它背后原理的思考。</p><p><img src="https://i.loli.net/2021/07/05/fCjDeTwZyztI3s6.png" alt="image-20210705111612040" style="zoom:50%;"></p><p><strong>论文标题：</strong>R-Drop: Regularized Dropout for Neural Networks</p><p><strong>论文链接：</strong><a href="https://arxiv.org/abs/2106.14448" target="_blank" rel="noopener">https://arxiv.org/abs/2106.14448</a></p><p><strong>代码链接：</strong><a href="https://github.com/dropreg/R-Drop" target="_blank" rel="noopener">https://github.com/dropreg/R-Drop</a></p><h2 id="1-SimCSE"><a href="#1-SimCSE" class="headerlink" title="1. SimCSE"></a>1. SimCSE</h2><p>《中文任务还是 SOTA 吗？我们给 SimCSE 补充了一些实验》[1] 中，我们已经对 SimCSE 进行了介绍。简单来说，SimCSE 是 NLP 的一种对比学习方案，对比学习的标准流程是同一个样本通过不同的数据扩增手段得到的结果视为正样本对，而 batch 内的所有其他样本视为负样本，然后就是通过 loss 来缩小正样本的距离、拉大负样本的距离了。</p><p>所以难度主要集中在数据扩增手段上。对于 NLP 来说，我们很难人工构建保证语义不变的数据扩增，所以 SimCSE 干脆不人工进行数据扩增，而是通过“Dropout 两次”的方式来得到同一个输入的不同特征向量，并将它们视为正样本对。奇怪的是，这个简单的“Dropout 两次”构造正样本，看上去是一种“无可奈何”的妥协选择，但消融实验却发现它几乎优于所有其他数据扩增方法，令人惊讶之余又让人感叹“大道至简”。</p><p><img src="https://i.loli.net/2021/07/05/Z8UKsXLWCAyp3dO.png" alt="image-20210705112154737" style="zoom:50%;"></p><p>在实现上，SimCSE 也相当简单，所谓“Dropout 两次”，只需要将样本重复地输入到模型，然后计算相应的 loss 就行了，如上图所示。由于 Dropout 本身的随机性，每个样本的 Dropout 模式都是不一样的，所以只要单纯地重复样本，就可以实现“Dropout 两次”的效果。</p><h2 id="2-R-Drop"><a href="#2-R-Drop" class="headerlink" title="2. R-Drop"></a>2. R-Drop</h2><p>从结果上来看，SimCSE 就是希望 Dropout对模型结果不会有太大影响，也就是模型输出对 Dropout 是鲁棒的。所以很明显，“Dropout 两次”这种思想是可以推广到一般任务的，这就是 R-Drop（Regularized Dropout）。</p><h3 id="2-1-分类问题"><a href="#2-1-分类问题" class="headerlink" title="2.1 分类问题"></a>2.1 分类问题</h3><p>在笔者看来，R-Drop 跟 SimCSE 是高度相关的，甚至 R-Drop 应该是受到了 SimCSE 启发的，不过 R-Drop 论文并没有引用 SimCSE，所以这就比较迷了。</p><p><img src="https://i.loli.net/2021/07/05/PkRl8QUn2uC1W5j.png" alt="image-20210705112225164" style="zoom: 80%;"></p><p>具体来说，以分类问题为例，训练数据为 ，模型为 ，每个样本的 loss 一般是交叉熵：</p><p>$\mathcal{L}_{i}=-\log P_{\theta}\left(y_{i} \mid x_{i}\right)$</p><p>在“Dropout 两次”的情况下，其实我们可以认为样本已经通过了两个略有不同的模型，我们分别记为 和 。这时候 R-Drop 的 loss 分为两部分，一部分是常规的交叉熵：</p><p>$\mathcal{L}_{i}^{(C E)}=-\log P_{\theta}^{(1)}\left(y_{i} \mid x_{i}\right)-\log P_{\theta}^{(2)}\left(y_{i} \mid x_{i}\right)$</p><p>另一部分则是两个模型之间的对称 KL 散度，它希望不同 Dropout 的模型输出尽可能一致：</p><p>$\mathcal{L}_{i}^{(K L)}=\frac{1}{2}\left[K L\left(P_{\theta}^{(2)}\left(y \mid x_{i}\right) | P_{\theta}^{(1)}\left(y \mid x_{i}\right)\right)+K L\left(P_{\theta}^{(1)}\left(y \mid x_{i}\right) | P_{\theta}^{(2)}\left(y \mid x_{i}\right)\right)\right]$</p><p>最终 loss 就是两个 loss 的加权和：</p><p>$\mathcal{L}_{i}=\mathcal{L}_{i}^{(C E)}+\alpha \mathcal{L}_{i}^{(K L)}$</p><p>也就是说，它在常规交叉熵的基础上，加了一项强化模型鲁棒性正则项。</p><h3 id="2-2-一般形式"><a href="#2-2-一般形式" class="headerlink" title="2.2 一般形式"></a>2.2 一般形式</h3><p>可能有些读者会问非分类问题应该将 KL 项替换为什么，事实上原论文并没有在非分类问题上进行实验，不过这里可以补充一下。我们可以留意到：</p><p>$-\log P_{\theta}\left(y_{i} \mid x_{i}\right)=K L\left(\right.$ one_hot $\left.\left(y_{i}\right) | P_{\theta}\left(y \mid x_{i}\right)\right)$</p><p>所以，上述 只不过是 KL 散度的反复使用，它的一般形式是：</p><p>$\mathcal{L}_{i}=\mathcal{D}\left(y_{i}, f_{\theta}^{(1)}\left(x_{i}\right)\right)+\mathcal{D}\left(y_{i}, f_{\theta}^{(2)}\left(x_{i}\right)\right)+\frac{\alpha}{2}\left[\mathcal{D}\left(f_{\theta}^{(2)}\left(x_{i}\right), f_{\theta}^{(1)}\left(x_{i}\right)\right)+\mathcal{D}\left(f_{\theta}^{(1)}\left(x_{i}\right), f_{\theta}^{(2)}\left(x_{i}\right)\right)\right]$</p><p>因此对于非分类问题，我们将 换成适当的度量（而不是 KL 散度）即可。</p><h2 id="3-实验效果"><a href="#3-实验效果" class="headerlink" title="3. 实验效果"></a>3. 实验效果</h2><p>我们先来看看 R-Drop 的实验结果。</p><p>R-Drop 的主要超参有三个：batch_size, $\alpha$ 和 Dropout 概率。batch_size 一 般取决于我们的算力，对个人来说调整空间不大; 原论文的 $\alpha$ 从 $1 \sim 5$ 都有，笔者自己的实验中，则取了 $\alpha=4$, 也没细调。至于 Dropout的概率，跟笔者在《中文任务还是 SOTA 吗？ 我们给 $\operatorname{SimCSE}$ 补充了一些实验》 [1] 所选的一样，设为 $0.3$ 效果比较好。</p><h3 id="3-1-论文报告"><a href="#3-1-论文报告" class="headerlink" title="3.1 论文报告"></a>3.1 论文报告</h3><p>说实话，原论文所报告的 R-Drop 的效果是相当让人惊艳的，这也是笔者不得不要介绍一波 R-Drop 的主要原因。原论文在 NLU、NLG、CV 的分类等多种任务上都对 R-Drop 做了对比实验，大部分实验效果都称得上“明显提升”。</p><p>官方实现：<a href="https://github.com/dropreg/R-Drop" target="_blank" rel="noopener">https://github.com/dropreg/R-Drop</a></p><p>下面截图一部分实验结果：</p><p><img src="https://i.loli.net/2021/07/05/Ff3U5wNhAv9kEjz.png" alt="image-20210705112718166" style="zoom:50%;"></p><p>▲ R-Drop在机器翻译任务上的效果</p><p><img src="https://i.loli.net/2021/07/05/tVo7znGCKvUDN5m.png" alt="image-20210705112750407" style="zoom: 50%;"></p><p>▲ R-Drop在GLUE任务上的效果</p><p>特别地，在机器翻译任务上，简单的“Transformer + R-Drop”超过了其他更加复杂方法的效果：</p><p><img src="https://i.loli.net/2021/07/05/gm1UzqLS9pB5KIy.png" alt="image-20210705112819739" style="zoom: 50%;"></p><p>▲ 机器翻译任务上不同方法的对比</p><p>论文还包括自动摘要、语言模型、图像分类等实验，以及关于超参数的一些消融实验，大家仔细看原论文就好。总的来说，R-Drop 的这份“成绩单”，确实足以让人为之点赞了。</p><h3 id="3-2-个人尝试"><a href="#3-2-个人尝试" class="headerlink" title="3.2 个人尝试"></a>3.2 个人尝试</h3><p>当然，笔者坚持的观点是“没有在中文测试过的模型是没有灵魂的”，一般情况下笔者都是在中文任务上亲自尝试过之后，才会写作分享。</p><p>个人实现：<a href="https://github.com/bojone/r-drop" target="_blank" rel="noopener">https://github.com/bojone/r-drop</a></p><p>有中文监督任务上，笔者实验了两个文本分类任务（CLUE 榜单的 IFLYTEK 和 TNEWS）。</p><p><img src="https://i.loli.net/2021/07/05/6WdlENPmujJQh8y.png" alt="image-20210705112907796" style="zoom:50%;"></p><p>和一个文本生成任务（CSL 标题生成，参考 <a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247504805&amp;idx=2&amp;sn=e0e149112e3318bf65d309e32db356b2&amp;chksm=96ea0c25a19d8533ec3d623547b415a58072b353037ac4cd95c68d9dc67f6eb48370684dc6dd&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Seq2Seq 中 Exposure Bias 现象的浅析与对策</a>）：</p><p><img src="https://i.loli.net/2021/07/05/epuUgz8yJSC6jxY.png" alt="image-20210705112924382" style="zoom:50%;"></p><p>可以看到，R-Drop 的结果足以 PK 在<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247504686&amp;idx=2&amp;sn=087dc7e98ede7960b3baacb20d55ce40&amp;chksm=96ea0caea19d85b8dccc52f07763b82f4aaffda6dd4cbdd784e682f13de6a11c6ced5b8df96f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">对抗训练浅谈：意义、方法和思考（附Keras 实现</a>）中介绍的著名正则化手段“对抗训练”和“梯度惩罚”了。</p><h3 id="3-3-实现要点"><a href="#3-3-实现要点" class="headerlink" title="3.3 实现要点"></a>3.3 实现要点</h3><p>相比于对抗学习等复杂正则化方法，R-Drop 的实现难度可谓是相当低了，这里以 bert4keras 为例，简单介绍一下如何将一个普通的训练脚本改为带 Dropout 的模式。</p><p>首先，是数据生成部分，改动如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">data_generator</span><span class="params">(DataGenerator)</span>:</span></span><br><span class="line">    <span class="string">"""数据生成器</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self, random=False)</span>:</span></span><br><span class="line">        batch_token_ids, batch_segment_ids, batch_labels = [], [], []</span><br><span class="line">        <span class="keyword">for</span> is_end, (text, label) <span class="keyword">in</span> self.sample(random):</span><br><span class="line">            token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)</span><br><span class="line">            <span class="comment"># batch_token_ids.append(token_ids)</span></span><br><span class="line">            <span class="comment"># batch_segment_ids.append(segment_ids)</span></span><br><span class="line">            <span class="comment"># batch_labels.append([label])</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">                batch_token_ids.append(token_ids)</span><br><span class="line">                batch_segment_ids.append(segment_ids)</span><br><span class="line">                batch_labels.append([label])</span><br><span class="line">            <span class="comment"># if len(batch_token_ids) == self.batch_size or is_end:</span></span><br><span class="line">            <span class="keyword">if</span> len(batch_token_ids) == self.batch_size * <span class="number">2</span> <span class="keyword">or</span> is_end:</span><br><span class="line">                batch_token_ids = sequence_padding(batch_token_ids)</span><br><span class="line">                batch_segment_ids = sequence_padding(batch_segment_ids)</span><br><span class="line">                batch_labels = sequence_padding(batch_labels)</span><br><span class="line">                <span class="keyword">yield</span> [batch_token_ids, batch_segment_ids], batch_labels</span><br><span class="line">                batch_token_ids, batch_segment_ids, batch_labels = [], [], []</span><br></pre></td></tr></table></figure><p>然后，自定义一个新 loss：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.losses <span class="keyword">import</span> kullback_leibler_divergence <span class="keyword">as</span> kld</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categorical_crossentropy_with_rdrop</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="string">"""配合上述生成器的R-Drop Loss</span></span><br><span class="line"><span class="string">    其实loss_kl的除以4，是为了在数量上对齐公式描述结果。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    loss_ce = K.categorical_crossentropy(y_true, y_pred)  <span class="comment"># 原来的loss</span></span><br><span class="line">    loss_kl = kld(y_pred[::<span class="number">2</span>], y_pred[<span class="number">1</span>::<span class="number">2</span>]) + kld(y_pred[<span class="number">1</span>::<span class="number">2</span>], y_pred[::<span class="number">2</span>])</span><br><span class="line">    <span class="keyword">return</span> K.mean(loss_ce) + K.mean(loss_kl) / <span class="number">4</span> * alpha</span><br></pre></td></tr></table></figure><p>最后把模型的 Dropout 打开，并用这个 data_generator 和 categorical_crossentropy_with_rdrop 来训练模型就行了。</p><h2 id="4-个人理解"><a href="#4-个人理解" class="headerlink" title="4. 个人理解"></a>4. 个人理解</h2><p>看完了让人赏心悦目的实验结果后，我们来啃一下理论。原论文提供了对 R-Drop 的一个理论分析，大致意思是 R-Drop 会促进参数的同化，从而起到正则化作用。不过个人感觉这个解释并不直观，而且还不够本质。下面笔者试图提供 R-Drop 的另外几个角度的理解。</p><h3 id="4-1-一致性"><a href="#4-1-一致性" class="headerlink" title="4.1 一致性"></a>4.1 一致性</h3><p>R-Dropout 可以看成是 Dropout 的改进，那 Dropout 有什么问题呢? 其实 Dropout 是典型的训练和预测不一致的方法。具体来说， Dropout 在训练阶段往 $\left(\right.$ 某些层的) 输入加上了乘性噪声，使得模型从 $f_{\theta}(x)$ 变成了 $f_{\theta}(x, \varepsilon)$, 其中 $\varepsilon$ 的每个元素有 $\mathrm{p}$的概率为 0 ，剩下 1-p 的概率为 $1 /(1-\mathrm{p})$ ，训练目标就是：</p><p>$\mathbb{E}_{(x, y) \sim \mathcal{D}} \mathbb{E}_{\varepsilon}\left[l\left(y, f_{\theta}(x, \varepsilon)\right)\right]$</p><p>这样训练之后，我们应该用哪个模型预测最好呢？不确定，但如果损失函数是 $l_{2}$距离的话，那么我们可以推出最佳预测模型应该是：</p><p>$\mathbb{E}_{\varepsilon}\left[f_{\theta}(x, \varepsilon)\right]$</p><p><strong>推导：</strong>如果用$l_{2}$ 损失，此时单个样本的损失是：</p><p>$\mathbb{E}_{\varepsilon}\left[\left|y-f_{\theta}(x, \varepsilon)\right|^{2}\right]=|y|^{2}-2\left\langle y, \mathbb{E}_{\varepsilon}\left[f_{\theta}(x, \varepsilon)\right]\right\rangle+\mathbb{E}_{\varepsilon}\left[\left|f_{\theta}(x, \varepsilon)\right|^{2}\right]$</p><p>注意，现在我们的问题是“模型训练完后应该用什么函数来预测”，所以 $f_{\theta}(x, \varepsilon)$ 是常数， $\mathrm{y}$ 才是要优化的变量，这只不过是一个 二次函数的最小值问题，容易解得 $y=\mathbb{E}_{\varepsilon}\left[f_{\theta}(x, \varepsilon)\right]$ 时损失函数最小。</p><p>我们假定这个结果能泛化到一般情况。上式告诉我们，带 Dropout 的模型的正确步骤是“模型融合”：</p><p>对同一个输入多次传入模型中（模型不关闭 Dropout），然后把多次的预测结果平均值作为最终的预测结果。</p><p>但我们一般情况下的预测方式显然不是这样的，而是直接关闭 Dropout 进行确定性的预测，这等价于预测模型由“模型平均”变成了“权重平均”：</p><p>$f_{\theta}\left(x, \mathbb{E}_{\varepsilon}[\varepsilon]\right)=f_{\theta}(x, 1)=f_{\theta}(x)$</p><p>这里的 1 指的是全 1 向量。所以，我们训练的是不同 Dropout 的融合模型，预测的时候用的是关闭 Dropout 的单模型，两者未必等价，这就是 Dropout 的训练预测不一致问题。</p><p>现在，我们就不难理解 R-Drop 了，它通过增加一个正则项，来强化模型对 Dropout 的鲁棒性，使得不同的 Dropout 下模型的输出基本一致，因此能降低这种不一致性，促进“模型平均”与“权重平均”的相似性，从而使得简单关闭 Dropout 的效果等价于多 Dropout 模型融合的结果，提升模型最终性能。</p><h3 id="4-2-连续性"><a href="#4-2-连续性" class="headerlink" title="4.2 连续性"></a>4.2 连续性</h3><p>本文开头就提到 R-Drop 与 SimCSE 的相似性，事实上它还跟另外一个方法相当相似，那便是“虚拟对抗训练（Virtual Adversarial Training，VAT）”。（不过 R-Drop 也没引 VAT，难道就只有笔者觉得像吗？？）</p><p>关于 VAT 的介绍，大家可以参考笔者之前的文章<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247507771&amp;idx=1&amp;sn=b92559c624c7c58e77a11c89bec5e99a&amp;chksm=96ea00bba19d89adb05dd98193e7371d43f090fcaf6be401e01c0109beee7920e84cb13274c9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练</a>。简单来说，VAT 也是通过一个正则项，使得模型对扰动更加鲁棒，增强模型本身的连续性（小的变化不至于对结果产生大的影响）。它们不同的地方在于加扰动的方式，VAT 只把扰动加入到输入中，并且通过对抗的思想提升扰动的针对性；R-Drop 的扰动则可以施加到模型的每一层中，并且扰动是随机的。</p><p>有读者可能想到了，VAT 可是主打半监督训练的，那是不是意味着 R-Drop 也可以做半监督训练？这部分原论文并没有实验，是笔者自己做的实验，答案是确实可以，跟 VAT 类似，R-Drop 新增的 KL 散度项是不需要标签的，因此可以无监督训练，混合起来就是半监督了，效果也还不错。下面是笔者的实验结果：</p><p><img src="https://i.loli.net/2021/07/05/UT8bI3qG54D19wt.png" alt="image-20210705113103693" style="zoom:50%;"></p><p>可以看到，R-Drop 的半监督效果完全不逊色于 VAT，而且它实现比 VAT 简单，速度也比 VAT 快！看来 VAT 有望退休了～ 直觉上来看，虽然 R-Drop 的扰动是随机的，但是 R-Drop 的扰动更多，所以它造成的扰动也会放大，也可能比得上 VAT 经过对抗优化的扰动，所以 R-Drop 能够不逊色于 VAT。</p><h3 id="4-3-非目标类"><a href="#4-3-非目标类" class="headerlink" title="4.3 非目标类"></a>4.3 非目标类</h3><p>一个比较直接的疑问是，如果我的模型够复杂，单靠交叉熵这一项，不能使得模型对 Dropout 鲁棒吗？KL 散度那一项造成了什么直接的区别？</p><p>事实上，还真的不能。要注意的是，交叉熵的训练目标主要是：让目标类的得分大于非目标类的得分，这样模型就能正确地把目标类预测出来了（参考<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247505145&amp;idx=1&amp;sn=42467a5475b64d3031a46594261600d2&amp;chksm=96ea0b79a19d826fb7e040968bbe928daf528d38e63d916b05cbc60a5d350755e619abc2aff8&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">将“softmax+交叉熵”推广到多标签分类问题</a>）。也就是说，如果只有交叉熵这一项，模型的训练结果顶多是：</p><blockquote><p>不同的 Dropout 下，目标类的得分都大于非目标类的得分。</p></blockquote><p>而不能做到：</p><blockquote><p>不同的 Dropout 下，每个类的得分一致。</p></blockquote><p>所以也就没有解决训练预测不一致的问题。从公式上来看，交叉熵（2）只跟目标类别有关，不关心非目标类的分布，假如目标类为第一个类别，那么预测结果是 [0.5, 0.2, 0.3] 或 [0.5, 0.3, 0.2]，对它来说都没区别。但对于 KL 散度项（3）来说就不一样了，每个类的得分都要参与计算，[0.5, 0.2, 0.3] 或 [0.5, 0.3, 0.2] 是有非零损失的。</p><h2 id="5-本文小结"><a href="#5-本文小结" class="headerlink" title="5. 本文小结"></a>5. 本文小结</h2><p>本文介绍了 R-Drop，它将“Dropout 两次”的思想用到了有监督任务中，每个实验结果几乎都取得了明显的提升。此外，笔者在自己的实验还发现，它在半监督任务上也能有不俗的表现。最后，分享了笔者对 R-Drop 的三个角度的思考。</p><h2 id="6-参考文献"><a href="#6-参考文献" class="headerlink" title="6. 参考文献"></a>6. 参考文献</h2><p>[1] <a href="https://kexue.fm/archives/8348" target="_blank" rel="noopener">https://kexue.fm/archives/8348</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;来源：&lt;a href=&quot;https://mp.weixin.qq.com/s/IvhGbFEMotpKJIUPExUklg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://mp.weixin.qq.com/s/IvhGbFEMotpKJIUP
      
    
    </summary>
    
      <category term="NLP, ML" scheme="http://yoursite.com/categories/NLP-ML/"/>
    
    
      <category term="NLP, ML" scheme="http://yoursite.com/tags/NLP-ML/"/>
    
  </entry>
  
  <entry>
    <title>Unifying Vision-and-Language Tasks via Text Generation</title>
    <link href="http://yoursite.com/2021/07/03/Unifying-Vision-and-Language-Tasks-via-Text-Generation/"/>
    <id>http://yoursite.com/2021/07/03/Unifying-Vision-and-Language-Tasks-via-Text-Generation/</id>
    <published>2021-07-03T10:16:18.000Z</published>
    <updated>2021-07-08T13:26:28.927Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/j-min/VL-T5" target="_blank" rel="noopener">code</a></p><h2 id="当前跨模态预训练模型存在的问题"><a href="#当前跨模态预训练模型存在的问题" class="headerlink" title="当前跨模态预训练模型存在的问题"></a>当前跨模态预训练模型存在的问题</h2><ul><li>当前跨模态预训练模型在做下游任务时，通常都是根据特定任务设计相应的 head 和 objective。例如，a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning.</li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>为了避免复杂的设计，本文提出了一个联合框架，可以在同一个结构中学习不同的任务。<strong style="color:red;">具体地，本文将判别式任务和生成式任务都转化为 Text  Generation task。</strong>本文的这种方法达到了近似SOTA的效果。</p><p>同时本文提出的框架可以在<strong>同一个参数</strong>下进行多任务训练，这种设置下，可以实现与单独训练特定任务达到相似的性能。(yaya解释，意思是说，这个模型在多任务的设置下训练之后，可以直接拿来去做各种任务，与单独训练特定任务的效果近似)</p><p>这种范式的好处：</p><ul><li>不需要为特定任务设计特定的head</li><li>对于一个新的任务，通过对input and output 进行 text rewrite即可，而不需要增加额外的参数或者是设计新的结构和训练目标。</li><li>由于预训练任务是生成式任务，因此相比于MLM这种理解型任务，文本生成能力更强。比如，当我们回答需要非简单答案的开放式问题时，这一点尤其有帮助，在这种情况下，判别性方法只能从<strong>预定义的频繁候选者集合中</strong>回答，而我们的模型可以生成开放式的自然语言答案。</li></ul><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>整体上，模型基于NLP的 T5 和 BART的 backbone 来设计。下图中以T5为例。</p><p>类似于T5，针对特定的任务，在输入文本的前面追加了关于任务的描述（task description）, 如下图中的 visual grounding.</p><p>模型设计上的一个比较特殊的点：we use the text embeddings of visual sentinel tokens as region id embeddings in Sec. 3.1. The embedding sharing enables our model to build the correspondence among query text, label text, and objects, which are useful in the grounding tasks。</p><p><img src="https://i.loli.net/2021/07/08/Yr5dLpGUzo6Rtl2.png" alt="image-20210708203404311"></p><h2 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h2><p>与T5类似，无论是判别式任务或是生成式任务，都会以自回归的生成式方式来解决。</p><p>如下图(a) 和 图(b) 以前的跨模态预训练模型需要为特定的任务设计特定的head, 但是，本文的方法，图(c) 和 图(d)  都以language modeling 生成式的方式来做。</p><p>we <font color="red"> <strong>formulate the task labels to corresponding text</strong></font>, and we learn these dif- ferent tasks by predicting label text with the same language modeling objective</p><p><img src="https://i.loli.net/2021/07/08/xFwyLG16aPCNMir.png" alt="image-20210708204113897"></p><h2 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h2><p>下图给出了预训练任务和下游任务的输入数据的格式</p><p>预训练任务中图文匹配任务也转化为生成 true/false。</p><p>预训练任务中 caption region 这个倒是挺新颖的</p><p>下游任务中，对于VQA and GQA 这两个任务，以前的方式都是多项选择的方式，但是本文中采用的是生成的方式。</p><p><img src="https://i.loli.net/2021/07/08/6z5B19PCcUJQbxT.png" alt="image-20210708204838585"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验一，预训练后的模型在各个下游任务上分别微调</p><p><img src="https://i.loli.net/2021/07/08/oKvQEH2TeXAz8LO.png" alt="image-20210708212303877"></p><p>实验二，微调下游任务时，采取多任务一起微调的方式</p><p>多任务一起微调效果也很不错，实现了一个模型参数可以处理多个任务，且不需要特定的head!!!</p><p><img src="C:\Users\shiyaya\AppData\Roaming\Typora\typora-user-images\image-20210708212215025.png" alt="image-20210708212215025"></p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li>存在的缺陷：不是端到端的，需要预先使用 faster r-cnn 提取 proposals; 虽然 ViLT是端到端的，但是ViLT 不方便做 visual grounding task。</li><li><font color="green">预训练任务中，有一个visual grounding task, 但是，region description 是如何得到的？？论文中有讲，但是没有看懂</font></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/j-min/VL-T5&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;当前跨模态预训练模型存在的问题&quot;&gt;&lt;a href=&quot;#当前跨模态预训练模型存在的问题&quot; 
      
    
    </summary>
    
      <category term="croos-moal" scheme="http://yoursite.com/categories/croos-moal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>Zero-Shot Detection via Vision and Language Knowledge Distillation</title>
    <link href="http://yoursite.com/2021/06/25/Zero-Shot-Detection-via-Vision-and-Language-Knowledge-Distillation/"/>
    <id>http://yoursite.com/2021/06/25/Zero-Shot-Detection-via-Vision-and-Language-Knowledge-Distillation/</id>
    <published>2021-06-25T09:28:47.000Z</published>
    <updated>2021-07-19T03:36:39.594Z</updated>
    
    <content type="html"><![CDATA[<p>来源：<a href="https://zhuanlan.zhihu.com/p/369153230" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/369153230</a></p><p>今年年初刚出的CLIP可以说是震惊了NLP和CV界，它让大白话搜图不再是梦想，就像这样：</p><p><img src="https://i.loli.net/2021/07/19/sXp4lcNJHfdTg6a.png" alt="image-20210719102532427" style="zoom:67%;"></p><p>以上是一个老哥用CLIP在unsplash数据集上做的大白话搜图引擎，有兴趣的同学们可以去感受一下：</p><p><a href="https://link.zhihu.com/?target=https%3A//github.com/haltakov/natural-language-image-search" target="_blank" rel="noopener">natural-language-image-searchgithub.com</a></p><p>其实CLIP做的事情很简单，简单说来就是<strong>学习出了一个图像和文本的共同空间</strong>，给定任意图像和文本都可以映射到这个空间中，然后这些图像和文本就可以<strong>直接计算相似度</strong>，于是就可以做到用大白话来搜图。换句话说，CLIP找到了一个方式<strong>填平了文本和视觉的gap</strong>。</p><p>那既然如此，可不可以进一步拓展，利用CLIP来做检测，比如我随便说一句话就可以检测出图像里面对应的物体，不管我这句话里面的物体属于任意类别呢？当然可以！而且谷歌已经做出来了，就在前天挂上了arXiv，不得不感叹他们真的太快了（CLIP才出了3个月啊）</p><p>这篇文章的名字就叫<strong>Zero-Shot Detection via Vision and Language Knowledge Distillation。</strong>zero-shot顾名思义，就是对于一个新类别，一张训练图像都不给的情况下训练出能检测这个类别物体的检测器；而knowledge distillation呢，指的就是从CLIP中蒸馏出知识来训练检测器（毕竟CLIP用了4亿个文本-图像对训练，肯定得好好利用啊）。</p><p>这个方法的名字叫做ViLD（Vision and Language knowledge Distillation），首先感受一下它的效果：</p><p><img src="https://i.loli.net/2021/07/19/QcueiMgxDAGhk2H.png" alt="image-20210719102620483"></p><p>这些玩具都是训练集里没有出现过的类别哦。</p><p>那么ViLD是怎么做的呢？框架图先贴出来：</p><p><img src="https://i.loli.net/2021/07/19/5g479jOJ2PB6ikQ.png" alt="image-20210719102700196"></p><p>定位网络用的是Mask R-CNN，BackBone提特征图，RPN提proposal，然后RoiAlign提proposal的特征就不用多说了。</p><p>训练时，重点在图像会用proposal进行裁剪，裁剪下来的图像会送入预训练好的CLIP中得到在其<strong>共同空间</strong>中的特征，训练的目标就是<strong>拉近proposal特征和CLIP得到的裁剪图像特征的距离</strong>，这个训练可以认为是一种蒸馏，把CLIP中的知识传递给了训练的模型。这样训练好的模型提取出的proposal特征也可以认为已经在CLIP的共同空间中了。这一部分对应于图中上半部分的Training<strong>。</strong></p><p>测试时，先把需要分类的文本标签通过其在CLIP共同空间的特征。然后图像中的proposal特征就可以<strong>直接与这些文本标签特征进行相似度计算</strong>（因为它们可以假设都位于CLIP的共同空间中），完成分类。</p><p>具体来看训练时候是怎么做的：</p><p><img src="https://i.loli.net/2021/07/19/WnESNvxVQCYmKRg.png" alt="image-20210719102730131"></p><p>直接看（b）部分，其中的ViLD-text部分就是在文本模态进行训练，比如当前的proposal对应的类别为“猫”，那么相应的，这个proposal特征应该和CLIP对“猫”这个文本生成的在共同空间中的特征相近。</p><p>ViLD-image则是在视觉模态进行训练，即proposal特征应该和根据这个proposal裁剪下来的图像在CLIP的共同空间的特征相近。</p><p>总结一下，就是在两个模态上对CLIP进行知识蒸馏，目的是将proposal特征映射到CLIP的共同空间中，其中文本模态由类别标签得到，视觉模态由裁剪的图像得到。</p><p>说下自己的感受吧，这个idea本来我刚刚想到，结果现在看到这篇文章，心里还是暗喜还好没开始做hhhhh。更多还是感叹大厂的速度是真的快，仅仅三个月就整出来了完成度和创新度都很不错的一篇文章。另外利用CLIP做跨模态肯定是以后的趋势了，毕竟用了4亿图像-文本对训练出来的模型填平了视觉和文本的gap，绝对大有搞头。</p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><p>本文使用了两个损失，一个是L1 loss,用于知识蒸馏，而cross entropy loss 则是为了使用CLIP预测的范式（使用匹配的方式来做分类预测）。</p><p>这两个损失是缺一不可的，仅使用L1 loss 来知识蒸馏是没有任何用的，因为这里并不是想获取更好的视觉特征，而是使用clip的预测范式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;来源：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/369153230&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/369153230&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;今年年初
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>理解对比学习和 SimCSE</title>
    <link href="http://yoursite.com/2021/06/22/%E7%90%86%E8%A7%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%92%8C-SimCSE/"/>
    <id>http://yoursite.com/2021/06/22/理解对比学习和-SimCSE/</id>
    <published>2021-06-22T13:03:55.000Z</published>
    <updated>2021-06-22T13:19:19.187Z</updated>
    
    <content type="html"><![CDATA[<p>来源：<a href="https://mp.weixin.qq.com/s/12UvfXnaB4NTy54wWIFZdQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/12UvfXnaB4NTy54wWIFZdQ</a></p><p>2020 年的 Moco 和 SimCLR 等，掀起了对比学习在 CV 领域的热潮，2021 年的 SimCSE，则让 NLP 也乘上了对比学习的东风。下面就尝试用 QA 的形式挖掘其中一些细节知识点，去更好地理解对比学习和 SimCSE。</p><ul><li><p>如何去理解对比学习，它和度量学习的差别是什么？</p></li><li><p>对比学习中一般选择一个 batch 中的所有其他样本作为负例，那如果负例中有很相似的样本怎么办？</p></li><li><p>infoNCE loss 如何去理解，和 CE loss有什么区别？</p></li><li><p>对比学习的 infoNCE loss 中的温度常数的作用是什么？</p></li><li><p>SimCSE 中的 dropout mask 指的是什么，dropout rate 的大小影响的是什么？</p></li><li><p>SimCSE 无监督模式下的具体实现流程是怎样的，标签生成和 loss 计算如何实现？</p></li></ul><h2 id="1-如何去理解对比学习，它和度量学习的差别是什么？"><a href="#1-如何去理解对比学习，它和度量学习的差别是什么？" class="headerlink" title="1. 如何去理解对比学习，它和度量学习的差别是什么？"></a><strong>1. 如何去理解对比学习，它和度量学习的差别是什么？</strong></h2><p>对比学习的思想是去拉近相似的样本，推开不相似的样本，而目标是要从样本中学习到一个好的语义表示空间。</p><p>论文 [1] 给出的 “Alignment and Uniformity on the Hypersphere”，就是一个非常好的去理解对比学习的角度。</p><p>好的对比学习系统应该具备两个属性：Alignment和Uniformity（参考上图）。</p><p>所谓“Alignment”，指的是相似的例子，也就是正例，映射到单位超球面后，应该有接近的特征，也即是说，在超球面上距离比较近；</p><p>所谓“Uniformity”，指的是系统应该倾向在特征里保留尽可能多的信息，这等价于使得映射到单位超球面的特征，尽可能均匀地分布在球面上，分布得越均匀，意味着保留的信息越充分。分布均匀意味着两两有差异，也意味着各自保有独有信息，这代表信息保留充分（参考自 [2]）。</p><p>度量学习和对比学习的思想是一样的，都是去拉近相似的样本，推开不相似的样本。但是对比学习是无监督或者自监督学习方法，而度量学习一般为有监督学习方法。而且对比学习在 loss 设计时，为单正例多负例的形式，因为是无监督，数据是充足的，也就可以找到无穷的负例，但<font color="red">如何构造有效正例才是重点。</font></p><p>而度量学习多为二元组或三元组的形式，如常见的 Triplet 形式（anchor，positive，negative），Hard Negative 的挖掘对最终效果有较大的影响。</p><h2 id="2-对比学习中一般选择一个-batch-中的所有其他样本作为负例，那如果负例中有很相似的样本怎么办？"><a href="#2-对比学习中一般选择一个-batch-中的所有其他样本作为负例，那如果负例中有很相似的样本怎么办？" class="headerlink" title="2. 对比学习中一般选择一个 batch 中的所有其他样本作为负例，那如果负例中有很相似的样本怎么办？"></a>2. <strong>对比学习中一般选择一个 batch 中的所有其他样本作为负例，那如果负例中有很相似的样本怎么办？</strong></h2><p>在无监督无标注的情况下，这样的伪负例，其实是不可避免的，首先可以想到的方式是去扩大语料库，去加大 batch size，以降低 batch 训练中采样到伪负例的概率，减少它的影响。</p><p>另外，神经网络是有一定容错能力的，像伪标签方法就是一个很好的印证，但前提是错误标签数据或伪负例占较小的比例。</p><p>PS：也确有人考虑研究过这个问题，可以参考论文 [3] [4]</p><h2 id="3-infoNCE-loss-如何去理解，和-CE-loss-有什么区别？"><a href="#3-infoNCE-loss-如何去理解，和-CE-loss-有什么区别？" class="headerlink" title="3. infoNCE loss 如何去理解，和 CE loss 有什么区别？"></a>3. <strong>infoNCE loss 如何去理解，和 CE loss 有什么区别？</strong></h2><p><strong>infoNCE loss</strong> 全称 info Noise Contrastive Estimation loss，对于一个 batch 中的样本 i，它的 loss 为：</p><p><img src="https://i.loli.net/2021/06/22/Omj8YCPkqeWwyp4.png" alt="image-20210622211019708" style="zoom:50%;"></p><p>要注意的是，<font color="red">log 里面的分母叠加项是<strong>包括了分子项</strong>的。</font> 分子是正例对的相似度，分母是正例对+所有负例对的相似度，最小化 infoNCE loss，就是去最大化分子的同时最小化分母，也就是最大化正例对的相似度，最小化负例对的相似度。</p><p>上面公式直接看可能没那么清晰，可以把负号放进去，分子分母倒过来化简一下就会很明了了。</p><p><strong>CE loss</strong>，Cross Entropy loss，在输入 p 是 softmax 的输出时：</p><p><img src="https://i.loli.net/2021/06/22/YFIfqETp1S7Kg83.png" alt="image-20210622211041814" style="zoom:50%;"></p><p>在分类场景下，真实标签 y 一般为 one-hot 的形式，因此，CE loss 可以简化成（i 位置对应标签 1）：</p><p><img src="https://i.loli.net/2021/06/22/saTEDlBWg1uUNAp.png" alt="image-20210622211108963" style="zoom:50%;"></p><p>看的出来，info NCE loss 和在一定条件下简化后的 CE loss 是非常相似的，但有一个区别要注意的是：</p><p>infoNCE loss 中的 K 是 batch 的大小，是可变的，是第 i 个样本要和 batch 中的每个样本计算相似度，而 batch 里的每一个样本都会如此计算，因此上面公式只是样本 i 的 loss。</p><p>CE loss 中的 K 是分类类别数的大小，任务确定时是不变的，i 位置对应标签为 1 的位置。不过实际上，infoNCE loss 就是直接可以用 CE loss 去计算的。</p><p>注：1）info NCE loss 不同的实现方式下，它的计算方式和 K 的含义可能会有差异；2）info NCE loss 是基于 NCE loss 的，对公式推导感兴趣的可以参考 [5]。</p><h2 id="4-对比学习的-infoNCE-loss-中的温度常数-t-的作用是什么？"><a href="#4-对比学习的-infoNCE-loss-中的温度常数-t-的作用是什么？" class="headerlink" title="4. 对比学习的 infoNCE loss 中的温度常数 t 的作用是什么？"></a>4. <strong>对比学习的 infoNCE loss 中的温度常数 t 的作用是什么？</strong></h2><p>论文 [6] 给出了非常细致的分析，知乎博客 [7] 则对论文 [6] 做了细致的解读，这里摘录它的要点部分：</p><p>温度系数的作用是调节对困难样本的关注程度：<strong>越小的温度系数越关注于将本样本和最相似的困难样本分开</strong>，去得到更均匀的表示。然而困难样本往往是与本样本相似程度较高的，很多困难负样本其实是潜在的正样本，过分强迫与困难样本分开会破坏学到的潜在语义结构，因此，温度系数不能过小。</p><p>考虑两个极端情况，温度系数趋向于 0 时，对比损失退化为只关注最困难的负样本的损失函数；当温度系数趋向于无穷大时，对比损失对所有负样本都一视同仁，失去了困难样本关注的特性。</p><p>还有一个角度：</p><p>可以把不同的负样本想像成同极点电荷在不同距离处的受力情况，距离越近的点电荷受到的库伦斥力更大，而距离越远的点电荷受到的斥力越小。</p><p>对比损失中，越近的负例受到的斥力越大，具体的表现就是对应的负梯度值越大 [4]。这种性质更有利于形成在超球面均匀分布的特征。</p><p>对照着公式去理解：</p><p><img src="https://i.loli.net/2021/06/22/QTPnuCLMc6Ug1sb.png" alt="image-20210622211420543" style="zoom:33%;"></p><p>温度系数很小时，越相似也即越困难的负例，对应的 就会越大，在分母叠加项中所占的比重就会越大，对整体 loss 的影响就会越大，具体的表现就是对应的负梯度值越大 [7]。</p><p>当然，这仅仅是提供了一种定性的认识，定量的认识和推导可以参见博客 [7]。</p><h2 id="5-SimCSE-中的-dropout-mask-指的是什么，dropout-rate-的大小影响的是什么？"><a href="#5-SimCSE-中的-dropout-mask-指的是什么，dropout-rate-的大小影响的是什么？" class="headerlink" title="5. SimCSE 中的 dropout mask 指的是什么，dropout rate 的大小影响的是什么？"></a>5. <strong>SimCSE 中的 dropout mask 指的是什么，dropout rate 的大小影响的是什么？</strong></h2><p>一般而言的 mask 是对 token 级别的 mask，比如说 BERT MLM 中的 mask，batch 训练时对 padding 位的 mask 等。</p><p>SimCSE 中的 dropout mask，对于 BERT 模型本身，是一种网络模型的随机，是对网络参数 W 的 mask，起到防止过拟合的作用。</p><p>而 SimCSE 巧妙的把它作为了一种 noise，起到数据增强的作用，因为同一句话，经过带 dropout 的模型两次，得到的句向量是不一样的，但是因为是相同的句子输入，最后句向量的语义期望是相同的，因此作为正例对，让模型去拉近它们之间的距离。</p><p>在实现上，因为一个 batch 中的任意两个样本，经历的 dropout mask 都是不一样的，因此，一个句子过两次 dropout，SimCSE 源码中实际上是在一个 batch 中实现的，即 [a,a,b,b…] 作为一个 batch 去输入。</p><p>dropout rate 大小的影响，可以理解为，这个概率会对应有 dropout 的句向量相对无 dropout 句向量，在整个单位超球体中偏移的程度，因为 BERT 是多层的结构，每一层都会有 dropout，这些 noise 的累积，会让句向量在每个维度上都会有偏移的，只是 p 较小的情况下，两个向量在空间中仍较为接近，如论文所说，“keeps a steady alignment”，保证了一个稳定的对齐性。</p><h2 id="6-SimCSE-无监督模式下的具体实现流程是怎样的，标签生成和-loss-计算如何实现？"><a href="#6-SimCSE-无监督模式下的具体实现流程是怎样的，标签生成和-loss-计算如何实现？" class="headerlink" title="6. SimCSE 无监督模式下的具体实现流程是怎样的，标签生成和 loss 计算如何实现？"></a>6. <strong>SimCSE 无监督模式下的具体实现流程是怎样的，标签生成和 loss 计算如何实现？</strong></h2><p>这里用一个简单的例子和 Pytorch 代码来说明：</p><p><strong>前向句子 embedding 计算：</strong></p><p>假设初始输入一个句子集 sents = [a,b]，每一句要过两次 BERT，因此复制成 sents = [a,a,b,b]。</p><p>sents 以 batch 的形式过 BERT 等语言模型得到句向量：batch_emb = [a1,a2,b1,b2]。</p><p><strong>batch 标签生成：</strong>标签为 1 的地方是相同句子不同 embedding 对应的位置。</p><p><img src="https://i.loli.net/2021/06/22/bE15TPnoGCRvFZk.png" alt="image-20210622211637062" style="zoom: 33%;"></p><p>pytorch 中的 CE_loss，要使用一维的数字标签，上面的 one-hot 标签可转换成：[1,0,3,2]。</p><p>可以把 label 拆成两个部分：奇数部分 [1,3…] 和偶数部分 [0,2…]，交替的每个奇数在偶数前面。因此实际生成的时候，可以分别生成两个部分再 concat 并 reshape 成一维。</p><p>pytorch 中 label 的生成代码如下：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造标签</span></span><br><span class="line"><span class="keyword">batch</span>_size = <span class="keyword">batch</span>_emb.size<span class="params">(0)</span></span><br><span class="line">y_<span class="literal">true</span> = torch.cat<span class="params">([torch.arange(1,batch_size,<span class="attr">step</span>=2,<span class="attr">dtype</span>=torch.long)</span><span class="string">.unsqueeze</span><span class="params">(1)</span>,</span><br><span class="line">                    torch.arange<span class="params">(0,batch_size,<span class="attr">step</span>=2,<span class="attr">dtype</span>=torch.long)</span><span class="string">.unsqueeze</span><span class="params">(1)</span>],</span><br><span class="line">                    dim=1)<span class="string">.reshape</span><span class="params">([batch_size,])</span></span><br></pre></td></tr></table></figure><p><strong>score 和 loss计算：</strong></p><p>batch_emb 会先 norm，再计算任意两个向量之间的点积，得到向量间的余弦相似度，维度是：[batch_size, batch_size]。</p><p>但是对角线的位置，也就是自身的余弦相似度，需要 mask 掉，因为它肯定是 1，是不产生 loss 的。</p><p>然后，要除以温度系数，再进行 loss 的计算，loss_func 采用 CE loss，注意 CE loss 中是自带 softmax 计算的。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算score和loss</span></span><br><span class="line"><span class="attr">norm_emb</span> = F.normalize(batch_emb, dim=<span class="number">1</span>, p=<span class="number">2</span>)</span><br><span class="line"><span class="attr">sim_score</span> = torch.matmul(norm_emb, norm_emb.transpose(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="attr">sim_score</span> = sim_score - torch.eye(batch_size) * <span class="number">1</span>e12</span><br><span class="line"><span class="attr">sim_score</span> = sim_score * <span class="number">20</span>        <span class="comment"># 温度系数为 0.05，也就是乘以20</span></span><br><span class="line"><span class="attr">loss</span> = loss_func(sim_score, y_<span class="literal">true</span>)</span><br></pre></td></tr></table></figure><p>完整代码：</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">loss_func</span> = nn.CrossEntropyLoss()</span><br><span class="line">def simcse_loss(batch_emb):</span><br><span class="line">    <span class="string">""</span><span class="string">"用于无监督SimCSE训练的loss</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    <span class="comment"># 构造标签</span></span><br><span class="line">    <span class="attr">batch_size</span> = batch_emb.size(<span class="number">0</span>)</span><br><span class="line">    <span class="attr">y_true</span> = torch.cat([torch.arange(<span class="number">1</span>, batch_size, <span class="attr">step=2,</span> <span class="attr">dtype=torch.long).unsqueeze(1),</span></span><br><span class="line">                        torch.arange(<span class="number">0</span>, batch_size, <span class="attr">step=2,</span> <span class="attr">dtype=torch.long).unsqueeze(1)],</span></span><br><span class="line">                       <span class="attr">dim=1).reshape([batch_size,])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算score和loss</span></span><br><span class="line">    <span class="attr">norm_emb</span> = F.normalize(batch_emb, <span class="attr">dim=1,</span> <span class="attr">p=2)</span></span><br><span class="line">    <span class="attr">sim_score</span> = torch.matmul(norm_emb, norm_emb.transpose(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="attr">sim_score</span> = sim_score - torch.eye(batch_size) * <span class="number">1</span>e12</span><br><span class="line">    <span class="attr">sim_score</span> = sim_score * <span class="number">20</span></span><br><span class="line">    <span class="attr">loss</span> = loss_func(sim_score, y_true)</span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure><p>注：看过论文源码 [8] 的同学可能会发现，这个和论文源码中的实现方式不一样，论文源码是为了兼容无监督 SimCSE 和有监督 SimCSE，并兼容有 hard negative 的三句输入设计的，因此实现上有差异。</p><p>看过苏神源码 [9] 的同学也会发现，构造标签的地方不一样，那是因为 keras 的 CE loss 用的是 one-hot 标签，pytorch 用的是数字标签，但本质一样。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</p><p>[2] <a href="https://zhuanlan.zhihu.com/p/367290573" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/367290573</a></p><p>[3] Debiased Contrastive Learning</p><p>[4] ADACLR: Adaptive Contrastive Learning Of Representation By Nearest Positive Expansion</p><p>[5] <a href="https://zhuanlan.zhihu.com/p/334772391" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/334772391</a></p><p>[6] Understanding the Behaviour of Contrastive Loss</p><p>[7] <a href="https://zhuanlan.zhihu.com/p/357071960" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/357071960</a></p><p>[8] <a href="https://github.com/princeton-nlp/SimCSE" target="_blank" rel="noopener">https://github.com/princeton-nlp/SimCSE</a></p><p>[9] <a href="https://github.com/bojone/SimCSE" target="_blank" rel="noopener">https://github.com/bojone/SimCSE</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;来源：&lt;a href=&quot;https://mp.weixin.qq.com/s/12UvfXnaB4NTy54wWIFZdQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://mp.weixin.qq.com/s/12UvfXnaB4NTy54w
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>GEM: A General Evaluation Benchmark for Multimodal Tasks</title>
    <link href="http://yoursite.com/2021/06/22/GEM-A-General-Evaluation-Benchmark-for-Multimodal-Tasks/"/>
    <id>http://yoursite.com/2021/06/22/GEM-A-General-Evaluation-Benchmark-for-Multimodal-Tasks/</id>
    <published>2021-06-22T00:53:34.000Z</published>
    <updated>2021-06-22T01:54:52.841Z</updated>
    
    <content type="html"><![CDATA[<h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><ul><li><p>类似于 GLUE，SuperGLUE 这种 language evaluation benchmark 可以评估纯语言预训练模型的能力，但是多模态方向上，多模态预训练模型的能力还没有相关的benchmark，本文提出了 <font color="red">多模态任务上 evaluation benchmark：<strong>GEM</strong>。</font></p></li><li><p>本文的多模态任务，包括 image-text <strong>GEM-I</strong>; video-text <strong>GEM-V</strong>。同时相比于当前存在的多模态数据集（例如 MSCOCO，Flickr30K，YouCook2, MSR-VTT）， <font color="red"><strong>GEM</strong> 规模上更大，且涵盖多种语言。</font></p></li><li><p>本文提供了两个多模态多语言预训练模型, <a href="https://arxiv.org/abs/2006.02635" target="_blank" rel="noopener">M3P</a> and <a href="https://arxiv.org/abs/2002.06353" target="_blank" rel="noopener">m-UniVL</a> 作为 GEM 的baseline. M3P是一个现成的多语言， image-text 预训练模型，m-UniVL是 本文对video-text预训练模型 UniVL 做的扩展</p></li></ul><h2 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h2><p>总结一下多模态预训练模型中，<strong>包含多语言</strong>的一些模型。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;本文的点&quot;&gt;&lt;a href=&quot;#本文的点&quot; class=&quot;headerlink&quot; title=&quot;本文的点&quot;&gt;&lt;/a&gt;本文的点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;类似于 GLUE，SuperGLUE 这种 language evaluation benchmark 可
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval</title>
    <link href="http://yoursite.com/2021/06/18/Frozen-in-Time-A-Joint-Video-and-Image-Encoder-for-End-to-End-Retrieval-1/"/>
    <id>http://yoursite.com/2021/06/18/Frozen-in-Time-A-Joint-Video-and-Image-Encoder-for-End-to-End-Retrieval-1/</id>
    <published>2021-06-18T01:56:24.000Z</published>
    <updated>2021-06-22T00:55:06.924Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>VinVL: Revisiting Visual Representations in Vision-Language Models</title>
    <link href="http://yoursite.com/2021/06/16/VinVL-Revisiting-Visual-Representations-in-Vision-Language-Models/"/>
    <id>http://yoursite.com/2021/06/16/VinVL-Revisiting-Visual-Representations-in-Vision-Language-Models/</id>
    <published>2021-06-16T08:51:45.000Z</published>
    <updated>2021-06-16T09:04:59.070Z</updated>
    
    <content type="html"><![CDATA[<h2 id="本文的任务"><a href="#本文的任务" class="headerlink" title="本文的任务"></a>本文的任务</h2><p>研究如何改善 V+L 跨模态预训练模型</p><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>通过改善目标检测器模型，来改善以物体 (object) 为中心的图像表示 (image representation)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;本文的任务&quot;&gt;&lt;a href=&quot;#本文的任务&quot; class=&quot;headerlink&quot; title=&quot;本文的任务&quot;&gt;&lt;/a&gt;本文的任务&lt;/h2&gt;&lt;p&gt;研究如何改善 V+L 跨模态预训练模型&lt;/p&gt;
&lt;h2 id=&quot;本文的点&quot;&gt;&lt;a href=&quot;#本文的点&quot; clas
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>UniT: Multimodal Multitask Learning with a Unified Transformer</title>
    <link href="http://yoursite.com/2021/06/15/UniT-Multimodal-Multitask-Learning-with-a-Unified-Transformer/"/>
    <id>http://yoursite.com/2021/06/15/UniT-Multimodal-Multitask-Learning-with-a-Unified-Transformer/</id>
    <published>2021-06-15T04:56:08.000Z</published>
    <updated>2021-06-16T02:22:02.690Z</updated>
    
    <content type="html"><![CDATA[<p>曾几何时，多模态预训练已经不是一个新的话题，各大顶会诸多论文仿佛搭上Visual和BERT，就能成功paper+=1，VisualBERT、ViLBERT层出不穷，傻傻分不清楚……这些年NLPer在跨界上忙活的不亦乐乎，提取视觉特征后和文本词向量一同输入到万能的Transformer中，加大力度预训练，总有意想不到的SOTA。</p><p>如何在多模态的语境中更细致准确地利用Transformer强大的表达能力呢？Facebook最新的 <strong><em>Transformer is All You Need</em></strong> 也许可以给你答案。</p><p><img src="https://i.loli.net/2021/06/15/7HAQCbFuxkGpOgt.png" alt="image-20210615125803091" style="zoom:67%;"></p><p>这篇貌似标题党的文章开宗明义，针对文本+视觉的多模态任务，用好Transformer就够了，与许多前作不同，这次提出的模型一个模型可以解决多个任务：目标检测、自然语言理解、视觉问答，各个模型板块各司其职、条理清晰：<strong>视觉编码器</strong>、<strong>文本编码器</strong>、<strong>特征融合解码器</strong>，都是建立在多层Transformer之上，最后添加为每个任务设计的<strong>处理器</strong>，通过多任务训练，一举刷新了多个任务的榜单。</p><p><strong>论文题目</strong>:<br><strong><em>Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer</em></strong></p><p><strong>论文链接</strong>:<br><em><a href="https://arxiv.org/pdf/2102.10772.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2102.10772.pdf</a></em></p><p><img src="https://i.loli.net/2021/06/15/EXpkQu8UT67eYrn.png" alt="image-20210615125848323" style="zoom: 67%;"></p><h2 id="文本编码器"><a href="#文本编码器" class="headerlink" title="文本编码器"></a>文本编码器</h2><p>用Transformer提取文本特征是个老生常谈的问题，从BERT石破天惊开始，纯文本领域近乎已被Transformer蚕食殆尽，所以该文也不能免俗，直接借用BERT的结构提取文本内容，区别在于，为了解决多个任务，在文本序列前添加了一个针对不同任务的参数向量，在最后输出隐藏状态到解码器时再去掉。</p><p><img src="https://i.loli.net/2021/06/15/l5z9uxCfsK3iEkD.png" alt="image-20210615125939485" style="zoom:50%;"></p><h2 id="视觉编码器"><a href="#视觉编码器" class="headerlink" title="视觉编码器"></a>视觉编码器</h2><p>本文将Transformer强大的表达能力运用到视觉特征的提取中，由于图片像素点数量巨大，首先通过基于卷积神经网络的ResNet-50提取卷积特征，极大程度上地降低了特征数量，最终得到的feature map大小为，然后用全联接层调整单个特征的维度到，再利用多层Transformer中的注意力机制提取各个feature之间的关系，由于Transformer的输入是序列，文章将拉成一条长为的序列，另外和文本编码器类似，同样添加了与下游任务相关的。</p><p><img src="https://i.loli.net/2021/06/15/8s9PnX7Zgz3piNS.png" alt="image-20210615130014598" style="zoom:67%;"></p><p>其中是调整维度的全联接层，是多层Transformer编码器。</p><h2 id="模态融合解码器"><a href="#模态融合解码器" class="headerlink" title="模态融合解码器"></a>模态融合解码器</h2><p>多模态的关键之一就在于怎么同时利用多个模态，在本文中是通过Transformer的解码器实现的，这个解码器首先将任务相关的query做self-attention，再将结果与文本编码器和视觉编码器的结果做cross-attention，针对单一模态的任务，选取对应编码器的输出即可，针对多模态的任务，取两个编码器输出的拼接。</p><p><img src="https://i.loli.net/2021/06/15/TJHRK84Ppv9jzYU.png" alt="image-20210615130046055" style="zoom:50%;"></p><h2 id="任务处理器-task-specific-output-head"><a href="#任务处理器-task-specific-output-head" class="headerlink" title="任务处理器(task-specific output head)"></a>任务处理器(task-specific output head)</h2><p>之前多模态预训练模型往往只针对某一项任务，而本文提出的一个模型可以解决多个文本+视觉任务，与BERT可以解决多个文本任务类似，本文的模型在模态融合解码器的结果上添加为每个任务设计的处理器，这个处理器相对简单，用于从隐藏状态中提取出与特定任务相匹配的特征。</p><ul><li><strong>目标检测</strong>：添加box_head和class_head两个前馈神经网络从最后一层隐藏状态中提取特征用来确定目标位置和预测目标类型。</li></ul><p><img src="https://i.loli.net/2021/06/15/jZhuaekyDWdP46w.png" alt="image-20210615130137970"></p><ul><li><strong>自然语言理解、视觉问答</strong>：通过基于全联接层的分类模型实现，将模态融合解码器结果的第一位隐藏状态输入到两层全联接层并以GeLU作为激活函数，最后计算交叉熵损失。</li></ul><p><img src="https://i.loli.net/2021/06/15/kCJYopFwB7MKRE4.png" alt="image-20210615130146288"></p><h2 id="实验与总结"><a href="#实验与总结" class="headerlink" title="实验与总结"></a>实验与总结</h2><p>本文提出的多模态预训练模型各个板块划分明确，通过多层Transformer分别提取特征，再利用解码器机制融合特征并完成下游任务，同时借助最后一层任务相关的处理器，可以通过一个模型解决多个任务，同时也让多任务预训练成为可能，并在实验中的各个数据集上得到了论文主要进行了两部分实验：</p><h3 id="多任务学习："><a href="#多任务学习：" class="headerlink" title="多任务学习："></a>多任务学习：</h3><p>这里的多任务涉及目标检测和视觉问答两个任务，在目标检测上运用COCO和VG两个数据集，在视觉问答上运用VQAv2数据集。对比了单一任务和多任务同时训练的结果，同时对比了不同任务共用解码器的结果。</p><p><img src="C:\Users\shiyaya\AppData\Roaming\Typora\typora-user-images\image-20210615130353457.png" alt="image-20210615130353457"></p><p>从结果中我们可以看出，单纯的使用多任务训练并不一定可以提高结果，不同任务间虽然相关但是却不完全相同，这可能是任务本身差异或者数据集的特性所导致，第二行和第五行可以很明显地看出COCO上的目标检测和VQAv2的视觉问答相结合后，结果有显著的下降，然而VG上的目标检测却能够和视觉问答很好地结合，通过三个数据集上的共同训练，可以得到最高的结果。</p><h3 id="多模态学习："><a href="#多模态学习：" class="headerlink" title="多模态学习："></a>多模态学习：</h3><p>这一实验中，为了体现所提出模型能够有效解决多个多种模态的不同任务，论文作者在之前COCO、VG、VQAv2的基础上，增加了单一文本任务GLUE的几个数据集（QNLI、QQP、MNLI、SST-2）和视觉推断数据集SNLI-VE，从数据集的数量上可以看出本文模型的全能性。与本文对比的有纯文本的BERT、基于Transformer的视觉模型DETR、多模态预训练模型VisualBERT。</p><p><img src="https://i.loli.net/2021/06/15/hSUibrVRlXz8KF2.png" alt="image-20210615130226872"></p><p>仔细看各个数据集上的结果，不难看出本文提出的模型其实并不能在所有数据集多上刷出SOTA，比如COCO上逊色于DETR，SNLI-VE逊色于VisualBERT，SST-2逊色于BERT，其他数据集上都有一定的提高，但是模型却胜在一个“全”字，模型的结构十分清晰明了，各个板块的作用十分明确，同时针对不同任务的处理器也对后续多模态任务富有启发性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;曾几何时，多模态预训练已经不是一个新的话题，各大顶会诸多论文仿佛搭上Visual和BERT，就能成功paper+=1，VisualBERT、ViLBERT层出不穷，傻傻分不清楚……这些年NLPer在跨界上忙活的不亦乐乎，提取视觉特征后和文本词向量一同输入到万能的Transf
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>SimCSE: Simple Contrastive Learning of Sentence Embeddings</title>
    <link href="http://yoursite.com/2021/06/11/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/"/>
    <id>http://yoursite.com/2021/06/11/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/</id>
    <published>2021-06-11T02:50:57.000Z</published>
    <updated>2021-06-11T02:50:57.626Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Non-Autoregressive Neural Machine Translation</title>
    <link href="http://yoursite.com/2021/06/07/Non-Autoregressive-Neural-Machine-Translation/"/>
    <id>http://yoursite.com/2021/06/07/Non-Autoregressive-Neural-Machine-Translation/</id>
    <published>2021-06-07T02:50:12.000Z</published>
    <updated>2021-06-12T03:44:40.846Z</updated>
    
    <content type="html"><![CDATA[<h2 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h2><p> <a href="https://homes.cs.washington.edu/~jkasai/2020-01-28/nat/" target="_blank" rel="noopener">https://homes.cs.washington.edu/~jkasai/2020-01-28/nat/</a></p><p>翻译版：<a href="https://zhuanlan.zhihu.com/p/110794460" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/110794460</a></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇博客针对当前使用<strong>非自回归</strong>方式处理<strong>机器翻译</strong>任务的相关论文进行总结。</p><h2 id="为什么要进行非自回归机器翻译？"><a href="#为什么要进行非自回归机器翻译？" class="headerlink" title="为什么要进行非自回归机器翻译？"></a>为什么要进行非自回归机器翻译？</h2><p>最近的一系列工作提出了非自回归机器翻译的方法 (NAT, <a href="https://arxiv.org/abs/1711.02281" target="_blank" rel="noopener">Gu et al. 2018</a>) 。NAT并行生成目标单词，这与标准自动回归翻译（AT）形成对比，后者可以预测以所有先前单词为条件的每个单词。虽然AT通常在相似配置下比NAT表现更好，但是NAT通过并行计算加快了推理速度。这种非自回归生成的一个非常成功的应用是Parallel WaveNet  (<a href="https://arxiv.org/abs/1711.10433" target="_blank" rel="noopener">Oord et al. 2017</a>)，将原始自回归Wavenet的速度提高了1000倍以上，并部署在了Google助手中。从NAT快速推断得到的收益可以允许在工业界的特定延迟和预算下部署更大，更深的Transformer模型。在这篇博客文章中，我将概述有关非自回归翻译的最新研究，并讨论我认为对进一步发展而言缺失或重要的内容。</p><h2 id="基本问题和可能的解决方法"><a href="#基本问题和可能的解决方法" class="headerlink" title="基本问题和可能的解决方法"></a>基本问题和可能的解决方法</h2><p>生成中的<a href="https://arxiv.org/abs/1711.02281" target="_blank" rel="noopener">多模式性(Multimodality)</a>对NAT提出了根本的挑战。我们都知道语言是高度多模态的(multimodal)。举一个小例子，英文句子<code>he is very good at Japanese</code>和<code>he speaks Japanese very well</code>是日语句子<code>彼は日本語が上手です</code>的两个有效译文。但是，看起很像的两个句子：<code>he speaks very good at Japanese</code>或<code>he is very good at very well</code>则没有任何意义。我们需要知道模型提交给它自己的两种可能的翻译是哪一种，但是在条件独立的解码中很难实现这一点。并行解码打破了条件依赖性，并经常导致输出不一致。文献中的一些工作已经提出解决NAT中Multimodality问题的方法。在这里，我对提出的方法进行了概括和分类。</p><h3 id="1-基于迭代（Iteration-based）的方法"><a href="#1-基于迭代（Iteration-based）的方法" class="headerlink" title="1. 基于迭代（Iteration-based）的方法"></a>1. 基于迭代（Iteration-based）<strong>的方法</strong></h3><p>解决并行解码问题的一种方法是迭代地优化模型输出(<a href="https://arxiv.org/abs/1802.06901" target="_blank" rel="noopener">Lee et al., 2018</a>; <a href="https://arxiv.org/abs/1904.09324" target="_blank" rel="noopener">Ghazvininejad et al., 2019</a>; <a href="https://homes.cs.washington.edu/~jkasai/2020-01-28/nat/Ghazvininejad et al., 2019" target="_blank" rel="noopener">Gu et al. 2019</a>; <a href="https://arxiv.org/pdf/2001.05136.pdf" target="_blank" rel="noopener">Kasai et al. 2020</a>)。在此框架中，我们放弃了完全可并行化的生成，而是在<strong>每次迭代中优化了先前生成的单词</strong>。由于通常我们所需的迭代次数比输出句子中单词的数量少得多，因此与自回归模型相比，迭代方法仍可以改善等待时间。这些论文均采用不同的方法进行细化，为了更清晰地理解，您可以参考<a href="https://arxiv.org/abs/1904.09324" target="_blank" rel="noopener">Ghazvininejad et al., 2019</a>  提出的一种典型的条件屏蔽语言模型（CMLM）。在给定源文本的情况下，使用目标端的BERT-style的掩蔽语言建模目标对CMLM进行训练，在推断中，我们Mask住<em>低置信度</em>的token，并在每次迭代中对其进行更新。 <a href="https://arxiv.org/pdf/2001.05136.pdf" target="_blank" rel="noopener">Kasai et al. 2020</a> 工作提出了DisCo Transformer，该Transformer可计算出这种掩盖语言建模MLM的有效替代方案。特别地，在给定其他reference token的任意子集的情况下，可以训练DisCo Transformer来预测每个输出token。可以将其视为一次模拟多个masking。我们证明了DisCo Transformer可以减少所需的迭代次数（从而减少解码时间），同时保持转换质量。</p><blockquote><p>[1] Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement</p><p>[2] Mask-Predict: Parallel Decoding of Conditional Masked Language Models. Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer</p><p>[3] Non-autoregressive Machine Translation with Disentangled Context Transformer. Jungo Kasai,  James Cross,  Marjan Ghazvininejad, Jiatao Gu</p></blockquote><h3 id="2-比NLL更好的训练目标"><a href="#2-比NLL更好的训练目标" class="headerlink" title="2. 比NLL更好的训练目标"></a>2. 比NLL更好的训练目标</h3><p>一些工作提出了对数负似然性NLL的替代损失函数。我的直觉是使用普通NLL损失进行训练无法捕捉高度多峰分布(multimodal distributions)的表征。这在某种程度上让人联想到生成对抗网络（GAN）中的对抗损失。在图像生成中，原始的L2重建损失将使模式崩溃并产生模糊的图像，当使用NLL损失训练NAT模型时，可能会发生类似情况。拟议的替代损失函数包括NAT模型与自回归教师之间的隐藏状态的距离(<a href="https://arxiv.org/abs/1909.06708" target="_blank" rel="noopener">Li et al. 2019</a>)，Ngram词袋差 (<a href="https://arxiv.org/pdf/1911.09320.pdf" target="_blank" rel="noopener">Shao et al. 2020</a>) 和辅助正则化 (<a href="https://arxiv.org/pdf/1902.10245.pdf" target="_blank" rel="noopener">Wang et al. 2019</a>). 。与基于迭代的方法相比，这一系列方法可以并行实现一次生成，但代价是性能大大降低。</p><h3 id="3-精简-部分的自回归解码"><a href="#3-精简-部分的自回归解码" class="headerlink" title="3. 精简/部分的自回归解码"></a>3. 精简/部分的自回归解码</h3><p>先前的工作还提出了将轻度或部分自回归模块整合到NAT模型中的方法。 <a href="https://arxiv.org/pdf/1803.03382.pdf" target="_blank" rel="noopener">Kaiser et al. 2018</a> 生成了较短序列的潜在变量，并在顶部进行了并行单词预测。Blockwise decoding和Insertion Transformer产生的局部自回归方式一个句子 (<a href="https://arxiv.org/abs/1811.03115" target="_blank" rel="noopener">Stern et al. 2018</a>, <a href="https://arxiv.org/abs/1902.03249" target="_blank" rel="noopener">2019</a>).  <a href="https://arxiv.org/abs/1910.11555" target="_blank" rel="noopener">Sun et al. 2019</a> 在变压器输出向量之上引入了factorized CRF层，并通过波束近似(beam approximation)进行了快速自回归解码。 <a href="https://arxiv.org/abs/1911.02215" target="_blank" rel="noopener">Ran et al. 2019</a> 引入了精简自回归源端重排序模块，以促进并行目标解码。请注意，他们还使用非自回归重排序模块显示了结果，但是性能却差得多。</p><h3 id="4-用潜在变量建模"><a href="#4-用潜在变量建模" class="headerlink" title="4. 用潜在变量建模"></a>4. 用潜在变量建模</h3><p>我们可以在此框架中解释许多模型。例如，可以将所有以预测长度(predicted length)为条件的NAT模型视为具有潜在变量的建模。但尤其是 <a href="https://arxiv.org/abs/1909.02480" target="_blank" rel="noopener">Ma et al. 2019</a>  使用生成流技术对目标句子的复杂分布进行建模。 <a href="https://arxiv.org/abs/1908.07181" target="_blank" rel="noopener">Shu et al. 2020</a> 开发出了具有确定性推论(deterministic inference)的连续潜在变量NAT模型。</p><h3 id="5-从自回归模型中蒸馏"><a href="#5-从自回归模型中蒸馏" class="headerlink" title="5. 从自回归模型中蒸馏"></a>5. 从自回归模型中蒸馏</h3><p>据我所知，几乎所有表现好的NAT模型都经过自回归模型 (e.g. <a href="https://arxiv.org/abs/1711.02281" target="_blank" rel="noopener">Gu et al. 2018</a>) 的序列级知识蒸馏(<a href="https://arxiv.org/abs/1606.07947" target="_blank" rel="noopener">Kim &amp; Rush 2016</a>）训练而成。虽然较大Transformer的蒸馏也有助于自回归机器翻译，尤其是在贪婪解码的情况下，但它的作用是是较大程度上简化NAT模型 (<a href="https://arxiv.org/pdf/2001.05136.pdf" target="_blank" rel="noopener">Kasai et al. 2020</a>)。 <a href="https://arxiv.org/abs/1911.02727" target="_blank" rel="noopener">Zhou et al. 2019</a> 研究了模型容量与蒸馏数据之间的关系，表明模型容量与蒸馏数据复杂性之间存在相关性。这表明知识蒸馏可以杀死原始数据中的某些模式，从而可以更好地训练NAT模型。</p><h2 id="悬而未决的问题和未来目标"><a href="#悬而未决的问题和未来目标" class="headerlink" title="悬而未决的问题和未来目标"></a>悬而未决的问题和未来目标</h2><p>在这里，我重点介绍了我个人好奇的非自回归机器翻译中的开放性问题。</p><ul><li><strong>我们需要蒸馏吗？</strong>蒸馏肯定是一次性的训练成本，但是如果每次更改训练数据或语言对时都必须这样做，则蒸馏成本可能会很高。我们是否可以利用原始数据获得合理的性能？</li><li><strong>我们需要预测目标长度吗？</strong>我仍然发现目标长度预测很奇怪。当前的许多NAT方法都要求目标长度预测并且以这个预测的长度作为条件。虽然长度预测为我们提供了在潜在变量空间中进行搜索的机会，但长度预测会破坏生成的灵活性。</li><li><strong>NAT可以胜过AT吗？</strong>我们已经看到，在相同的配置下，AT的性能通常优于NAT。但是，NAT可以做得更好吗？或者更实际的说，在相同的延迟预算下，NAT是否能明显胜过AT？NAT可以使用更大的配置。</li><li><strong>预训练和NAT。</strong>在非自回归机器翻译中使用大规模预训练的掩蔽语言模型MLM可能比在自回归翻译中使用更容易。NAT中的解码器（例如条件屏蔽语言模型）看起来更像BERT。</li><li><strong>训练和推理中的隔阂(bridge)。</strong>在迭代NAT框架中，<strong><strong style="color:red;">训练和推理之间经常会出现差距</strong></strong>。For example, a conditional language model (CMLM) is trained to predict masked tokens given the other <strong>gold</strong> observed tokens. 最近一项成功的尝试是对CMLM进行SMART训练 (<a href="https://arxiv.org/abs/2001.08785" target="_blank" rel="noopener">Ghazvininejad et al. 2020</a>) ，他们训练模型以从先前的预测误差中恢复。这种方法可普及到基于迭代的NAT。</li><li><strong>向结构化预测学习。</strong>NLP中已经在结构化预测（例如语法和语义解析）方面投入了很多精力。我们可以从结构化预测的方法中学习以更好地处理生成中的条件依赖性吗？训练和推理之间的上述差距是句法分析中研究的一个问题 (e.g. dynamic oracle, <a href="https://www.aclweb.org/anthology/C12-1059/" target="_blank" rel="noopener">Goldberg &amp; Nivre 2012</a>)。我怀疑从结构化预测中还会吸取更多教训。</li></ul><h2 id="ACL-2021"><a href="#ACL-2021" class="headerlink" title="ACL 2021"></a>ACL 2021</h2><p>【ACL findings】Progressive Multi-Granularity Training for Non-Autoregressive  Translation<br><strong>标题</strong>：非自回归翻译的渐进式多粒度训练</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://zhuanlan.zhihu.com/p/110794460" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/110794460</a></p><p><a href="https://homes.cs.washington.edu/~jkasai/2020-01-28/nat/" target="_blank" rel="noopener">https://homes.cs.washington.edu/~jkasai/2020-01-28/nat/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;来源&quot;&gt;&lt;a href=&quot;#来源&quot; class=&quot;headerlink&quot; title=&quot;来源&quot;&gt;&lt;/a&gt;来源&lt;/h2&gt;&lt;p&gt; &lt;a href=&quot;https://homes.cs.washington.edu/~jkasai/2020-01-28/nat/&quot; tar
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>The multimodality problem in NAT</title>
    <link href="http://yoursite.com/2021/06/07/The-multimodality-problem-in-NAT/"/>
    <id>http://yoursite.com/2021/06/07/The-multimodality-problem-in-NAT/</id>
    <published>2021-06-07T02:40:43.000Z</published>
    <updated>2021-06-12T08:57:41.913Z</updated>
    
    <content type="html"><![CDATA[<h2 id="multimodality-problem"><a href="#multimodality-problem" class="headerlink" title="multimodality problem"></a>multimodality problem</h2><p>非自回归神经机器翻译系统(NAT)通过打破自回归性，并行地生成所有目标词，大幅度地提高了推断速度。然而，现有的NAT模型由于<strong>多峰问题</strong>，与自回归神经网络机器翻译模型相比，翻译质量仍有很大差距。</p><p><strong><strong style="color:red;">什么是多峰问题</strong></strong>，举个简单的例子将汉语句子“干/得/好/！”翻译成英文，可以翻译成“Good job !”或者“Well done !”。由于<strong style="color:blue;">非自回归模型</strong>的条件独立性假设，推断时第一个词“Good”和“Well”的概率是差不多大的，如果第二个词“job”和“done”的概率也差不多大，会使得模型生成出“Good done !”或者“Well job !”这样错误的翻译。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation</p><p>Syntactically Supervised Transformers for Faster Neural Machine Translation</p><p>ICML 2021 (Oral)：Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;multimodality-problem&quot;&gt;&lt;a href=&quot;#multimodality-problem&quot; class=&quot;headerlink&quot; title=&quot;multimodality problem&quot;&gt;&lt;/a&gt;multimodality problem&lt;/
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>最新机器翻译进展</title>
    <link href="http://yoursite.com/2021/06/07/%E6%9C%80%E6%96%B0%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E8%BF%9B%E5%B1%95/"/>
    <id>http://yoursite.com/2021/06/07/最新机器翻译进展/</id>
    <published>2021-06-07T02:10:19.000Z</published>
    <updated>2021-06-07T04:24:20.202Z</updated>
    
    <content type="html"><![CDATA[<h2 id="AAAI-2021"><a href="#AAAI-2021" class="headerlink" title="AAAI  2021"></a>AAAI  2021</h2><p>在 AAAI2021 上同样涌现了许多关于机器翻译任务的研究工作，几乎所有的工作都是基于Transformer模型展开讨论。这里对机器翻译在AAAI2021上的最新研究进展进行总结：</p><h2 id="1-引入语法信息"><a href="#1-引入语法信息" class="headerlink" title="1. 引入语法信息"></a><strong>1. 引入语法信息</strong></h2><p>尽管依托于模型本身本文就能从海量数据中捕获到语言之间的映射关系，但研究人员一直在探索如何将句法、语义等先验知识有效地融入到模型中，并指导模型取得进一步的性能突破。传统的做法通常使用外部工具从训练样本中构造句法树等先验知识，之后在编码端、解码端分别融入先验知识。SyntAligner[1]采取一种自监督双语句法对齐方法，让模型在高维空间中对源语-目标语的句法结构进行精确对齐，从而最大限度地利用对齐后的句法结构之间的互信息提高翻译的性能。</p><blockquote><p>Self-supervised Bilingual Syntactic Alignment for Neural Machine Translation</p></blockquote><h2 id="2-无监督机器翻译"><a href="#2-无监督机器翻译" class="headerlink" title="2. 无监督机器翻译"></a><strong>2. 无监督机器翻译</strong></h2><p>无监督机器翻译同样是机器翻译中备受关注的研究热点。在现实世界中，除了部分富资源语言（如英语，汉语，德语，俄语，印地语等），更多的语言本身受众较小，缺乏海量的双语平行语料进行监督学习。因此，如何在这种资源匮乏，甚至零资源的条件下，学习语言之间的映射是极具挑战的。目前无监督机器翻译通常采用迭代式的back-translation。此外，利用预训练的技术手段能够有效地加快模型的收敛，提高翻译的正确性。[2]通过在构造伪数据的过程中对合成的句子进行正则化约束能够有效地改善翻译的性能。</p><blockquote><p>Empirical Regularization for Synthetic Sentence Pairs in Unsupervised Neural Machine Translation</p></blockquote><h2 id="3-多语言翻译"><a href="#3-多语言翻译" class="headerlink" title="3. 多语言翻译"></a><strong>3. 多语言翻译</strong></h2><p>伴随着机器翻译的发展，研究人员逐渐开始探索不局限于双语句对之间的翻译。多语言模型通过一个模型实现多个语种之间的翻译能够有效降低多语言翻译部署成本。同时将一种源语言翻译成多种不同的目标语言是多语言翻译最常见的场景之一。SimNMT[3]提出了一种同步交叉交互解码器，即在每个目标语生成时，可以依赖未来的信息，以及其他目标语言的历史和未来的上下文信息，充分利用语言内与语言间的信息。</p><h2 id="4-语音翻译"><a href="#4-语音翻译" class="headerlink" title="4. 语音翻译"></a><strong>4. 语音翻译</strong></h2><p>语音翻译直接将源语的语音翻译成目标语言的文本。传统的方法中，采用语音识别和机器翻译级联的方法来解决这一问题。但是具有延迟高，占用存储大，以及容易产生错误累积的问题，很多工作开始关注直接使用端到端的语音到文本的模型来解决这一问题。对于跨模态之间的语言映射，为了让单一的模型充分学习模态之间的关联信息，往往需要引入更多的跨模态和跨语言的特征，造成了沉重的负担，同时单纯的用于端到端模型的语音到文本数据较少，无法充分利用语言识别和机器翻译的数据。为了解决这些问题，COSTT[4]作为一种通用的框架同时结合了级联模型与端到端模型的优点，能够更好地利用大规模双语平行语料，在多个测试集上取得了最优的效果。</p><p>同声传译是一种实时的语言翻译场景，对翻译时延的要求更加严格。目前主流的手段是采用Wait-K策略，但仍然存在由于重复编码导致的训练慢，以及缺少对未来信息建模的问题。Future-guided Training[5]采取unidirectional Transformer方式来避免重复编码，并引入averaged embedding来满足当前词与过去词之间的信息交互。同时利用知识精炼的手段让网络充分利用未来的信息，从而达到更准确的预测。</p><h2 id="5-领域适应"><a href="#5-领域适应" class="headerlink" title="5. 领域适应"></a><strong>5. 领域适应</strong></h2><p>在神经机器翻译中，通过微调来做领域的迁移是一种常见的方法。但是，无约束的微调需要非常仔细的超参数调整，否则很容易在目标域上出现过拟合，导致在通用领域上的性能退化。PRUNE-TUNE[6]是一种基于渐变修剪的领域适应算法。它学习微小的特定于领域的子网以进行调优，通过调整它相应的子网来适应一个新的领域。有效缓解了在微调过中的过拟合和退化问题。</p><p>此外，领域适应与其他方法相结合也是研究的一个热点。元学习对于低资源神经机器翻译(NMT)的有效性已经得到了充分的验证。但是元训练的NMT系统在未见领域中的翻译性能仍然较差。Meta-Curriculum Learning[7]是一种新的面向领域适应的元课程学习方法。在元训练过程中，NMT首先从各个领域学习相似的知识，以避免早期陷入局部最优，最后学习针对不同领域学习个性化的知识，以提高模型对领域特定知识学习的鲁棒性。</p><h2 id="6-解码加速：轻量模型-非自回归解码"><a href="#6-解码加速：轻量模型-非自回归解码" class="headerlink" title="6. 解码加速：轻量模型/非自回归解码"></a><strong>6. 解码加速：轻量模型/非自回归解码</strong></h2><p>过参数化的（超大规模）模型能够有效提升神经机器翻译的性能，但是庞大的存储开销和高昂的计算复杂度使得这类模型无法直接部署到边缘设备(如手机，翻译笔，离线翻译机等)上。早期为了提高模型对未登录词的覆盖度往往使用更大的词表，同时增大了词嵌入矩阵的存储开销，以及构建词表上概率分布时对计算资源的消耗。针对该问题，Partial Vector Quantization[8]提出了一种部分矢量量化的方法，通过压缩词嵌入降低softmax层的计算复杂度，同时使用查找操作来替换softmax层中的大部分乘法运算，在保障翻译质量的同时大大减少了词嵌入矩阵的参数和softmax层的计算复杂度。</p><p>近期，深层模型在神经机器翻译中取得突破性进展，但伴随着层数的堆叠同样面临上述问题。GPKD[9]中提出一种基于群体置换的知识蒸馏方法将深层模型压缩为浅层模型，该方法可以分别应用与编码端与解码端达到模型压缩和解码加速的目的。文中探讨了一种深编码器-浅解码器的异构网络， 其既能保证翻译的准确度，同时满足工业生产的推断时延需求。此外采用子层跳跃的正则化训练方法缓解随着网络加深带来的过拟合问题。</p><p>此外，沿着减少解码端计算复杂度的研究方向，例如Averaged Attention Network（ACL2018）和Sharing Attention Network（IJCAI2019），Compressed Attention Network[10]采取压缩子层的方式，将解码器每一层中分离的多个子层压缩成一个子层，进而简化解码端的计算复杂度，达到解码加速的目的。这种方式在深编码器-浅解码器的结构上取得了进一步的加速增益。</p><p>上述的工作通过轻量化模型提高推断速度，本质上在解码过程中还是采用自回归的方式。相比之下非自回归解码同样是一种有效的解码加速手段。非自回归神经机器翻译系统(NAT)通过打破自回归性，并行地生成所有目标词，大幅度地提高了推断速度。然而，现有的NAT模型由于<strong>多峰问题</strong>，与自回归神经网络机器翻译模型相比，翻译质量仍有很大差距。<strong>什么是多峰问题</strong>，举个简单的例子将汉语句子“干/得/好/！”翻译成英文，可以翻译成“Good job !”或者“Well done !”。由于非自回归模型的条件独立性假设，推断时第一个词“Good”和“Well”的概率是差不多大的，如果第二个词“job”和“done”的概率也差不多大，会使得模型生成出“Good done !”或者“Well job !”这样错误的翻译。ReorderNAT[11]提出一个新颖的NAT框架，通过显式地建模重排序信息来指导非自回归解码。区别于传统方法，根据源语的繁衍率来构造解码端的输入，ReorderNAT在编码器和解码器中间引入了重排序机制。该机制将源语的表示按照目标语的语序进行重新组合，减少解码器对语序的再加工。</p><h2 id="7-评测方法及应用"><a href="#7-评测方法及应用" class="headerlink" title="7. 评测方法及应用"></a><strong>7. 评测方法及应用</strong></h2><p>除了针对机器翻译系统的研究外，如何有效的评估机器翻译系统的性能也是一个重要的研究方向。通常情况下我们使用BLEU作为译文质量评估的常用指标，但是在很多应用场景中，并没有可以对比的参考译文。机器翻译质量评估(QE)便是在不依赖任何参考译文的情况下预测机器翻译质量的一项任务。在QE任务中，通常使用预测器-估计器框架(Predictor-Estimator)。使用预训练的预测器作为特征提取器，再通过评估器对译文进行评估。但是预测器和估计器在训练数据和训练目标上都存在差距，这使得QE模型不能更直接地从大量平行语料库中受益。<strong>DirectQE</strong>[12]中提出了一个新框架，通过生成器在构造QE伪数据，使用额外的探测器在生成的数据上进行训练，并为QE任务设定了新的学习目标，将原本分离的过程进行整合。</p><p>同样机器翻译本身也可以作为工具应用于其他的任务。研究人员为了解决问答任务的数据稀缺问题，通过机器翻译方法来构造多语言问答数据[13]。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <strong>Self-supervised Bilingual Syntactic Alignment for Neural Machine Translation</strong></p><p>[2] Empirical Regularization for Synthetic Sentence Pairs in Unsupervised Neural Machine Translation</p><p>[3] Synchronous Interactive Decoding for Multilingual Neural Machine Translation</p><p>[4] Consecutive Decoding for Speech-to-text Translation</p><p>[5] Future-Guided Incremental Transformer for Simultaneous Translation</p><p>[6] Finding Sparse Structure for Domain Specific Neural Machine Translation</p><p>[7] Meta-Curriculum Learning for Domain Adaptation in Neural Machine Translation</p><p>[8] Accelerating Neural Machine Translation with Partial Word Embedding Compression</p><p>[9] Learning Light-Weight Translation Models from Deep Transformer</p><p>[10] An Efficient Transformer Decoder with Compressed Sub-layers</p><p>[11] <strong>Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information</strong></p><p>[12] <strong>DirectQE: Direct Pretraining for Machine Translation Quality Estimation</strong></p><p>[13] Multilingual Transfer Learning for QA Using Translation as Data Augmentation</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;AAAI-2021&quot;&gt;&lt;a href=&quot;#AAAI-2021&quot; class=&quot;headerlink&quot; title=&quot;AAAI  2021&quot;&gt;&lt;/a&gt;AAAI  2021&lt;/h2&gt;&lt;p&gt;在 AAAI2021 上同样涌现了许多关于机器翻译任务的研究工作，几乎所有的工作
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Masked Non-Autoregressive Image Captioning</title>
    <link href="http://yoursite.com/2021/06/06/Masked-Non-Autoregressive-Image-Captioning/"/>
    <id>http://yoursite.com/2021/06/06/Masked-Non-Autoregressive-Image-Captioning/</id>
    <published>2021-06-06T06:53:49.000Z</published>
    <updated>2021-06-06T08:21:43.522Z</updated>
    
    <content type="html"><![CDATA[<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>以 Non-Autoregressive 的方式来做 Image Captioning</p><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><ul><li><p>自回归的方式做生成任务存在的问题</p><p>自回归解码导致了一些问题，如连续的错误积累、生成缓慢、不恰当的语义和缺乏多样性。 </p><p><strong>顺序解码很容易从训练数据中 copy tokens 来提高语法准确性</strong>，这很容易造成语义错误，而且在图像字幕的生成方面缺乏多样性。</p></li><li><p>非自回归的方式</p><p>非自回归解码已被提出来解决神经机器翻译（NMT）的生成速度较慢的问题。但直接应用到多模态描述生成任务上不是很直接：由于对真实目标分布的间接建模，不可避免地引入了另一个问题，被称为 “多模态问题”。</p><p>多模态问题【网络】：一种特殊的<strong>问题</strong>，其中不存在唯一的全局解决方案。可以在搜索空间周围找到多个全局优化或一个具有多个局部优化（或峰值）的全局优化。即，一对多问题。</p><p>多模态问题【本文】：完全的条件独立会导致对真实目标分布的近似度低。complete conditional independence results in the poor approximation to the true target distribution。</p></li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>本文提出 masked non-autoregressive decoding 来解决 自回归解码 和 非自回归解码 中存在的问题。</p><ul><li><p>训练阶段</p><p>对于输入句子，以几种 （K） 比例进行掩码。</p></li><li><p>在推理阶段</p><p>在推理过程中，从一个<strong>完全被掩盖</strong>的序列到一个<strong>完全没有被掩盖</strong>的序列，以一种合成的方式，分几个 （K） 阶段平行地生成字幕。</p></li></ul><p>实验证明，我们提出的模型可以更有效地保留语义内容，并可以生成更多样性的标题。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://i.loli.net/2021/06/06/GagfcOSPqskTiEW.png" alt="image-20210606152813892"></p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>使用CNN提取的特征图，或者是 使用目标检测器检测得到的 object features.</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>与传统的Tansformer 来做seq-to-seq 是一样的，只是移去了 decoder的 autoregressive mask ， 即decoder 中的每个token，是双向attention。</p><h3 id="Masked-non-autoregressive-decoding"><a href="#Masked-non-autoregressive-decoding" class="headerlink" title="Masked non-autoregressive decoding"></a>Masked non-autoregressive decoding</h3><p>如图1</p><p>训练阶段，以 K 种比例进行掩码，比如 K = [1, 0.6, 0.2],其中 k=1, 代表全部掩码</p><p>测试阶段，使用 K stage，从全部掩码到部分掩码，来一步一步的优化caption。</p><details><summary>细节-增强鲁邦性</summary>我们还以一定比例的随机词来替代 [MASK] token 或 ground-truth token。 在我们的实验中，由于标题的长度相对较短，我们只是在每个非完全屏蔽的输入序列中用一个随机的词替换一个词。 使用随机词可以增强标记的上下文表示，并通过在训练期间引入噪声标记来提高推理过程的稳健性，因为模型在推理的早期阶段很容易产生错误的标记。</details><h3 id="作者分析"><a href="#作者分析" class="headerlink" title="作者分析"></a>作者分析</h3><p>我们提供了更多的分析和讨论，关于模型在不同比例的掩蔽序列中所学习的内容，以及模型在推理过程中不同阶段的预测。 我们进一步讨论了自回归和屏蔽式非自回归解码之间的内在差异。</p><ul><li><p>在训练阶段</p><ul><li>掩码的比例大时，会输出视觉单词</li><li>掩码的比例小时，会对语法进行修正</li></ul></li><li><p>在测试阶段</p><p>在推理过程中，the masked non-autoregressive 解码过程很好地反映了模型在训练过程中所学习的内容。 在早期阶段，该模型倾向于在语言组织较差的图像中生成包含高频率（如 “a”、”on”）和突出的视觉线索（如物体、颜色名词和重要动词）的caption，而在后期阶段，该模型可以通过采用训练好的双向语言模型来选择最合适的词来连接子序列的两边，从而生成语义和语法上正确的标题。 如图1 </p></li><li><p>自回归和屏蔽式非自回归解码之间的内在差异</p><p>在推理过程中，自回归解码和掩码非自回归解码的区别在于掩码非自回归解码自然接近人类的语言生成。更具体地说，人类首先在大脑中生成视觉场景的关键词，然后选择其他词来连接不同的部分，并按照语言规则组成整个句子。这是一个<strong>先视觉再语言</strong>的生成过程，视觉信息奠定了字幕的基础，语言信息辅助以组合的方式而不是顺序的方式形成最终的字幕，这样会更好地保留有意义的语义信息。<strong>掩码非自回归解码一步生成整个句子，因此前面标记的质量不会显着影响后面的标记，这从根本上缓解了自回归解码中存在的顺序错误累积</strong>。相比之下，自回归解码是一个从左到右逐字的生成过程，因此后面步骤生成的标记在很大程度上取决于前面步骤的标记，一旦前面的标记不合适，就容易出现顺序错误累积。更糟糕的是，它只有一次机会生成整个标题，而无法调整前面不适当的标记。因此，自回归解码在保持流畅性方面相当不错，但难以准确说出图像丰富的显着语义内容。</p></li></ul><h2 id="Inference-rules"><a href="#Inference-rules" class="headerlink" title="Inference rules"></a>Inference rules</h2><ul><li><p>推理阶段每个 stage, 如何确定mask 哪些token？或者保留哪些token ?</p><p><strong>保留信息量最大的 token</strong> 并屏蔽每个阶段生成的字幕中的其他位置以生成新的屏蔽输入序列至关重要。</p><p>在本文中，我们采用了一种直接的方法。在这种方法中，在这种方法中，那些不包括在高频率的标记集中的 tokens (tf-idf的思想) ，以及具有高概率且与迄今所选标记不重复的 tokens ，被指定为高度优先保留的 tokens 。例如，在图 <a href="https://www.arxiv-vanity.com/papers/1906.00717/#S3.F1" target="_blank" rel="noopener">1中</a>，我们保留了第一阶段输出序列的 “二”、“鸭子”、“游泳” 和“水”。此外，对最后阶段生成的 caption 进行处理，选择与之前选择的 tokens 不重复的 tokens 。 </p></li><li><p>推理阶段，caption的长度是如何确定的？</p><p>关于推理过程中 length of caption 的确定, 我们首先计算训练数据中的长度的分布情况，然后从这个分布中选择一个随机的长度 T 作为标题。 随后。由T个 [MASK] token 组成的序列被送入模型，这样，一个完整的标题就可以最终被解码。 另一个选择是，我们直接为所有图像设置一个固定的序列长度。 该模型将根据长度自动强制生成粗略或精细的标题，但具有类似的语义信息。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;任务&quot;&gt;&lt;a href=&quot;#任务&quot; class=&quot;headerlink&quot; title=&quot;任务&quot;&gt;&lt;/a&gt;任务&lt;/h2&gt;&lt;p&gt;以 Non-Autoregressive 的方式来做 Image Captioning&lt;/p&gt;
&lt;h2 id=&quot;存在的问题&quot;&gt;&lt;a href
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>Mask-Predict: Parallel Decoding of Conditional Masked Language Models</title>
    <link href="http://yoursite.com/2021/06/06/Mask-Predict-Parallel-Decoding-of-Conditional-Masked-Language-Models/"/>
    <id>http://yoursite.com/2021/06/06/Mask-Predict-Parallel-Decoding-of-Conditional-Masked-Language-Models/</id>
    <published>2021-06-06T06:38:51.000Z</published>
    <updated>2021-06-06T06:45:31.307Z</updated>
    
    <content type="html"><![CDATA[<p>Facebook发表在EMNLP 2019上的工作</p><h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>机器翻译</p><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>非自回归的生成方式</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>这篇文章将masked language model和iterative refinement进行了结合。并提出了更精确的解码方法。</p><p>具体来说，在训练时，这篇文章采用了和masked language model一样的设定。即，随机mask掉目标语句中的一些单词作为decoder input，mask的策略也和Bert中的一致。而decoder output则是这些位置上被mask掉的原始单词。其中，与Bert对每个句子固定mask掉15%的单词不同的是，这篇文章中被mask单词的数目是在从1到句子长度的范围中随机选取的。这样做的好处会在decoding时显现出来。</p><p><img src="https://i.loli.net/2021/06/06/bBgAcQlLFVSt6Es.jpg" alt="img"></p><p>图二：Mask-Predict 解码示意图</p><p>在预测时，这篇文章提出了基于mask and predict的解码方法，是文章的主要贡献。其实在masked language model这个框架下，解码方法是水到渠成的，即每次迭代时，都在当前翻译结果上mask掉一些词，再预测这些词即可。这里有几个点需要特别考虑。一是如何决定mask掉哪些词。文中给出的解决方法是选取top k个解码时置信度最小的词，把他们mask掉再重新预测。二是如何确定mask掉多少个词，也就是如何选取k。文中给出了基于迭代次数递减的策略，即<strong>在第一次解码时，将所有位置都置为[MASK]</strong>，同时预测所有位置的词。<strong>在之后解码迭代中，按照随迭代次数线性衰减的方式确定需要mask的单词个数</strong>：</p><p>$n=N \cdot \frac{T-t}{T}$</p><p>其中N是目标语句的长度，T和t则分别是总的迭代次数和当前迭代次数。上图中展示了解码过程的一个例子。</p><p>上面提到对每个句子，训练时每个epoch确定被mask单词数目时都是随机从 [1，句子长度] 中选取得到的。由于在解码时，被mask单词的个数是线性递减的，即会从 [句子长度，1] 依次递减。因此，训练时这样选取被mask单词的数目可以增加模型的capacity，让模型能处理任意个单词被mask掉时的情况，从而更符合解码时的策略，减少bias并达到更好的效果。</p><p>在确定目标语句的长度时，这篇文章采用了与上篇文章相同的方法，即通过encoder output来预测目标语句的长度，和golden目标语句的长度作为额外的loss function单独训练。</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a><strong>Results</strong></h2><p><img src="https://i.loli.net/2021/06/06/xongXjtwZciSe42.jpg" alt="img" style="zoom:50%;"></p><p>从上表中可以看到，这篇文章在WMT14 En-De和WMT16 En-Ro上均达到了SOTA。但有一点存疑的是，由于文章中也采用了knowledge distillation来训练non-autoregressive模型，而其采用的autoregressive teacher模型均为比较强的模型（28.6 on WMT14 En-De and 34.28 on WMT16 En-Ro），没有控制变量采用和baseline相同效果的autoregressive teacher。而一般来说，teacher越强训练出的non-autoregressive模型也会越强，因此<strong>这篇文章良好的效果应该也有一部分是得益于其选择了较强的teacher</strong>。</p><p>文中也做了在WMT17 En-Zh上的实验，以及对迭代次数和mutiple length candidates的分析，这里就不一一赘述了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Iterative refinement无疑是一个改善 $O(1)$非自回归模型的良好方向，在增加了有限的复杂度（通常是 $O(10)$)的代价下，非自回归模型的翻译质量得到了大幅提高。在这个方向中， Mask-Predict这篇文章给出了一个优良的解码范例，即每次迭代并不会预测所有的单词，而是预测置信度较低的数个单词。</p><p>但这样的解码范例也可能并不是最优的。因为模型给出的概率上的置信度有时候并不会反应真正的翻译质量。如何证明/设计与翻译质量挂钩的解码策略也是一个值得思考的问题。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Facebook发表在EMNLP 2019上的工作&lt;/p&gt;
&lt;h2 id=&quot;任务&quot;&gt;&lt;a href=&quot;#任务&quot; class=&quot;headerlink&quot; title=&quot;任务&quot;&gt;&lt;/a&gt;任务&lt;/h2&gt;&lt;p&gt;机器翻译&lt;/p&gt;
&lt;h2 id=&quot;本文的点&quot;&gt;&lt;a href=&quot;#本文的点
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
</feed>
