<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShiYaya</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-02-26T01:15:46.853Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ShiYaya</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[DeepFuse] HKU’s Multimodal Machine Translation System for VMT’20</title>
    <link href="http://yoursite.com/2021/02/26/DeepFuse-HKU%E2%80%99s-Multimodal-Machine-Translation-System-for-VMT%E2%80%9920/"/>
    <id>http://yoursite.com/2021/02/26/DeepFuse-HKU’s-Multimodal-Machine-Translation-System-for-VMT’20/</id>
    <published>2021-02-26T01:15:34.000Z</published>
    <updated>2021-02-26T01:15:46.853Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>A Closer Look at the Robustness of Vision-and-Language Pre-trained Models</title>
    <link href="http://yoursite.com/2021/02/24/A-Closer-Look-at-the-Robustness-of-Vision-and-Language-Pre-trained-Models/"/>
    <id>http://yoursite.com/2021/02/24/A-Closer-Look-at-the-Robustness-of-Vision-and-Language-Pre-trained-Models/</id>
    <published>2021-02-24T02:12:45.000Z</published>
    <updated>2021-02-25T03:32:27.292Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>大规模的预训练多模态transformer将最新的视觉-语言任务推进到了一个新的高度。虽然在标准任务上实现了令人印象深刻的性能，但是，迄今为止，任然不清楚这些预训练模型的鲁棒性。</p><p>为了进行调查，我们针对现有的预训练模型对4种不同类型的V + L特定模型的鲁棒性进行了全面的评估：(i) Linguistic Variation; (ii) Logical Reasoning; (iii) Visual Content Manipulation; and (iv) Answer Distribution Shift. 有趣的是，by standard model finetuning，预训练的V+L模型相比于task-specific 模型展示出更好的鲁棒性。</p><p>为了<strong>进一步增强模型的鲁棒性</strong>，本文提出了<strong>MANGO</strong>，一个具有泛化性且鲁棒的方法，可以在embedding space 学习a Multimodal Adversarial Noise GeneratOr 以愚弄pre-trained V+L models。与以往针对一种特定类型的鲁棒性的研究不同，MANGO具有任务不可知性，并且可以针对各种任务（旨在评估鲁棒性的广泛方面）对预训练模型进行通用性能提升。</p><p> 全面的实验表明，MANGO在9个鲁棒性基准中有7个达到了最新水平，大大超过了现有方法。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>当前的 V+L pre-tranining model取得了很大的进展在各种 V+L tasks，但是这些benchmark 在测试集和数据集上的分布常常是相似的，textual query 几乎没有 linguistic variation, 使用干净的自然图像，而没有任何visual content manipulation。 因此，尽管这些标准基准对于通用模型评估有效，但仍<strong>缺乏明确评估模型鲁棒性的能力</strong>。（在本文中，我们不关注 adversarial robustness，<strong style="color:red;">因为目前没有可用的adversarial benchmark</strong>。因此，我们在已有的robustness benchmark上进行观测，这些benchmark 设有挑战性的设置，并且经过了人类的验证）</p><p>(i) VQA-Rephrasings[56] for <strong>linguistic variation</strong>;</p><p>(ii) VQA-LOL (Compose and Supplement) [18], VQA-Introspect [54] and GQA [25] for <strong>logical reasoning</strong>; </p><p>(iii) IV-VQA and CVVQA [2] for <strong>visual content manipulation</strong>;  </p><p>(iv) VQA-CP v2 [3] and GQA-OOD [31] for <strong>answer distribution shift</strong>.</p><h3 id="当前方法存在的问题"><a href="#当前方法存在的问题" class="headerlink" title="当前方法存在的问题"></a>当前方法存在的问题</h3><p>VILLA，在multimodal embedding 加入对抗扰动，<strong>projected gradient descent（PGD） attack training（AT）</strong> 可以在 linguistic variation and visual content manipulation 增强鲁棒性，但是在训练集和测试集之间有显著的数据分布差异时，会有收效甚微的影响甚至drop model performence。</p><h3 id="本文方法简介"><a href="#本文方法简介" class="headerlink" title="本文方法简介"></a>本文方法简介</h3><p>为了在所有方面都实现鲁棒性，本文提出了 MANGO，通过在multi-modal embedding space 引入adversarial noise来增强鲁棒性。</p><p><img src="https://i.loli.net/2021/02/24/yDSgc5JwWFHCU8h.png" alt="image-20210224120151245"></p><p>如图 figure 1a所示，不使用PGD来生成对抗扰动，而是使用一个基于可训练神经网络来学习一个adversarial noise generator。与 VILLA相同，在embedding space 加入扰动，因为本文的目标 是对抗训练的最终结果，而不是制造对抗样本。</p><p>【1】本文要学习的是一个 universial noise generator，但是在VILLA中使用的PGD方法是针对每个特定样本来生成的，本文提出的noise generator 是通用的，对输入训练样本是不加区别的。【2】而PGD的方法是<strong>耗时</strong>的，而本文提出的方法是轻量级的，不需要梯度计算中的重复迭代。同时，为了使能多样性的对抗embedding，本文进一步提出随机对image regions 和 textual tokens掩码。</p><h3 id="本文的贡献"><a href="#本文的贡献" class="headerlink" title="本文的贡献"></a>本文的贡献</h3><ul><li>第一个系统性的分析pre-trained V+L 模型的鲁棒性</li><li>提出了 MANGO，一个generic and efficient 对抗训练方法来增强 V+L model 鲁棒性</li><li>实验结果证明了本文提出方法的鲁棒性。</li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h4><p>Perturbing clean images with Gaussian noise. we use Gaussian noise augmentation as a simple baseline to investigate model robustness under V+L setting. Instead of adding noise to raw image pixels as in [52], we add perturbations directly to the embeddings.</p><h4 id="Adversarial-Noise-Generator-our"><a href="#Adversarial-Noise-Generator-our" class="headerlink" title="Adversarial Noise Generator (our)"></a><strong>Adversarial Noise Generator</strong> (our)</h4><p>Adding Gaussian noise to clean image-text pairs 可以补充训练样本。但是，随着训练的持续，模型可以逐渐的适应这种扰动，因为扰动都是从同一个 Gaussian noise distribution 中采样来的。</p><p>为了得到 harder perturbations，本文提出了一个可学习的 adversarial noise generator。对抗性噪声发生器将高斯噪声样本作为输入，通过可学习神经网络产生对抗性噪声样本。</p><p>Intuitively, to maximally fool the backbone network, 【1】we want to <strong>maximize prediction errors on these adversarially perturbed samples.</strong> 【2】In the meantime, we want the model to possess <strong>less confidence in its predictions on perturbed samples</strong> than clean samples, to promote harder adversarial examples。因此，<strong>adversarial noise generator 的目标是</strong>最大化这两个损失的求和：【1】task-speficic loss 【2】KL loss, which measures the distance between the predicted answer distribution of perturbed samples and that of clean samples.</p><p>另一方面，the trained model 旨在通过将对抗性生成的嵌入作为数据增强来最大程度地减少这两种损失。</p><p>综合上述两种方面，提出了如下的min-max game:</p><script type="math/tex; mode=display">\min _{\boldsymbol{\theta}} \max _{\boldsymbol{\phi}_{v}(\boldsymbol{v}, \boldsymbol{w}, \boldsymbol{y}) \sim \mathcal{D} \boldsymbol{\alpha} \in \mathcal{N}(\mathbf{0}, \mathbf{1})} \mathbb{E}\left[\mathcal{L}_{s t d}\left(\boldsymbol{\theta}, \boldsymbol{\phi}_{v}\right)+\beta \mathcal{R}_{k l}\left(\boldsymbol{\theta}, \boldsymbol{\phi}_{v}\right)\right]</script><p>where $\beta$ is a hyper-parameter, and</p><script type="math/tex; mode=display">\begin{array}{l}\mathcal{L}_{s t d}\left(\boldsymbol{\theta}, \boldsymbol{\phi}_{v}\right)=\mathcal{L}_{\mathrm{BCE}}\left(f_{\boldsymbol{\theta}}\left(\boldsymbol{v}+g_{\boldsymbol{\phi}_{v}}(\boldsymbol{\alpha}), \boldsymbol{w}\right), \boldsymbol{y}\right) \\\mathcal{R}_{k l}\left(\boldsymbol{\theta}, \boldsymbol{\phi}_{v}\right)=\mathcal{L}_{k l}\left(f_{\boldsymbol{\theta}}\left(\boldsymbol{v}+g_{\phi_{v}}(\boldsymbol{\alpha}), \boldsymbol{w}\right), f_{\boldsymbol{\theta}}(\boldsymbol{v}, \boldsymbol{w})\right)\end{array}</script><p>在训练时，迭代跟新 an outer loop of the backbone network and an inner loop of noise generator.</p><p>本文提出的adversarial noise generator 是轻量级的，仅仅存在 a few linear layers。相比于一个深层模型，这种轻量模型更容易陷入局部最优。因此，定期地，we replace the learned noise generator with a new one trained from scratch。每次，new  generator is trained against the latese learned parameters of the backbone.</p><h4 id="Random-Masking"><a href="#Random-Masking" class="headerlink" title="Random Masking"></a>Random Masking</h4><p>虽然 adversarial noise generator 可以产生具有挑战性，更加多样化的噪声扰动，但是不会改变训练样本的内在统计（例如，问题长度和image regions的分布）。然而，实际上，在robustness benchmark 的训练和测试集中存在这种 significant mismatch。比如，the average length of questions in VQA-LOL 测试集是VQA V2 训练集的 2-3倍。</p><p>为了补偿这种统计上的不匹配，我们建议在向图像和单词嵌入中添加对抗性噪声时，<strong>随机掩盖图像区域</strong> and <strong>随机插入[MASK]令牌</strong>。</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>进行实验分析了 pre-trained V+L model 的鲁棒性和 本文提出的MANGO framework 的有效性。本文使用UNITER作为 backbone，并将 MANGO与 UNITER和 VILLA 在9个 robustness datasets + VQA v2 dataset上进行了比较。本文在这10个 benchmark上进行研究，<strong style="color:red;"><strong>因为目前在其他任务上没有这种 robustness dataset。</strong></strong></p><p>VILLA 在预训练阶段和微调阶段都采用了 对抗训练，而本文只是在微调阶段（即针对特定任务）</p><p><img src="https://i.loli.net/2021/02/25/HwjDn2voZfWKFOk.png" alt="image-20210225095811994"></p><h3 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h3><p>investigate the adversarial robustness of pre-trained V+L models.</p><p>（在本文中，我们不关注 adversarial robustness，<strong style="color:red;">因为目前没有可用的adversarial benchmark</strong>。因此，我们在已有的robustness benchmark上进行观测，这些benchmark 设有挑战性的设置，并且经过了人类的验证）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;p&gt;大规模的预训练多模态transformer将最新的视觉-语言任务推进到了一个新的高度。虽然在
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>UNIMO Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning</title>
    <link href="http://yoursite.com/2021/02/23/UNIMO-Towards-Unified-Modal-Understanding-and-Generation-via-Cross-Modal-Contrastive-Learning/"/>
    <id>http://yoursite.com/2021/02/23/UNIMO-Towards-Unified-Modal-Understanding-and-Generation-via-Cross-Modal-Contrastive-Learning/</id>
    <published>2021-02-23T03:27:15.000Z</published>
    <updated>2021-02-23T12:00:49.165Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>VX2TEXT End-to-End Learning of Video-Based Text Generation From Multimodal Inputs</title>
    <link href="http://yoursite.com/2021/02/23/VX2TEXT-End-to-End-Learning-of-Video-Based-Text-Generation-From-Multimodal-Inputs/"/>
    <id>http://yoursite.com/2021/02/23/VX2TEXT-End-to-End-Learning-of-Video-Based-Text-Generation-From-Multimodal-Inputs/</id>
    <published>2021-02-23T02:49:31.000Z</published>
    <updated>2021-02-23T07:28:59.717Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li>本文提出了一个框架 for text generation from multimodal inputs consisting of video plus text, speech, or audio。</li><li><p>为了利用 transformer networks，每个模态通过一个 learnable tokenzier 首先转换为 a set of language embeddinngs。这将使得我们的方法可以在语言空间执行多模态融合，从而消除了对ad-hoc cross-modal fusion modules 的需要。</p></li><li><p>为了解决在连续输入（例如视频或音频）上tokenization 的不可微性，我们利用了一种放松方案，该方案可进行端到端训练。</p></li><li><p>进一步地，不像先前的 encoder-only models。本文提出的网络包括一个 autoregressive decoder来生成 open-ended text。同时在语言空间执行多模态融合，这使我们的方法完全具有生成性，并使其<strong>直接适用于不同的“video + $x $ to text” 问题，而无需为每个任务设计专门的网络.</strong></p></li><li><p>本文提出的框架不仅概念简单，而且效果显着。实验结果证明，our approach based on a single architecture 在三个video basedd text-generation task （captioning, question answering and audio-visual scene-aware dialog）上实现了最好的性能，而且本文提出的方法不需要任何的预训练任务。</p></li></ul><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>While this and a few other recent works [55] have leveraged decoders for text-generation from multimodal inputs, we believe <strong>we are the first</strong> to empirically demonstrate via systematic ablation the performance improvements achieved with generative learning with decoding, compared to discriminative learning applied to the same encoder model.</p><p>当前的multimodal transformer-based models inspired by the success of pretext tasks in the language domain（预训练任务）。这些工作，依赖消耗大的预训练任务。但是本文提出的VX2TEXT 可以在 unified language space 执行 跨模态融合，这不需要multimodal pretext pretraining.</p><blockquote><p>Hero: Hierarchical encoder for video language omni-representation pre-training</p><p>Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.</p><p>Videobert: A joint model for video and language representation learning, 2019.</p><p>Lxmert: Learning crossmodality encoder representations from transformers.</p><p>Unified vision-language pre-training for image captioning and vqa</p></blockquote><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>本文提出的方法可以概括为三步: (1) 利用一个 预训练的 modality-specific classifiers来为每个模态获得最可能的类别预测。(2) 将预测类别的textual names 经由本文提出的可微分tokenization scheme 嵌入到一个语义语言空间，这将使得整个系统可以端到端的训练（including the modality-specific classifiers)。（3）最终，使用一个generative encoder-decoder language model 将 多个模态的，embedding vector 映射到 free-form text，这将使得 不同形式的 ”video+$x$ to text” 问题变形为一个 sequence-to-sequence task。</p><h4 id="Differentiable-Tokenization"><a href="#Differentiable-Tokenization" class="headerlink" title="Differentiable Tokenization"></a>Differentiable Tokenization</h4><ul><li>We first leverage modality-specific classifiers trained  to predict a large set of categories over <strong>predefined language vocabularies</strong>.</li><li>虽然概念上是简单的，但是这个方法有一些缺点。第一，预训练的 modality-specific classifiers 可能不能泛化到目标数据。第二，每个分类器中选择top categories，这一操作是不可微分的，这阻止我们针对 target task 来微调modality-specific classifiers。</li><li>为了解决这些限制，本文提出了一个 differentiable tokenization scheme，这个方案可以在整个系统（modality specific classifer + sequence-to-sequence model）上进行端到端的训练。</li><li><strong>将预测类别的textual names 嵌入到一个语义语言空间</strong>：（1）对于每个模态的类别概率输出，采样top $K_m$个类别。（2）将采样的类别名称嵌入到语言空间：$\mathbf{e}_{m}^{k}=\mathbf{W}_{m}^{T} \mathbf{c}_{m}^{k}$，the embedding transformation  $\mathbf{W}_{m}$ can be initialized using a pretrained language embedding space </li></ul><h3 id="Generative-Encoder-Decoder"><a href="#Generative-Encoder-Decoder" class="headerlink" title="Generative Encoder-Decoder"></a>Generative Encoder-Decoder</h3><p>上一阶段，将不同的模态嵌入到了一个相同的语言空间，因此，现在可以使用一个<strong>text encoder</strong>来融合多模态信息。将多个模态得到的embedding vectors 组成一个长为L的序列，并结合<strong>task token</strong> 输入到 <strong>text encoder</strong>，并生成一个长为L的序列，该序列从多个模态中捕捉到了task  specific information。</p><p>将得到的新序列送入 decoder 中来做text generation。本文提出的decoder使用auto-regressive的方式。</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ul><li>使用 teacher-forcing 和 cross-entropy 来训练模型</li></ul><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul><li>大部分先前的 multimodal transformer 依赖 task-specific heads 来处理不同的任务。具体而言，为生成式任务设计的heads 通常与 判别式任务是不同的。但是，本文提出的VX2TEXT 可以同时处理这两种任务，而不需要改变结构</li><li>对于生成式任务，captioning and video dialog，使用 beam search and greedy decoding 来生成句子。</li><li>对于判别式任务，QA on TVQA，模型需要从候选答案中挑选出一个最可能的答案。在这种情况下，本文include the entire set of candidate answers as additional input to the model (using separator tokens to mark them)。然后评估每个候选答案，根据autoregressive decoder对它们输出的概率分布。</li></ul><h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><p>We use T5-base [39] as our text transformer including the text token embedding layer, the encoder and the decoder. We use pretrained weights provided in HuggingFace [50]</p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>简要总结本文有效的点</li><li>（1）提出将不同的模态，通过一个modality-specific classifier 映射到语言空间。（2）提出了一个端到端训练的模式，同时可以将 classifier 一起训练，这样解决了 迁移，泛化性不好的问题。（3）为了可以进行端到端的训练，采取了一些技术方案。we leverage the Gumbel-Softmax trick [18] and a differentiable approximation of tokenization [8].</li></ul><blockquote><p>Eric Jang, Shixiang Gu, and Ben Poole. <strong>Categorical reparameterization with gumbel-softmax.</strong>  arXiv preprint arXiv:1611.01144, 2016.</p><p>Yoshua Bengio, Nicholas L´eonard, and Aaron Courville. <strong>Estimating or propagating gradients through stochastic neurons for conditional computation.</strong> arXiv preprint arXiv:1308.3432, 2013.</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;本文提出了一个框架 for text generation from multi
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>Adaptive Offline Quintuplet Loss for Image-Text Matching</title>
    <link href="http://yoursite.com/2021/02/21/Adaptive-Offline-Quintuplet-Loss-for-Image-Text-Matching/"/>
    <id>http://yoursite.com/2021/02/21/Adaptive-Offline-Quintuplet-Loss-for-Image-Text-Matching/</id>
    <published>2021-02-21T08:00:22.000Z</published>
    <updated>2021-02-21T08:01:38.586Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>现有的image-text matching 的方法一般使用<strong>在线</strong>负样本和三元组损失来训练模型。对于mini-batch中的 image 或 text anchor, 模型被训练以希望区分与anchor 相对应的 positive sample 和 most confusing negative sample。这种策略能够提高模型区分image 和 text 之间细粒度的对应或者是不对应。</p><p>但是，这种方法存在几个缺陷。（1）负样本的选择策略，给模型提供了较少的机会：从<strong>很难区分的样本</strong>中学习。（2）训练的模型从训练集到测试集的泛化性较差。（3）The penalty lacks hierarchy and adaptiveness for hard negatives with different “hardness” degrees。</p><p>在本文中，（1）我们提出了一个从整个训练集中采样 <strong>negative offline samples</strong> 的解决方法。这种方法，提供了 “harder” offline negatives than online hard negatives 让模型来区分。（2）基于 <strong>the offline hard negatives,</strong> 一个五元组损失被提出来，以提高模型的泛化性。（3）另外，提出了一个新颖的损失函数来结合 <strong>the knowledge of</strong> positives, online hard negatives and online hard negatives.</p><p>由于本文提出的方法，不是创建了一个新颖的模型，而是在采集样本与损失函数上做的改进，因此，本文在三个最好的模型上添加了本文提出的模块，并报告了实验结果，证明了本文提出方法的有效性。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li>本文使用了<strong>两轮训练</strong>来增加<strong>offline “harder” negatives</strong>。在第一轮，本文使用原始的 online triple loss来训练matching model。 然后，使用训练好的模型，对于训练集中的 image or text anchor，模型预测它们与训练中的负样本的相似性分数，并对这些负样本进行排序。在第二轮，对于mini-batch 中的每个anchor，本文从 top negative list中采取采样offline negatives. 在这个过程中，多种offline hard negative pairs被构建，这些负样本与anchor之间共享或者不共享common elements。</li><li>进一步地，本文修改了损失函数，将offline hard negative pairs的信息融合到 online triplet loss中。The complete training loss实现了对(positive pairs， offline hard negatives、online hard negatives)  <strong>分等级</strong>和<strong>自适应</strong>的惩罚，</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h3&gt;&lt;p&gt;现有的image-text matching 的方法一般使用&lt;strong&gt;在线&lt;/strong&gt;负样本和三元组损失来训练模型。对于mini
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="Image-Text Matching" scheme="http://yoursite.com/categories/cross-modal/Image-Text-Matching/"/>
    
    
      <category term="cross-modal,Image-Text Matching" scheme="http://yoursite.com/tags/cross-modal-Image-Text-Matching/"/>
    
  </entry>
  
  <entry>
    <title>VMSMO Learning to Generate Multimodal Summary for Video-based News Articles</title>
    <link href="http://yoursite.com/2021/02/21/VMSMO-Learning-to-Generate-Multimodal-Summary-for-Video-based-News-Articles/"/>
    <id>http://yoursite.com/2021/02/21/VMSMO-Learning-to-Generate-Multimodal-Summary-for-Video-based-News-Articles/</id>
    <published>2021-02-21T07:52:40.000Z</published>
    <updated>2021-02-23T07:29:14.533Z</updated>
    
    <content type="html"><![CDATA[<h2 id="VMSMO-Learning-to-Generate-Multimodal-Summary-for-Video-based-News-Articles"><a href="#VMSMO-Learning-to-Generate-Multimodal-Summary-for-Video-based-News-Articles" class="headerlink" title="VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles"></a>VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles</h2><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>当前受欢迎的多媒体新闻格式是提供<strong>一个视频和相对应的文章</strong>。 这种格式广泛应用于 new media (CNN and BBC)，social media (Twitter and Weibo)。<br>在这种情况下，自动选择<strong>合适的视频封面</strong>并生成相应的<strong>文章摘要</strong>可帮助编辑人员节省时间，并使读者更有效地做出决定。</li><li>因此，在本文中，we propose the task of <strong>Video-based Multimodal Summarization with Multimodal Output</strong> (VMSMO) to tackle such a problem. </li><li>此任务中的主要挑战是使用文章的语义共同对视频的时间依赖性进行建模。</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>视频的封面应该是整个视频最显著的要素，而文本摘要应该是从原文章中提取出来的重要信息。因为视频和文章都关注于同一个报告内容中的相同事件，因此这两种信息形式在 summarizing 过程中应该是<strong>互为补充的</strong>。但是由于视频和文章是不同的模态（空间），如何充分的探索视频中的时域依赖与文章中的语义内容<strong>之间的关系</strong>仍然是一个问题。</p><p>因此，在本文中，我们提出了一个模型（DIMS）。该模型通过在过程中实施一个 dual interaction strategy 来同时summarize video and article。（1）使用 RNN 来编码 text and video. （2）设计了一个对偶交互模块（a dual interaction module）来让视频和文章相互交互。具体地，包括一个conditional self-attention mechanism 该模块可以在文章的指导下学习 local video representation。还包括一个global-attention mechanism 来学习 high level representation of video-aware article and article-aware video。（3）最后，based on fusion repersentation multimodal generator generates textual summary and cover image。</p><p>为了证明本文提出模型的有效性，本文从社会媒体网站上收集了 a large-scale news article-summary dataset associated with video-cover。实验证明，在当前广泛使用的评价标准上，DIMS可以显著的超过当前最好的baseline methods.</p><h2 id="本文的贡献"><a href="#本文的贡献" class="headerlink" title="本文的贡献"></a>本文的贡献</h2><ul><li>提出了一个任务，需要同时生成一个视频封面和一个文本摘要。</li><li>针对该任务，提出了一个模型，该模型可以同时建模视频中对的时域依赖和文章中的语义信息。</li><li>本文提出了一个大规模的数据集。在该数据集上，本文提出的方法在 automatic and human evaluation上都显著好于 baseline methods。</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><ul><li><p>Text Summarization: (1) Extractive models：从原文中提取一句话来表征整个文章的内容。（2）abstractive models：generate a summary from scratch</p></li><li><p>Multimodal Summarization：（1）结合多模态的输入，生成更高的texual summaries。（2）Multimodal<br>summarization with multimodal output 这一方向，研究的相对较少。[zhu 2018] 提出同时输出一个 textual summary 和 从6个候选中挑选出来的most relevent image。[zhu 2020] 增加了一个多模态的目标函数。</p><p>但是，在实际应用中，我们通常需要为包含数百帧的视频选择封面图。 因此，视频中帧之间的时间相关性不能通过几种静态编码方法简单地建模。</p><blockquote><p>[zhu 2018] Msmo: multimodal summarization with multimodal output. EMNLP/IJCNLP. </p><p>[zhu 2020] Multimodal summarization with guidance of multimodal reference.</p></blockquote></li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://i.loli.net/2021/02/21/KqO2uQEkZj1fyPB.png" alt="image-20210221114344434"></p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>本文从 <strong>微博</strong> 上收集了做 VMSMO task 的数据集。视频的平均时长是1分钟，帧率是25fps。对于文本，文章的平均长度是96个字，文章summary的长度是12个字。整体上，有184k 个样本被收集，180k作为训练集，2.4k作为验证集，2.4k作为测试集。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ul><li><p>compare baseline：本文提出的方法与summarization baseline 和 VQA baseline 进行比较。</p></li><li><p>evaluation metrics：<br>（1）评估生成的summary：standard full-length <strong>Rouge F1</strong>. R-1, R-2, and R-L refer to unigram, bigrams, and the longest common2 subsequence respectively.</p><p>（2）评估chosen cover frame: mean average precision （MAP）and recall at precision（$R_n@k$）。$R_n@k$用来评估是否positive sample 被排在n candidatas的前k个位置。</p></li><li><p>实验结果证明，（1）本文提出的方法相比于 baseline methods 都要好。（2）本文提出了一个联合损失，同时训练两个任务，可以看做是一个 Multi-task。在本文的实验中，探索了，如果分别训练这两个任务，是怎样的结果。从倒数第二列中可以发现，我们采用的多任务方式训练，效果会好。（3）在本文实验中探索了conditional self-attention 和 global-attention对效果的影响。从最后一列的实验结果可以发现，self-attention 模块对挑选 封面图有很大的贡献，global attention对生成文本摘要有很大的贡献。</p></li></ul><p><img src="https://i.loli.net/2021/02/21/PcFyBkTZNQlHY8m.png" alt="image-20210221154509463"></p><h2 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h2><ul><li>本文最后说到，可以结合video  script (subtitles) 来做该任务。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;VMSMO-Learning-to-Generate-Multimodal-Summary-for-Video-based-News-Articles&quot;&gt;&lt;a href=&quot;#VMSMO-Learning-to-Generate-Multimodal-Summary
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>BART</title>
    <link href="http://yoursite.com/2021/02/19/BART/"/>
    <id>http://yoursite.com/2021/02/19/BART/</id>
    <published>2021-02-19T09:59:22.000Z</published>
    <updated>2021-02-21T07:54:06.268Z</updated>
    
    <content type="html"><![CDATA[<h1 id="BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and-Comprehension"><a href="#BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and-Comprehension" class="headerlink" title="BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"></a>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>本文提出了一个<strong>denoising autoencoder</strong> (BART)来预训练一个 sequence-to-sequence model。It is implemented as a sequence-to-sequence model<br>with a bidirectional encoder over corrupted text and a<br>left-to-right autoregressive decoder</li><li>BART 通过两点来训练：（1）使用一个<strong>arbitrary noising function</strong> 来对原始文本添加噪声。（2）让模型去学习重构原始文本。</li><li>BART使用<strong>一个标准的 transformer-based neural machine translation architecture</strong>。由于采用了 <strong style="color:blue;">bidirectional encoder</strong> 可以看做是对BERT的推广，由于其采用了 <strong style="color:blue;">the left-to-right decoder</strong> 也可以看做是对GPT2的推广，同时也采用了许多现在广泛使用的预训练方案。</li><li>本文对许多 noising approaches 进行评估，发现最好的方案是对原始的句子进行打乱顺序。本文采用了一个 novel in-filling scheme，<strong>使用spans of text 来代替 a single mask token</strong>。</li><li>实验结果：（1）当对文本生成进行微调之后，BART 对生成式任务的性能尤其的好。BART对理解型任务也表现很好。（2）BART与RoBERTa的性能相匹配，在 GLUE 和 SQuAD 上有可比较的training resources，在一系列  abstractive dialogue, question answering, and summarization tasks 取得了最新成果，并在ROUGE上获得了多达6个点的提升。（3）BART在机器翻译任务上，仅使用 target language 来预训练。在BLEU 指标上，获得了比 back translation system 1.1个点的提升。</li><li>消融实验：使用BART framework，采用不同的预训练方案，来确定影响了最终任务性能的关键因素是什么。</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>现存方法的一个问题：However, these methods typically focus on particular types of end tasks (e.g. span prediction, generation, etc.), limiting their applicability.<br>yaya: 现存的方法可以只限定对某一类任务有效，如只对 comprehensive tasks有效，而对 generation tasks无法使用。</li><li><p>在本文中，提出了一个可以结合 <strong style="color:red;">bidirectional</strong> and <strong style="color:blue;">auto-regressive</strong> Transformers. BART是使用<strong style="color:blue;">序列到序列</strong>模型构建的<strong style="color:red;">去噪自动编码器</strong>，BART可以适用于非常广泛的最终任务。</p></li><li><p>本文提出框架的优点：噪声的灵活性，可以将任意转换应用于原始文本，包括更改其长度。</p></li><li><p>本文对许多 noising approaches 进行评估，发现最好的方案是 shuffling the order of the<br>original sentences。本文采用了一个 novel in-filling scheme，使用spans of text 来代替 a single mask token。<br>本文提出的方法通过强制模型对更多的总体句子长度进行推理，并对输入进行更远距离的转换，从而泛化了BERT中的两个预训练任务（word masking and next sentence prediction）</p></li><li><p>BART还开辟了关于微调的新思路。本文提出了一种新的机器翻译方案，其中BART模型堆叠在其他几个transformer layers 之上。这些层经过培训，从而将外语从本质上翻译为噪声英语，可以通过BART进行传播，从而将BART用作预训练的目标方语言模型。</p></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p><strong>BART</strong> is a denoising autoencoder that maps a corrupted document to the original document it was derived from. </p><p>It is implemented as <strong>a sequence-to-sequence model</strong> with <strong>a bidirectional encoder</strong> over corrupted text and <strong>a left-to-right autoregressive decoder</strong>. For pre-training, we optimize the negative log likelihood of the original document.</p><p><img src="https://i.loli.net/2021/02/19/BCSUOQkGX5KcJTR.png" alt="image-20210219151202969" style="zoom:50%;"></p><h2 id="Pre-training-BART"><a href="#Pre-training-BART" class="headerlink" title="Pre-training BART"></a>Pre-training BART</h2><p>Unlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to apply any type of document corruption.</p><p>解释：BERT 对句子中的token进行随机掩码，采用的特定类型的 noising schemes，而 BART 可以应用任意类型的corruption。<strong>极端情况下，当原句中的所有信息都丢失时，BART可以看做是一个 language model。</strong></p><p>本文采用了 几个先前提出的变换，和几个自己新提出的变换。</p><p><img src="https://i.loli.net/2021/02/19/LxrawUIdKp8TyFX.png" alt="image-20210219152433064"></p><p>（1）Token Masking.</p><p>（2）Token Deletion. 模型必须去决定哪个在位置丢失了。</p><p>（3）Text Infilling. 采样了多个 text spans, spans lengths 从一个泊松分布（$\lambda$ = 3）中随机采样。每个span被替换成一个[MASK]。<strong>Text infilling teaches the model to predict how many tokens are missing from a span.</strong> yaya:仅仅是预测[MASK 位置处有几个tokens，而不需要预测具体点的tokens是什么]</p><p>（4）Sentence Permutation. 根据停止符，将一个document分成多个句子，然后打乱句子的顺序，</p><p>（5）Document Rotation. A token is chosen uniformly at random, and the document is rotated so that it begins with that token. This task trains the model to identify the start of the document.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and-Comprehension&quot;&gt;&lt;a href=&quot;#BART-Denoi
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Evaluating Models&#39; Local Decision Boundaries via Contrast Sets</title>
    <link href="http://yoursite.com/2021/02/19/Evaluating-Models-Local-Decision-Boundaries-via-Contrast-Sets/"/>
    <id>http://yoursite.com/2021/02/19/Evaluating-Models-Local-Decision-Boundaries-via-Contrast-Sets/</id>
    <published>2021-02-19T09:56:22.000Z</published>
    <updated>2021-02-21T07:53:56.122Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文标题：</strong>Evaluating Models’ Local Decision Boundaries via Contrast Sets</p><p><strong>论文链接：</strong><a href="https://arxiv.org/abs/2004.02709" target="_blank" rel="noopener">https://arxiv.org/abs/2004.02709</a></p><p><strong>数据集：</strong><a href="https://allennlp.org/contrast-sets" target="_blank" rel="noopener">https://allennlp.org/contrast-sets</a></p><p><strong>Main Contribution：</strong>训练集与测试集 i.i.d 的假设使得模型很难泛化，文章提出了在原始测试集构建 contrast test set 的方法，可以真实的评估模型的语言能力。</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a><strong>Motivation</strong></h2><p>这里用了一个 toy example 和一个真实示例来表示作者想要提出并解决的问题。</p><p><strong>Toy Example</strong></p><p>考虑二维的情况，下图中的两分类问题需要一个非常复杂的 decision boundary。</p><p><img src="https://i.loli.net/2021/02/19/sfK5PyknNElIVoJ.png" alt="image-20210219175052164" style="zoom:33%;"></p><p>但是在很多情况下，由于采样的 biased，我们很可能得到如下的数据集：</p><p><img src="https://i.loli.net/2021/02/19/PBUDYm6LK7S3IN5.png" alt="image-20210219175104378" style="zoom:33%;"></p><p>网络通过一个很简单的 decision boundary 就可以将它们分类，而由于训练测试数据集独立同分布，虽然这个 decision boundary 非常差，但它在测试集表现得非常好。理想情况下，如果我们完整采样整个数据集，所有问题都迎刃而解，但这显然是很难做到的。为了正确的测试模型的能力，作者提出了对测试集做 perturbation 的方法：对测试集的每一个实例，我们生成一系列与之类似的测试样本（Contrast Set：下图中的灰色圆圈）。</p><p><img src="https://i.loli.net/2021/02/19/CdF9GOxAJe2aEv3.png" alt="image-20210219175121207" style="zoom:33%;"></p><p><strong>Complex NLP Task</strong></p><p>我们很难用图把 NLP task 中存在的问题进行描述，但是有很多工作支撑了这一观点。比较有意思的示例为在 SNLI 数据集中，表明单词”睡觉”，”电视”和”猫”几乎从来没有同时出现数据中，但是它们经常出现在 contradiction 的例子中。所以 model 很容易的学到“同时出现’睡觉’和’猫’的句子都是 contradiction sentence，并且这一分类标准工作得很好”。 </p><p>在初始数据收集过程中完全消除这些差距将是非常理想化的，在一个非常高维的空间中，语言有太多的可变性。相反，该文使用 Contrast Set 来填补测试数据中的空白，从而给出比原始数据提供的更全面的评估。</p><h2 id="Contrast-sets"><a href="#Contrast-sets" class="headerlink" title="Contrast sets"></a><strong>Contrast sets</strong></h2><p>假设我们现在为测试样本 构建 Contrast Set，有两个要点 (i) 构建样本距离与  小于某个阈值。(ii) Label 与 <strong>不一致</strong>。下图是在 NLVR2 数据集上的一些实例，在这里，句子和图像都通过一些很简单的方式进行修改（例如，通过改变句子中的一个词或找到一个相似但有区别的词），从而使输出标签发生变化。</p><p><img src="https://i.loli.net/2021/02/19/tlN7BZIVnDCsYed.png" alt="image-20210219175240236" style="zoom:50%;"></p><p>我们需要注意，contrast set 和 adversarial examples 是不一样的，对抗样本的目的是对句子/图像做 perturbation，但是保持原标签不变。 </p><p>不过文章中如何计算样本距离，阈值的确定，label 是否发生变化，都是由 expert 给出的。</p><h2 id="How-to-Create-Contrast-Sets"><a href="#How-to-Create-Contrast-Sets" class="headerlink" title="How to Create Contrast Sets"></a><strong>How to Create Contrast Sets</strong></h2><p>作者用了三个数据集来展示 Contrast Sets 的构造过程。</p><p><strong>DROP</strong></p><p>DROP 是一个阅读理解数据集，旨在涵盖对段落中的数字进行组合推理，包括过滤、排序和计数，以及进行数值运算。数据主要来自 (i) Wikipedia (ii) 美国足球联赛的描述。(iii) 人口普查结果说明。(iv) 战争摘要。作者发现数据集中存在明显的 bias，比如一旦问题是”How many…”，结果很多情况都是 2。关于事件顺序的问题通常遵循段落的线性顺序，而且大部分问题不需要理解。 </p><p>作者从三个方面改进这个数据集：</p><ul><li>关于足球联赛的问题往往需要推理和比较（比如询问两场比赛得分的差值），但是其他类型的数据很少需要推理比较，因此作者为他们提供额外的需要推理比较的问题；</li><li>将问题的部分语义颠倒，类似于 shortest 变为 longest, later 变为 earlier, How many countries 变为 which countries 等等；</li><li>改变事件发生的顺序，使得与事件顺序相关的问题推理难度增加。</li></ul><p><strong>NLVR2</strong></p><p>给模型一对图像与一个句子，判断这句话正确与否。这个数据集的特点在于 compositional reasoning，我们需要模型理解图像中的物体的属性，物体与物体的关系，物体与场景的关系。 </p><p>我们通过修改句子或用网络搜索中获得自由许可的图片替换其中一张图片来构建 NLVR2 的 Contrast Set。 </p><p>比如将句子”The leftimage contains twice the number of dogs as theright image”改为“The left image containsthree timesthe number of dogs as the right image”。或者对一个图像对，将原本 4 条狗的图像换成其他数目。也可以对一些量词比如”at least one”改为”exactly one”，或者实体”dogs”改为”cats”，或者属性”yellow”改为“red”。</p><p><strong>UD  Parsing</strong> </p><p>这是一个 dependency parsing 的数据集。作者想要通过这个数据集证明 Contrast set 不仅在 high-level 的 NLP 任务中有效，也在语义分析的任务中有效。具体方法可以查看原文。</p><p><img src="https://i.loli.net/2021/02/19/CzWXEFJs63dDHAq.png" alt="image-20210219175335245"></p><p>可以看到，再加上 Contrast Set 之后，SOTA models 的性能都有了显著的下降。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;论文标题：&lt;/strong&gt;Evaluating Models’ Local Decision Boundaries via Contrast Sets&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文链接：&lt;/strong&gt;&lt;a href=&quot;https://arxiv.
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP,evaluation" scheme="http://yoursite.com/tags/NLP-evaluation/"/>
    
  </entry>
  
  <entry>
    <title>On calibration of modern neural networks</title>
    <link href="http://yoursite.com/2021/01/07/On-calibration-of-modern-neural-networks/"/>
    <id>http://yoursite.com/2021/01/07/On-calibration-of-modern-neural-networks/</id>
    <published>2021-01-07T04:00:37.000Z</published>
    <updated>2021-01-07T04:22:12.667Z</updated>
    
    <content type="html"><![CDATA[<p>On Calibration of Modern Neural Networks</p><h2 id="Calibration-一个工业价值极大，学术界却鲜有研究的问题！"><a href="#Calibration-一个工业价值极大，学术界却鲜有研究的问题！" class="headerlink" title="Calibration: 一个工业价值极大，学术界却鲜有研究的问题！"></a>Calibration: 一个工业价值极大，学术界却鲜有研究的问题！</h2><p>原创 kid丶 <a href="javascript:void(0" target="_blank" rel="noopener">夕小瑶的卖萌屋</a>;) </p><blockquote><blockquote><blockquote><blockquote><p>文 | kid丶(知乎作者)<br>编 | 夕小瑶</p></blockquote></blockquote></blockquote></blockquote><p>尽管深度学习给工业界带来了一波上线春天，但是总有很多比较难的业务，模型反复迭代后准确率依然达不到预期的产品标准，难以满足用户期望。</p><p>以下为工业界常见讨（si）论（b）场景：</p><p>R&amp;D小哥哥一顿调参输出，RoBERTa都用上了，终于将模型从80%准确率提升到了90%，但是PM小姐姐说，“不行！咱们必须要达到95%准确率才能上线！否则就是对用户和产品逼格的伤害！”</p><p>怎么办呢？</p><p>熟悉工业界上线套路的小伙伴马上就能给出答案，那就是 <strong><em>提高模型决策的阈值！</em></strong> PM小姐姐只是根据产品标准定义了模型准确率（或者说精确率，precision），但是并不在乎召回率有多高（毕竟模型只要没上线，就相当于召回率为0）。</p><p>那么基于上面的思路：假如模型的softmax输出可靠，比如二分类场景，模型softmax之后1类的输出是0.92，能表征模型有92%的把握说这是个正例，并且模型的这个把握是精准的，那么PM小姐姐说要达到95%准确率，那我们就疯狂提高模型的决策阈值就好了，这样把那些不确定性高的样本砍掉了，模型准确率自然就上来了。</p><p>然而，神经网络并不一定这么靠谱，你看模型的测试集输出的话，却常常发现模型要么以99.999的概率输出来判定正例，要么0.0001的概率输出来判定负例，基本没有样本落在0.1~0.9区间内。那么这时候上面的思路就失效了。</p><p>那么有没有办法<strong>让模型的softmax输出能真实的反映决策的置信度呢？</strong> 这个问题，就被称为Calibration问题（直译是叫“校准”）。</p><p>故事要从一篇发表于2017年的ICML顶会论文开始，目前这篇论文引用量1001。</p><p><strong>论文标题：</strong></p><p>On Calibration of Modern Neural Networks</p><p><strong>链接：</strong></p><p><a href="https://arxiv.org/pdf/1706.04599.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.04599.pdf</a></p><p>Arxiv访问慢的小伙伴可以在【<strong>夕小瑶的卖萌屋</strong>】后台回复关键词【<strong><em>0106</em></strong>】下载论文pdf~</p><h2 id="神经网络的-overconfidence"><a href="#神经网络的-overconfidence" class="headerlink" title="神经网络的 overconfidence"></a>神经网络的 overconfidence</h2><p><img src="https://i.loli.net/2021/01/07/cDzC2UkMQGL3y8I.png" alt="image-20210107121456693" style="zoom: 50%;"></p><p>首先，让咱们来思考一个普通图像分类任务。对于一张“koala”的图像，在经过神经网络后会得到 logits 输出 ，经过 softmax 层后得到对各类别的预测的后验概率，接着我们选择概率最大的类别（ koala）输出为最后的预测类别。这里，最终的预测类别 ，其对应的置信度为 。在大多情况下，我们只关心类别的预测 有多准，根本不 care 置信度是怎样的。然而，在一些实际应用场景下，置信度的度量也同样重要。例如：</p><p><img src="https://i.loli.net/2021/01/07/HILjXfdVBk7v1hw.png" alt="image-20210107121544230" style="zoom: 50%;"></p><p>如上图，对于自动驾驶中的目标识别任务，车辆的前方出现了一个人，神经网络会将其识别成塑料袋，此时输出的置信度为50%（低于阈值），则可通过其它传感器进行二次的正确识别（识别为人）。但想想看，若神经网络对塑料袋预测的置信度为90%会怎样？再例如：</p><p><img src="https://i.loli.net/2021/01/07/3CUylrwH4u6csDG.png" alt="image-20210107121606617" style="zoom:50%;"></p><p>使用 Resnet 模型简单的对一些图片任务进行训练，收敛后的模型对测试集的平均置信度高达80%-85%，然而只有将近70%的图片能被正确分对（红色代表分错，绿色代表分对）。这意味着啥？训练好的模型好像有点盲目自信，即出现 <strong>overconfidence</strong> 现象，或者可以称为模型的准确率和置信度不匹配（<strong>miscalibration</strong>）。</p><h2 id="预期校准误差（ECE）"><a href="#预期校准误差（ECE）" class="headerlink" title="预期校准误差（ECE）"></a>预期校准误差（ECE）</h2><p>直观的来看，模型的准确率应当和置信度相匹配。一个完美校准的模型可定义成如下所示：</p><p>即，模型置信度 等于概率 的条件下模型的预测 为真实标记 的概率同样也为 。因此，本文提出一个新的度量方式叫做 <strong>预期校准误差（Expected Calibrated Error, ECE）</strong> 来描述模型学习的匹配程度：很简单，其实就是将前面那个完美校准模型的等式写成差的期望的形式。我们将期望进一步展开可得到：</p><p>其中： 这里的 代表着一个个根据置信度区间划分的一个个桶（用来装样本的），如下图所示：</p><p><img src="https://i.loli.net/2021/01/07/KS6p2nRIhMdtgjv.png" alt="image-20210107121742204" style="zoom:50%;"></p><p>例如，我们将置信区间平均划分成5份，然后将样本按照其置信度挨个送到对应的桶中，分别计算每个桶中的平均置信度和准确率，两者的差值（Gap）的期望就是所定义的 <strong>ECE。</strong></p><p><strong>读到这的读者</strong>应该能逐步体会本文想干一件啥事了。本文首先引出这样一个问题，深度模型在学习过程中出现准确率和置信度的严重不匹配问题，接着提出了一个合理的评价指标来描述模型学习的匹配程度，所以接下来，它要提出方法来想办法<strong>最小化期望校准误差（ECE）。</strong></p><h2 id="什么原因导致神经网络出现准确率与置信度不匹配？"><a href="#什么原因导致神经网络出现准确率与置信度不匹配？" class="headerlink" title="什么原因导致神经网络出现准确率与置信度不匹配？"></a>什么原因导致神经网络出现准确率与置信度不匹配？</h2><p>然而 <strong>ECE</strong> 是没办法直接最小化的，因此本文尝试着做一些探索性的实验来观察啥因素会使得模型的 ECE 变大。本文分别从三个方面上去进行实验：</p><p><img src="https://i.loli.net/2021/01/07/kniAWZNqaVBwQPG.png" alt="image-20210107121834360"></p><p>▲网络复杂度对ECE的影响</p><p><strong>网络复杂度对 ECE 的影响：</strong> 首先，作者使用两个模型（LeNet和ResNet）分别对CIFAR-100数据集进行了训练，准确率分别为55.1%和69.4%，ResNet 在预测性能上完爆LeNet。然而，ResNet 置信度（右图蓝色+红色部分）的分布和准确率（右图蓝色部分）出现了严重的不匹配，导致二者的 Gap （红色部分）非常大。**注意完美校准模型的分布应当是蓝色部分刚好和对角线重合，且没有红色 Gap 部分。</p><p><img src="https://i.loli.net/2021/01/07/OQX8Ir64DqyBwdl.png" alt="image-20210107114831568" style="zoom: 33%;"></p><p>▲网络的宽度和深度对ECE的影响</p><p><strong>网络宽度和深度对 ECE 的影响：</strong> 在得知模型复杂度会影响模型的 ECE 后，作者紧接着做了网络宽度和深度对模型 ECE 和错误率（Error）的影响。可以看到，在控制变量前提下，单方面的增加网络的深度和宽度均会使得模型的 Error 降低，这是我们所期望的；然而，ECE也会同样的随着上升。<strong>换句话来说，一昧的增加模型复杂度能有效的提高模型的预测性能，但同样带来的问题是模型的 overconfidence 问题愈发严重。</strong></p><p><img src="https://i.loli.net/2021/01/07/v1S9YrfQBt5sl4O.png" alt="image-20210107114951774" style="zoom:33%;"></p><p>▲归一化和权重衰减对ECE的影响</p><p><strong>normalization 和 weight decay 对 ECE 的影响：</strong> 接着的实验也是我们为提高模型性能经常使用的 batch normalization 和 loss regularization。<strong>左图：</strong> 使用 batch normalization 会有效的提升模型的性能，但同时也会提升模型的 ECE。<strong>右图：</strong> weight decay 通常用来调节 L2 正则的权重衰减系数，随着其系数的增加相当于更多的强调模型参数 w 要尽可能的小，能有效的防止模型过拟合。<strong>该现象表明，模型越不过拟合，其ECE是越小的，也就是说模型越不会 overconfidence ；换句话说，模型对样本的拟合程度和对样本的置信度是息息相关的，拟合得越好，置信度越高，所以 ECE 越大。（个人理解，欢迎评论区指正~）</strong></p><h2 id="我们该如何对模型进行校准呢？"><a href="#我们该如何对模型进行校准呢？" class="headerlink" title="我们该如何对模型进行校准呢？"></a>我们该如何对模型进行校准呢？</h2><p><img src="https://i.loli.net/2021/01/07/8WvRUEQtXg6iseY.png" alt="image-20210107115025920" style="zoom:33%;"></p><p>作者接下来又做了一个很有意思的实验，在CIFAR-100上训练模型500个 epoch，其中在第250个 epoch 和第375个 epoch 下调节学习率，观察测试集上的 test error 和 test NLL 的变化情况。Test NLL 的定义如图中所示，它其实等价于测试集上的交叉熵。这个实验啥意思呢？我调节了一下学习率后，测试性能得到了提升，但是测试集上的交叉熵却出现了过拟合现象（出现了反常的上升现象）。<strong>有意思的点来了！</strong> 有人肯定会 argue 不是说好本文研究的是overconfidence嘛？即模型的置信度太高而准确率过低，这里对 NLL overfitting 岂不是好事，因为负对数似然上升了等价于模型的置信度的降低了。<strong>注意：这里的</strong> <strong>是对正确类上的置信度，而前面的实验是对预测类的置信度</strong> <strong>！其实认真想想，是一个意思，前面之所以 confident 很高的样本准确率很低，正是因为其在正确类别上的置信度太低导致的！！（这部分卡了很久）</strong></p><p>该结果可以表明，模型置信度和准确率的不匹配很大可能的原因来自于模型对 NLL 的过拟合导致的。所以，咋办呢？最小化 NLL 呗。</p><p><img src="https://i.loli.net/2021/01/07/gzr4juPYwMFyiSH.png" alt="image-20210107121901763"></p><p>此时，本文提出在验证集上对带 temperature 参数的 softmax 函数进行校准。即我们训练完模型后，最小化 NLL 来学习 temperature 参数，注意到对该项的优化并不会影响模型预测的准确率，只会对模型的 confidence 进行校准。最终的结果是这样的，详细可参考论文。</p><p><img src="https://i.loli.net/2021/01/07/dDNYosvc2tprZaE.png" alt="image-20210107115122188"></p><h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a><strong>讨论</strong></h2><p>上述得实验结果我觉得对很多研究领域都是很有启发意义的。</p><ol><li>模型的置信度应当是和准确率匹配的，这样的模型我觉得才是有意义的，否则以很高置信度进行很离谱的预测错误的模型会让人感觉这个模型好像什么都会、又好像什么都不会。</li><li>ECE 的指标是否能反应样本的一些性质，例如难易程度、是否为噪声等。</li><li>该文章是间接的去优化ECE的，能否有直接优化的形式，或者主动学习里面能否考虑这一点来挑选样本？</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;On Calibration of Modern Neural Networks&lt;/p&gt;
&lt;h2 id=&quot;Calibration-一个工业价值极大，学术界却鲜有研究的问题！&quot;&gt;&lt;a href=&quot;#Calibration-一个工业价值极大，学术界却鲜有研究的问题！&quot; clas
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>End-to-End Learning of Visual Representations from Uncurated Instructional Videos</title>
    <link href="http://yoursite.com/2020/12/30/End-to-End-Learning-of-Visual-Representations-from-Uncurated-Instructional-Videos/"/>
    <id>http://yoursite.com/2020/12/30/End-to-End-Learning-of-Visual-Representations-from-Uncurated-Instructional-Videos/</id>
    <published>2020-12-30T09:16:44.000Z</published>
    <updated>2020-12-30T09:21:01.480Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/12/30/yfHDiZusbKvM2AY.png" alt="image-20201230171856275" style="zoom:67%;"></p><p><img src="https://i.loli.net/2020/12/30/C1HmZXKLzFlBDGM.png" alt="image-20201230171925959" style="zoom:67%;"></p><p><img src="https://i.loli.net/2020/12/30/3AudkSImHtnrwha.png" alt="image-20201230172037294"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2020/12/30/yfHDiZusbKvM2AY.png&quot; alt=&quot;image-20201230171856275&quot; style=&quot;zoom:67%;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.
      
    
    </summary>
    
      <category term="Video Pre-trained Models" scheme="http://yoursite.com/categories/Video-Pre-trained-Models/"/>
    
    
      <category term="Video Pre-trained Models" scheme="http://yoursite.com/tags/Video-Pre-trained-Models/"/>
    
  </entry>
  
  <entry>
    <title>Blank Language Model</title>
    <link href="http://yoursite.com/2020/12/30/Blank-Language-Model/"/>
    <id>http://yoursite.com/2020/12/30/Blank-Language-Model/</id>
    <published>2020-12-30T03:48:12.000Z</published>
    <updated>2020-12-30T05:23:27.601Z</updated>
    
    <content type="html"><![CDATA[<h3 id="yaya-序"><a href="#yaya-序" class="headerlink" title="yaya 序"></a>yaya 序</h3><h4 id="直接做文本填充任务的模型"><a href="#直接做文本填充任务的模型" class="headerlink" title="直接做文本填充任务的模型"></a>直接做文本填充任务的模型</h4><ul><li><p>目前已知的做文本填充的模型有ilm, BLM</p><blockquote><p><strong>ilm</strong>: <strong>(ACL 2020) Enabling Language Models to Fill in the Blanks</strong></p><p><strong>BLM</strong>:  <strong>(EMNLP 2020) Blank Language Models</strong></p><p>Text Infilling</p><p>Langsmith An Interactive Academic Text Revision System</p><p>MaskGAN Better Text Generation via Filling in the__</p><p> (ACL 2020) INSET Sentence Infilling with INter-SEntential Transformer</p></blockquote></li></ul><h4 id="直接可以做文本填充任务的模型"><a href="#直接可以做文本填充任务的模型" class="headerlink" title="直接可以做文本填充任务的模型"></a>直接可以做文本填充任务的模型</h4><ul><li><p>其次，做文本任务 对抗的一些任务，其实也可以看做文本填充</p><ul><li>AdvExpander Generating Natural Language Adversarial Examples by Expanding Text</li><li>BERT-ATTACK Adversarial Attack Against BERT Using BERT</li><li>Generate Your Counterfactuals Towards Controlled Counterfactual Generation for Text</li></ul></li><li><p>另外 padlepadle/ ERNIE 的预训练任务，不使用 sub-word 来随机掩码，而是使用 fragment来做随机掩码，其预训练模型，也是可以用来做文本填充的。但是可能性能没有那么可靠</p></li></ul><h3 id="BLM介绍"><a href="#BLM介绍" class="headerlink" title="BLM介绍"></a>BLM介绍</h3><p>来源： <a href="https://mp.weixin.qq.com/s/cVUT4FMpgqARuWf5dWY0bA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/cVUT4FMpgqARuWf5dWY0bA</a></p><p>讲者提出了填空语言模型（Blank Language Model, BLM），<strong>该模型通过动态创建和填充空白来生成序列</strong>。空白用于控制要扩展序列的那一部分，使BLM成为各种文本编辑任务的理想选择。<strong>该模型可以从单个空白或在指定位置带有空白的部分完成文本开始，迭代地确定要在空白中放入哪个单词以及是否插入新的空白，并在没有新的空白填充时停止生成。</strong>使用边缘似然的下界可以有效地训练BLM。在填充缺失文本的任务上，BLM在准确性和流利性方面均明显优于所有其他方法。在情感迁移和修复古文字的实验中，证明该框架具有广泛的应用潜力。</p><h4 id="一、动机：传统语言模型的局限性"><a href="#一、动机：传统语言模型的局限性" class="headerlink" title="一、动机：传统语言模型的局限性"></a><strong>一、动机：传统语言模型的局限性</strong></h4><p>传统的语言模型通常是从左到右对文本序列进行建模，其模式是，首先生成第一个词，然后以第一个词作为condition来生成第二个词，继而以第二个词为condition生成第三个词，如此迭代计算后一个词。</p><p>该方法的优势在于算法简单、有效。但大多情况下，并不需要从头开始生成文本，而是已有部分文本，想自动生成剩下的部分内容。比如，文本编辑，是基于已有的draft，修改文本中任意位置的内容；或是template filling，针对一些表格等具有固定格式的文件，比如医疗或者法律文件等进行填空；或是text restoration，比如一些文件可能在任意位置缺失相关内容，需要做的是复原损失部分。由于Left-to-Right Language Model仅仅考虑blanks左边的文本语境，不能很好地对这些应用进行建模。</p><p><img src="https://i.loli.net/2020/12/30/b4piu9OP6x3VINC.png" alt="image-20201230120139264" style="zoom:50%;"></p><p><strong>因此，讲者提出了Blank Language Model（BLM）</strong>，其输入形式为任意文本，blanks可存在于文本的任意位置；每一个blank可以对应任意多个单词；且BLM模型会结合上下文语境决定需要填充几个单词。BLM与Mask Language Model的区别在于，Mask Language Model的一个mask只能对应一个单词，因此如果预先不知道blank中间丢失多少个单词，就不能使用Mask Language Model来实现。</p><p><img src="https://i.loli.net/2020/12/30/uN759vob1MEhyRO.png" alt="image-20201230120251358" style="zoom:50%;"></p><h4 id="二、实现：什么是BLM"><a href="#二、实现：什么是BLM" class="headerlink" title="二、实现：什么是BLM?"></a>二、实现：什么是BLM?</h4><h4 id="1）BLM工作原理介绍"><a href="#1）BLM工作原理介绍" class="headerlink" title="1）BLM工作原理介绍"></a><strong>1）BLM工作原理介绍</strong></h4><p><strong>BLM具体是如何实现填空的呢？</strong>首先存在一个画布，有单词和blank，其中blank用来控制单词能被放置的位置；然后BLM模型在这个画布上动态进行修改，每一步都会选择一个待填充的blank。</p><p>由于每个blank可以对应任意数量的词，因此当填充这个word w之后，还要决定是不是只填一个字，或者在w左边、右边、还是两边各补上blank。由此每个blank就可以继续迭代，且被扩展成任意多个词，当没有blank留下，模型达到终止条件。其实现步骤如图3所示。</p><p><img src="https://i.loli.net/2020/12/30/41uMQY2dqSroxZD.png" alt="image-20201230120357144" style="zoom:50%;"></p><p>yaya 觉得以下这个例子更好一些</p><p><img src="https://i.loli.net/2020/12/30/1N9C5R4ecJM6ADB.png" alt="image-20201230120728774" style="zoom: 50%;"></p><p><strong>讲者举如下例子，进一步说明模型的实现步骤。</strong>原句是They also have <strong><strong> which </strong></strong>.通过从canvas中一步步选择合适的word和blank，直至句子中没有新的blank生成为止，实现句子填空。</p><p><strong>继而，讲者概括BLM的工作方式</strong>，类似于一个Grammar，如图4所示。其与传统语法<strong>Context-Free Grammar</strong>的区别在于：讲者所提出的模型在<strong>Production rules</strong>上面的概率分布是取决于模型参数和当前的<strong>context</strong>。</p><p><img src="https://i.loli.net/2020/12/30/ucYZU5Ld7T9tysv.png" alt="image-20201230120451316" style="zoom: 50%;"></p><h4 id="2）BLM框架介绍"><a href="#2）BLM框架介绍" class="headerlink" title="2）BLM框架介绍"></a>2）BLM框架介绍</h4><p>首先采用transformer模块，将input通过这个transformer以得到一系列的representation，每个representation包含有其context信息；</p><p>然后将所有blank位置的representation通过Linear project和softmax得到blanks上的distribution，选择其中一个（假设是第二个）；</p><p>进一步将被选中的blank representation投影到整个vocabulary，由此可预测一个词，比方说预测really；</p><p>最后，将blank representation和被预测词的word embedding进行拼接，都输入给一个MLP进行如图所示的四种情况分类。假设是第四种情况，得到<strong><strong> really </strong></strong>，然后将其fill到第二个Blank中，并继续这个过程直至没有新的blank生成。</p><p><img src="https://i.loli.net/2020/12/30/9jZpWxPyeuR3Cdn.png" alt="image-20201230121033747" style="zoom: 50%;"></p><p>从一个初始的blank到一个没有blank的complete text这个过程称为<strong>trajectory</strong>，包括每一步的canvas和action；每一步的action包括了choose a blank，predict a word 以及create new blanks。</p><p>如果一个句子有n个单词，那么会有n！种不同的trajectory都可以生成它，每一种trajectory对应着不同的word insertion order。如图6所示的trajectory就对应着word insertion order “3 1 10 6 2 8 4 7 5 9” 。</p><p>因此，生成一个句子x的概率就是以所有trajectory/word insertion order生成它的概率之和，公式中的Sn为所有的n排列。而给定order，就可以确定每一步的canvas和action，概率可相应分解为每一步的概率的乘积。</p><p><img src="https://i.loli.net/2020/12/30/k4ZMPjR5TmFycXA.png" alt="image-20201230130236667" style="zoom:50%;"></p><p><strong>如何高效的训练BLM呢？</strong>讲者对上述的likelihood进行估计，对等式两边取log，并借助图7中蓝色不等式可得到最终评估的loss。但同时讲者介绍到该训练方式与left-to-right language model相比并不高效，原因在于：Left-to-Right Language Model在一个pass中是同时计算了n个词的Loss，而上述公式在一个pass中只计算了一个action loss。</p><p>基于此，讲者进一步介绍到ctx,o只与word insertion order o的前t个词有关，因此可在一个pass中，将前t个order相同且在t+1时不同的trajectory进行组合，共同计算loss。同时由于EOt+2:n不会影响第t步的action和canvas，则可改写最终的loss表达，实现在一个pass中计算期望n/2个action loss。</p><p><img src="https://i.loli.net/2020/12/30/2xnYKAswB6RO47o.jpg" alt="图片"></p><p>综上所述，BLM的training的规则如图8所示。讲者通过一个实例，详细介绍了模型的训练过程，给定sentence x，先sample t，再sample order o1:t，由此可构建动态画布ctx,o，并按照图7的loss进行训练。</p><p><img src="https://i.loli.net/2020/12/30/lUAgSP1uOX9r7sH.png" alt="image-20201230130440892" style="zoom:50%;"></p><p>训练好BLM后，可采用greedy decoding或beam search来填充输入文本中的空白。需要注意的是，greedy decoding/beam search寻找的不是具有最大边缘似然p(x;θ)的sentence，而是具有最大联合似然p(x;o;θ)的sentence 和trajectory。</p><h4 id="三、实验"><a href="#三、实验" class="headerlink" title="　三、实验"></a>　三、实验</h4><p>讲者主要进行了4种不同的实验验证模型的有效性，分别为<strong>Text infilling，Ancient text restoration，Sentiment transfer以及Language modeling。</strong>下述针对<strong>Text infilling</strong>的实验进行阐述。</p><h5 id="1）Text-infilling"><a href="#1）Text-infilling" class="headerlink" title="1）Text infilling"></a>1）Text infilling</h5><p>选择Yahoo Answers（100k的文档，且最大长度的为200个单词）作为实验数据集；在该数据集上随机mask掉比例为r的tokens，将连续的masked tokens用blank代替，由此得到的数据集作为测试数据集。在评价指标方面，分别选择Accuracy和Fluency来评价模型的有效性。具体是指通过计算初始文档与模型filling后文档间的BLEU score表征Accuracy；采用经过预训练的Left-to-Right Language Model计算perplexity表征Fluency。</p><p><strong>讲者分别选择以下模型作为在Baseline models:</strong></p><p><strong>a.BERT+LM</strong></p><p>指采用BERT模型得到每一个blank的representation，并将其输入给Left-to-Right Language Model生成不同blanks对应的tokens。</p><p><strong>b.Masked Language Model with oracle length (MLM)</strong></p><p>由于模型需要知道每一个blank对应单词个数，因此给定oracle length数量的masked tokens来代替blanks；此外MLM模型同时预测masked tokens时是相互独立的，并没有考虑filling的部分之间的consistency，因此使用most-confident-first heuristic来autoregressively一个一个进行填写。</p><p><strong>c.Insertion Transformer (Stern et al., 2019)</strong></p><p>该模型支持动态单词插入，与BLM不同，该模型可在所有位置插入，因此强制模型只在指定位置进行插入，避免出现较高的失败率。</p><p><strong>d.Seq2seq-full/fill (Donahue et al., 2020)</strong></p><p>指直接输出infill之后的句子序列，或者直接输出代替blanks的tokens并用分隔符“|”划分。这两种方法具有较高的失败率。</p><p><img src="https://i.loli.net/2020/12/30/2LU8x5dlnCcOJmH.png" alt="image-20201230131800545" style="zoom:50%;"></p><p>图a)</p><p><img src="https://i.loli.net/2020/12/30/aZrSFqPjty67nXu.png" alt="image-20201230131732735" style="zoom:50%;"></p><p>图 b)</p><p>图9 Text infilling 实验结果</p><p>实验结果如图9所示，其中图a)说明当mask ratio 越来越高时，所有模型的BLUE score都会降低，同时BLM表现出最高的BLUE score；图b)中BLM与InsT模型的perplexity要低于original data的perplexity，且当mask ratio 越来越高时，模型最终的output比原始data要更加typical，则perplexity会越来越低。</p><h4 id="四、结论与展望"><a href="#四、结论与展望" class="headerlink" title="四、结论与展望"></a>四、结论与展望</h4><p>讲者提出了BLM可以用来灵活地生成文本，其生成方式是动态创建以及填充blanks。实验表明BLM模型在Text infilling，Ancient text restoration和Sentiment transfer上具有很好的表现。</p><p>BLM也具有广阔的应用前景，比如Template filling，information fusion以及assisting human writing；同时BLM也可被扩展为conditional model，可以用来edit和refine机器翻译，或者在dialogue中根据给定内容生成一句话；最后可以探究BLM跟MLM/BERT在representation的学习上的差异性。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/8ld6oicYkZZtRCNqJf5KRcmmFmYpBTS6hibiad1Gyib2x0Lt4qGWjuMCERiaGibUT5ibK0ezrHch0nMNJqDnibRgELdMRQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><p>图12 结论与展望</p><h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a><strong>参考文献：</strong></h3><p>Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. 2019. Insertion transformer: Flexible sequence generation via insertion operations. arXiv preprint arXiv:1902.03249.</p><p>Chris Donahue, Mina Lee, and Percy Liang. 2020. Enabling language models to fill in the blanks. arXiv preprint arXiv:2005.05339.</p><h3 id="论文地址："><a href="#论文地址：" class="headerlink" title="论文地址："></a><strong>论文地址：</strong></h3><p><a href="https://arxiv.org/pdf/2002.03079.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2002.03079.pdf</a></p><h3 id="Github地址："><a href="#Github地址：" class="headerlink" title="Github地址："></a><strong>Github地址：</strong></h3><p><a href="https://github.com/Varal7/blank_language_model" target="_blank" rel="noopener">https://github.com/Varal7/blank_language_model</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;yaya-序&quot;&gt;&lt;a href=&quot;#yaya-序&quot; class=&quot;headerlink&quot; title=&quot;yaya 序&quot;&gt;&lt;/a&gt;yaya 序&lt;/h3&gt;&lt;h4 id=&quot;直接做文本填充任务的模型&quot;&gt;&lt;a href=&quot;#直接做文本填充任务的模型&quot; class=&quot;head
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="Text-Filling" scheme="http://yoursite.com/categories/NLP/Text-Filling/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>图像描述任务的实际应用</title>
    <link href="http://yoursite.com/2020/12/24/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/"/>
    <id>http://yoursite.com/2020/12/24/图像描述任务的实际应用/</id>
    <published>2020-12-24T04:07:45.000Z</published>
    <updated>2020-12-24T04:09:01.208Z</updated>
    
    <content type="html"><![CDATA[<h3 id="CaptionBot"><a href="#CaptionBot" class="headerlink" title="CaptionBot"></a>CaptionBot</h3><p>from: <a href="https://www.theguardian.com/technology/2016/apr/14/captionbot-microsoft-latest-ai-experiment-it-isnt-racist" target="_blank" rel="noopener">https://www.theguardian.com/technology/2016/apr/14/captionbot-microsoft-latest-ai-experiment-it-isnt-racist</a></p><p>简介：用户上传一张图片到CaptionBot Service，该系统针对该图片自动的生成一个caption。用户可以评分生成的caption 是否准确。</p><blockquote><p>The idea is that you upload a photo to the service, and it tries to automatically generate a caption that describes what the algorithm sees. You are then able to rate how accurately it has detected what was on display. It learns from the rating, and in theory, the captions get better.</p></blockquote><p>The bot, from <a href="http://go.theguardian.com/?id=114047X1572903&amp;url=https%3A%2F%2Fwww.microsoft.com%2Fcognitive-services&amp;sref=https://www.theguardian.com/technology/2016/apr/14/captionbot-microsoft-latest-ai-experiment-it-isnt-racist" target="_blank" rel="noopener">Microsoft’s Cognitive Services team</a>, is the result of <a href="http://go.theguardian.com/?id=114047X1572903&amp;url=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F264408%2FImageCaptionInWild.pdf&amp;sref=https://www.theguardian.com/technology/2016/apr/14/captionbot-microsoft-latest-ai-experiment-it-isnt-racist" target="_blank" rel="noopener">some hefty research</a> into how to model objects in photographs so that a computer can understand them. They claim that their system can recognise “a broad range of visual concepts” and also performs entity extraction so that it can recognise celebrities. </p><p><img src="https://i.loli.net/2020/12/24/kJ31yRYX5dKH64g.jpg" alt="Taylor Swift and Kanye West both identified in Microsoft’s CaptionBot app"></p><h3 id="Seeing-AI-项目"><a href="#Seeing-AI-项目" class="headerlink" title="Seeing AI 项目"></a><strong>Seeing AI</strong> 项目</h3><p>微软研究院的研究员们不仅在寻找识别图像的方法，还在为图像进行描述。这项研究结合了图像识别技术与自然语言处理技术，能帮助视障人士获得对图像的准确描述，还可能帮助那些需要图像信息却无法直接看到图像的人——比如正在开车的司机。</p><p>Seeing AI项目组中的Margaret Mitchell是一名专攻自然语言处理的研究员，也是图像描述领域顶尖的研究者之一。她说，她和同事们正在寻找方法，让计算机可以用更加人性化的方式来描述图像。例如，计算机可以将一个场景准确地描述为“一群人坐在一起”，但真人可能会将这一场景描述为“一群人坐在一起享受美好时光。”<strong>目前的挑战就是让这项技术懂得一张图像中哪些是对人们最重要、最值得描述的内容。</strong>“<strong>一张图像中有什么，和我们如何谈论一张图像可是完全不同的两回事</strong>，”Mitchell说。</p><p>微软的另一些研究员们正在努力让最新的图像识别工具提供更深入的图片解释。例如，与单纯地将图片描述为“一个男人和一个女人坐在一起”相比，对人们更有帮助的描述可能是：“奥巴马和希拉里·克林顿正在摆pose拍照”。今天人们在网上搜索图片时，绝大多数情况下搜索引擎会根据与图片相关的文字内容，从而得到美国名媛金·卡戴珊或“霉霉”泰勒·斯威夫特的照片，这些搜索结果主要依据文本内容。而微软的资深研究员张磊及郭彦东等研究员正在开发一套借助机器学习识别名人、政治家和公众人物的系统，这套系统会根据图像本身的元素，而非与图像相关的文字内容来进行图像识别。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;CaptionBot&quot;&gt;&lt;a href=&quot;#CaptionBot&quot; class=&quot;headerlink&quot; title=&quot;CaptionBot&quot;&gt;&lt;/a&gt;CaptionBot&lt;/h3&gt;&lt;p&gt;from: &lt;a href=&quot;https://www.theguardian
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>DirectQE Direct Pretraining for Machine Translation Quality Estimation</title>
    <link href="http://yoursite.com/2020/12/24/DirectQE-Direct-Pretraining-for-Machine-Translation-Quality-Estimation/"/>
    <id>http://yoursite.com/2020/12/24/DirectQE-Direct-Pretraining-for-Machine-Translation-Quality-Estimation/</id>
    <published>2020-12-24T04:06:01.000Z</published>
    <updated>2020-12-30T02:21:51.049Z</updated>
    
    <content type="html"><![CDATA[<p>AAAI2021论文：基于数据生成的机器翻译质量评估方法</p><p>来源：<a href="https://mp.weixin.qq.com/s?__biz=Mzg3ODA0NTA2OA==&amp;mid=2247484204&amp;idx=1&amp;sn=1b24bb494110fff68beedde3cb422066&amp;chksm=cf18f1cff86f78d9bdd2d883201af98c4296ec45fe8fae91509421b226ccee923ae7bd158451&amp;mpshare=1&amp;scene=1&amp;srcid=1224CwmzTjgig5X1t5fCbU0T&amp;sharer_sharetime=1608779809453&amp;sharer_shareid=ab44667880fa06ced8bfa560d1d64d36&amp;key=22efb95c8c04cdfbe43c8f83a69b76927ad01c4ab53016600f84450f07570ee6fb6b4747790c342eb1180e349f3fbc3fa8a1ab1219b80a6d725aacd79413c692d47bc3d763cd18ca87eaf3dd823648170186d2f2873bc5c2ef24ff36d3b519a54723506b60b7d4161912fd28659694c1100797e597da6bfe583dad4ef0c80802&amp;ascene=1&amp;uin=MjQwOTk2MzUwOA%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=A8kVBtMx05%2Bw41YTdHRse0I%3D&amp;pass_ticket=5UOCG2UJoSjTMMc7Gh82YYMCZ5iDMMdAZnDIpIDdHbP7%2FsA%2FS9I%2B%2FHaK1%2BTsQBXL&amp;wx_header=0" target="_blank" rel="noopener">南大NLP</a></p><h3 id="研究问题"><a href="#研究问题" class="headerlink" title="研究问题"></a>研究问题</h3><p>目前机器翻译在各个领域得到广泛应用，不同的机器翻译系统，对于同样的原文句子可能会给出不同的翻译结果（如表1所示），质量有好有坏。该如何自动评价译文的翻译质量呢？</p><div class="table-container"><table><thead><tr><th>原文</th><th>黄河之水天上来，奔流到海不复回</th></tr></thead><tbody><tr><td>翻译1</td><td>The water of the Yellow River comes from the sky, rushes to the sea and never returns</td></tr><tr><td>翻译2</td><td>The Yellow River never comes back to the sea</td></tr><tr><td>翻译3</td><td>The Yellow River comes from the sky, runs to the sea and never comes back</td></tr><tr><td>翻译4</td><td>The water of the Yellow River came from the sky and ran to the sea</td></tr><tr><td>翻译5</td><td>The water of the Yellow River comes from the sky, and the waves rush to the East China Sea and never look back</td></tr></tbody></table></div><p>表格 1：机器翻译的不同结果</p><h3 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h3><p>目前的评价指标主要有两类。一类需要依赖参考译文，比如BLEU、TER、Meteor等，主要依赖机器翻译译文和参考译文之间的匹配程度，<strong style="color:red;">但参考译文在现实应用场景下往往难以获取。</strong>第二类评价指标不需要依赖参考译文，如质量评估Quality Estimation (QE)，仅通过原文和机器译文对翻译质量进行估计。</p><p>QE的粒度有很多种，包括词级别、句子级别、短语级别、文档级别等，<strong style="color:red;">本文主要关注词级别与句子级别QE任务。</strong>词级别QE将译文中的每一个词标记为“Ok”或“Bad“；句子级别QE给每一个句子标注一个[0, 1]之间的打分（0表示很好，1表示很差），这些标记都由人工标记得到，或基于人工编辑结果得到。表2展示了一个英-中语向的QE数据示例。</p><div class="table-container"><table><thead><tr><th>原文（英）</th><th>this insubordination earned him a now famous reprimand from the King .</th></tr></thead><tbody><tr><td>机器译文（中）</td><td>这种 <strong style="color:blue;">不 服从命令</strong> 的 态度 使 他 <strong style="color:blue;">赢得 了 国王 现在 著名</strong> 的 <strong style="color:blue;">训斥</strong> .</td></tr><tr><td>词级别标记</td><td>O B B O O O O B B B B B O B B</td></tr><tr><td>句子级别标记</td><td>0.6429</td></tr></tbody></table></div><p>表格 2：QE数据示例（WMT20英-中）</p><p>早期QE任务依赖人工设计的特征，例如原文、译文中的单词数量，词频等，但是该方法的适应性较弱，效果较差。<strong style="color:red;">随后有研究者使用神经网络对QE数据进行端到端建模，取得了一定的成绩。</strong>神经网络需要大规模数据进行训练，但是<strong style="color:red;">QE数据由于需要人工进行标记，暂时规模较小</strong>（万句级别），这限制了神经网络的训练。<strong style="color:red;">目前最流行的QE模型利用知识迁移技术，从无QE标记但具有大规模（百万句级别）的平行语料中迁移QE任务所需要的知识。</strong></p><p>Predictor-Estimator是一种流行的基于知识迁移的QE框架，它是一种两阶段模型（如图一所示）。第一阶段，预测器（Predictor）将在平行语料上进行预训练，其预训练任务一般为“词预测”类型的任务。第二阶段，使用预测器提取QE句对的特征，通过评估器（Estimator）学习如何在这些特征上拟合QE标记。</p><p><img src="https://i.loli.net/2020/12/30/hLHD7IOQgznTZJd.png" alt="image-20201230101547991" style="zoom:33%;"></p><p>我们可以使用神经机器翻译模型NMT（Kim et al. 2017, Fan et al. 2018, Zhou et al. 2019）或者预训练语言模型PLM（Kepler et al. 2019, Kim et al. 2019）来作为预测器，使用LSTM模型作为评估器。</p><p><strong style="color:red;">该框架的问题在于，其两阶段之间存在差异，包括数据的差异和训练目标的差异。</strong> 数据的差异是指，<strong style="color:blue;">预测器</strong>在大规模平行语料上训练，平行语料由原文和正确译文组成；<strong style="color:blue;">评估器</strong>在QE数据上训练，句对由原文和包含错误的机器翻译译文组成。训练目标的差异是指，预测器是在做“词预测”任务；评估器是在预测词和句子的质量。那么预测器的预训练过程与目标QE任务存在差异，会导致学习不到QE任务真正需要的知识，无法充分利用大规模双语平行数据。</p><h3 id="本文提出的方法"><a href="#本文提出的方法" class="headerlink" title="本文提出的方法"></a>本文提出的方法</h3><p>QE模型的现存问题主要是，1.大规模神经网络训练参数依赖大量数据；2.数据分布及训练目标的差异可能对两阶段训练带来不利影响。</p><p>为了解决这两个问题，我们采取的改进方向是：</p><ol><li><p>使用相同/相似的数据进行预训练；</p></li><li><p>使用相同/相似的预训练目标。</p></li></ol><p>QE数据中包含一些翻译噪音，QE的训练目标需要质量标签。那么如何基于平行语料，获得带有一定噪音的数据，并且可以获得噪音数据的质量标签？</p><p>我们的解决方法是，<strong style="color:green;">首先基于平行数据训练生成器（generator）进行词改写任务；接着对平行语料进行词改写，从而引入一定量的可控噪音并利用可控噪音自动生成质量标签。最终可以将这些生成数据提供给判别器（detector）直接为QE任务进行预训练。</strong> 接下来具体介绍生成器的训练与生成过程。</p><p>首先，我们以Masked Language Model (MLM)的方式训练生成器。给定平行句对，随机的隐藏（mask）译文中某个位置的词，然后让模型预测被隐藏的词（如图2所示）。</p><p><img src="https://i.loli.net/2020/12/30/UfN9cyzTM8CZb6n.png" alt="image-20201230101626106" style="zoom: 50%;"></p><p>生成器训练结束后，我们将使用生成器对平行语料进行转化，具体分为两个步骤。</p><ol><li>生成伪造机器翻译译文。给定平行语料并隐藏译文中某个位置的词，让生成器进行预测并输出概率分布，根据概率分布采样新的词替换被隐藏词（如图3所示），即完成了对被隐藏词的改写。</li></ol><p><img src="https://i.loli.net/2020/12/30/HJvEwk2ZhzyC7KB.png" alt="image-20201230101742561" style="zoom:50%;"></p><ol><li><p>生成对应标签。</p><p>根据译文中的词是否被改写来获得词级别标签（见公式1）。</p><p>根据译文中被改写词的比例获得句子级别标签q’（见公式2）。</p><p>公式1： $o_{j}^{\prime}=\left\{\begin{array}{ll}1, &amp; \text { if } y_{j}=y_{j}^{\prime} \\ 0, &amp; \text { otherwise }\end{array}\right.$</p><p>公式2： $q^{\prime}=1-\frac{\operatorname{sum}\left(\mathbf{O}^{\prime}\right)}{\operatorname{len}\left(\mathbf{O}^{\prime}\right)}$</p></li></ol><p>通过生成器，我们能够将大规模平行语料转化为更大规模的伪造QE数据。比起平行译文，伪造的机器翻译译文在数据分布上与QE中的译文更加接近。同时，伪造QE数据针对每一个词有表示“是否由机器生成“的标签，对整个句子有表示”句子改写程度“的标签，形式上与QE数据类似。<strong style="color:green;">基于大规模伪造QE数据以及真实QE数据，我们将使用同样的训练目标，对判别器（Detector）分别在伪造数据上进行预训练、在真实数据上微调参数。最终也只需要使用判别器来做QE分数预测。</strong></p><blockquote><p>即，生成的数据，仅仅是在estimator 上提供预训练数据</p><p>即本文，并没有对 predictor做改动，是对estimator 的训练策略上做了改动</p></blockquote><p>我们对比了Predictor-Estimator框架中的两种具体实现。一种是基于NMT的QE模型，具体实现仿照QE Brain模型(Fan et al. 2018)；一种是基于PLM的QE模型，具体使用的PLM模型来自于huggingface。在本文的实现中，我们所提出的<strong>DirectQE</strong>参数量是最小的。</p><p>实验结果如表3，4，5所示，可以发现我们的模型在绝大多数情况下都是具有优势的。</p><p>表格 3：单模型结果（英-德）</p><p><img src="https://i.loli.net/2020/12/30/4OZDeJgo7CmGX5l.png" alt="image-20201230102042709"></p><p>表格 4：集成模型结果（英-德）</p><p><img src="https://i.loli.net/2020/12/24/UkC5rKy9ZoWh8Ab.png" alt="image-20201224114950396" style="zoom:33%;"></p><p>表格 5：单模型结果（英-中、英-俄）</p><p><img src="https://i.loli.net/2020/12/24/87bizODk5VYjdxQ.png" alt="image-20201224115010524" style="zoom: 25%;"></p><p>为了找出我们模型性能具体的增长点，我们按照错误词的比例划分了真实QE数据集，并评估了模型在数据每个部分的性能。如图4所示，在翻译质量存在问题时（错误词比例&gt;12.5%），DirectQE的性能更好。</p><p>图表 4：模型在不同错误比例数据上的性能对比</p><p><img src="https://i.loli.net/2020/12/30/SOFaCUxnjHIuV4X.png" alt="image-20201230101815866"></p><p>为了研究预训练使用的数据分布对QE性能的影响，我们使用基于NMT的QE模型，并且将其中训练预测器用到的平行语料替换为生成器制造的伪造机器翻译译文，其余部分均保持不变。从表6中可以看出，使用伪造译文的模型性能有所上升，说明对于QE任务而言，使用伪造译文预训练比平行译文更好。</p><p>表格 6：使用伪造译文/平行译文训练基于NMT的QE系统</p><p><img src="https://i.loli.net/2020/12/24/r2zCSgNepfovnET.png" alt="image-20201224115311979" style="zoom: 33%;"></p><p>为了研究预训练数据质量对QE性能的影响，我们测试了不同质量的数据下QE性能，这里数据质量具体指是译文质量，可以体现在替换词比例上（在相同替换策略下，替换词比例越大，译文质量越差）。从图5中可以看出，伪造译文质量太好或者太坏都不利于最终QE的性能。伪造译文质量太好（替换比例很低），句子将接近于平行语料本身，数据中几乎没有噪音；而伪造译文质量太差时，会破坏句子结构，与真实QE译文数据分布有较大差异。图5中红点表示使用随机噪音替换被隐藏词，此时的译文质量很差，可以看到QE性能也很低。</p><p>图表 5：不同伪造译文质量下的QE模型性能</p><p><img src="https://i.loli.net/2020/12/30/kSiwhoP47EzGx9Y.png" alt="image-20201230101837656" style="zoom: 33%;"></p><p>在固定规模的平行语料上，生成器每一次采样会产生不同的伪造QE数据，最终用于训练判别器的数据规模是超百万级别的，且更多样化。为了研究多样性的价值，我们使用生成器产生了固定数量的伪造QE数据，对比了在固定生成的数据上以及持续生成的数据上预训练的模型性能。结果（图6）显示，<strong style="color:red;">伪造QE数据的多样性对提升模型性能来说很重要。</strong></p><p>图6：</p><p><img src="https://i.loli.net/2020/12/30/nh5G1rxMRPpDgmO.png" alt="image-20201230101857603" style="zoom:50%;"></p><p>词级别QE任务需要判别当前词的质量，那么模型在建模当前词时，包含更多当前词的信息是有必要的。为了体现模型隐层表示含有当前词信息的多与少，我们计算了隐层表示与当前词之间的互信息（模型指判别器/预测器）。在图7中可以看到，DirectQE学到的表示中包含有更多当前词的信息。</p><p>图表 7：模型隐层表示 v.s. 当前词信息</p><p><img src="https://i.loli.net/2020/12/30/b43AOK7aHm1QklR.png" alt="image-20201230101937525" style="zoom:33%;"></p><p>假设当模型针对下游任务进行微调时，模型的隐层表示改变越小，则原始的表示更适合该下游任务。为了研究是否DirectQE可以学习到更加适合QE任务的表示，我们测试了在真实QE数据上微调前后，DirectQE和基于NMT的QE模型的隐层表示之间的相似度。表7显示，DirectQE隐层表示的相似度较高，说明DirectQE可以学到更加适合QE任务的表示。</p><p>表格 7：微调前后隐层表示变化</p><p><img src="https://i.loli.net/2020/12/30/F5c2lrWGv8wuMnP.png" alt="image-20201230101958503" style="zoom:33%;"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>大规模的QE模型需要大规模数据进行参数训练。现有的两阶段方法，由于数据和训练目标差异，无法最大化利用大规模平行语料知识。我们提出一种直接为QE模型进行预训练的框架（DirectQE）——使用生成器由平行语料得到伪QE数据，而后使用判别器，在伪QE数据上进行预训练，并且使用真实QE数据微调。我们模型的优势是，参数规模更小，模型性能更好，并且易于使用。</p><p>未来，我们将考虑使用更多样化的方式来构造伪QE数据，进一步缓解数据差异带来的影响，最大程度利用大规模语料，提升QE模型性能。</p><h3 id="可查看的参考文献"><a href="#可查看的参考文献" class="headerlink" title="可查看的参考文献"></a>可查看的参考文献</h3><ul><li><p>Predictor-Estimator: Neural Quality Estimation Based on Target Word Prediction for Machine Translation</p><p><a href="https://unbabel.github.io/OpenKiwi/cli/train_predictor_estimator.html" target="_blank" rel="noopener">https://unbabel.github.io/OpenKiwi/cli/train_predictor_estimator.html</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;AAAI2021论文：基于数据生成的机器翻译质量评估方法&lt;/p&gt;
&lt;p&gt;来源：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=Mzg3ODA0NTA2OA==&amp;amp;mid=2247484204&amp;amp;idx=1&amp;amp;sn=1b2
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>BERT-ATTACK Adversarial Attack Against BERT Using BERT</title>
    <link href="http://yoursite.com/2020/12/01/BERT-ATTACK-Adversarial-Attack-Against-BERT-Using-BERT/"/>
    <id>http://yoursite.com/2020/12/01/BERT-ATTACK-Adversarial-Attack-Against-BERT-Using-BERT/</id>
    <published>2020-12-01T07:47:01.000Z</published>
    <updated>2020-12-01T09:30:26.130Z</updated>
    
    <content type="html"><![CDATA[<h1 id="复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法"><a href="#复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法" class="headerlink" title="复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法"></a>复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法</h1><p>针对离散数据（例如文本）的对抗攻击比连续数据（例如图像）更具挑战性，因为很难使用基于梯度的方法生成对抗样本。当前成功的文本攻击方法通常在字符或单词级别上采用启发式替换策略，替换时难以保持语义一致性和语言流畅性。在本文中，作者提出了BERT-Attack，这是一种高质量且有效的方法，可以使用以BERT为例的MLM预训练语言模型来生成对抗性样本。作者使用BERT对抗其微调模型和其他预训练模型，以误导目标模型，使其预测错误。作者的方法在成功率和扰动百分比方面均优于最新的攻击策略，并且生成的对抗性样本很流利，并且在语义一致。而且作者的方法计算成本低，可以大规模生成。</p><p>本期AI TIME PhD直播间，我们有幸邀请到了复旦大学 NLP group2019级研究生李林阳分享他的观点。</p><p>李林阳：复旦大学 NLP group2019级研究生；导师为邱锡鹏教授；</p><hr><h2 id="一、针对文本任务的攻击"><a href="#一、针对文本任务的攻击" class="headerlink" title="一、针对文本任务的攻击"></a><strong>一、针对文本任务的攻击</strong></h2><p>尽管深度学习取得了成功，但最近的研究发现神经网络容易受到对抗样本的攻击，这些对抗样本是对原始输入进行细微扰动而制成的。尽管对抗性样本对于人而言几乎不可察觉，但是它们会误导神经网络进行错误的预测。针对对抗性攻击的学习可以提升神经网络的可靠性和健壮性，在计算机视觉领域，攻击策略及其防御措施都得到了很好的探索，但由于语言的离散性，对文本的对抗性攻击较为困难，难以保证语法流利且语义一致。</p><p><img src="https://i.loli.net/2020/12/01/S3DQcwvOEAt1uma.jpg"> </p><p> 表1 BERT-Attack方法生成样本的例子</p><p>当前对文本的成功攻击通常采用启发式规则来修改单词的字符，并用同义词替换单词。</p><p>之前的研究包括使用word embedding生成替换词；对原有句子的短语进行添加或删除；使用人工构建的规则进行词语替换。<strong>尽管上述方法取得了良好的效果，但在攻击成功率，语法正确性和语义一致性等方面，仍有很大的改进空间</strong>。此外，这些方法的替换策略通常很简单，受限于特定任务。</p><p>本文提出了一种有效且高质量的对抗样本生成方法：BERT-Attack，使用BERT作为生成器生成对抗样本。BERT-Attack的核心算法包括<strong>两个阶段</strong>：<strong>在给定输入序列中查找易受攻击的单词，然后用如BERT的生成器来生成易受攻击单词的替代词。</strong> BERT能够捕捉文本的上下文语义，因此生成的样本更为流畅且合理。作者将BERT这样的MLM语言模型用作生成器，并找到让BERT模型得到最大错误预测风险的扰动。另外，本文的方法只需要一次生成器前向，而且无需反复使用语言模型对对抗样本进行评分，速度有一定改进。表1展示了该攻击方法在几个数据集上的生成文本样例。</p><h2 id="二、BERT-ATTACK攻击方法"><a href="#二、BERT-ATTACK攻击方法" class="headerlink" title="二、BERT-ATTACK攻击方法"></a><strong>二、BERT-ATTACK攻击方法</strong></h2><p><img src="https://i.loli.net/2020/12/01/cOpDH2hWr81uzMS.png" alt="image-20201201154053279"></p><p>图1. BERT-ATTACK替换策略一步的样例</p><p>本文提出BERT-Attack，它使用原始BERT模型制作对抗性样本以对抗微调的BERT模型。对抗样本的生成包括两个步骤：（1）找出针对目标模型的易受攻击的单词，（2）用语义相似且语法正确的单词替换它们，直到成功攻击为止。具体而言：</p><p><strong>1.寻找易受攻击词(Vulnerable Words)</strong></p><p>作者给句子中的每一个词一个评分，得分与易受攻击程度呈正比，该评分按照去掉该词的句子在判别器上的输出结果的扰动程度给出。作者使用目标模型（微调的BERT或其他神经模型）的logit输出作为判别器。易受攻击词定义为序列中对最终输出logit有重要影响的单词。令表示输入语句，表示目标模型输出的正确标签y的logit，重要性得分定义为</p><p>$I_{w_{i}}=o_{y}(S)-o_{y}\left(S_{\backslash w_{i} j}\right.)$</p><p>其中，</p><p>$S_{\backslash w_{i}}=\left[w_{0} ; \ldots ; w_{i-1} ;[M A S K] ; w_{i+1} ; \ldots\right]$</p><p>就是将该词替换成“[MASK]”。然后，对降序排名，获取其中的前百分之的词组成可替换词表，记为L。</p><h2 id="2-BERT生成器的优点"><a href="#2-BERT生成器的优点" class="headerlink" title="2.BERT生成器的优点"></a><strong>2.BERT生成器的优点</strong></h2><p>找到易受攻击的单词后，将列表L中的单词一一替换，以寻找可能误导目标模型的干扰。以前的替换方法包括同义词词典，POS检查器，语义相似性检查器等。但是因为替换的时候只有词表，不考虑上下文，因此需要用传统语言模型给替换单词的句子打分。由于换一个词就得评价一次，时间成本比较高。</p><p>作者利用BERT进行单词替换，可确保所生成的句子相对流利且语法正确，还保留了大多数语义信息。此外，掩码语言模型的预测是上下文感知的，因此可以动态搜索扰动，而不是简单的同义词替换。而且针对一个词而言，仅通过一个前向即可产生候选文本，无需再用语言模型来对句子评分，提升了效率。</p><h2 id="3-替换策略"><a href="#3-替换策略" class="headerlink" title="3.替换策略"></a><strong>3.替换策略</strong></h2><p><img src="https://i.loli.net/2020/12/01/MIsY1JPptlRxGB5.png" alt="image-20201201153433807" style="zoom:50%;">图2 BERT-ATTACK替换算法</p><p>如图1所示，作者输入原句子给BERT，并根据BERT输出生成候选词。<strong style="color:red;">注意这里不用[MSAK]替换被攻击词语</strong>，其原因作者给出了如下解释：1. 有些词语替换后，和原句子几乎一样流畅但是语义可能变更。例如给定一个序列“I like the cat”，如果遮盖cat这个词，那么MLM模型很难预测原始单词cat，因为如“I like the dog”一样很流畅。2. MASK掉给定的单词后，每个候选词都需要运行一遍BERT前向，时间成本太高。</p><p>令M代表BERT模型，为原序列，是利用BERT的分词器分完词的序列，将H输入BERT中得到输出预测。使用top-K策略选择可能的替换词预测，其中K是超参数。作者遍历所有候选易攻击词表L生成替换词表。</p><p>由于BERT使用字节对编码（BPE）分词，候选词可能会被分开，因此还需要将所选单词与BERT中相应的子单词对齐。</p><p>针对未被分开的单个单词，作者使用相应的前K个预测候选逐一尝试替换，并使用NLTK过滤其中的停用词，另外对于情感分类任务候选词可能包括同义词和反义词，作者使用同义词词典过滤反义词。然后将替换完成的句子重新输入判别器，如果判别器给出与原label相反的判断那么输出该句子作为攻击句；否则，从筛选出的候选词中选择一个对logit影响最大的。</p><p>针对字词组（sub-word 应该不能翻译为字词组），由于无法直接获取其替代词，作者使用子词组合中所有词的预测中找到合适的词替代。作者首先使用MLM模型分析整个词组的易攻击程度，然后再选出词组的top-k组合。剩余过程与单个单词一致。</p><p><strong>三、实验结果</strong></p><p><strong>3.1 数据集和评价指标</strong></p><p>为了衡量所生成样本的质量，作者设计了几种评估指标：</p><p>●成功率（success rate）：攻击样本的判别器准确率。</p><p>●扰动百分比（perturbed percentage）更改文本的占比。</p><p>●每个样本的查询数量（query number per sample）一个样本生成对抗样本的需要访问判别器的次数。</p><p>●语义相似度（semantic similarity）使用通用句子编码器（Universal Sentence Encoder）评价的句子相似度。</p><p><img src="https://i.loli.net/2020/12/01/jgXqBTRfJYniZFe.png" alt="image-20201201153907478" style="zoom:50%;">表2 实验结果</p><p><strong>3.2 实验结果</strong></p><p>如表2所示，BERT-Attack方法成功欺骗了其下游的微调模型。在文本分类和自然语言推断任务中，经过微调的BERT均无法正确地对生成的对抗样本进行分类，攻击后的平均准确度低于10％。同时，扰动百分比小于10％，明显小于以前的工作，BERT-Attack方法更有效且更不易察觉。查询数量也要少得多。</p><p>另外可以观察到，由于扰动百分比非常低，因此通常更容易攻击评论分类任务。BERT-Attack仅替换少数几个单词就可能误导判别器。由于平均序列长度相对较长，因此判别器倾向于仅按序列中的几个词进行判断，这不是人类预测的自然方式。因此，这些关键字的干扰将导致目标模型的预测不正确，从而揭示了该模型的脆弱性。</p><p><strong>3.3人工验证</strong></p><p>为了进一步评估生成的对抗性样本，作者人工评估了流利性，语法以及语义保留方面生成的样本的质量。</p><p><img src="https://i.loli.net/2020/12/01/oDVS914hYZWJNje.png" alt="image-20201201153955115" style="zoom:50%;"></p><p>作者要求三名标注人员对生成的对抗性样本和原始序列的混合句子的语法正确性进行评分（1-5分），然后将原始文本和对抗文本混在一起进行人工预测。在IMDB和MNLI数据集中，作者分别选择100个原始样本和对抗样本验证。对于IMDB，将多数类作为人类预测标签，对于MNLI，则使用标注人员之间的平均分数。从表2中可以看出，对抗性样本的语义分数和语法分数接近原始样本。MNLI<strong>任务数据长且更加复杂（存在句子对（sentence pair）之间，重复出现的词汇较多，而基于替换的对抗样本则破坏了这种相同词汇的对应关系</strong>），使标注人员难以正确预测，因此其准确性要比简单的句子分类任务低。作者同样做了大量消融实验，实验结果表明该对抗方法生成的样本迁徙性强，生成速度快。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>在这项工作中，作者提出了一种高质量有效的BERT-Attack方法，以使用BERT<strong>掩蔽语言模型（masked-LM）生成对抗性样本。</strong>实验结果表明，该方法在保持最小扰动的同时，取得了较高的成功率。然而，从屏蔽语言模型生成的候选者有时可能是反义词或与原始单词无关，从而导致语义损失。因此，增强语言模型以生成更多与语义相关的扰动可能是将来完善BERT-Attack的一种可能解决方案。</p><hr><blockquote><p>整理：李键铨<br>排版：杨梦蒗<br>审稿：李林阳</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法&quot;&gt;&lt;a href=&quot;#复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法&quot; class=&quot;headerlink&quot; title=&quot;复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法&quot;&gt;&lt;/a&gt;复旦
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</title>
    <link href="http://yoursite.com/2020/10/28/Beyond-Accuracy-Behavioral-Testing-of-NLP-Models-with-CheckList/"/>
    <id>http://yoursite.com/2020/10/28/Beyond-Accuracy-Behavioral-Testing-of-NLP-Models-with-CheckList/</id>
    <published>2020-10-28T04:09:31.000Z</published>
    <updated>2020-10-28T04:11:33.512Z</updated>
    
    <content type="html"><![CDATA[<p>转载：<a href="https://dy.163.com/article/FH8NB44C0511DPVD.html" target="_blank" rel="noopener">https://dy.163.com/article/FH8NB44C0511DPVD.html</a></p><p>现在，ACL2020各个奖项都已悉数公布，对此AI科技评论做了详细报道。其中，最受人瞩目的当属最佳论文奖，今年该奖项由微软团队的 《Beyond Accuracy: Behavioral Testing of NLP Models with CheckList》一举拿下。</p><p>　　小编看到论文题目的第一眼就觉得哪些有些不对，于是赶紧通读了一下文章，嗯~确实不太对，这貌似和之前我们熟悉的NLP“大力出奇迹”的模型套路不太一样啊？</p><p>　　那么这篇论文到底讲了什么呢，又何以摘得桂冠呢？</p><p>　　论文解读以外，我们进一步对论文的第二作者吴彤霜进行了专访，以更深入地了解最佳论文团队背后的工作。</p><h3 id="论文方法一览"><a href="#论文方法一览" class="headerlink" title="论文方法一览"></a><strong>论文方法一览</strong></h3><p>　　我们从论文的题目入手来了解一下这篇论文在讲什么。</p><p>　　首先是”Beyond Accuracy”：这是在说要超越Accuracy，这里Accuracy说的是NLP模型在各大数据集和任务上跑出的准确率，也即是性能的一种度量。</p><p>　　那既然要超越它总要有一个理由:</p><p>　　1.评估所用的训练-验证-测试划分集来估计模型的准确性时保留的数据集往往不全面。</p><p>　　2.测试集中往往包含与训练数据相同的偏差，这种方式可能高估了模型在真实世界的性能</p><p>　　3.通过Accuracy一刀切的方式很难找出模型失败在哪里，以及如何修复它。</p><p>　　对此本文提出的Beyond 方式又是如何呢？</p><p>　　Behavioral Testing of NLP Models with CheckList！也即用CheckList对NLP模型做行为测试。</p><h4 id="1、We-should-test-NLP-models"><a href="#1、We-should-test-NLP-models" class="headerlink" title="1、We should test NLP models"></a><strong>1、We should test NLP models</strong></h4><p>　　训练NLP模型的主要目标之一是泛化，虽然Accuracy是评价泛化的主要方法，但它往往高估了NLP模型的性能，用于评估模型的替代方法要么侧重于单个任务，要么侧重于特定的行为，benchmark的准确性不足以评估NLP模型。</p><p>　　除此之外许多额外的评估方法已经被提出来了，例如评估对噪声或对抗性变化的鲁棒性、公平性、逻辑一致性、可解释、诊断数据集和交互式错误分析。然而，这些方法要么侧重于单个任务，如问答或自然语言推理，要么侧重于一些能力（如鲁棒性），因此没有提供关于如何评估模型的全面指导。</p><p>　　因此在这这篇论文中，作者提出了CheckList(检查表)，一种新的评估方法和配套工具，用于NLP模型的综合行为测试。</p><h4 id="2、Software-engineering-gt-NLP"><a href="#2、Software-engineering-gt-NLP" class="headerlink" title="2、Software engineering-&gt;NLP"></a><strong>2、Software engineering-&gt;NLP</strong></h4><p>　　软件工程研究提出了测试复杂软件系统的各种范式和工具。特别是“行为测试”（黑盒测试）是指在不了解内部结构的情况下，通过验证输入输出行为来测试系统的不同能力。虽然有明显的相似之处，但软件工程的许多见解还没有应用到NLP模型中。</p><p>　　作者借鉴软件工程中行为测试的原理提出了CheckList：一种和模型、任务都无关的测试方法，它使用三种不同的测试类型来测试模型的各个功能。</p><p>　　作者用三个任务的测试来说明检查表的效用，识别商业和SOTA模型中的关键错误。在一项用户研究中，一个负责商业情绪分析模型的团队在一个经过广泛测试的模型中发现了新的、可操作的bug。在另一个用户研究中，使用CheckList的NLP实践者创建了两倍多的测试，发现的bug几乎是没有检查表的用户的三倍。</p><p>　　<img src="https://i.loli.net/2020/10/28/qk64rwGIt1lBJWE.png" alt="img"></p><p>　　图1</p><h4 id="3、What-is-CheckList"><a href="#3、What-is-CheckList" class="headerlink" title="3、What is CheckList"></a><strong>3、What is CheckList</strong></h4><p>　　CheckList包括一个通用语言能力和测试类型的矩阵，有助于全面的测试构思，以及一个快速生成大量不同测试用例的软件工具。从概念上讲，用户通过填写矩阵中的单元格来“检查”模型（图1），每个单元格可能包含多个测试。CheckList应用了“测试与实现脱钩”的行为测试原则，即将模型视为一个黑盒，允许对不同数据上训练的不同模型进行比较，或者对不允许访问训练数据或模型结构的第三方模型进行比较。</p><h4 id="4、What-to-test：capabilities"><a href="#4、What-to-test：capabilities" class="headerlink" title="4、What to test：capabilities"></a><strong>4、What to test：capabilities</strong></h4><p>　　CheckList通过提供适用于大多数任务的语言能力列表，指导用户测试什么。CheckList引入了不同的测试类型，比如在某些干扰下的预测不变性，或者一组“健全性检查”的性能。</p><p>　　虽然测试单个组件是软件工程中的常见实践，但现代NLP模型很少一次只构建一个组件。相反，CheckList鼓励用户考虑如何在手头的任务上表现出不同的自然语言能力，并创建测试来评估这些能力的模型。例如，词汇+POS能力取决于一个模型是否具有必要的词汇，以及它是否能够恰当地处理具有不同词性的单词对任务的影响。对于情绪，我们可能需要检查模型是否能够识别带有积极、消极或中性情绪的单词，方法是验证它在“这是一次很好的飞行”等示例上的行为。</p><p>　　基于此，作者建议用户至少考虑以下性能（capabilities）：</p><p>　　词汇+POS（任务的重要单词或单词类型）</p><p>　　Taxonomy（同义词、反义词等）</p><p>　　健壮性（对拼写错误、无关更改等）</p><p>　　NER（正确理解命名实体）</p><p>　　公平性</p><p>　　时态（理解事件顺序）</p><p>　　否定</p><p>　　共指（Coreference），</p><p>　　语义角色标记（理解诸如agent、object等角色）</p><p>　　逻辑（处理对称性、一致性和连词的能力）。</p><p>　　通过以上，CheckList实现包括多个抽象，帮助用户轻松生成大量测试用例，例如模板、词典、通用扰动、可视化和上下文感知建议。然而此功能列表并非详尽无遗，而是用户的一个起点，用户还应提供特定于其任务或域的附加功能。</p><h4 id="5、How-to-test"><a href="#5、How-to-test" class="headerlink" title="5、How to test"></a><strong>5、How to test</strong></h4><p>　　作者提示用户使用三种不同的测试类型来评估每个功能：最小功能测试、不变性和定向期望测试（矩阵中的列）。</p><p>　　1）最小功能测试（MFT）:它是受软件工程中单元测试的启发的一组简单的示例（和标签）的集合，用于检查功能中的行为。MFT类似于创建小而集中的测试数据集，尤其适用于在模型使用快捷方式处理复杂输入而不实际掌握功能的情况下进行检测。</p><p>　　<img src="https://i.loli.net/2020/10/28/2bFIGQ17wgVB3su.png" alt="img"></p><p>　　2）不变性测试（INV）：当对输入应用保留标签的扰动并期望模型预测保持不变时。不同的功能需要不同的扰动函数，例如，更改NER情感功能的位置名称，或者引入输入错误来测试健壮性能力。</p><p>　　<img src="https://i.loli.net/2020/10/28/c4Wa9IZzusY1Kj7.png" alt="img"></p><p>　　3）定向期望测试（DIR）：与不变性测试类似，只是标签会以某种方式发生变化。例如，我们预计，如果我们在针对某家航空公司的推文末尾添加“You are lame.”（图1C），情绪不会变得更积极。</p><p>　　<img src="https://i.loli.net/2020/10/28/Y78FjaetnpdgOJb.png" alt="img"></p><h4 id="6、可视化效果"><a href="#6、可视化效果" class="headerlink" title="6、可视化效果"></a><strong>6、可视化效果</strong></h4><p>　　调用test.visual_summary()</p><p>　　<img src="https://dingyue.ws.126.net/2020/0711/f079207eg00qdafs60095d200hr005pg00it0061.gif" alt="img"></p><p>　　在代码中调用suite.summary()（与test.summary相同）或suite.visual_summary_table() 显示测试结果如下：</p><p>　　<img src="https://dingyue.ws.126.net/2020/0711/c3640090g00qdafs701u7d200hr00bcg00it00c0.gif" alt="img"></p><p>　　模型保存和加载：精简至极！</p><p>　　<img src="https://i.loli.net/2020/10/28/KQDGslUnygeLESV.png" alt="img"></p><h4 id="7、更方便的大规模生成测试用例"><a href="#7、更方便的大规模生成测试用例" class="headerlink" title="7、更方便的大规模生成测试用例"></a><strong>7、更方便的大规模生成测试用例</strong></h4><p>　　用户可以从头开始创建测试用例，也可以通过扰动现有的数据集来创建测试用例。从头开始可以更容易地为原始数据集中可能未充分表示或混淆的特定现象创建少量高质量测试用例。然而，从头开始编写需要大量的创造力和努力，这通常会导致测试覆盖率低或者生成成本高、耗时长。扰动函数很难编写，但同时生成许多测试用例。为了支持这两种情况，作者提供了各种抽象，从零开始扩展测试创建，并使扰动更容易处理。</p><h4 id="8、使用CheckList测试SOTA模型"><a href="#8、使用CheckList测试SOTA模型" class="headerlink" title="8、使用CheckList测试SOTA模型"></a><strong>8、使用CheckList测试SOTA模型</strong></h4><p>　　作者通过付费API 检查了以下商业情绪分析模型：微软的文本分析、谷歌云的自然语言和亚马逊的Constract。我们还检查了在SST-23（acc:92.7%和94.8%）和QQP数据集（acc:91.1%和91.3%）上微调的BERT base和RoBERTa base。对于MC，作者使用了一个经过预训练的大BERT 微调阵容，达到93.2 F1。</p><p>　　<img src="https://i.loli.net/2020/10/28/pn7OqdKUx4shRLD.png" alt="img"></p><h4 id="9、测试商业系统"><a href="#9、测试商业系统" class="headerlink" title="9、测试商业系统"></a><strong>9、测试商业系统</strong></h4><p>　　作者联系了负责微软服务销售的通用情绪分析模型的团队（表1中的q）。由于它是一个面向公众的系统，模型的评估过程比研究系统更全面，包括公开可用的基准数据集以及内部构建的重点基准（例如否定、emojis）。此外，由于该服务已经成熟，拥有广泛的客户群，因此它经历了许多错误发现（内部或通过客户）和后续修复的周期，之后在基准测试中添加了新的示例。</p><p>　　作者的目标是验证检查表是否会增加价值，即使在这样的情况下，模型已经用当前的实践进行了广泛的测试。</p><p>　　作者邀请小组参加了一个持续约5小时的检查表会议。该团队集思广益地进行了大约30个测试，涵盖了所有功能。</p><p>　　从质量上讲，该小组称检查表非常有用：</p><p>　　（1）他们测试了他们没有考虑过的能力；</p><p>　　（2）他们测试了他们考虑过但不在benchmark中的能力；</p><p>　　（3）甚至他们有基准的能力（例如否定）也用检查表进行了更彻底和系统的测试。</p><p>　　他们发现了许多以前未知的错误，他们计划在下一个模型迭代中修复这些错误。最后，他们表示，他们肯定会将检查表纳入他们的开发周期，并要求访问我们的实现。</p><h4 id="10、用户研究"><a href="#10、用户研究" class="headerlink" title="10、用户研究"></a><strong>10、用户研究</strong></h4><p>　　作者进行了一项用户研究，以在一个更可控的环境中进一步评估检查表的不同子集，并验证即使是没有任务经验的用户也能获得洞察并发现模型中的错误。</p><p>　　尽管用户在使用CheckList时不得不解析更多的指令和学习新的工具，但他们同时为模型创建了更多的测试。</p><p>　　在实验结束时，作者要求用户评估他们在每个特定测试中观察到的失败的严重程度，研究结果令人鼓舞：有了检查表的子集，没有经验的用户能够在2小时内发现SOTA模型中的重大缺陷。此外，当被要求对检查表的不同方面进行评分时（1-5分），用户表示，测试环节有助于他们进一步了解模型，功能帮助他们更彻底地测试模型，模板也是如此。</p><p>　　评估特定语言能力的一种方法是创建挑战性数据集。我们的目标不是让检查表取代挑战或基准数据集，而是对它们进行补充。CheckList保留了挑战集的许多优点，同时也减轻了它们的缺点：用模板从头开始编写示例提供了系统控制，而基于扰动的INV和DIR测试允许在未标记的自然发生的数据中测试行为。</p><p>　　最后用户研究表明，CheckList可以轻松有效地用于各种任务：用户在一天内创建了一个完整的测试套件进行情绪分析，两个小时内创建了的MFTs，这两个都揭示了之前未知的严重错误。</p><h3 id="专访吴彤霜：最佳论文何以花落此家"><a href="#专访吴彤霜：最佳论文何以花落此家" class="headerlink" title="专访吴彤霜：最佳论文何以花落此家"></a>专访吴彤霜：最佳论文何以花落此家</h3><p>　　到这里我们大概明白了这篇论文到底在讲什么，但是我们还是心存疑惑，何以它能获得最佳论文殊荣？</p><p>　　<strong>以下为专访实录：</strong></p><p>　　<strong>AI 科技评论：</strong>首先恭喜您和您的团队斩获ACL2020最佳论文！我们想先了解一下这项工作的完成大概花了多长时间，把软件测试带入NLP模型检测的想法是最近才有的吗还是说之前就有了最近才实现？</p><p>　　<strong>吴彤霜：</strong>这个项目最早是一作快博士毕业时我们开始合作的，中间因为各种原因搁置了一段时间，实际做大概一年吧。引用软件测试应该可以算是一个新的想法。以前有很多nlp模型分析的论文本质上也可以说是我们论文里提到的那种“行为测试” (behavioral testing)，比如各种NLI challenge set。只不过这些工作大部分是针对某一个任务在做某个特定种类的分析，每次都要从头开始。我们工作其中的一个主要的一个贡献就是把这些分析做归一化，提供一个测试框架+开源系统。</p><p>　　<strong>AI 科技评论：</strong>这项测试系统是不是可以理解为对抗扰动系统啊？或者相比有什么优势呢？</p><p>　　<strong>吴彤霜：</strong>不变性测试 (INVariant test) 可以相当于扰动，就是模型在预测一个句子s和经修改后的版本s’时结果类似。CheckList还支持别的测试类别 (test type)：定向测试 (DIRectional test) 是用来测试预测结果的特定变化，最小功能测试 (Min Func Test) 不需要有配对的例子，只要一个能生成单个测试例句的模板就可以了。</p><p>　　只和INV（不变性测试 ）相比而言，现在NLP的大部分对抗句经常是在改拼写或者会产生乱码，比较难保证句子的连贯性，而能保证句子连贯性的居多是改近义词 (it’s a good movie -&gt; it’s a great movie)。CheckList因为允许自定义改写函数 (perturbation function)，所以可以更多样地测试模型的性能，比如看情感分析模型是否能辨认虚拟语气 (it’s a bad movie -&gt; it would have been a good movie)。这种测试也更系统化，因为可以生成很多对改写方法类似的句子/测试用例 (test case)。</p><p>　　当然相应的checklist的自动化就比较差，需要人来定义这些测试 :)</p><p>　　<strong>AI 科技评论：</strong>请问你们团队成员之前有过软件测试的经验吗，在CheckList的设计环节有什么亮点以及代码实现过程中有什么难点？</p><p>　　<strong>吴彤霜：</strong>应该不算有经验，没有在工业界实战过，顶多就是在软工课写单元测试，所以最开始其实也认真学习了一下软工 :)</p><p>　　设计上我觉得最大的亮点是对于性能 (capability) 的定义 。我们遇到一个瓶颈就是试图给每个NLP task设计需要测试的不同方面，但这样就非常繁琐，也失去了可拓展性。直到后来我们和其他researcher聊的时候意识到其实大部分的模型需要测试的“capability”基本比较稳定，只是不同任务里对标签的影响会有区别，比如[改主语]对NLI影响会比对情感分析要大。这样一个稳定的capability集合就让整个框架干净了很多。</p><p>　　开源上，其实NLP部分还是比较整洁的，但是为了让大家能比较流畅地在notebook里浏览和回顾test集，我们下了很大功夫研究怎们做交互插件，是一个大坑，但是最终效果还挺好看的，可以到readme里看看preview感受一下，哈哈。</p><p>　　写作上，因为marco在微软，我们很幸运能近水楼台找微软情感分析的工程组来做用户研究，让我们真的看到了CheckList在已经被认为是大规模测试过的模型仍然很有用。</p><p>　　<strong>AI 科技评论：</strong>很开心你们把这项工作开源，我想这项工作只是一个开始对吗？（大家都可以在你们开源的代码上进行哪些尝试和改进呢，比如自定义测试模板之类）</p><p>　　<strong>吴彤霜：</strong>最重要的是希望能看到大家提出的测试实例！其实比起很多NLP模型，CheckList是一个比较依靠人的力量的项目，测试者仔细设计实例才能用它最大程度暴露模型可能存在的缺陷。我们考虑的一个想法是希望可以做一个类似模型排行榜的测试榜，大家可以上传分享自己的测试集，甚至是顶或者踩别人的测试，最终让每个任务都能有一个比较稳定的测试集，也方便模型间的比较。</p><p>　　其次，我们也很期待看到大家会不会有关于如何让CheckList更自动化的想法，实现一键测试这个终极目标 :)</p><p>　　以及更研究向的：</p><p>　　我个人对于设计更稳定的测试也很感兴趣。CheckList对具体实例比较敏感，不同人在测试同一个模型性能时，如果实例设计不同，最终测试结果可能会有一些偏差。有没有什么交互手段能帮助测试者尽量穷尽一个性能所有的相关改写？甚至还有没有什么办法能慢慢形成一些自动的测试拓展？这个可能也和上面提到的自动化有一些关系。</p><p>　　最后测试带来的一个恒久不变的问题：so what？一个模型有问题之后，应该用什么样的标准来决定一个模型是不是可以被公开部署 (比如可能公平性测试的容错率可能远低于拼写错误)？应该如何改进它？</p><p>　　<strong>AI 科技评论：</strong>请问软件测试的思想只适用于NLP领域吗 ，在CV领域可行吗，应该怎么去设计测试系统？</p><p>　　<strong>吴彤霜：</strong>我相信是可行的！抽象来讲，本文图1的这种框架似乎能直接套用在CV上。</p><p>　　比如说一个最简单的狗和狼的分类，这个模型首先得能辨认有动物出现 (MFT)，然后改变图片的背景应该不影响预测 (INV)，但改变动物的头的形状应该是要影响的 (DIR)。vision里的“改写”效果其实比NLP好很多，也许更好用也说不定 :)</p><p>　　对设计系统而言，我觉得比较重要的是抽取基本组件。在NLP版本的CheckList里有一个重要组件就是写生成template/模板；也许在vision里则是需要提供一些基础像素之类的。</p><p>　　当然也可以考虑除了行为和单元测试之外的测试思想，比如如果是pipeline模型，考虑如何设计集成测试也许也会很有用 :)</p><p>　　<strong>AI 科技评论：</strong>可以简单介绍一下你们的团队成员吗，以及你们的近期工作、未来研究方向？</p><p>　　<strong>吴彤霜：</strong>隆重介绍一下没有出镜的一作吧，marco也是华大的博士，2018年毕业以后就加入了微软研究院，主要在做模型可解释性和分析，之前很有名的LIME（一种解释机器学习模型的方法——Local Interpretable Model-Agnostic Explanations）就是出自他手。除了CheckList，他今年在CVPR上也有一篇合作论文，是分析vqa model的稳定性的。现在主要在做vision模型的错误分析以及模型比较。</p><p>　　我们现在也在合作一个新工作，这项工作更多是关于如何人去探索模型的可解释性。虽然现在主要做的都是人如何检查模型，但是我们对于模型如何能反过来规范人或者帮助人也很感兴趣 :) 三四作Carlos和Sameer都是marco的导师，分别是ML和NLP的大佬。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>　　虽然CheckList目前也有一些不足比如CheckList不能直接用于非行为问题，例如数据版本控制问题、标记错误、注释器偏差、最坏情况下的安全问题或缺乏可解释性。</p><p>　　但是不可否认的是，使用CheckList创建的测试可以应用于任何模型，这使得它很容易被纳入当前的基准测试或评估pipeline中。用户研究表明，CheckList很容易学习和使用，对已经对模型进行了长时间测试的专家用户以及在任务中缺乏经验的实践者都很有帮助。</p><p>　　另外对吴同学的专访，我们相信， 本篇论文工作确实开创地把软件测试系统引入NLP模型的测试之中并且提供了完善的测试工具。 这将会给社区和企业带来很大的商业价值，比如CheckList测试工具将会节省很大的人力成本。</p><p>　　最后，我们相信，这种系统引进软件测试的思想也将会在CV乃至整个AI领域大有作为。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载：&lt;a href=&quot;https://dy.163.com/article/FH8NB44C0511DPVD.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://dy.163.com/article/FH8NB44C0511DPVD.
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>N-Gram模型</title>
    <link href="http://yoursite.com/2020/10/27/N-Gram%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/10/27/N-Gram模型/</id>
    <published>2020-10-27T09:22:01.000Z</published>
    <updated>2020-10-31T13:49:29.760Z</updated>
    
    <content type="html"><![CDATA[<p>转载：<a href="https://zhuanlan.zhihu.com/p/32829048" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32829048</a></p><h2 id="一、什么是n-gram模型"><a href="#一、什么是n-gram模型" class="headerlink" title="一、什么是n-gram模型"></a><strong>一、什么是n-gram模型</strong></h2><p>N-Gram是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。</p><p>每一个字节片段称为gram，对所有gram的出现频度进行统计，并且按照事先设定好的阈值进行过滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量维度。</p><p>该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。</p><p>说完了n-gram模型的概念之后，下面讲解n-gram的一般应用。</p><h2 id="二、n-gram模型用于评估语句是否合理"><a href="#二、n-gram模型用于评估语句是否合理" class="headerlink" title="二、n-gram模型用于评估语句是否合理"></a><strong>二、n-gram模型用于评估语句是否合理</strong></h2><p>如果我们有一个由 m 个词组成的序列（或者说一个句子），我们希望算得概率 $P(w_1, w_2,…,w_m)$，根据链式规则，可得</p><p><img src="https://i.loli.net/2020/10/27/ye3X8vh6LcKTzjr.png" alt="image-20201027172437915"></p><p>这个概率显然并不好算，不妨利用马尔科夫链的假设，即当前这个词仅仅跟前面几个有限的词相关，<strong><em>因此也就不必追溯到最开始的那个词，这样便可以大幅缩减上述算式的长度\</em></strong>。即</p><p><img src="https://i.loli.net/2020/10/27/XjDPInlQY9GN7ZU.png" alt="image-20201027172458291" style="zoom: 67%;"></p><p><strong><em>这个马尔科夫链的假设为什么好用？我想可能是在现实情况中，大家通过真实情况将n=1，2，3，….这些值都试过之后，得到的真实\</em></strong>的效果和时间空间的开销权衡之后，发现能够使<strong><em>用。\</em></strong></p><p>下面给出一元模型，二元模型，三元模型的定义：</p><p>当 n=1, 一个一元模型（unigram model)即为 ：</p><p><img src="https://i.loli.net/2020/10/27/UMNPucLtdw3zxgo.png" alt="img"></p><p>当 n=2, 一个二元模型（bigram model)即为 ：</p><p><img src="https://i.loli.net/2020/10/27/UmZkyzIK4fws7Nj.png" alt="img"></p><p>当 n=3, 一个三元模型（trigram model)即为</p><p><img src="https://i.loli.net/2020/10/27/Vr1LZIG7snCvokK.png" alt="img"></p><p>然后下面的思路就很简单了，在给定的训练语料中，利用贝叶斯定理，将上述的条件概率值（<strong>因为一个句子出现的概率都转变为右边条件概率值相乘了</strong>）都统计计算出来即可。下面会给出具体例子讲解。这里先给出公式：</p><p><img src="https://i.loli.net/2020/10/27/locAQnW9bGpXa3K.png" alt="img"></p><p>对第一个进行解释，后面同理,如下：</p><p><img src="https://i.loli.net/2020/10/27/iGbljDacPTSWu92.png" alt="image-20201027172711422"></p><p>下面给出具体的例子。</p><h2 id="三、二元语言模型判断句子是否合理"><a href="#三、二元语言模型判断句子是否合理" class="headerlink" title="三、二元语言模型判断句子是否合理\"></a><strong><em>三、二元语言模型判断句子是否合理\</em></strong></h2><p><strong><em>下面例子来自于：\</em></strong><a href="https://link.zhihu.com/?target=http%3A//blog.csdn.net/baimafujinji/article/details/51281816" target="_blank" rel="noopener">自然语言处理中的N-Gram模型详解 - 白马负金羁 - CSDN博客</a>和《北京大学 常宝宝 以及 The University of Melbourne “Web Search and Text Analysis” 课程的幻灯片素材》</p><p>假设现在有一个语料库，我们统计了下面的一些词出现的数量</p><p><img src="https://i.loli.net/2020/10/27/ufWOseUyrAm9qV3.png" alt="img"></p><p>下面的这些概率值作为已知条件：</p><p><img src="https://i.loli.net/2020/10/27/G2Y6zKZNxBCitVo.png" alt="img"></p><p><img src="https://pic1.zhimg.com/80/v2-a7c0d77143e0c997abd45e1535eaeb8c_1440w.jpg" alt="img"></p><p>$p(want|<s>) = 0.25$</s></p><p>下面这个表给出的是基于Bigram模型进行计数之结果</p><p><img src="https://i.loli.net/2020/10/27/G2Y6zKZNxBCitVo.png" alt="img"></p><p><img src="https://i.loli.net/2020/10/27/az7ewSWFYZGUbIu.png" alt="img"></p><p>例如，其中第一行，第二列 表示给定前一个词是 “i” 时，当前词为“want”的情况一共出现了827次。据此，我们便可以算得相应的频率分布表如下。</p><p><img src="https://i.loli.net/2020/10/27/9WhUqaZtcC3xDn7.jpg" alt="img"></p><p>比如说，我们就以表中的$p(eat|i)=0.0036$这个概率值讲解，从表一得出“i”一共出现了2533次，而其后出现eat的次数一共有9次，$p(eat|i)=p(eat,i)/p(i)=count(i,eat)/count(i)=9/2533 = 0.0036$</p><p>下面我们通过基于这个语料库来判断$s1=“<s> i want english food</s>” $ 与 $s2 = “<s> want i english food</s>“$哪个句子更合理：通过例子来讲解是最人性化的，我在网上找了这么久发现这个例子最好：</p><p><strong>首先来判断$p(s1)$</strong></p><p>$P(s1)=P(i|<s>)P(want|i)P(english|want)P(food|english)P(</s>|food)$</p><p>=$0.25×0.33×0.0011×0.5×0.68=0.000031$</p><p><strong>再来求$p(s2)$</strong></p><p>$P(s2)=P(want|<s>)P(i|want)P(english|want)P(food|english)P(</s>|food)$</p><p>=$0.25×0.0022×0.0011×0.5×0.68 = 0.00000002057$</p><p><strong>通过比较我们可以明显发现0.00000002057&lt;0.000031,也就是说s1= “i want english food&lt;/s&gt;”更像人话。</strong></p><p><strong>再深层次的分析，我们可以看到这两个句子的概率的不同，主要是由于顺序i want还是want i的问题，根据我们的直觉和常用搭配语法，i want要比want i出现的几率要大很多。所以两者的差异，第一个概率大，第二个概率小，也就能说的通了。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32829048&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/32829048&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;一
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>PPL-语句通顺</title>
    <link href="http://yoursite.com/2020/10/27/PPL-%E8%AF%AD%E5%8F%A5%E9%80%9A%E9%A1%BA/"/>
    <id>http://yoursite.com/2020/10/27/PPL-语句通顺/</id>
    <published>2020-10-27T03:37:20.000Z</published>
    <updated>2020-11-05T00:36:17.976Z</updated>
    
    <content type="html"><![CDATA[<h3 id="语句通顺-一些调研"><a href="#语句通顺-一些调研" class="headerlink" title="语句通顺 - 一些调研"></a>语句通顺 - 一些调研</h3><ul><li><p>BERT模型通过在大量语料的训练可以判断一句话是否通顺</p></li><li><p>理解 NNLM <a href="https://zhuanlan.zhihu.com/p/65446874" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/65446874</a></p><p>以上推理就是</p><ol><li>用词汇的联合概率表达一个语句是否通顺；</li><li>将计算联合概率转换为计算条件概率；</li><li>将条件概率由不定长度的且一般较大的t维降到一般较小的n-1维;</li></ol></li><li><p><a href="https://blog.csdn.net/blmoistawinde/article/details/104966127" target="_blank" rel="noopener">https://blog.csdn.net/blmoistawinde/article/details/104966127</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/76912493" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76912493</a></p></li></ul><h3 id="PPL-评价指标"><a href="#PPL-评价指标" class="headerlink" title="PPL-评价指标"></a>PPL-评价指标</h3><p>在得到不同的语言模型（一元语言模型、二元语言模型….）的时候，我们如何判断一个语言模型是否好还是坏，一般有两种方法：</p><p>1、一种方法将其应用到具体的问题当中，比如机器翻译、speech recognition、spelling corrector等。然后看这个语言模型在这些任务中的表现（extrinsic evaluation，or in-vivo evaluation）。但是，这种方法一方面难以操作，另一方面可能非常耗时，可能跑一个evaluation需要大量时间，费时难操作。</p><p>2、针对第一种方法的缺点，大家想是否可以根据与语言模型自身的一些特性，来设计一种简单易行，而又行之有效的评测指标。于是，人们就发明了perplexity这个指标。</p><p><img src="https://i.loli.net/2020/10/27/UcxVWKjtlCi5Tw7.png" alt="image-20201027113452950" style="zoom: 25%;"></p><p>困惑度（perplexity）的基本思想是：<strong>给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，</strong></p><p>由公式可知，<strong>句子概率越大，语言模型越好，迷惑度越小。</strong></p><p>$P(W_1, W_2, … , W_t)$</p><p>= $P(W_t | W_{t-1}, … , W_2, W_1) * P(W_{t-1}, … , W_2, W_1))$</p><p>= $P(W_t | W_{t-1}, … , W_2, W_1)$ <em> $P(W_{t-1} | W{t-2}, …, W_2, W_1) $ </em> $ P(W{t-2}, …, W_2, W_1)$</p><p>= $P(W_t | W_{t-1}, … , W_2, W_1)$ <em> $ P(W_{t-1} | W_{t-2}, …, W_2, W_1) $ </em> $ … $ <em> $ P(W_2 | W_1) </em> P(W_1)$</p><p>一些 ngram 模型经 训练文本后在测试集上的困惑度值：</p><p><img src="https://i.loli.net/2020/10/31/bYtXO3hgJs8TE6f.jpg" alt="img" style="zoom: 50%;"></p><ul><li>求通俗解释NLP里的perplexity是什么？ - 习翔宇的回答 - 知乎 <a href="https://www.zhihu.com/question/58482430/answer/412012509" target="_blank" rel="noopener">https://www.zhihu.com/question/58482430/answer/412012509</a></li><li>也可以用交叉熵损失函数来表示</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;语句通顺-一些调研&quot;&gt;&lt;a href=&quot;#语句通顺-一些调研&quot; class=&quot;headerlink&quot; title=&quot;语句通顺 - 一些调研&quot;&gt;&lt;/a&gt;语句通顺 - 一些调研&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;BERT模型通过在大量语料的训练可以判断一句话是否通顺&lt;/
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Visual Grounding in Video for Unsupervised Word Translation</title>
    <link href="http://yoursite.com/2020/10/18/Visual-Grounding-in-Video-for-Unsupervised-Word-Translation/"/>
    <id>http://yoursite.com/2020/10/18/Visual-Grounding-in-Video-for-Unsupervised-Word-Translation/</id>
    <published>2020-10-18T10:53:46.000Z</published>
    <updated>2020-10-18T10:54:21.489Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>我们的目标是使用视觉基础来改进语言之间的非监督词映射。其核心思想是通过学习母语教学视频中未配对的嵌入语，在两种语言之间建立一种共同的视觉表达。</p><p>本文的工作就是<strong>向机器提供不同的教学视频</strong>，这些视频的内容是人们用本国语言的教学视频。比如说，说中文和英文教别人榨橙汁的教学视频。这类视频有两个特点：视频网站上<strong>大量存在</strong>和<strong>内容相似度高</strong>，非常适合用于训练。但是这些视频也有一些弊端，会有很多无关废话（如“观众老爷们记得素质三连哦~”）。</p><p>即使如此，这种基于视觉的翻译提高了翻译的精度。</p><h3 id="Unsupervised-Multilingual-Learning"><a href="#Unsupervised-Multilingual-Learning" class="headerlink" title="Unsupervised Multilingual Learning"></a>Unsupervised Multilingual Learning</h3><p><img src="https://i.loli.net/2020/10/18/qzv2QN3bd8oInSx.png" alt="image-20201018172741937"></p><p>一个无监督的系统，该系统通过将语言嵌入视频中翻译单词。其中，不需要任何配对数据来学习翻译。</p><p><strong>Our method</strong> is unsupervised in that it learns the correspondences between two languages $X$ and $Y$ (e.g. English and French) without any parallel (paired) corpora.</p><p>given two distinct collections of instructional videos, i.e. n videos narrated with language $X$and another m different videos with language $Y$.</p><p><strong>Our goal</strong> is to learn to map languages $X$ and $Y$ by leveraging the shared visual modality $Z$ – the videos.</p><p><strong>Loss function</strong></p><p><img src="https://i.loli.net/2020/10/18/uYrymIKnJwvL2G6.png" alt="image-20201018173014409" style="zoom: 25%;"></p><h4 id="Multilingual-Visual-Embedding-Architecture"><a href="#Multilingual-Visual-Embedding-Architecture" class="headerlink" title="Multilingual Visual Embedding: Architecture"></a>Multilingual Visual Embedding: Architecture</h4><p><img src="https://i.loli.net/2020/10/18/XU2slICkuhyLBRT.png" alt="image-20201018172904212" style="zoom:33%;"></p><p><strong>yaya:</strong>  通过 视觉将两种语言做一种映射是存在困难的。文中列出了三点：<br>（1）learning video-text embeddings from instructional videos is difficult as the speech in these videos is only loosely related to the scene.</p><p>（2）in multilingual setting, such errors compound since both languages have this low video-text relevance;<br>（3）visually similar videos may not be semantically similar.</p><p>因此本文不同video 作为桥梁直接学习两种语言的映射，而是采取了间接的方式：we learn a joint (monolingual) video-text embedding space from instructional videos.</p><p>对于一种语言X, 学习视频及其字幕的映射，对于另一种语言，也学习一种映射，同时，在这种语言上加一个Adaptlayer, 使得 X和Y 能够映射到一个共同的空间。</p><p><strong>模型细节：</strong></p><p>其中X编码器 = WordEmbed + （Liner + ReLU MaxPool) + Linear</p><p>（WordEmbed层，度向量的转换；Linear层，建立与 Joint Embedding Space的映射）</p><p>而Y编码器则多了一个调整层（AdaptLayer），进行的是跨语言共享模型的权重分配，尽量让Y语言的词和X语言的词有相似的嵌入。</p><h4 id="MUVE-Improving-Unsupervised-Translation"><a href="#MUVE-Improving-Unsupervised-Translation" class="headerlink" title="MUVE: Improving Unsupervised Translation"></a>MUVE: Improving Unsupervised Translation</h4><p>略</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;p&gt;我们的目标是使用视觉基础来改进语言之间的非监督词映射。其核心思想是通过学习母语教学视频中未配
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[VIVO] Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training</title>
    <link href="http://yoursite.com/2020/10/18/VIVO-Surpassing-Human-Performance-in-Novel-Object-Captioning-with-Visual-Vocabulary-Pre-Training/"/>
    <id>http://yoursite.com/2020/10/18/VIVO-Surpassing-Human-Performance-in-Novel-Object-Captioning-with-Visual-Vocabulary-Pre-Training/</id>
    <published>2020-10-18T04:11:10.000Z</published>
    <updated>2020-10-31T13:49:42.322Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>VIVO可以在没有文本标签的数据上进行文本和图像的多模态预训练，摆脱了对配对图文数据的依赖，可以直接利用ImageNet等数据集的类别标签。借助VIVO，模型可以学习到物体的视觉外表和语义之间的关系，建立视觉词表。</p><p>这个视觉词表是啥呢？其实就是一个图像和文本的联合特征空间，在这个特征空间中，语义相近的词会聚类到一起，如金毛和牧羊犬，手风琴和乐器等。</p><p>预训练建好词表后，模型只需在有少量共同物体的配对图文的数据上进行微调，模型就能自动生成通用的模板语句，使用时，即使出现没见过的词，也能从容应对，相当于把图片和描述的各部分解耦了。</p><p>所以VIVO既能利用预训练强大的物体识别能力，也能够利用模板的通用性，从而应对新出现的物体。</p><p><img src="https://i.loli.net/2020/10/17/gakiQWDp3YAITCd.png" alt="image-20201017185401941" style="zoom:50%;"></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文要针对describe novel objects which are unseen in caption-labeled training data。This paper presents VIsual VOcabulary pretraining (VIVO) that performs pre-training in the absence of caption annotations。</p><p>By breaking the dependency of paired image-caption training data in VLP, VIVO can leverage large amounts of <strong>paired image-tag data</strong> to learn <strong>a visual vocabulary</strong>.<br>This is done by pre-training a <strong>multi-layer Transformer model</strong> that learns to align image-level tags with their corresponding image region features. Given that tags are not ordered, we employ <strong>the Hungarian matching loss</strong> for tag prediction optimization. </p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="https://i.loli.net/2020/10/17/JuOMYRtzW7LEgfo.png" alt="image-20201017193144259"></p><h4 id="VIVO-Pre-training"><a href="#VIVO-Pre-training" class="headerlink" title="VIVO Pre-training"></a>VIVO Pre-training</h4><p>We pre-train the Transformer model on a large-scale dataset with abundant tags, e.g., the Open Images training set with <strong>6.4K classes of image-level tags.</strong></p><p><strong>The training objective</strong> is to predict the missing (masked) tags given a bag of image-level tags and image regions. </p><p>We denote the training set: N images $I_i$ and their corresponding tags $G_i$. 一个image有多个tags.</p><p>use <strong>bi-directional attention mask</strong> in VIVO pre-training.</p><h4 id="Fine-tuning-and-Inference"><a href="#Fine-tuning-and-Inference" class="headerlink" title="Fine-tuning and Inference"></a>Fine-tuning and Inference</h4><p>After pre-training, the Transformer model is fine-tuned on a dataset where both captions and tags are available, e.g., the COCO set annotated with tags from 80 object classes and captions.</p><p>the input to the model during <strong>fine-tuning is a triplet of image region features $V$, a set of tags $T$ and a  caption $C$</strong>, where $V$ and $T$ are constructedin the same way as described in pre-training, and $C$ is a sequence of tokens. During fine-tuning, we <strong>randomly mask outsome of the tokens in a caption sentence</strong> for  prediction, and optimize the model parameters using the cross-entropy loss.</p><p>during fine-tuning we apply <strong>the uni-directional attention mask</strong> on a caption sequence to prevent the positions from attending to subsequent positions.</p><p>During inference, we first extract image region features and detect tags from a given image. Then the model is applied to <strong>generate a sequence, one token at a time,</strong> until it outputs the end of sentence token or reaches the maximum length.</p><p><strong>detect tags</strong> ：We use an object detector trained on the Open Images dataset （500 classes bboxes）to detect object tags for all datasets.</p><p><strong style="color:red;"><strong>yaya：</strong> tags detector的限制，仅能输出 500个类别tags, 因此，novel objects 的生成也是受到限制的</strong></p><p>以下这个表就可以说明问题，当不预训练时，是第一行的数据；当仅使用tags detector 的500个类时，是第二行的数据；当使用open-image 所有的 6.4k 个类时，是第三行的数据。因此，在inference阶段，使用 tag detector 来提供tags 是存在问题的。至少限制了模型的性能。</p><p><strong>改进</strong>：本文的model，pre-training, fine-tune，都是在一个fix model上。但是pre-training 的目的，仅仅是为了构建 image-tag vocabulary, 可以先构建，然后再离线使用！！！</p><p><img src="https://i.loli.net/2020/10/18/eyKJkcQPibhX3nS.png" alt="image-20201018115538016" style="zoom:33%;"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;VIVO可以在没有文本标签的数据上进行文本和图像的多模态预训练，摆脱了对配对图文数据的依赖，可以直接利用ImageNet等数据集的类别标签。
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
      <category term="region-word-embedding" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/region-word-embedding/"/>
    
    
      <category term="region-word-embedding" scheme="http://yoursite.com/tags/region-word-embedding/"/>
    
  </entry>
  
  <entry>
    <title>Vokenization Improving Language Understanding with Contextualized, Visual-Grounded Supervision</title>
    <link href="http://yoursite.com/2020/10/18/Vokenization-Improving-Language-Understanding-with-Contextualized-Visual-Grounded-Supervision/"/>
    <id>http://yoursite.com/2020/10/18/Vokenization-Improving-Language-Understanding-with-Contextualized-Visual-Grounded-Supervision/</id>
    <published>2020-10-18T04:10:43.000Z</published>
    <updated>2021-02-19T10:00:35.736Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><h4 id="现存问题"><a href="#现存问题" class="headerlink" title="现存问题"></a>现存问题</h4><p>人类学习语言都是结合多模态信息，但是当前的 language pre-training frameworks 仅通过自监督的方式，学习语言这一种模态。</p><p>虽然这种自监督的方式取得了很大的成功，但是它们没有利用grounding information from external visual word.</p><blockquote><p>Emily M Bender and Alexander Koller. 2020. <strong>Climbing towards nlu: On meaning, form, and understanding in the age of data.</strong> In ACL.</p><p>Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata,<br>Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. 2020. <strong>Experience grounds language.</strong> In EMNLP</p></blockquote><h4 id="本文的解决"><a href="#本文的解决" class="headerlink" title="本文的解决"></a>本文的解决</h4><p><img src="https://i.loli.net/2020/10/17/1cz3MHUdNXA9o5J.png" alt="image-20201017095623630" style="zoom:33%;"></p><p>本文：介绍了一个 <strong>视觉</strong>监督语言模型，如图1，该模型使用 language tokens 作为输入，使用token-related images 作为视觉监督。本文将这些images称作 vokens，which act as visualizations of the corresponding tokens.</p><p>假若a large aligned token-voken dataset 存在，那么模型可以通过voken-prediction task 从这些vokens中进行学习。但是不幸的是，不存在这种大型数据集，主要是有两个挑战：(1) 视觉性单词与 其他非视觉性单词之间，数量上存在很大的差异。如，在visually-grounded language datasets中仅有120M tokens, 但是在BERT的训练数据中有3300M tokens。grounded language 一般会更短，偏向于instructive descriptions, 因此在句子长度和有效词的数量上与其他语言类型的分布不同。(2) 自然语言中的大部分单词是 not visually grounded，因此对是否建立一个 visual supervision的数据集提出了质疑。粗略估计，英语维基百科中 grounded tokens 的比例仅为大约28％。 这种 low grounded ratio 导致以前方法中的视觉监控覆盖率低。</p><p><img src="https://i.loli.net/2020/10/17/27sWCBNOiqp13Vz.png" alt="image-20201017111702738"></p><p>为解决以上的两个挑战，本文提出了一个 <strong>vokenization method, that contextually maps the tokens to the visualized tokens (i.e., vokens) by retrieval.</strong>  而不是直接使用具有visually grounded的语言数据集来监督语言模型。</p><p>解决第一个挑战：(1) relative small datasets to train the <strong>vokenization processor</strong> (2) generate vokens for large language corpora.<br>our visually-supervised language model will take the input supervision from these large datasets, thus <strong>bridging the gap between different data sources,</strong> which solves the first challenge.</p><p>解决第一个挑战：low grounded ratio 的第二个挑战似乎是语言的固有特征。 但是，我们发现，考虑到它的上下文，可以将一些非可视化的tokens 有效地映射到相关图像。by our contextual token-image matching model (defined in Sec. 3.2) inside our vokenization processor, where we map tokens to images by viewing the sentence as the context.</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>Using our proposed vokenizer with a <strong>contextualized</strong> token-image matching model, we generate vokens for English Wikipedia. </p><p>Supervised by these generated vokens, we show consistent improvements upon a BERT model on several diverse NLP tasks such as GLUE (Wang et al., 2019), SQuAD (Rajpurkar et al., 2016), and SWAG (Zellers et al., 2018).  We also show the transferability of our vokens to other frameworks (i.e., RoBERTa).</p><h3 id="Vokenization"><a href="#Vokenization" class="headerlink" title="Vokenization"></a>Vokenization</h3><p><img src="https://i.loli.net/2020/10/17/fUWtQpIO8kZjAcY.png" alt="image-20201017120304025" style="zoom:50%;"></p><p>we <strong>retrieve an image for a token</strong> from a set of images $X$ = {$x_1; x_2; … ; x_n$} regarding a token-image-relevance scoring function $r_\theta(w_i; x; s)$. This scoring function $r_\theta(w_i; x; s)$, parameterized by $\theta$</p><h4 id="Contextual-Token-Image-Matching-Model"><a href="#Contextual-Token-Image-Matching-Model" class="headerlink" title="Contextual Token-Image Matching Model"></a>Contextual Token-Image Matching Model</h4><p>输入：The model takes a sentence $s$ and an image $x$ as input.</p><p>输出：The output $r_\theta(w_i; x; s)$ is the relevance score between the token $w_i \in s$ and the image $x$ while considering the whole sentence $s$ as a context.</p><p>Model: an inner product of the language feature representation $f_\theta(w_i; s)$ and the visual feature representation $g_\theta(x)$: $r_\theta(w_i; x; s)$ = $f_\theta(w_i; s)^T$ $g_\theta(x)$</p><p>token-image paris: 使用MS-COCO image caption pairs， 将caption中的所有tokens的vokens 都指定为该 image.</p><p>Training: 训练模型，maximizing the relevance score of these aligned token-image pairs over unaligned pairs. 使用 hinge loss.</p><h3 id="Visually-Supervised-Language-Models"><a href="#Visually-Supervised-Language-Models" class="headerlink" title="Visually-Supervised Language Models"></a>Visually-Supervised Language Models</h3><p>Based on these vokens, we propose a new pre-training task for language: voken classification.</p><h4 id="The-Voken-Classification-Task"><a href="#The-Voken-Classification-Task" class="headerlink" title="The Voken-Classification Task"></a>The Voken-Classification Task</h4><p><img src="https://i.loli.net/2020/11/04/j32ZMfNUpHWwez4.png" alt="image-20201104160728232"></p><p>BERT 的结果，会在每个token $w_i$的位置输出一个localized feature representation ${h_i}$，因此这将会很容易增加一个 token-level classification task, 而不需要修改模型的结构。Suppose the vokens come<br>from a finite set $X$, we convert the hidden output to ${h_i}$ a probability distribution ${p_i}$ with a linear layer and a softmax layer. </p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>受到这篇文章对的影响，是否可以结合视频，设计一个这种模型，比如有一些动词，仅能在视频中体现出来。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;h4 id=&quot;现存问题&quot;&gt;&lt;a href=&quot;#现存问题&quot; cla
      
    
    </summary>
    
    
      <category term="region-word-embedding" scheme="http://yoursite.com/tags/region-word-embedding/"/>
    
  </entry>
  
</feed>
