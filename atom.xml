<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShiYaya</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-05-09T07:00:00.329Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ShiYaya</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Learning to Evaluate Image Captioning</title>
    <link href="http://yoursite.com/2020/05/09/Learning-to-Evaluate-Image-Captioning/"/>
    <id>http://yoursite.com/2020/05/09/Learning-to-Evaluate-Image-Captioning/</id>
    <published>2020-05-09T06:33:59.000Z</published>
    <updated>2020-05-09T07:00:00.329Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>本文提出一个可学习的图像描述评价指标。</p><p><strong>Motivation</strong>: 由于当前的评价指标不是很完美，不能处理句子中存在的所有的病理行为，或者是说，当遇到某些病理行为时，则不能正常工作，比如，SPICE对字幕的语义很敏感，但往往会忽略其句法质量，SPICE倾向于对带有重复子句的长句子给予高分。每个评估指标都有其众所周知的盲点，基于规则的指标通常不灵活，无法应对新的病理病例。</p><p>因此本文提出，使用几种数据增强的方式，来扩展出很多的存在特征几种病理问题的对抗样本，并纳入训练过程中，使得训练出来的评价指标对于这些对抗样本更加的鲁棒。（即，可以识别出这些对抗样本的能力）</p><h4 id="How-to-Use-the-Proposed-Metric-in-Practice"><a href="#How-to-Use-the-Proposed-Metric-in-Practice" class="headerlink" title="How to Use the Proposed Metric in Practice"></a>How to Use the Proposed Metric in Practice</h4><p>由于涉及到需要学习 ，则评价指标的训练的数据分布 与 被测试的captioning dataset 之间存在差异。</p><p>本文解决: 假设要评估 coco  <strong>test</strong> captioning, 则将该份submission 分成两半，一半用于scratch 训练该评价指标，另外一半则使用该训练好的评价指标得到得分；然后交替，则得到了所有的得分！</p><h4 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h4><ul><li>(1) One direction of future work could aim to capture the heterogeneous nature of human annotated captions and incorporate such information into captioning evaluation.  <strong>Human annotated captions 带有人的个性</strong></li><li>(2) Another direction for future work could be training a caption generator together with the proposed evaluation metric (discriminator) in a generative adversarial setting. <strong>captioning model 与提出的评价指标，一起生成对抗的训练</strong></li><li>(3) Finally, gameability is definitely a concern, not only for our learning based metric, but also for other rule-based metrics. Learning to be more robust to adversarial examples is also a future direction of learning based evaluation metrics.  <strong>对 对抗样本更加的鲁棒，是基于学习的评价指标的一个未来的方向</strong></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h4&gt;&lt;p&gt;本文提出一个可学习的图像描述评价指标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivation&lt;
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Cross-modal Coherence Modeling for Caption Generation</title>
    <link href="http://yoursite.com/2020/04/22/Cross-modal-Coherence-Modeling-for-Caption-Generation/"/>
    <id>http://yoursite.com/2020/04/22/Cross-modal-Coherence-Modeling-for-Caption-Generation/</id>
    <published>2020-04-22T02:44:15.000Z</published>
    <updated>2020-04-23T11:27:08.374Z</updated>
    
    <content type="html"><![CDATA[<h4 id="现在图像描述中存在的问题"><a href="#现在图像描述中存在的问题" class="headerlink" title="现在图像描述中存在的问题"></a>现在图像描述中存在的问题</h4><ul><li>标注方式上：让工作人员标注出image 对应的text。</li><li>这导致的问题：（1）Unfortunately, such dedicated annotation efforts cannot yield enough data for training robust generation models; the resulting generated captions are plagued by content<br>hallucinations (Rohrbach et al., 2018; Sharma et al., 2018) that effectively preclude them for being used in real-world applications. </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;现在图像描述中存在的问题&quot;&gt;&lt;a href=&quot;#现在图像描述中存在的问题&quot; class=&quot;headerlink&quot; title=&quot;现在图像描述中存在的问题&quot;&gt;&lt;/a&gt;现在图像描述中存在的问题&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;标注方式上：让工作人员标注出image 对应的t
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</title>
    <link href="http://yoursite.com/2020/04/16/Oscar-Object-Semantics-Aligned-Pre-training-for-Vision-Language-Tasks/"/>
    <id>http://yoursite.com/2020/04/16/Oscar-Object-Semantics-Aligned-Pre-training-for-Vision-Language-Tasks/</id>
    <published>2020-04-16T08:14:45.000Z</published>
    <updated>2020-04-17T01:26:40.517Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><ul><li>现在基于bert 来处理的vision-language task 存在的问题：现在的方将image region features 和 text features 拼起来，然后利用自我注意机制以暴力方式学习图像区域和文本之间的语义对齐。（1）<strong>由于没有显示的region 与 text poses之间的对齐监督，因此是一种弱监督的任务。</strong> （2）另外，vision region常常过采样(region之间有重叠)，从而带来噪声和歧义（由于重叠，导致region之间的特征区分性不大），这将会使得vision-language task任务更加具有挑战性。</li><li>本文通过引入从images中检测出的object tags 作为anchor points来减轻images 和 text 之间语义对齐的学习。</li><li>本文提出了一个新的vision-language pre-training method <strong>OSCAR</strong> ，设计训练样本是一个三元组：（word sequence, a set of object tags, and a set of image region features. ）</li><li>Motivated by: the salient objects in an image can be accurately detected by modern object detectors, and that these objects are often mentioned in the paired text.</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gdvtabmqe2j30qh0f247k.jpg" alt="搜狗截图20200416190213.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;现在基于bert 来处理的vision-lang
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>A negative case analysis of visual grounding methods for VQA</title>
    <link href="http://yoursite.com/2020/04/15/A-negative-case-analysis-of-visual-grounding-methods-for-VQA/"/>
    <id>http://yoursite.com/2020/04/15/A-negative-case-analysis-of-visual-grounding-methods-for-VQA/</id>
    <published>2020-04-15T10:46:54.000Z</published>
    <updated>2020-04-16T00:26:21.276Z</updated>
    
    <content type="html"><![CDATA[<h4 id="yaya简述"><a href="#yaya简述" class="headerlink" title="yaya简述"></a>yaya简述</h4><p>在VQA任务中，现在的方法尝试希望模型在回答问题时，同时能够关注到相对应的正确的物体（出发点：当模型关注到正确的物体时，能够更好的帮助模型选择出正确的答案）。于是，基于这样的方式，提出了一些方法 [1] [2]. 但是本文发现即便在模型中给了vision grounding 的监督，但是模型的grounding 能力却未必很好。那么提升VQA性能的真正原因其实是这个监督，仅仅是一种正则化效果。</p><p>作者使用了Grounding using irrelevant cues；Grounding using fixed random cues；Grounding using variable random cues 来说明，即使是错误的监督信息，相比于正确的监督也不会使得性能下降很多。</p><p>作者使用Regularization by zeroing out answers  来说明，给损失函数中加一个正则化项，使得training accuracy下降，就会达到正则化的效果，其VQA的性能与用grounding 监督的效果差距也不大。这就证明了使用grounding来监督，其实仅仅是起到了正则化的效果。</p><h4 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h4><ul><li><p>未来的方法必须设法通过使用与本文中介绍的类似的实验设置来验证性能增益不是源于spurious source.</p></li><li><p>创建一个数据集，使得能够评估  if methods are able to focus on relevant information.</p></li><li><p>Use tasks  that explicitly test grounding, e.g., in visual query detection an agent must output boxes around any regions of a scene that match the natural language query .</p></li></ul><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p>[1] Ramprasaath R Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi Parikh. <strong>Taking a hint: Leveraging explanations to make vision and language models more grounded.</strong>  In ICCV 2019. </p><p>[2] Jialin Wu and Raymond Mooney. <strong>Self-critical reasoning for robust visual question answering.</strong> In NeurIPS 2019</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;yaya简述&quot;&gt;&lt;a href=&quot;#yaya简述&quot; class=&quot;headerlink&quot; title=&quot;yaya简述&quot;&gt;&lt;/a&gt;yaya简述&lt;/h4&gt;&lt;p&gt;在VQA任务中，现在的方法尝试希望模型在回答问题时，同时能够关注到相对应的正确的物体（出发点：当模型关注到正
      
    
    </summary>
    
      <category term="Grounding 相关" scheme="http://yoursite.com/categories/Grounding-%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="Grounding 相关" scheme="http://yoursite.com/tags/Grounding-%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models</title>
    <link href="http://yoursite.com/2020/04/01/Egoshots-an-ego-vision-life-logging-dataset-and-semantic-fidelity-metric-to-evaluate-diversity-in-image-captioning-models/"/>
    <id>http://yoursite.com/2020/04/01/Egoshots-an-ego-vision-life-logging-dataset-and-semantic-fidelity-metric-to-evaluate-diversity-in-image-captioning-models/</id>
    <published>2020-04-01T12:55:53.000Z</published>
    <updated>2020-04-02T00:39:08.259Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul><li><p>当前的image caption dataset 存在的问题</p><p>图像字幕模型已经能够生成语法正确且易于理解的句子。但是，大多数字幕传达的信息有限，因为所使用的模型是在数据集上训练的，而该数据集并未为日常生活中存在的所有可能的对象提供字幕。由于缺少先验信息，因此大多数字幕仅偏向场景中出现的少数几个对象，因此限制了它们在日常生活中的使用。在本文中，我们试图证明当前现有图像字幕模型的偏向性，并提出一个新的图像字幕数据集<em>Egoshots</em>，由978张不带字幕的现实生活图像组成。我们进一步利用最先进的预训练图像字幕和对象识别网络来注释我们的图像并显示现有作品的局限性。</p></li><li><p>当前的standard metric存在的问题</p><p>此外，为了评估所生成字幕的质量，我们提出了一种新的图像字幕度量标准，即基于对象的<em>语义保真度</em>（SF）。现有的图像字幕度量标准只能在存在其相应注释（reference captions）的情况下评估字幕。但是，SF允许评估为图像生成的字幕而没有注释，这对于现实生活中生成的字幕非常有用。</p></li></ul><h4 id="SF"><a href="#SF" class="headerlink" title="SF"></a>SF</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gdel85qhz8j30se0g97eb.jpg" alt="搜狗截图20200401212845.png"></p><h4 id="Annotation-Pipline-and-Sementic-Fidelity-Metric"><a href="#Annotation-Pipline-and-Sementic-Fidelity-Metric" class="headerlink" title="Annotation Pipline and Sementic Fidelity Metric"></a>Annotation Pipline and Sementic Fidelity Metric</h4><ul><li><p>annotation pipline</p><p>使用三个预训练好的caption model: Show Attend And Tell (SAT), nocaps: novel object captioning at scale (NOC), and Decoupled Novel Object Captioner (DNOC) 在新的数据集Egoshots上进行captioning 任务。</p></li><li><p>sementic fidelity metric</p><p>我们提出了一种称为<em>语义保真</em>度的新图像字幕指标。SF考虑了两个元素：1）生成的字幕与图像中检测到的对象的语义接近度； 2）相对于检测到的对象实例数量的对象多样性。假设有一个最新的准完美对象检测器，通过考虑这两组（带字幕和检测到的）实体（即对象）之间的语义亲密性，当一个模型输出的caption中包含了并没有出现在image scene中的objects时，将进行惩罚。</p><p>公式：<img src="http://ww1.sinaimg.cn/large/006uWRWVly1gdelh0afpej3055021a9x.jpg" alt="搜狗截图20200401213725.png"></p><p>对于图像i，si是其预测字幕c i中的名词词与OD检测到的对象名词之间的语义相似性，＃O是O O D的基数，＃N是名词的数量（表示对象in N i）存在于caption i中。SF的范围为[0，1]：SF接近1的字幕传达更多信息，并且在语义上更接近于要字幕的场景（就字幕所涉及的对象而言）。</p><p>关于si的计算：Recent works (Mikolov et al., 2013; Conneau et al., 2017) show the ability of word embeddings that is transforming a word into its vectored form efficiently capture the semantic closeness of two given words. The SF metric uses this approach to calculate such semantic similarity between the noun words and objects in an image.</p><p>上述公式存在一个假设：that #O ≥ #N (Assumption 1) for all images. This approach to compute SF will work only assuming robust object detectors satisfying enough scene annotation granularity.  </p><p>同时为保证分母不为0，还需要一个假设：Assumption 2: #O ！= 0 (i.e., the object detector can at least detect one object in the image). </p><h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4></li><li><p>在上述新提出的metric中，如果使用SF执行字幕评估，则良好通用化且鲁棒的对象检测模型将扮演最重要的角色。（a well generalized and robust object detection model plays the most important role if<br>the evaluation of captions is performed using SF. ）</p></li><li><p>在物体检测器发生故障的情况下，度量是不可靠的。由于SF将无法惩罚字幕模型，因为它不能依赖忠实（即足够鲁棒）的对象检测器（＃O = 0，假设2损坏），因此无法应用SF。</p></li></ul><h4 id="Appendix-指标限制"><a href="#Appendix-指标限制" class="headerlink" title="Appendix: 指标限制"></a>Appendix: 指标限制</h4><ul><li><p>我们必须注意到度量标准的一些局限性，应加以补充/扩展为（1）解释字幕的动词和其他句法元素（当前只考虑了名词）；（2）根据解释的质量对字幕进行评分，并考虑图像中相同类型的对象相对于字幕中存在的对象的数量。诸如（Cohen17）之类的特定计数模型是有关如何增强此处提出的无标签数据集注释管道的特定示例。</p><p>应该在更有针对性的应用程序使用案例中评估指标，例如，在诸如导航，对目标用户（如盲人）的有用性。</p></li></ul><h4 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h4><ul><li>其实本文尝试使用一个open-domain dataset 来测试在 in-domain 上训练的captioning model的泛化性能。但是这本身就存在问题！因为，model本身就会受限于训练数据，因此这里却希望它有很强的泛化性能，这本身就太难为model了。<code>eg: 不能要求一个学了小学课程的人来做高中生的题目</code></li></ul><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ul><li><p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. </p><p>Distributed Representations of Words and Phrases and their  Compositionality. In NIPS, 2013. </p></li><li><p>Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herve J ´ egou. ´<br>Word Translation Without Parallel Data. ArXiv, abs/1710.04087, 2017 </p></li><li><p>Joseph Paul Cohen, Genevieve Boucher, Craig A. Glastonbury, Henry Z. Lo, and Yoshua Bengio.<br>Count-ception: Counting by Fully Convolutional Redundant Counting. In The IEEE International<br>Conference on Computer Vision (ICCV) Workshops, Oct 2017. </p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;当前的image caption dataset 存在的问题&lt;/p&gt;
&lt;p
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>(ACL 2019)Putting Evaluation in Context: Contextual Embeddings improve Machine Translation Evaluation</title>
    <link href="http://yoursite.com/2020/04/01/ACL-2019-Putting-Evaluation-in-Context-Contextual-Embeddings-improve-Machine-Translation-Evaluation/"/>
    <id>http://yoursite.com/2020/04/01/ACL-2019-Putting-Evaluation-in-Context-Contextual-Embeddings-improve-Machine-Translation-Evaluation/</id>
    <published>2020-04-01T09:35:03.000Z</published>
    <updated>2020-04-01T09:43:09.981Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>没有认真阅读本篇文章，但是其中提到了尝试去拟合Human judgements，这一训练方案。</p><p> （1）We treat the human reference translation and the MT output as the premise and hypothesis, respectively 。</p><p>（2）Using squared error as part of regression loss – being better suited to Pearson’s r — and might be resolved through a different loss. Using hinge loss over pairwise preferences which would better reflect Kendall’s Tau</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;没有认真阅读本篇文章，但是其中提到了尝试去拟合Human judgements，这一训练方案。&lt;/p&gt;
&lt;p&gt; （1）We treat the human reference translation and the MT output as the pre
      
    
    </summary>
    
      <category term="自然语言理解" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3/"/>
    
    
      <category term="自然语言理解" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3/"/>
    
  </entry>
  
  <entry>
    <title>BERTScore: Evaluating Text Generation with BERT</title>
    <link href="http://yoursite.com/2020/04/01/BERTScore-Evaluating-Text-Generation-with-BERT/"/>
    <id>http://yoursite.com/2020/04/01/BERTScore-Evaluating-Text-Generation-with-BERT/</id>
    <published>2020-04-01T03:22:30.000Z</published>
    <updated>2020-04-01T09:10:45.905Z</updated>
    
    <content type="html"><![CDATA[<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>我们提出BERTScore，这是一种用于文本生成的自动评估指标。</p><p>类似于通用指标，BERTScore计算候选句子中每个token与参考中每个token的相似性得分。但是，我们不是使用精确匹配，而是使用上下文化的BERT embedding 来计算相似度。</p><p>我们对几种机器翻译和图像字幕基准进行了评估，结果表明BERTScore与人为判断的关联性比现有指标更好，通常甚至大大超过特定于任务的监督指标。</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>在本文中，我们将重点放在句子级别的生成评估上，并提出了：BERTScore，这是一种基于预训练的BERT上下文嵌入 （bert）的评估指标。 BERTScore将两个句子之间的相似度计算为它们的标记之间的余弦相似度的加权汇总。</p><p>基于n-gram matching metric 的常见缺陷：</p><ul><li><p>semantically-correct phrases are penalized because they differ from the surface form of the reference.</p><p>解决： In contrast to string matching (e.g., in BLEU) or matching heuristics (e.g., in METEOR), we compute similarity using contextualized token embeddings, which have been shown to be effective for paraphrase detection  </p></li><li><p>n-gram models fail to capture distant dependencies and penalize semantically-critical ordering changes.</p><p>解决： contextualized embeddings are trained to effectively capture distant dependencies and ordering  </p></li></ul><p>实验结果：（1）In machine translation, BERTSCORE shows stronger system-level and segment-level correlations<br>with human judgments than existing metrics on multiple common benchmarks.（2）BERTSCORE is well-correlated with human annotators for image captioning, surpassing SPICE.</p><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><ul><li>见论文，比较好理解</li></ul><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ul><li>Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance  <ul><li>同样尝试使用contextual word embeddings  来构建一个metric.</li></ul></li><li>Putting evaluation in context: Contextual embeddings improve machine translation evaluation. In ACL, 2019.  </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h4&gt;&lt;p&gt;我们提出BERTScore，这是一种用于文本生成的自动评估指标。&lt;/p&gt;
&lt;p&gt;类似于通用指标，BERTScore计算候选句子中每个toke
      
    
    </summary>
    
      <category term="自然语言理解" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3/"/>
    
    
      <category term="自然语言理解" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3/"/>
    
  </entry>
  
  <entry>
    <title>Grounded Situation Recognition</title>
    <link href="http://yoursite.com/2020/03/31/Grounded-Situation-Recognition/"/>
    <id>http://yoursite.com/2020/03/31/Grounded-Situation-Recognition/</id>
    <published>2020-03-31T02:19:26.000Z</published>
    <updated>2020-04-01T03:21:19.045Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Grounded-Situation-Recognition-Task"><a href="#Grounded-Situation-Recognition-Task" class="headerlink" title="Grounded Situation Recognition Task"></a>Grounded Situation Recognition <strong>Task</strong></h4><h5 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h5><ul><li><p>以前的situation recognition task: </p><p><strong>Situation Recognition</strong> is the task of recognizing the activity happening in an image, the actors and objects involved in this activity, and the roles they play. Semantic roles describe how objects in the image participate in the activity described by the verb. </p><p>While situation recognition addresses <strong><em>what</em></strong> is happening in an image, <strong><em>who</em></strong> is playing a part in this and <strong><em>what</em></strong> their roles are, it does not address a critical aspect of visual understanding: <strong>where</strong> the involved entities lie in the image. </p></li><li><p>本文：We address this shortcoming and present <strong>Grounded Situation Recognition (GSR)</strong>, a task that builds upon situation recognition and requires one to not just identify the situation observed in the image but also visually ground the identified roles within the corresponding image.</p></li></ul><h4 id="Challenge-of-Grounded-Situation-Recognition-GSR"><a href="#Challenge-of-Grounded-Situation-Recognition-GSR" class="headerlink" title="Challenge of Grounded Situation Recognition (GSR)"></a>Challenge of Grounded Situation Recognition (GSR)</h4><ul><li><em>语义显著性</em>：与识别图像中的所有实体不同，它需要在呈现的<strong>主要活动的背景下</strong>识别关键对象和参与者。</li><li><em>语义稀疏性</em>：GSR存在语义稀疏性问题，  在训练中很少见到role and groundings 的许多组合。这一挑战要求模型从有限的数据中学习。</li><li><em>Ambiguity</em>：将角色定位到图像中通常需要消除在同一类别下的多个观察到的实体之间的歧义。</li><li><em>Scale</em>：grounded entities 的比例尺变化很大，图像中也缺少某些实体（在这种情况下，模型负责检测这种缺失）。</li><li><em>Hallucination</em>：标记语义角色并grounding 通常需要弄清物体的存在，因为它们可能被完全遮挡或不在屏幕上。</li></ul><h4 id="Situations-With-Groundings-SWiG-dataset"><a href="#Situations-With-Groundings-SWiG-dataset" class="headerlink" title="Situations With Groundings (SWiG) dataset"></a>Situations With Groundings (SWiG) dataset</h4><p><a href="https://prior.allenai.org/assets/project-content/gsr/gsr_banner.png" target="_blank" rel="noopener"><img src="https://prior.allenai.org/assets/project-content/gsr/gsr_banner.png" alt="SWiG examples">A sample of images from the SWiG dataset</a></p><p>We present the Situations With Groundings (SWiG) Dataset for training and evalutation on the GSR task. This dataset builds upon the <a href="https://homes.cs.washington.edu/~ali/papers/SituationRecognition.pdf" target="_blank" rel="noopener">Situation Recognition dataset</a> presented by Yatskar et al. The SWiG dataset contains approximately 125,000 images. Each image is associated with one verb. Three different annotators then label each <strong>entity</strong> in the frame associated with that <strong>verb</strong> and mark the <strong>location</strong> of the entity in the image. All three labels for each role are given in the SWiG dataset as well as an average of the three localizations.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Grounded-Situation-Recognition-Task&quot;&gt;&lt;a href=&quot;#Grounded-Situation-Recognition-Task&quot; class=&quot;headerlink&quot; title=&quot;Grounded Situation Rec
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Multi-task Collaborative Network for Joint Referring Expression Comprehension and Segmentation</title>
    <link href="http://yoursite.com/2020/03/30/Multi-task-Collaborative-Network-for-Joint-Referring-Expression-Comprehension-and-Segmentation/"/>
    <id>http://yoursite.com/2020/03/30/Multi-task-Collaborative-Network-for-Joint-Referring-Expression-Comprehension-and-Segmentation/</id>
    <published>2020-03-30T09:26:57.000Z</published>
    <updated>2020-03-31T01:40:39.416Z</updated>
    
    <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>VIOLIN: A Large-Scale Dataset for Video-and-Language Inference</title>
    <link href="http://yoursite.com/2020/03/28/VIOLIN-A-Large-Scale-Dataset-for-Video-and-Language-Inference/"/>
    <id>http://yoursite.com/2020/03/28/VIOLIN-A-Large-Scale-Dataset-for-Video-and-Language-Inference/</id>
    <published>2020-03-28T02:43:22.000Z</published>
    <updated>2020-03-31T01:43:47.276Z</updated>
    
    <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Object Hallucination in Image Captioning </title>
    <link href="http://yoursite.com/2020/03/25/Object-Hallucination-in-Image-Captioning/"/>
    <id>http://yoursite.com/2020/03/25/Object-Hallucination-in-Image-Captioning/</id>
    <published>2020-03-25T12:19:50.000Z</published>
    <updated>2020-03-27T03:03:36.395Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul><li>当前captioning task 存在的问题<ul><li>当前的caption model 目前存在的问题是，生成的句子中出现的object 常常是在corresponding vision scene 中没有出现到的。</li><li>当前使用的评价指标只能评估 candidate caption 与 gt captions 之间的一个相似性，不能捕捉到candidate caption 与 image information之间的relevance. </li></ul></li></ul><ul><li>因此本文进行的一个工作：<ul><li>We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination.  </li></ul></li></ul><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><ul><li><p>（一）关于 object hallucination 的四点讨论</p><ul><li><p>从人类的角度</p><ul><li>丢失对显著物体的描述是一个 failure mode，但是captions is summaries，<strong>因此一般不期待其描述出场景中的所有的objects.</strong> 另外在人类的标注数据中，也不偏向于标注出所有出现在scene 中的objects</li><li>研究报告表明，human judgements 比较不待见那些caption中包含了image content中未出现的 object，*<em>Correctness is more important to human judges than specificity.   *</em></li><li>Many visually impaired who *<em>value correctness over coverage, *</em>hallucination is an obvious concern.  </li></ul></li><li><p>从模型的角度</p><ul><li>object hallucination  揭示了caption model 存在的一个问题，可能caption model并没有对视觉场景学习到一个很好的视觉表征，而是对损失函数过拟合。</li></ul></li></ul></li><li><p>（二）本文要研究的问题</p><ul><li>本文研究当前captioning models中存在的object hallucination现象</li><li>考虑了几个关键问题：<ul><li>(1) <strong>Which models are more prone to hallucination?</strong>  spanning different architectures and learning objectives.   <ul><li>一个新的评价指标来评估object hallucination：CHAIR (Caption Hallucination Assessment with Image Relevance)  </li></ul></li><li>(2) *<em>What are the likely causes of hallucination?   *</em><ul><li>造成object hallucination这一现象的原因主要有两点：visual misclassification and over-reliance on language priors  <ul><li>提出：image and language model consistency scores  </li></ul></li></ul></li><li>(3) <strong>How well do the standard metrics capture hallucination?</strong>  <ul><li>当前的评价指标并不能很好的捕捉到object hallucination 这一现象。</li></ul></li></ul></li></ul></li></ul><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><h5 id="CHAIR-Metric"><a href="#CHAIR-Metric" class="headerlink" title="CHAIR Metric"></a>CHAIR Metric</h5><ul><li>同时使用GT sentence 和 coco image segmentation这两个信息 to measure object hallucination。</li></ul><p>  <img src="http://ww1.sinaimg.cn/large/006uWRWVly1gd7d25xmjcj30h605k0te.jpg" alt="搜狗截图20200326152719.png"></p><h5 id="Image-Consistency"><a href="#Image-Consistency" class="headerlink" title="Image Consistency"></a>Image Consistency</h5><ul><li>对比 <strong>caption model 与 image (alone) model</strong> 两个模型对于预测objects 之间的一致性误差。</li></ul><h5 id="Language-Consistency"><a href="#Language-Consistency" class="headerlink" title="Language Consistency"></a>Language Consistency</h5><ul><li>对比 <strong>caption model 与 sentence (alone) model</strong> 两个模型对于预测下一个word 之间的一致性误差。</li></ul><h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><h5 id="Which-Models-Are-More-Prone-To-Hallucination"><a href="#Which-Models-Are-More-Prone-To-Hallucination" class="headerlink" title="Which Models Are More Prone To Hallucination?"></a>Which Models Are More Prone To Hallucination?</h5><ul><li>一般情况下 ，在标准的evaluation metrics 上表现性能好的模型，在CHAIR metric 上也能表现的比较好，即object hallucination现象相对较弱。但是当模型基于 cider 进行强化学习的训练之后，则不是这种一致的现象。</li><li>（1）使用attention 的模型更加偏向于有较低的object hallucination；NBT模型在标准的evaluation metrics 上表现性能没有topdown-BB 好，但是CHAIR性能却更好，原因在于其使用的pre-trained  detector 与 captioning dataset is in a same domain 。</li><li>（2）当模型基于 cider 进行强化学习的训练之后，将会增加hallucination 的数量。</li><li>（3）LRCN Model 比 FC Model有更多的object hallucination，说明仅仅在第一个 time step 输入视觉特征，将会产生更少的object hallucination </li><li>（4）the GAN loss actually helps decrease hallucination.  the GAN loss encourages sentences to be human-like。</li><li>（5）CE loss: beam size 5, object hallucination 会比 lower beam size 小很多；self-critical loss: beam size sometimes leads to worse performance on CHAIR.   即object hallucination 数量会更多。</li></ul><h5 id="What-Are-The-Likely-Causes-Of-Hallucination"><a href="#What-Are-The-Likely-Causes-Of-Hallucination" class="headerlink" title="What Are The Likely Causes Of Hallucination?"></a>What Are The Likely Causes Of Hallucination?</h5><ul><li><p>We rely on the deconstructed TopDown models to analyze the impact of model components on hallucination  </p><ul><li>通过设计的 几个 deconstructed TopDown models 的分析可以看出，使得object hallucination 数量减少的原因是：due to access to feature maps with spatial locality, not the actual attention mechanism.  </li><li>LRCN Model 比 FC Model 有更多的object hallucination，说明仅仅在第一个 time step 输入视觉特征，将会产生更少的object hallucination 。作者在文中对这一现象给出的解释是，在每一步都输入视觉特征 fc_feature, 而不是 spatial feature, 这将导致对视觉特征的过拟合。</li></ul></li><li><p>Investigate what causes hallucination using the deconstructed TopDown models and the image consistency and language consistency scores. </p><ul><li>We note that models with less hallucination tend to make errors consistent with the image model, whereas models with more hallucination tend to make errors consistent with the  language model.  这说明有更少object hallutition 的models 有更强的能力从Image 中提取知识到句子生成过程中。</li><li>在Robust split 上进行实验发现，所有models之间的language consistency 差异度不大；相比于 Karpathy split，相对应下的models image consistency 有所下降。这是由于 Robust split 在测试集上，会出现 novel compositions of objects at test time. 使得所有的模型有很强的language prior.</li></ul></li><li><p>在训练过程中，分析FC model 的image/language consistency，结果发现在训练开始，与language model 的一致性更好，随着训练的结束，与 image model 的一致性更好。这说明，模型首先学习生成流畅的语言，而后再去学习结合视觉信息。</p><h5 id="How-Well-Do-The-Standard-Metrics-Capture-Hallucination"><a href="#How-Well-Do-The-Standard-Metrics-Capture-Hallucination" class="headerlink" title="How Well Do The Standard Metrics Capture Hallucination?"></a>How Well Do The Standard Metrics Capture Hallucination?</h5></li><li><p>作者分析了 standard metric 与 CHAIRs  之间的 pearsom correlation coefficient ，结果发现 SPICE 的一致性更好。</p></li><li><p>object hallucination can not be always predicted based on the traditional sentence metrics.  </p></li><li><p><strong>与当前使用的standard metrci 互为补充，可以提升与人类评分的一致程度。</strong></p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gd8997jm24j30gi08375g.jpg" alt="搜狗截图20200327100117.png"></p><p>这个意思是说，第一列，单独分析各个automatic metric 与 human judgement 的一致性。第二/三列，各个评价指标分别加上1-CHs/ 1-CHi 之后再与human judgement 计算一致性。可以发现，一致性得到提升。即 <strong>CHAIR is complementary to standard metrics</strong></p></li></ul><h5 id="Does-hallucination-impact-generation-of-other-words"><a href="#Does-hallucination-impact-generation-of-other-words" class="headerlink" title="Does hallucination impact generation of other words?"></a>Does hallucination impact generation of other words?</h5><ul><li>Hallucinating objects 影响句子生成的质量，不仅是由于 object 没有被正确的预测，也是由于hallucinated word 影响到了生成的其他的words.</li><li>通过比较TopDown 和 TD-Restrict 生成的句子可以分析这个现象。We find that after the hallucinated word is generated, the following words in the sentence are different 47.3% of  the time.  </li><li>一旦一个hallucination words 被生成，则其又会由于language prior(hallucinating a “cat” leading to hallucinating<br>a “chair”  )，产生更多的hallucination words 。</li></ul><h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4><ul><li><p>the popular self critical loss increases CIDEr score, but also the amount of hallucination. </p></li><li><p>CHAIR complements the standard sentence metrics in capturing human preference( judgements ).  </p></li><li><p>Models with stronger image consistency frequently hallucinate fewer objects, suggesting that strong visual processing is important for  avoiding hallucination.  </p></li><li><p><strong>Advises for captioning task:</strong> 仅使用CE-loss/ standard sentence metrics来优化，不太能解决object hallucination 这个问题，若同时以 image relevance 来优化，会更好。</p></li></ul><h4 id="yaya-总结"><a href="#yaya-总结" class="headerlink" title="yaya 总结"></a>yaya 总结</h4><ul><li><p>在设计评价指标上给我的几点启发</p><ul><li>(1) 不需要要求machine generated caption 可以概括所有的objects which have been occurred in the vision scene</li><li>(2) machine generated caption 进行评价时，正确性比全面性更加重要</li></ul></li><li><p>本文的一个巧妙的点</p><ul><li>本文为了查看object hallucination，使用COCO的80个类。对于candidata caption 首先将其token， 然后调整成单数形式，然后使用同义词的思想，去跟COCO 的80个类别进行匹配。</li><li>另外对于GT sentences，也提出一个list，这个地方不太知道它说的什么意思？？？？？  </li></ul></li><li><p>GVD 好像也类似的提到过类似的思想</p></li><li><p>关于the image consistency and language consistency scores.</p><ul><li>在这个得分的计算方式上，是否还有什么可以改进的地方？</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;当前captioning task 存在的问题&lt;ul&gt;
&lt;li&gt;当前的capti
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>NLP: 自编码 and 自回归</title>
    <link href="http://yoursite.com/2020/03/24/NLP-%E8%87%AA%E7%BC%96%E7%A0%81-and-%E8%87%AA%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2020/03/24/NLP-自编码-and-自回归/</id>
    <published>2020-03-24T05:44:04.000Z</published>
    <updated>2020-03-31T02:21:34.096Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p><a href="https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff" target="_blank" rel="noopener">https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff</a>      </p></li><li><p>这篇博文写的不错<br><a href="https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff" target="_blank" rel="noopener">https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.infoq.cn/article/4SRM7UMV
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions</title>
    <link href="http://yoursite.com/2020/01/16/VIFIDEL-Evaluating-the-Visual-Fidelity-of-Image-Descriptions/"/>
    <id>http://yoursite.com/2020/01/16/VIFIDEL-Evaluating-the-Visual-Fidelity-of-Image-Descriptions/</id>
    <published>2020-01-16T01:41:25.000Z</published>
    <updated>2020-01-16T14:31:11.909Z</updated>
    
    <content type="html"><![CDATA[<h3 id="当前指标存在的问题"><a href="#当前指标存在的问题" class="headerlink" title="当前指标存在的问题"></a>当前指标存在的问题</h3><ul><li>BLEU, ROUGE, Meteor, CIDEr 这些指标， 他们依靠精确的字符串匹配来测量 condidate 文本和reference文献之间的surface-level 、 n-gram 重叠。当 references 有限的情况下，这会导致样本稀疏问题 （reference数量对 metric 得分有很大影响，因为reference 数量越多，多样性更好）。Meteor 通过匹配字典和释义表中的同义词来部分解决此问题，但受限于此类字典的可用性，也不能很好地适用于其他的 language。SPICE and BAST 通过计算语义级别的相似性来解决 exact string matching。但是这个方法严重的依赖于语言资源，例如 parsers, semantic role labellers, tailored rules, 使其很难适应到不同的语言和领域。</li></ul><h3 id="仅仅使用-reference-description-来-评估-image-description-的缺点"><a href="#仅仅使用-reference-description-来-评估-image-description-的缺点" class="headerlink" title="仅仅使用 reference description 来 评估 image description 的缺点"></a>仅仅使用 reference description 来 评估 image description 的缺点</h3><ul><li>受限于 reference 的数量，可能会造成<strong>样本稀缺</strong>的问题。</li><li>reference description 是<strong>主观的，有歧义的</strong>，可能不能涵盖 image 中所有的关键信息，可能只包含 image content 的一个子集。<strong>使用 object labels 可以解决这个问题</strong> </li><li>references 可能含有错误的信息。</li></ul><h3 id="基于-object-information-来-作为评价指标的优点"><a href="#基于-object-information-来-作为评价指标的优点" class="headerlink" title="基于 object information 来 作为评价指标的优点"></a>基于 object information 来 作为评价指标的优点</h3><ul><li><strong>少的标注时间消耗</strong>： 若仅使用 multiple descriptions 来作为参考，则必然需要人类为 每个 image 来标注 多个 descriptions，在标注数据上需要花费很多时间。且为每个 image 标注的description 数量越多，评估的越准确，则也需要更多的标注时间。</li><li>但是若使用基于 object imformation ， 则可以使用 predicted objects 或者 object annotations</li></ul><h3 id="Modelling-object-importance-with-reference-descriptions"><a href="#Modelling-object-importance-with-reference-descriptions" class="headerlink" title="Modelling object importance with reference descriptions"></a>Modelling object importance with reference descriptions</h3><ul><li><p>human reference 可以作为一种guidance 提供信息 – 人类对该张图片关注的重点在哪里。</p></li><li><p>与 CIDEr 很相似，都是考虑与 human reference 之间的consensus 信息，但是有以下两点不同：  </p><p>（1）使用reference 来建模object的重要性，而不是直接将 候选与 参考进行比较。<br>（2）在语义空间使用word embedding来执行 word matching，而不是直接在计算表面的匹配度（eg: n-gram）   </p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;当前指标存在的问题&quot;&gt;&lt;a href=&quot;#当前指标存在的问题&quot; class=&quot;headerlink&quot; title=&quot;当前指标存在的问题&quot;&gt;&lt;/a&gt;当前指标存在的问题&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;BLEU, ROUGE, Meteor, CIDEr 这些指标， 他们依
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>TIGEr: Text-to-Image Grounding for Image Caption Evaluation</title>
    <link href="http://yoursite.com/2020/01/15/TIGEr-Text-to-Image-Grounding-for-Image-Caption-Evaluation/"/>
    <id>http://yoursite.com/2020/01/15/TIGEr-Text-to-Image-Grounding-for-Image-Caption-Evaluation/</id>
    <published>2020-01-15T02:20:39.000Z</published>
    <updated>2020-01-15T06:31:18.079Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li><p>当前在图像描述领域使用的 automatic metric 仅仅考虑了 gt 与 pred sentence 之间的匹配度。这就存在问题：（1）给出的 references 可能不能  fully cover the image content。（2）自然语言本质上就是有歧义的（模棱两可的）</p></li><li><p>因此提出了 TIGEr，该指标，（1）不仅可以评估 pred caption 与  image content 之间的匹配度，（2）也能评估 pred caption 与 gt caption 之间的匹配度</p></li><li><p>（1） 对于  pred caption 与 gt caption， 使用预训练的 image-text grounding model 来 grounds the content of texts。然后分别比较 relevance ranking 和 distribution of grounding weights。</p></li><li><p>（2）计算 pred 与 gt caption 之间的匹配度时，不采用 n-gram matching，而是将他们映射到一个共同的语义空间，再对得到的映射向量进行比较。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;当前在图像描述领域使用的 automatic metric
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Graph Matching Networks for Learning the Similarity of Graph Structured Objects</title>
    <link href="http://yoursite.com/2019/12/20/Graph-Matching-Networks-for-Learning-the-Similarity-of-Graph-Structured-Objects/"/>
    <id>http://yoursite.com/2019/12/20/Graph-Matching-Networks-for-Learning-the-Similarity-of-Graph-Structured-Objects/</id>
    <published>2019-12-20T08:32:12.000Z</published>
    <updated>2019-12-20T09:30:49.974Z</updated>
    
    <content type="html"><![CDATA[<ul><li>ICML 2019</li></ul><h3 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h3><ul><li>本文主要是提出了一种 新的方法来计算图的相似度问题</li><li>普通的方法分别单独计算 graph vector，而后再计算graph 之间的相似性</li><li>新提出的方法在计算  graph vector 时考虑了 cross graph matching vector来得到 更具有判别性的 graph vector，从而更好的用于 计算 graph similarity.</li></ul><h3 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h3><ul><li>Graph Edit Distance（GED）：文中对问题进行了简化，两个graph（G1, G2），相同数量的节点数和边数, 如何变动一个图中的edge(i, j) 到 edge(i’, j’) 才能使两个graph 完全一样。以下是 GED=1 的例子。</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ga3ats0xyzj308p07jaaw.jpg" alt="搜狗截图20191220170651.png"></p><ul><li>该文主要是想解决graph 的相似性问题，而不是真正的要求解出来需要几步的 graph edit distance。因此对问题做了如下的设定：<code>positive pair:（原图G，对G变动一条边：G1）</code>，<code>negative pair: （原图G，对G变动两条边：G1）</code></li><li>positive pair 认为这两个 graph 是相似的，label=1; 而negative pair认为两个graph 是不相似的, label=-1。</li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ga3b3udt8bj30pe0bsgnq.jpg" alt="搜狗截图20191220171642.png"></p><ul><li><p>一般的计算两个graph之间的相似性问题采用上图中的左图的方法，分别单独计算出 graph vector，而后再计算 vector space similarity</p></li><li><p>而本文：计算两个graph之间的匹配，然后互相作为补充特征（cross-graph matching vector），得到更加 <strong>discriminative</strong>  graph representation， 从而更加有效的graph 之间的相似度问题。</p></li><li><p>yaya: 文中使用的是 <code>difference between node_i and its closest neighbor in the other graph</code>  来计算  <code>cross-graph matching vector</code> 。我认为还可以有其他的方法或许会更加有效。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;ICML 2019&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;简述&quot;&gt;&lt;a href=&quot;#简述&quot; class=&quot;headerlink&quot; title=&quot;简述&quot;&gt;&lt;/a&gt;简述&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;本文主要是提出了一种 新的方法来计算图的相似度问题&lt;/li&gt;
&lt;li
      
    
    </summary>
    
      <category term="图卷积网络" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="图卷积网络" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Quality Estimation for Image Captions Based on Large-scale Human Evaluations</title>
    <link href="http://yoursite.com/2019/12/12/Quality-Estimation-for-Image-Captions-Based-on-Large-scale-Human-Evaluations/"/>
    <id>http://yoursite.com/2019/12/12/Quality-Estimation-for-Image-Captions-Based-on-Large-scale-Human-Evaluations/</id>
    <published>2019-12-12T06:14:06.000Z</published>
    <updated>2019-12-12T06:55:51.636Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Quality-Estimation-QE-of-image-captions"><a href="#Quality-Estimation-QE-of-image-captions" class="headerlink" title="Quality Estimation (QE) of image-captions"></a>Quality Estimation (QE) of image-captions</h3><ul><li>本文提出了在图像描述领域一个新的问题，Quality Estimation。由于当前的 automatic metric 非常依赖 ground-truth references，因此当一个模型训练好后，若是对一个 unseen images which don’t have gt sentence 进行描述，则无法对该描述进行评价。    </li></ul><h3 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h3><h4 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h4><ul><li>（1）首先在 Conceptual Captions dataset  上训练多个 image-captioning model （这个数据集会在比 coco上训练的captioning model 更好），captioning model的差异可以体现在 image feature extraction model，object detection提取的object 数量，caption decoder。    </li><li>（2）以上的 image-captioning model 可以为一个image 提供多个 sentence，作者对image 进行了脱敏处理    </li></ul><h4 id="数据打分及处理"><a href="#数据打分及处理" class="headerlink" title="数据打分及处理"></a>数据打分及处理</h4><ul><li><p>（1）这些 image-caption pairs 放到 crowdsource.google上让大家对这些 captioning，进行评价：好、坏或者跳过。每个image-captioning pair 被分配给10个人进行打分     </p></li><li><p>（2）得到收集的 rating image-captioning pairs 之后，对 unique image，将10个评分进行处理， using the equation y = round(mean(ri) ∗ 8)/8.     </p></li></ul><h4 id="QE-Model"><a href="#QE-Model" class="headerlink" title="QE Model"></a>QE Model</h4><ul><li>本文作者设计了两个模型（并把这两个模型进行融合）来处理 QE task。一个是使用到 image-captioning model，两一个是不使用   </li><li>（1）使用image-captioning model：<strong>Confidence-based Features QE Model</strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVgy1g9txhzr2swj30yc0i642k.jpg" alt="搜狗截图20191212143453.png"></li><li>（2） 不使用 image-captioning model：<strong>Generation-independent Bilinear QE model</strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVgy1g9txhzmt4rj30xs0apq5u.jpg" alt="搜狗截图20191212143515.png"></li></ul><h4 id="Spearman’s-ρ-Analysis"><a href="#Spearman’s-ρ-Analysis" class="headerlink" title="Spearman’s ρ Analysis"></a>Spearman’s ρ Analysis</h4><ul><li>该文的主要目的就是希望在 没有gt sentence 的情况下，对 unseen-image 进行描述时，可以给出一个caption 的评分。或者是说，该captioning与 人类的描述的相近程度。</li><li>该任务也是希望提出一个 machine learning metric similar to human evaluation （trained-metric），则对该模型好坏的一个的评判就是这个模型给出的评分与人类评分的相近程度。</li><li>predict： 模型对image-caption pair 的评分， Gt:  人类给出的评分</li><li>指标：Spearman’s correlation.  <a href="https://github.com/ShiYaya/spearman-rank" target="_blank" rel="noopener">my github explanation</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Quality-Estimation-QE-of-image-captions&quot;&gt;&lt;a href=&quot;#Quality-Estimation-QE-of-image-captions&quot; class=&quot;headerlink&quot; title=&quot;Quality Estima
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>pth: save in py2, but load in py3</title>
    <link href="http://yoursite.com/2019/12/11/pth-save-in-py2-but-load-in-py3/"/>
    <id>http://yoursite.com/2019/12/11/pth-save-in-py2-but-load-in-py3/</id>
    <published>2019-12-11T06:28:46.000Z</published>
    <updated>2019-12-11T06:38:09.904Z</updated>
    
    <content type="html"><![CDATA[<h3 id="在torch-load-pth-时出现的问题：UnicodeDecodeError-39-utf-8-39-codec-can-39-t-decode-byte-0xba-in-position-0-invalid-start-byte"><a href="#在torch-load-pth-时出现的问题：UnicodeDecodeError-39-utf-8-39-codec-can-39-t-decode-byte-0xba-in-position-0-invalid-start-byte" class="headerlink" title="在torch.load(*.pth) 时出现的问题：UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xba in position 0: invalid start byte"></a>在torch.load(*.pth) 时出现的问题：<code>UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xba in position 0: invalid start byte</code></h3><ul><li><p>经过网络查询，发现是由于该文件是在 python2 下保存的，但是现在却是在python3下读取，而导致的错误</p></li><li><p>有的人给出了下面的解决方案(但是对于我是无效的)：    </p><p>来自：<a href="https://github.com/CSAILVision/places365/issues/25" target="_blank" rel="noopener">https://github.com/CSAILVision/places365/issues/25</a></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools import partial</span><br><span class="line">import pickle</span><br><span class="line">pickle.load = partial(pickle.load, <span class="attribute">encoding</span>=<span class="string">"latin1"</span>)</span><br><span class="line">pickle.Unpickler = partial(pickle.Unpickler, <span class="attribute">encoding</span>=<span class="string">"latin1"</span>)</span><br><span class="line">model = torch.load(model_file, <span class="attribute">map_location</span>=lambda storage, loc: storage, <span class="attribute">pickle_module</span>=pickle)</span><br></pre></td></tr></table></figure></li></ul><ul><li><p><strong>这里给出我的解决办法</strong>   </p><p>（1） 在python2 环境下读取该文件，然后用 pickle来保存   </p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tmp_data = torch.<span class="built_in">load</span>(model_file)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'tmp.pickle'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> <span class="built_in">file</span>:</span><br><span class="line">    pickle.dump(tmp_data, <span class="built_in">file</span>, protocol=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>（2）换到python3环境下，再读取pickle文件，再用torch.load来保存（这一点或可以省略）</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'tmp.pickle'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> <span class="built_in">file</span>:</span><br><span class="line">    tmp_data = pickle.<span class="built_in">load</span>(<span class="built_in">file</span>, encoding=<span class="string">'latin1'</span>)</span><br><span class="line">    </span><br><span class="line">torch.save(tmp_data, tmp_model.pth)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;在torch-load-pth-时出现的问题：UnicodeDecodeError-39-utf-8-39-codec-can-39-t-decode-byte-0xba-in-position-0-invalid-start-byte&quot;&gt;&lt;a href=&quot;#在t
      
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video</title>
    <link href="http://yoursite.com/2019/12/02/Weakly-Supervised-Spatio-Temporally-Grounding-Natural-Sentence-in-Video/"/>
    <id>http://yoursite.com/2019/12/02/Weakly-Supervised-Spatio-Temporally-Grounding-Natural-Sentence-in-Video/</id>
    <published>2019-12-02T03:36:02.000Z</published>
    <updated>2019-12-03T07:13:39.369Z</updated>
    
    <content type="html"><![CDATA[<ul><li><strong>ACL 2019</strong></li></ul><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li><p>image grounding取得了很大的进步，但是将该任务迁移到视频上，需要对视频中的每帧都标注region，这个工程量是巨大的。</p></li><li><p>为了避免这种标注工作的工程量，一些<strong>weakly-supervised</strong> video grouding工作【1】【2】被提出来，他们只提供了video-sentence pairs，没有提供 fine-grained regional annotations。在他们的 video grounding任务中，他们仅仅对名词和代词在 视频的静态帧进行grounding。</p></li><li><p>但是这种 grounding存在问题，比如sentence: A brown and white dog is lying on the grass and then it stands up. 但是帧中出现了多个狗，而我们给出的要搜索的对象仅仅是一个名词： ‘dog’，没有其他更多的信息，来进行更加具体地定位，那么就有可能定位错误。另外只对一张静态帧进行定位，也无法捕捉到object在时域上的动态变化。</p></li><li><p>基于上述的分析，本文提出了一个在video grounding上 weakly-supervised 的新任务：<strong>weakly-supervised spatio-temporally grounding sentence in video (WSSTG).</strong>    </p></li></ul><h3 id="Weakly-supervised-spatio-temporally-grounding-sentence-in-video"><a href="#Weakly-supervised-spatio-temporally-grounding-sentence-in-video" class="headerlink" title="Weakly-supervised spatio-temporally grounding sentence in video"></a>Weakly-supervised spatio-temporally grounding sentence in video</h3><ul><li>Specifically, given a natural sentence and a video, we aim to localize a spatio-temporal tube (i.e., a sequence of bounding boxes) ,（本文中作者将tube 称作 instance）</li><li>yaya: 相比于之前的video-grounding任务，同是 weakly-supervised，但是有两点不同：（1）是句子级别的描述，对要定位的对象的描述更加具体，而不是仅仅是个noun。（2）是要定位出一个 spatial-temporal tube，而不是仅在一张静态帧中定位出一个bbox。</li><li>这两点不同同时带来了优势和挑战</li><li>（1）细节性的描述可以消除歧义，但是如何捕捉句子中的语义并在video中定位出来是一个难题；（2）相比于在静态帧中定位一个bbox, 而是在video中定位一个tube,更能呈现出一个object在时域上的动态。但是，如何利用和建模tube的时空特性以及它们与句子的复杂关系提出了另一个挑战。</li><li>compared with 【2】: different from 【2】，whose text input consists of nouns/pronouns and output is a bounding box in a specific frame, we aim  to ground a natural sentence and output a spatio-temporal tube in the video. </li></ul><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul><li>提出了一个新任务：weakly-supervised spatio-temporally grounding sentence in video</li><li>针对该任务提出了一个method：提出了一个Attentive interactor利用 tube(instance) 与 sentence之间的细粒度的关系来计算 匹配度；提出了一个diversity loss来加强 reliable instance-sentence pairs 并惩罚 unreliable ones。</li><li>在VID object detection dataset 数据集的基础上，对tube(instance) 增加了description</li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><ul><li>该任务是 给出一个 a natural sentence query <strong>q</strong> and a video <strong>v</strong> 来定位一个spatial-temporal tube，作者也将这个tube 称作 instance。</li><li>由于是弱监督，因此仅仅只给出 video-sentence pair，细粒度的regional annotations不给出！</li><li>将该任务转为一个 Multiple instance learning problem。给定一个video，首先由instance generator【3】来生成一组instance proposals，然后再根据语义相似性来匹配 natural sentence query 和 instance。  </li></ul><h4 id="Instance-Extraction"><a href="#Instance-Extraction" class="headerlink" title="Instance Extraction"></a>Instance Extraction</h4><ul><li><strong>Instance Generation</strong> ：  先由faster rcnn提取object proposals，假设每帧提取N个proposal ， 然后根据【3】得到N个spatial-temporal tube</li><li><strong>Feature Representation</strong> ：I3D-RGB， I3D-Flow， frame-level RoI pooled feature   </li></ul><h4 id="Attentive-Interactor"><a href="#Attentive-Interactor" class="headerlink" title="Attentive Interactor"></a>Attentive Interactor</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g9io2sodynj30i40hk0v9.jpg" alt="搜狗截图20191202204720.png"></p><ul><li>（1）分别对 sequential visual features 和 sequential textual features 经过LSTM进行编码，LSTM每个step输出的隐层状态作为新的representation，得到新的visual feature 和 sentence representation</li><li>（2）依次以visual feature中的每个隐状态作为查询，以 sentence 所有隐状态作为key 和 value，输入Attention中，则得到了<strong>visual guided sentence feature</strong>。（直观的理解：在给定某一个视觉特征，用attention去分析要关注哪一个word）  </li></ul><h4 id="Matching-Behavior-Characterization"><a href="#Matching-Behavior-Characterization" class="headerlink" title="Matching Behavior Characterization"></a>Matching Behavior Characterization</h4><ul><li>用余弦函数计算 <code>i-th</code> visual feature 和 visual guided sentence features 之间的 匹配度</li><li>对所有的step 加和，则得到instance proposal 与 sentence 之间的匹配度</li></ul><h3 id="Training-Loss"><a href="#Training-Loss" class="headerlink" title="Training Loss"></a>Training Loss</h3><ul><li><p>论文对这里介绍的比较详细，参见论文。</p></li><li><p><strong>ranking loss</strong>： aiming at distinguishing aligned video-sentence pairs from the unaligned ones.  这个损失是希望不匹配的video-sentence之间计算出来的匹配度差一些，比如给网络输入不与该视频对应的句子。</p></li><li><p><strong>novel diversity loss</strong> ：to strengthen the matching behaviors between reliable instance-sentence pairs and penalize the unreliable ones from the aligned video-sentence pair.  这个损失主要是希望对一个video，在计算tube 与 sentence之间的匹配度时，希望不同的 tube之间的差异性（diversity）大一些！</p></li></ul><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul><li>一个video 给出了N个 tube proposal，当计算完匹配度之后，选取匹配度最大的那个proposal，然后计算与GT之间的 overlap【4】，若overlap 大于一个阈值，则任务预测正确。</li></ul><h3 id="Yaya-Analysis："><a href="#Yaya-Analysis：" class="headerlink" title="Yaya Analysis："></a>Yaya Analysis：</h3><ul><li><p><strong>此类任务可提升的point</strong></p></li><li><p>更好的 detector来获取 object proposal</p></li><li><p>更好的算法来获取 tube proposal</p></li><li><p>设计算法更好滴计算 sentence 与 tube proposal 匹配度！</p></li><li><p>对 rank loss 给予更多的约束，像此文：提出了一个novel  diversity loss</p></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li>【1】De-An Huang, Shyamal Buch, Lucio Dery, Animesh Garg, Li Fei-Fei, and Juan Carlos Niebles. 2018. <strong>Finding “it”: Weakly-supervised reference-aware visual grounding in instructional videos</strong>. In CVPR. </li><li>【2】Luowei Zhou, Nathan Louis, and Jason J Corso. 2018. <strong>Weakly-supervised video object grounding from text by loss weighting and object interaction</strong>. BMVC. </li><li>【3】Georgia Gkioxari and Jitendra Malik. 2015. <strong>Finding action tubes</strong>. In CVPR, pages 759–768. </li><li>【4】Masataka Yamaguchi, Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. 2017. <strong>Spatio-temporal person retrieval via natural language queries</strong>. In ICCV. </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ACL 2019&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation
      
    
    </summary>
    
      <category term="Visual Grounding" scheme="http://yoursite.com/categories/Visual-Grounding/"/>
    
    
      <category term="Viusal Grounding" scheme="http://yoursite.com/tags/Viusal-Grounding/"/>
    
  </entry>
  
  <entry>
    <title>Finding It: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos</title>
    <link href="http://yoursite.com/2019/12/02/Finding-It-Weakly-Supervised-Reference-Aware-Visual-Grounding-in-Instructional-Videos/"/>
    <id>http://yoursite.com/2019/12/02/Finding-It-Weakly-Supervised-Reference-Aware-Visual-Grounding-in-Instructional-Videos/</id>
    <published>2019-12-02T03:23:38.000Z</published>
    <updated>2019-12-02T03:37:20.039Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Learning to Generate Grounded Visual Captions without Localization Supervision</title>
    <link href="http://yoursite.com/2019/12/01/Learning-to-Generate-Grounded-Visual-Captions-without-Localization-Supervision/"/>
    <id>http://yoursite.com/2019/12/01/Learning-to-Generate-Grounded-Visual-Captions-without-Localization-Supervision/</id>
    <published>2019-12-01T04:15:21.000Z</published>
    <updated>2019-12-01T10:47:20.570Z</updated>
    
    <content type="html"><![CDATA[<h3 id="ICLR-2020-under-view"><a href="#ICLR-2020-under-view" class="headerlink" title="ICLR 2020 under view"></a>ICLR 2020 under view</h3><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li><p>问题：在captioning 任务中，当前的评价指标并不能很好的反应生成的句子与该视频之间的契合度（Groud），有可能生成的句子只是基于在训练过程中学习到的priors（一种统计特性，而不是基于该视频本身）</p></li><li><p>当前模型对于 groud 这个任务，存在的困难：（1）由于当前的 language model 常使用 attention 机制来关注某一个 region，以此来预测下一个生成的单词。换句话说，就是在不知道将会生成什么单词的情况下，却要先定位region， 另外，一篇论文 [1] 提出，attention机制关注的region与人类所关注的并不一致（2）更难的是：传入 attention网络的是 RNN 的 hidden_state，由于 RNN 的记录历史的特性，这个输入包括的是过去所有的信息，而不是针对于某一个individual word。  </p></li></ul><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul><li>不同于 GVD，该文不使用 annotation bbox 作为监督信号，而是使用了 decoder + localizer + redecoder的结构来自我监督（self-supervision）</li><li>由于其自监督的特性，在一些infrequent word上该文的方法比监督的方法，效果更好</li><li>不仅使用一般的为每个 object class 计算 grounding accuracy， 还提出了一个新的指标：为每个sentence 计算grounding accuracy。</li></ul><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ul><li><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g9heo6n8glj318k0lp0ze.jpg" alt="搜狗截图20191201183637.png"></p></li><li><p><strong>分阶段训练</strong></p></li><li><p>（1）正常的 encoder-decoder先训练 ~30个epoch</p></li><li><p>（2）在正常的基础上进行添加。 （a）<strong>re-localize</strong>: language_lstm 会得到y1, y2, …, yT 个预测的序列，将这些序列作为attention机制中的 查询向量，赋给每个region一个attention系数，这样就可以在每一个step重构attention系数分配，这样也解决了motivation中提到的问题，即attention是由某一个individual word 而计算得来的。（b）每个step 有了attention对齐之后的attention_region， 再输入到language_LSTM中，得到<strong>再次预测的sequence of word</strong>。</p></li><li><p>在这第二阶段，就是两个loss 交叉熵损失进行权重加和来训练</p></li><li><p>可以发现一个问题，对于visual-words 和 non-visual-words都进行了re-localize。实际上对于，on a 等这类词汇，并不需要在image中找到 grounded region。 该文作者在补充材料里给出了一些额外的实验， eg, 将这些non-visual words 进行抑制，不计算reconstruction loss, 或者给这些localized region representation重新赋给invalid representaion。但是实验表明，在Flickr30 上性能（caption and ground）有提升，但是在 activity上（caption 没变化，ground下降）。</p></li><li><p>但是作者并没有给出分析，我个人总觉得实验设计的不完善，分析的也不多。</p></li></ul><h3 id="Measuring-grounding-per-generated-sentence"><a href="#Measuring-grounding-per-generated-sentence" class="headerlink" title="Measuring grounding per generated sentence"></a>Measuring grounding per generated sentence</h3><ul><li>提该指标的原因：（Such metrics （F1all, F1loc） are extremely stringent as captioning models are generally biased toward certain words in the vocabulary, given the long-tailed distribution of words. ）</li></ul><h3 id="Analysis-Grounding-performance-when-using-a-better-object-detector"><a href="#Analysis-Grounding-performance-when-using-a-better-object-detector" class="headerlink" title="Analysis:  Grounding performance when using a better object detector."></a>Analysis:  Grounding performance when using a better object detector.</h3><ul><li>在 Flickr30k Entities 上进行实验，分析 better detector 对 grounding性能的影响</li><li>（1）使用 GT box (ubrealistically) ，进行实验，发现 caption metric 和 grounding accuracy都有提升</li><li>（2）在 Flickr30k上训练一个detector（之前使用的是在 visual genome上训练好的），进行实验，发现，使得caption metirc下降，（作者分析：由于在本数据集上进行训练，得到的 the ROI features and their associated object predictions 更偏向于 该数据中的  the annotated object words 却不能很好地泛化以预测 diverse captions， 从而导致了captioning 指标下降）</li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv Batra.  <strong>Human attention in visual question answering: Do humans and deep networks look at the same regions?</strong>  Computer Vision and Image Understanding, 163:90–100, 2017. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;ICLR-2020-under-view&quot;&gt;&lt;a href=&quot;#ICLR-2020-under-view&quot; class=&quot;headerlink&quot; title=&quot;ICLR 2020 under view&quot;&gt;&lt;/a&gt;ICLR 2020 under view&lt;/h3&gt;&lt;
      
    
    </summary>
    
      <category term="视频描述" scheme="http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="视频描述" scheme="http://yoursite.com/tags/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
</feed>
