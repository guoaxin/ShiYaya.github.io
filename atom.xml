<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShiYaya</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-08-14T02:07:02.733Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ShiYaya</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>glob 之 **</title>
    <link href="http://yoursite.com/2019/08/13/glob-%E4%B9%8B/"/>
    <id>http://yoursite.com/2019/08/13/glob-之/</id>
    <published>2019-08-13T06:25:21.000Z</published>
    <updated>2019-08-14T02:07:02.733Z</updated>
    
    <content type="html"><![CDATA[<ul><li>该篇主要介绍glob的一些使用小技巧</li></ul><h3 id="想要获得某个文件目录下所有-指定文件格式-的所有文件"><a href="#想要获得某个文件目录下所有-指定文件格式-的所有文件" class="headerlink" title="想要获得某个文件目录下所有 指定文件格式 的所有文件"></a>想要获得某个文件目录下所有 <strong><em>指定文件格式</em></strong> 的所有文件</h3><ul><li><p>假设有一个文件环境如下图所示</p><p><img src="https://i.loli.net/2019/08/14/sjTANPfDuV6cord.png" alt="搜狗截图20190814100532.png"></p></li></ul><ul><li><p>比如想要获得<code>/userhome/dataset/MSVD/YouTubeClips/YouTubeClips</code> 下 <code>.avi</code>格式的所有文件</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">path</span> = <span class="string">'/userhome/dataset/MSVD/YouTubeClips/YouTubeClips/'</span></span><br><span class="line">glob.glob(<span class="built_in">path</span> + <span class="string">'*.avi'</span>)</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>想要获得某目录下的所有子目录中的所有指定文件格式的所有文件</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">path</span> = <span class="string">'/userhome/dataset/MSVD/YouTubeClips/'</span></span><br><span class="line">glob.glob(<span class="built_in">path</span> + <span class="string">'**/'</span> + <span class="string">'*.avi'</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">path</span> = <span class="string">'/userhome/dataset/MSVD/'</span></span><br><span class="line">glob.glob(<span class="built_in">path</span> + <span class="string">'**/'</span> + <span class="string">'**/'</span> + <span class="string">'*.avi'</span>)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;该篇主要介绍glob的一些使用小技巧&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;想要获得某个文件目录下所有-指定文件格式-的所有文件&quot;&gt;&lt;a href=&quot;#想要获得某个文件目录下所有-指定文件格式-的所有文件&quot; class=&quot;headerlink&quot; title=&quot;想
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title> pytorch clone() vs copy_()</title>
    <link href="http://yoursite.com/2019/08/06/pytorch-clone-vs-copy/"/>
    <id>http://yoursite.com/2019/08/06/pytorch-clone-vs-copy/</id>
    <published>2019-08-06T07:05:25.000Z</published>
    <updated>2019-08-06T07:06:02.565Z</updated>
    
    <content type="html"><![CDATA[<p><code>clone</code>() → Tensor</p><ul><li>反向传播时，将会返回到原来的变量上<br>Returns a copy of the <code>self</code> tensor. The copy has the same size and data type as <code>self</code>.</li><li>NOTE</li><li>Unlike copy_(), this function is recorded in the computation graph. Gradients propagating to the cloned tensor will propagate to the original tensor.</li></ul><hr><p><code>copy_</code>(<em>src</em>, <em>non_blocking=False</em>) → Tensor</p><ul><li><p>只是值得复制<br>Copies the elements from <code>src</code> into <code>self</code> tensor and returns <code>self</code>.</p></li><li><p>The <code>src</code> tensor must be <a href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics" target="_blank" rel="noopener">broadcastable</a> with the <code>self</code> tensor. It may be of a different data type or reside on a different device.</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;code&gt;clone&lt;/code&gt;() → Tensor&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;反向传播时，将会返回到原来的变量上&lt;br&gt;Returns a copy of the &lt;code&gt;self&lt;/code&gt; tensor. The copy has the same siz
      
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>实验中遇到的问题及解决</title>
    <link href="http://yoursite.com/2019/08/05/%E5%AE%9E%E9%AA%8C%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3/"/>
    <id>http://yoursite.com/2019/08/05/实验中遇到的问题及解决/</id>
    <published>2019-08-05T11:22:00.000Z</published>
    <updated>2019-08-05T11:41:45.360Z</updated>
    
    <content type="html"><![CDATA[<h3 id="问题-1"><a href="#问题-1" class="headerlink" title="问题 1"></a>问题 1</h3><ul><li>问题描述：首先表现为：在pycharm debug下和在running模式下的实验结果不一致，<br><br>后来，在训练阶段将预训练的模型保存下来，载入evaluate.py 文件中再次进行评估，得到的分数与在训练阶段评估的分数不一致</li><li>解决思路：由于第二个现象，更加容易解决，因此先解决他，师兄提出一个办法，将保存的模型再次载入，这样就可以有两个网络，然后比较两个网络的数据是在哪里出现差异的，这样可以找到问题。</li><li>解决办法：</li></ul><ol><li>在训练一个epoch 后，将模型保存了下来，然后用两个网络，一个时train.py中重新加载这个网络，一个是在evaluate.py中加载这个网络，将得到的结果，进行比较，（看输出的结果是否一致），然后发现，在一些video 输出的结果是一样的，在一些video是不一样的。<br></li><li>找到那些video对应的结果不一样的所对应的iteration，在该iteration打印出了网络中的部分变量的数据，发现，在dataloader的数据就是不一样的.<br></li><li>那么问题就是出现在数据加载上。通过对数据加载部分的代码进行调试，发现，仅在num_workers=0时，两个dataloader的数据才一样，而采用多线程的话，两个dataloader的数据不完全一样。而又在其他的代码上测试，多线程不会影响数据加载，那么问题就是出现在，自己设计的dataset上，<br></li><li>又发现在加载h5py文件时，没有取切片，而self.critical pytorch代码时加上了的，通过加上切片 <code>[:]</code> 发现在多线程时，是正常的。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;问题-1&quot;&gt;&lt;a href=&quot;#问题-1&quot; class=&quot;headerlink&quot; title=&quot;问题 1&quot;&gt;&lt;/a&gt;问题 1&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;问题描述：首先表现为：在pycharm debug下和在running模式下的实验结果不一致，&lt;br&gt;&lt;br&gt;后
      
    
    </summary>
    
      <category term="问题总结" scheme="http://yoursite.com/categories/%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="问题总结" scheme="http://yoursite.com/tags/%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>Bridging the Gap between Training and Inference for Neural Machine Translation</title>
    <link href="http://yoursite.com/2019/08/04/Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation/"/>
    <id>http://yoursite.com/2019/08/04/Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation/</id>
    <published>2019-08-04T12:53:17.000Z</published>
    <updated>2019-08-04T13:51:20.419Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Multi-Label Image Recognition with Graph Convolutional Networks</title>
    <link href="http://yoursite.com/2019/08/02/Multi-Label-Image-Recognition-with-Graph-Convolutional-Networks/"/>
    <id>http://yoursite.com/2019/08/02/Multi-Label-Image-Recognition-with-Graph-Convolutional-Networks/</id>
    <published>2019-08-02T13:22:06.000Z</published>
    <updated>2019-08-03T04:07:09.308Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Motivation：建模-label-之间的依赖"><a href="#Motivation：建模-label-之间的依赖" class="headerlink" title="Motivation：建模  label 之间的依赖"></a>Motivation：建模  label 之间的依赖</h3><ul><li>使用GCN来建模label之间的依赖</li><li>有向图</li><li>每个节点用 label 的词向量来表达</li></ul><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><h4 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h4><ul><li><p>GCN 的输入：GCN 的 输入是label的word embedding，使用预训练的glove vector，如果label 是含有多个词的，那么对这多个词的词向量取平均，</p></li><li><p>GCN的输出<code>C*D</code>是为了得到一个分类器，<code>C</code>是类别数，<code>D</code>是image representation的维度，</p></li><li><p>邻接矩阵：a<sub>ij</sub>用条件概率来表示：当label<sub>i</sub>出现时，label<sub>j</sub>出现的概率，因此这不是一个对称矩阵，具体地论文中还给出了更加细节的修改。</p></li></ul><h4 id="image-representation"><a href="#image-representation" class="headerlink" title="image representation"></a>image representation</h4><ul><li>使用 ResNet101 得到 conv5层的输出，再经过全局池化得到一个<code>D</code>维度的特征向量</li></ul><h4 id="multi-label-classifier"><a href="#multi-label-classifier" class="headerlink" title="multi-label classifier"></a>multi-label classifier</h4><ul><li>将上两步的输出进行矩阵相乘，就可以得到 计算的multi-label</li></ul><p><img src="https://i.loli.net/2019/08/03/cdwYEWSF9q6tk3p.png" alt="搜狗截图20190802221229.png"></p><h3 id="不同点-vs-semi-supervised-gcn"><a href="#不同点-vs-semi-supervised-gcn" class="headerlink" title="不同点 vs semi-supervised gcn"></a>不同点 vs semi-supervised gcn</h3><p>1.</p><ul><li>不同于一般的GCN，输入节点的特征，和边，经过GCN之后，得到的是更新后的节点特征</li><li>本文GCN的输出<code>C*D</code>是为了得到一个分类器，<code>C</code>是类别数，<code>D</code>是image representation的维度，</li><li>GCN 的 输入是label的word embedding，使用预训练的glove vector，如果label 是含有多个词的，那么对这多个词的词向量取平均</li></ul><p>2.</p><ul><li>一般的GCN的邻接矩阵是预先定义好的，</li><li>但是本文的邻接矩阵：need to construct the <code>A</code> from scrach</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Motivation：建模-label-之间的依赖&quot;&gt;&lt;a href=&quot;#Motivation：建模-label-之间的依赖&quot; class=&quot;headerlink&quot; title=&quot;Motivation：建模  label 之间的依赖&quot;&gt;&lt;/a&gt;Motivation
      
    
    </summary>
    
      <category term="图卷积网络" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="图卷积网络" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>word2vec</title>
    <link href="http://yoursite.com/2019/08/02/word2vec-1/"/>
    <id>http://yoursite.com/2019/08/02/word2vec-1/</id>
    <published>2019-08-02T04:51:02.000Z</published>
    <updated>2019-08-02T04:51:02.386Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>word2vec</title>
    <link href="http://yoursite.com/2019/08/01/word2vec/"/>
    <id>http://yoursite.com/2019/08/01/word2vec/</id>
    <published>2019-08-01T12:56:45.000Z</published>
    <updated>2019-08-02T14:37:15.464Z</updated>
    
    <content type="html"><![CDATA[<h3 id="使用one-hot-来作为词向量"><a href="#使用one-hot-来作为词向量" class="headerlink" title="使用one-hot 来作为词向量"></a>使用one-hot 来作为词向量</h3><ul><li>存在一个缺点，即，两个单词之间的余弦相似度为0，因为one-hot是两两正交的形式。</li><li>但是相似度为0，显然是不对的</li></ul><h3 id="word2vet"><a href="#word2vet" class="headerlink" title="word2vet"></a>word2vet</h3><ul><li><p>跳字模型：中心词生成背景词</p></li><li><p>连续词袋模型：背景词生成中心词</p></li><li><p>这两个模型存在的问题：在softmax中，由于分母是对整个vocab进行求和，导致反向传播的计算量非常大</p></li><li><p><a href="https://www.bilibili.com/video/av18512944/" target="_blank" rel="noopener">相关教程</a></p></li></ul><p>预训练模型</p><ul><li>glove</li><li>fasttext</li><li><a href="https://www.bilibili.com/video/av18795160/?spm_id_from=333.788.videocard.0" target="_blank" rel="noopener">相关教程</a></li><li>spacy</li><li><a href="https://shiyaya.github.io/2019/07/16/Spacy工具包/" target="_blank" rel="noopener">https://shiyaya.github.io/2019/07/16/Spacy%E5%B7%A5%E5%85%B7%E5%8C%85/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;使用one-hot-来作为词向量&quot;&gt;&lt;a href=&quot;#使用one-hot-来作为词向量&quot; class=&quot;headerlink&quot; title=&quot;使用one-hot 来作为词向量&quot;&gt;&lt;/a&gt;使用one-hot 来作为词向量&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;存在一个缺点，即
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>FPN</title>
    <link href="http://yoursite.com/2019/08/01/FPN/"/>
    <id>http://yoursite.com/2019/08/01/FPN/</id>
    <published>2019-08-01T06:47:34.000Z</published>
    <updated>2019-08-01T06:47:34.036Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>VideoGraph: Recognizing Minutes-Long Human Activities in Videos</title>
    <link href="http://yoursite.com/2019/07/30/VideoGraph-Recognizing-Minutes-Long-Human-Activities-in-Videos/"/>
    <id>http://yoursite.com/2019/07/30/VideoGraph-Recognizing-Minutes-Long-Human-Activities-in-Videos/</id>
    <published>2019-07-30T13:15:55.000Z</published>
    <updated>2019-08-02T13:23:11.520Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>当前基于CNN或者non-lcoal的方法，可以建模 temporal concepts，但是却不能建模分钟级长的时域依赖。</li><li>学习一个无向图，节点和边都是直接从video中得到，而不需要进行单独的节点标注。</li><li>这里的节点是：组成activity的一个unit-action，比如 “煎鸡蛋” 这个activity里的 “打破鸡蛋” 。</li><li>边，表示 (units-action) 运动单元之间的时域关系</li></ul><h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><ul><li>建模长范围的activity</li><li>捕捉到细节信息</li></ul><h3 id="Vs-Video-as-space-time-region-graph"><a href="#Vs-Video-as-space-time-region-graph" class="headerlink" title="Vs  Video as space-time region graph"></a>Vs  <code>Video as space-time region graph</code></h3><ul><li>Video as space-time region graph： 需要提取 key objects</li><li>Video graph：自动的从video中学到 nodes</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;当前基于CNN或者non-lcoal的方法，可以建模 tempo
      
    
    </summary>
    
      <category term="行为识别" scheme="http://yoursite.com/categories/%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB/"/>
    
    
      <category term="行为识别" scheme="http://yoursite.com/tags/%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>安装pytorch_geometricc</title>
    <link href="http://yoursite.com/2019/07/30/%E5%AE%89%E8%A3%85pytorch-geometricc/"/>
    <id>http://yoursite.com/2019/07/30/安装pytorch-geometricc/</id>
    <published>2019-07-30T07:15:51.000Z</published>
    <updated>2019-08-12T12:49:33.840Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p><a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html#frequently-asked-questions" target="_blank" rel="noopener">官方链接</a></p></li><li><p>下面是截取自官方</p></li></ul><h2 id="Directly-Installation"><a href="#Directly-Installation" class="headerlink" title="Directly Installation"></a>Directly Installation</h2><p>We have outsourced a lot of functionality of PyTorch Geometric to other packages, which needs to be installed in advance. These packages come with their own CPU and GPU kernel implementations based on the newly introduced <a href="https://github.com/pytorch/extension-cpp/" target="_blank" rel="noopener">C++/CUDA extensions</a> in PyTorch 0.4.0.</p><p>Note</p><p>We do not recommend installation as root user on your system python. Please setup an <a href="https://conda.io/docs/user-guide/install/index.html/" target="_blank" rel="noopener">Anaconda/Miniconda</a> environment or create a <a href="https://www.docker.com/" target="_blank" rel="noopener">Docker image</a>.</p><p>Please follow the steps below for a successful installation:</p><ol start="0"><li><p>Added  by yaya:</p><ul><li><p>may be you can select a conda environments, will be more fine</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3<span class="number">-5.0</span><span class="number">.0</span>-Linux-x86_64.sh</span><br><span class="line">conda create -n pytorch_geometric python=<span class="number">3.7</span> -y</span><br><span class="line">source activate pytorch_geometric</span><br></pre></td></tr></table></figure></li><li><p>after into env: pytorch_geometric</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">pip</span> <span class="selector-tag">install</span> <span class="selector-tag">torch-1</span><span class="selector-class">.1</span><span class="selector-class">.0-cp37-cp37m-manylinux1_x86_64</span><span class="selector-class">.whl</span>  # <span class="selector-tag">download</span> <span class="selector-tag">at</span> <span class="selector-tag">first</span></span><br><span class="line"><span class="selector-tag">pip</span> <span class="selector-tag">installl</span> <span class="selector-tag">numpy-1</span><span class="selector-class">.17</span><span class="selector-class">.0-cp37-cp37m-manylinux1_x86_64</span><span class="selector-class">.whl</span>  # <span class="selector-tag">download</span> <span class="selector-tag">at</span> <span class="selector-tag">first</span></span><br><span class="line"><span class="selector-tag">pip</span> <span class="selector-tag">install</span> <span class="selector-tag">scipy-1</span><span class="selector-class">.3</span><span class="selector-class">.0-cp37-cp37m-manylinux1_x86_64</span><span class="selector-class">.whl</span>  # <span class="selector-tag">download</span> <span class="selector-tag">at</span> <span class="selector-tag">first</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><ol><li><p>Ensure that at least PyTorch 1.1.0 is installed:</p><blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; $ python -c <span class="string">"import torch; print(torch.__version__)"</span></span><br><span class="line">&gt; <span class="meta">&gt;&gt;&gt; </span><span class="number">1.1</span>.<span class="number">0</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote></li><li><p>Ensure CUDA is setup correctly (optional):</p><blockquote><ol><li><p>Check if PyTorch is installed with CUDA support:</p><blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;    &gt; $ python -c <span class="string">"import torch; print(torch.cuda.is_available())"</span></span><br><span class="line">&gt;    &gt; <span class="meta">&gt;&gt;&gt; </span>True</span><br><span class="line">&gt;    &gt;</span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote><blockquote><ol start="2"><li><p>Add CUDA to <code>$PATH</code> and <code>$CPATH</code> (note that your actual CUDA path may vary from <code>/usr/local/cuda</code>):</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda/bin:<span class="variable">$PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">echo</span> <span class="variable">$PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; &gt;&gt;&gt; /usr/<span class="built_in">local</span>/cuda/bin:...</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">export</span> CPATH=/usr/<span class="built_in">local</span>/cuda/include:<span class="variable">$CPATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">echo</span> <span class="variable">$CPATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; &gt;&gt;&gt; /usr/<span class="built_in">local</span>/cuda/include:...</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt;</span></span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote><blockquote><ol start="3"><li><p>Add CUDA to <code>$LD_LIBRARY_PATH</code> on Linux and to <code>$DYLD_LIBRARY_PATH</code> on macOS (note that your actual CUDA path may vary from <code>/usr/local/cuda</code>):</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda/lib64:<span class="variable">$LD_LIBRARY_PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">echo</span> <span class="variable">$LD_LIBRARY_PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; &gt;&gt;&gt; /usr/<span class="built_in">local</span>/cuda/lib64:...</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">export</span> DYLD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda/lib:<span class="variable">$DYLD_LIBRARY_PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">echo</span> <span class="variable">$DYLD_LIBRARY_PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; &gt;&gt;&gt; /usr/<span class="built_in">local</span>/cuda/lib:...</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt;</span></span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote><blockquote><ol start="4"><li><p>Verify that <code>nvcc</code> is accessible from terminal:</p><blockquote><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="quote">&gt;    &gt; $ nvcc --version</span></span><br><span class="line"><span class="quote">&gt;    &gt; &gt;&gt;&gt; 10.0</span></span><br><span class="line"><span class="quote">&gt;    &gt;</span></span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote><blockquote><ol start="5"><li><p>Ensure that PyTorch and system CUDA versions match:</p><blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;    &gt; $ python -c <span class="string">"import torch; print(torch.version.cuda)"</span></span><br><span class="line">&gt;    &gt; <span class="meta">&gt;&gt;&gt; </span><span class="number">10.0</span></span><br><span class="line">&gt;    &gt; </span><br><span class="line">&gt;    &gt; $ nvcc --version</span><br><span class="line">&gt;    &gt; <span class="meta">&gt;&gt;&gt; </span><span class="number">10.0</span></span><br><span class="line">&gt;    &gt;</span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote></li><li><p>Install all needed packages:</p><blockquote><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="symbol">$</span> you can see <span class="number">4.</span> first (optional)</span><br><span class="line">&gt; <span class="symbol">$</span> pip install --verbose --<span class="keyword">no</span>-cache-dir torch-scatter</span><br><span class="line">&gt; <span class="symbol">$</span> pip install --verbose --<span class="keyword">no</span>-cache-dir torch-sparse</span><br><span class="line">&gt; <span class="symbol">$</span> pip install --verbose --<span class="keyword">no</span>-cache-dir torch-cluster</span><br><span class="line">&gt; <span class="symbol">$</span> pip install --verbose --<span class="keyword">no</span>-cache-dir torch-spline-conv (optional)</span><br><span class="line">&gt; <span class="symbol">$</span> pip install torch-geometric</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote></li><li><p>added by yaya:<br>may be you can pip install scipy at first ,because above need it.</p></li></ol><h2 id="Docker-install"><a href="#Docker-install" class="headerlink" title="Docker install"></a>Docker install</h2><ul><li><a href="https://github.com/rusty1s/pytorch_geometric/tree/master/docker" target="_blank" rel="noopener">https://github.com/rusty1s/pytorch_geometric/tree/master/docker</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html#frequently-asked-questions&quot; target=&quot;_blank&quot; 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>GCN_LSTM  vs  SGAE</title>
    <link href="http://yoursite.com/2019/07/30/gcn-on-captioning/"/>
    <id>http://yoursite.com/2019/07/30/gcn-on-captioning/</id>
    <published>2019-07-30T02:55:21.000Z</published>
    <updated>2019-07-31T13:13:19.284Z</updated>
    
    <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Video Description: A Survey of Methods, Datasets and Evaluation Metrics</title>
    <link href="http://yoursite.com/2019/07/29/Video-Description-A-Survey-of-Methods-Datasets-and-Evaluation-Metrics/"/>
    <id>http://yoursite.com/2019/07/29/Video-Description-A-Survey-of-Methods-Datasets-and-Evaluation-Metrics/</id>
    <published>2019-07-29T02:21:32.000Z</published>
    <updated>2019-07-29T13:00:31.749Z</updated>
    
    <content type="html"><![CDATA[<h3 id="视频描述仍然处于起步阶段的原因"><a href="#视频描述仍然处于起步阶段的原因" class="headerlink" title="视频描述仍然处于起步阶段的原因"></a>视频描述仍然处于起步阶段的原因</h3><ul><li>对视频描述模型的分析是困难的，很难去判别是visual feature 亦或是 language model 哪个做的贡献大</li><li>当前的数据集，既没有包含足够的视觉多样性，也没有复杂的语言结构</li><li>当前的凭据指标并不能非常正确的去评估生成的句子与人类生成的句子之间的一致程度</li></ul><h3 id="the-difficulty-of-video-caption"><a href="#the-difficulty-of-video-caption" class="headerlink" title="the difficulty of video caption"></a>the difficulty of video caption</h3><ul><li>并不是在video中的所有object 都是与description相关的，可能其只是背景中的一个元素。    </li><li>此外，还需要objects的运动信息，以及 事件，动作，对象之间的因果关系。   </li><li>视频中的action可能有不同的长度，不同的action之间，可能有重叠。    </li></ul><h3 id="Sequence-Learning-based-Video-Captioning-Methods"><a href="#Sequence-Learning-based-Video-Captioning-Methods" class="headerlink" title="Sequence Learning based Video Captioning Methods"></a>Sequence Learning based Video Captioning Methods</h3><h4 id="CNN-RNN-based"><a href="#CNN-RNN-based" class="headerlink" title="CNN-RNN-based"></a>CNN-RNN-based</h4><ul><li><p>第一个 end-to-end：</p><p>S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. Mooney, and K. Saenko. 2014. Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729, (2014).    </p><img src="https://i.loli.net/2019/07/29/5d3ea016090c918345.png" alt="图片1.png" title="图片1.png"></li><li><p>S2VT （变长输入，变长输出）</p><p>I. Sutskever, O. Vinyals, and Q. V. Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems. 3104-3112.    </p><img src="https://i.loli.net/2019/07/29/5d3ea01536b3144846.png" alt="图片2.png" title="图片2.png">   </li><li><p>TA ( 加入C3D[1] )</p><p>L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and A.Courville. 2015. Describing videos by exploiting temporal structure. In IEEE ICCV    </p><img src="https://i.loli.net/2019/07/29/5d3ea016a248c95582.png" alt="图片3.png" title="图片3.png">  </li><li><p>LSTM-E （making a common visual-semantic-embedding ）</p><p>Y. Pan, T. Mei, T. Yao, H. Li, and Y. Rui. 2016. Jointly modeling embedding and translation to bridge video and language. In IEEE CVPR. </p><img src="https://i.loli.net/2019/07/29/5d3ea421aaf9013065.png" alt="图片4.png" title="图片4.png"></li></ul><ul><li><p>GRU-EVE  ( short fourier transform)</p><p>N. Aafaq, N. Akhtar, W. Liu, S. Z. Gilani and A. Mian. 2019. Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning. In IEEE CVPR.    </p><img src="https://i.loli.net/2019/07/29/5d3ea0163113561600.png" alt="搜狗截图20190729152752.png" title="搜狗截图20190729152752.png">   </li><li><p>h-RNN<br>H. Yu, J. Wang, Z. Huang, Y. Yang, and W. Xu. 2016. Video paragraph captioning using hierarchical recurrent neural networks. In IEEE CVPR.</p><img src="https://i.loli.net/2019/07/29/5d3ea63af2e0354548.png" alt="图片5.png" title="图片5.png"></li></ul><h4 id="RL-based"><a href="#RL-based" class="headerlink" title="RL-based"></a>RL-based</h4><ul><li><p>Z. Ren, X. Wang, N. Zhang, X. Lv, and L. Li. 2017. Deep reinforcement learning-based image captioning with embedding reward. arXiv preprint arXiv:1704.03899, (2017).</p></li><li><p>Y. Chen, S. Wang, W. Zhang, and Q. Huang. 2018.  ==Less Is More: Picking Informative Frames for Video Captioning.==  arXiv preprint arXiv:1803.01457, (2018).</p><p>提出了一个基于强化学习的方法，来选择 key informative frames 来表达一个 complete video ，希望这样的操作可以忽略掉噪声和不必要的计算。</p></li><li><p>L. Li and B. Gong. 2018. End-to-End Video Captioning with Multitask Reinforcement Learning. arXiv preprint arXiv:1803.07950,<br>(2018).</p></li><li><p>R. Pasunuru and M. Bansal. 2017. Reinforced video captioning with entailment rewards. arXiv preprint arXiv:1708.02300, (2017).</p></li><li><p>S. Phan, G. E. Henter, Y. Miyao, and S. Satoh. 2017. Consensusbased Sequence Training for Video Captioning. arXiv preprint arXiv:1712.09532, (2017).</p></li><li><p>X. Wang, W. Chen, J. Wu, Y. Wang, and W. Y. Wang. 2017.  ==Video Captioning via Hierarchical Reinforcement Learning.==  arXiv preprint arXiv:1711.11135, (2017).</p><p>在 decoder阶段，使用 深度强化学习，这个方法证明可以捕捉到视频内容中的细节，并生成细粒度的description，但是！这个方法相对于当前的baseline 没有多大的提高。（我自己还需要再看看， 使用DRL的motivation）</p></li></ul><h3 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h3><ul><li><p><a href="https://blog.csdn.net/joshuaxx316/article/details/58696552" target="_blank" rel="noopener">参考链接</a></p></li><li><p>BLEU、ROUGE、METEOR  来源于 机器翻译</p></li><li><p>CIDEr、SPICE 来源于图像描述   </p></li></ul><h4 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a>BLEU</h4><ul><li><a href="https://blog.csdn.net/allocator/article/details/79657792" target="_blank" rel="noopener">BLEU参考链接</a></li><li>==BLEU实质是对两个句子的共现词频率计算==，但计算过程中使用好些技巧，追求计算的数值可以衡量这两句话的一致程度。 </li><li>BLEU容易陷入常用词和短译句的陷阱中，而给出较高的评分值。本文主要是对解决BLEU的这两个弊端的优化方法介绍。</li><li>缺点</li></ul><ol><li>　不考虑语言表达（语法）上的准确性； </li><li>　 测评精度会受常用词的干扰； </li><li>　 短译句的测评精度有时会较高； </li><li>　没有考虑同义词或相似表达的情况，可能会导致合理翻译被否定；</li></ol><h4 id="ROUGE"><a href="#ROUGE" class="headerlink" title="ROUGE"></a>ROUGE</h4><img src="https://i.loli.net/2019/07/29/5d3ed71f2086769963.png" alt="20170228224903951.png" title="20170228224903951.png"><h4 id="METEOR"><a href="#METEOR" class="headerlink" title="METEOR"></a>METEOR</h4><img src="https://i.loli.net/2019/07/29/5d3edcce1761442736.png" alt="20170228225011405.png" title="20170228225011405.png">   <h4 id="CIDEr"><a href="#CIDEr" class="headerlink" title="CIDEr"></a>CIDEr</h4><img src="https://i.loli.net/2019/07/29/5d3edcce646d089162.png" alt="20170228225056046.png" title="20170228225056046.png"><h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><img src="https://i.loli.net/2019/07/29/5d3edd503479c20027.png" alt="搜狗截图20190729194921.png" title="搜狗截图20190729194921.png">    <h3 id="当前的瓶颈："><a href="#当前的瓶颈：" class="headerlink" title="当前的瓶颈："></a>当前的瓶颈：</h3><h4 id="缺乏有效的评价指标"><a href="#缺乏有效的评价指标" class="headerlink" title="缺乏有效的评价指标"></a>缺乏有效的评价指标</h4><ul><li><p>我们的调查显示，阻碍这一研究进展的一个主要瓶颈是缺乏有效和有目的设计的视频描述评价指标。目前，无论是从机器翻译还是从图像字幕中，都采用了现有的度量标准，无法衡量机器生成的视频字幕的质量及其与人类判断的一致性。改进这些指标的一种方法是增加引用语句的数量。我们认为，从数据本身学习的目的构建的度量标准是推进视频描述研究的关键。    </p></li><li><p>王鑫也曾说：human evaluation在video captioning任务中是有必要的       </p><h4 id="视觉特征部分的瓶颈"><a href="#视觉特征部分的瓶颈" class="headerlink" title="视觉特征部分的瓶颈"></a>视觉特征部分的瓶颈</h4></li><li><p>在一个video中，可能出现多个activity，但是caption model只能检测出部分几个，导致性能下降。   </p></li><li><p>可能这个video中 action 的持续时间较长，但是，当前的video representation方法只能捕捉时域较短的运动信息（eg:C3D），因此不能很好地提取视频特征。   </p></li><li><p>大多数特征提取器只适用于静态或平稳变化的图像，因此难以处理突然的场景变化。目前的方法通过表示整体视频或帧来简化视觉编码部分。可能需要进一步探索注意力模型，以关注视频中具有重要意义的空间和时间部分。   </p></li><li><p>当前的encoder 与 decoder 部分，并 ==不是端到端的==，需要先提取 video representation再进行decoder，这样分布进行，而不是端到端的训练是不好的！    </p></li></ul><h3 id="captioning-model-的可解释性不足"><a href="#captioning-model-的可解释性不足" class="headerlink" title="captioning model 的可解释性不足"></a>captioning model 的可解释性不足</h3><ul><li>举个例子：当我们从包含“白色消防栓”的帧中看到视频描述模型生成的标题“红色消防栓”时，很难确定颜色特征是视觉特征提取器编码错误还是由于使用的语言模型bias( 由于有过多的训练数据是“红色消防栓)。<img src="https://i.loli.net/2019/07/29/5d3ee4996cf7480633.png" alt="搜狗截图20190729202028.png" title="搜狗截图20190729202028.png"></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li>[1] D. Tran, L. D. Bourdev, R. Fergus, L. Torresani, and M. Paluri. 2014. C3D: Generic Features for Video Analysis. CoRR abs/1412.0767, (2014). </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;视频描述仍然处于起步阶段的原因&quot;&gt;&lt;a href=&quot;#视频描述仍然处于起步阶段的原因&quot; class=&quot;headerlink&quot; title=&quot;视频描述仍然处于起步阶段的原因&quot;&gt;&lt;/a&gt;视频描述仍然处于起步阶段的原因&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;对视频描述模型的分析是困
      
    
    </summary>
    
      <category term="视频描述" scheme="http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="视频描述" scheme="http://yoursite.com/tags/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Temporal Deformable Convolutional Encoder-Decoder Networks for Video Captioning</title>
    <link href="http://yoursite.com/2019/07/28/Temporal-Deformable-Convolutional-Encoder-Decoder-Networks-for-Video-Captioning/"/>
    <id>http://yoursite.com/2019/07/28/Temporal-Deformable-Convolutional-Encoder-Decoder-Networks-for-Video-Captioning/</id>
    <published>2019-07-28T13:51:41.000Z</published>
    <updated>2019-07-29T00:55:09.280Z</updated>
    
    <content type="html"><![CDATA[<h3 id="当前存在的问题"><a href="#当前存在的问题" class="headerlink" title="当前存在的问题"></a>当前存在的问题</h3><ul><li>RNN 存在梯度消失和梯度下降的问题</li><li>RNN 的本质的循环依赖，限制了其并行计算</li><li>因此本文提出了 ==Temporal Deformable Convolutional Encoder-Decoder Networks (dubbed as TDConvED) ==that fully employ convolutions in both encoder and decoder networks for video captioning. </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;当前存在的问题&quot;&gt;&lt;a href=&quot;#当前存在的问题&quot; class=&quot;headerlink&quot; title=&quot;当前存在的问题&quot;&gt;&lt;/a&gt;当前存在的问题&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;RNN 存在梯度消失和梯度下降的问题&lt;/li&gt;
&lt;li&gt;RNN 的本质的循环依赖，限制了
      
    
    </summary>
    
      <category term="视频描述" scheme="http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="视频描述" scheme="http://yoursite.com/tags/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Motion Guided Spatial Attention for Video Captioning</title>
    <link href="http://yoursite.com/2019/07/28/Motion-Guided-Spatial-Attention-for-Video-Captioning/"/>
    <id>http://yoursite.com/2019/07/28/Motion-Guided-Spatial-Attention-for-Video-Captioning/</id>
    <published>2019-07-28T07:04:27.000Z</published>
    <updated>2019-07-29T01:03:59.809Z</updated>
    
    <content type="html"><![CDATA[<h3 id="当前的问题"><a href="#当前的问题" class="headerlink" title="当前的问题"></a>当前的问题</h3><ul><li>spatial attention 很少有人去探索</li><li>motion information 被利用通常是使用3D-CNNs来作为另外一种模态</li></ul><h3 id="本文的工作"><a href="#本文的工作" class="headerlink" title="本文的工作"></a>本文的工作</h3><ul><li>两个贡献： MGSA、GARU</li><li>The proposed MGSA utilize motion information between consecutive frames by applying CNN to stacked optical flows. </li><li>In addition, a gated recurrent unit named GARU is designed to adaptively relate spatial attention maps across time. <img src="https://i.loli.net/2019/07/28/5d3d877e9c0d546970.png" alt="搜狗截图20190728193057.png" title="搜狗截图20190728193057.png">    </li></ul><h3 id="Encoder-部分我的理解"><a href="#Encoder-部分我的理解" class="headerlink" title="Encoder 部分我的理解"></a>Encoder 部分我的理解</h3><ul><li>对一个video 采取N帧，对这N帧提取appearences feature，得到<code>N*H*W*D</code>的特征向量</li><li>以每帧为中心，采取连续的M帧，这M帧计算optical flow，并将这个<code>N*M</code>帧的optical flow images送入CNN中，得到<code>N*H*W*1</code>的特征向量。</li><li>==构造一个长度为N的GRU时域序列，每次送入一帧==  appearence feature 和 optical flow cnn feature，并得到一个输出,维度为<code>H*W</code>，</li><li>该输出作为一个attention系数，并与 ==当前帧== frame feature 相乘。得到一个为该帧的每个像素点（<code>H*W</code>）分配的权重系数。即进行加权求和，则可以得到该帧的spatial attention</li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><img src="https://i.loli.net/2019/07/28/5d3d87b91afaa36288.png" alt="搜狗截图20190728193156.png" title="搜狗截图20190728193156.png" width="440px" height="400px">    <h3 id="Experimental-results"><a href="#Experimental-results" class="headerlink" title="Experimental results"></a>Experimental results</h3><ul><li>这里只是想提一点，就是有一些论文在MSR-VTT上的实验结果，是使用了==音频信息==。<img src="https://i.loli.net/2019/07/28/5d3d80835814750581.png" alt="搜狗截图20190728190107.png" title="搜狗截图20190728190107.png"></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><p>基于 spatial attention的 video captioning model</p></li><li><p>Li, X.; Zhao, B.; and Lu, X. 2017. MAM-RNN: multi-level attention model based RNN for video captioning. In IJCAI, 2208–2214. </p></li><li><p>Yang, Z.; Han, Y.; and Wang, Z. 2017. Catching the temporal regions-of-interest for video captioning. In ACM MM, 146–153. attention, spatial. </p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;当前的问题&quot;&gt;&lt;a href=&quot;#当前的问题&quot; class=&quot;headerlink&quot; title=&quot;当前的问题&quot;&gt;&lt;/a&gt;当前的问题&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;spatial attention 很少有人去探索&lt;/li&gt;
&lt;li&gt;motion informatio
      
    
    </summary>
    
      <category term="视频描述" scheme="http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="视频描述" scheme="http://yoursite.com/tags/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning</title>
    <link href="http://yoursite.com/2019/07/27/Object-aware-Aggregation-with-Bidirectional-Temporal-Graph-for-Video-Captioning/"/>
    <id>http://yoursite.com/2019/07/27/Object-aware-Aggregation-with-Bidirectional-Temporal-Graph-for-Video-Captioning/</id>
    <published>2019-07-27T12:53:36.000Z</published>
    <updated>2019-07-28T07:04:55.749Z</updated>
    
    <content type="html"><![CDATA[<h3 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h3><ul><li>本文旨在于捕捉基于object的运动信息(capture object-based trajectory)，以前向流为例，以第一帧中的object regions 作为anchor， 来寻找在其他帧中相对应的regions， 计算该anchor 与 第i帧中的regions的相似性，然后相似性最大的那个region，认为是与anchor一致的objects， 然后将他们组成一组。反向流类似。  </li><li>这个捕捉运动信息的思想与 【Learning Video Representations from Correspondence Proposals】中的很相似。   </li></ul><h3 id="当前存在的问题"><a href="#当前存在的问题" class="headerlink" title="当前存在的问题"></a>当前存在的问题</h3><ul><li>当前的工作主要使用 global frame 或者是 salient regions而不是使用specific objects，那么将不能捕捉到每个object 的细节的时域动态。</li></ul><h3 id="文章的主要工作"><a href="#文章的主要工作" class="headerlink" title="文章的主要工作"></a>文章的主要工作</h3><h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><ul><li>constructs ==bidirectional temporal graph== to extract  the temporal trajectories for each object instance, which captures the detailed temporal dynamics in video content.    </li><li>==aggregation process on object regions==, which can capture the object-aware semantic information， 这里主要是得到了 VLAD[5, 6] representation   </li></ul><h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><ul><li><p>对object VLAD representation实施了temporal attention 和 object attentin</p></li><li><p>对 frames VLAD representation 实施了 temporal attention</p></li><li><p>然后分别进行nn.linear 线性变换后，相加</p></li><li><p>再与word_embedding相加送入GRU</p><img src="https://i.loli.net/2019/07/28/5d3d37324d6d283934.png" alt="搜狗截图20190728134820.png" title="搜狗截图20190728134820.png">   </li></ul><h3 id="本文的性能分析"><a href="#本文的性能分析" class="headerlink" title="本文的性能分析"></a>本文的性能分析</h3><ul><li>可以准确的描述video，比如关键的objects。</li><li><strong>但是！不能很好地去描述 objects 之间的交互</strong></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li>【NetVLAD】Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: Cnn architecture for weakly supervised place recognition. In CVPR, pages 5297–5307, 2016.</li><li>【SeqVLAD】Youjiang Xu, Yahong Han, Richang Hong, and Qi Tian. Sequential video vlad: Training the aggregation locally and temporally. IEEE Transactions on Image Processing (TIP), 27(10):4933–4944, 2018</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;我的理解&quot;&gt;&lt;a href=&quot;#我的理解&quot; class=&quot;headerlink&quot; title=&quot;我的理解&quot;&gt;&lt;/a&gt;我的理解&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;本文旨在于捕捉基于object的运动信息(capture object-based trajectory)，以前
      
    
    </summary>
    
      <category term="视频描述" scheme="http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="视频描述" scheme="http://yoursite.com/tags/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Pointing Novel Objects in Image Captioning</title>
    <link href="http://yoursite.com/2019/07/26/Pointing-Novel-Objects-in-Image-Captioning/"/>
    <id>http://yoursite.com/2019/07/26/Pointing-Novel-Objects-in-Image-Captioning/</id>
    <published>2019-07-26T11:40:20.000Z</published>
    <updated>2019-07-27T09:42:59.022Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><img src="https://i.loli.net/2019/07/27/5d3c1676f301a18995.png" alt="搜狗截图20190727171628.png" title="搜狗截图20190727171628.png"><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><ul><li>当前的模型，都是以image-caption对来进行训练，因此训练模型只能输出in-domain objects，但是，在实际应用中有些图片含有丰富的信息，但是用现有的模型却不能充分的表达。   </li></ul><h4 id="解决1"><a href="#解决1" class="headerlink" title="解决1"></a>解决1</h4><ul><li>希望可以生成新的words,(没有出现在training dataset)   </li><li>本文提出了解决办法：用object learner 来扩增标准的deep caption 结构。即，由一个图像分类任务，则可以得到该图像中出现的obects。这可以作为一个补充信息，加入到当前现有的deep caption Model 中。   </li><li>具体地：（1）标准的LSTM decoder 会输出一个predicted word,  （2）objects learner 通过一个copying layer 也可以得到一个预测单词。那么该选谁，本文并不硬选择，而是软选择，即给一个系数，来给这两个分配个概率，然后加和。这个选择的过程称为 <strong>Pointing Mechanism</strong>   </li><li>loss:   <img src="https://i.loli.net/2019/07/27/5d3c1c012cccc48971.png" alt="搜狗截图20190727173939.png" title="搜狗截图20190727173939.png">   </li></ul><h4 id="解决2"><a href="#解决2" class="headerlink" title="解决2:"></a>解决2:</h4><ul><li>希望将image中的所有信息，在句子中都可以覆盖到</li><li>提出了一个新的损失。target caption中含有 n词，即对应到image 中的 objects。那么希望生成的句子中含有的n词信息能够包含image中所有出现到的objects（即 target caption中的所有名词）</li><li>那么可以根据预测的单词是否生成了 target caption 中的名词，来计算损失（文章中这里在计算损失的时候忽略了语法结构，即不要求名词出现的在句子中的位置，只要求出现就可以）.   </li><li>loss:</li></ul><img src="https://i.loli.net/2019/07/27/5d3c1c754b56a83488.png" alt="搜狗截图20190727174134.png" title="搜狗截图20190727174134.png">    <img src="https://i.loli.net/2019/07/27/5d3c1c7536e6674709.png" alt="搜狗截图20190727174147.png" title="搜狗截图20190727174147.png"> ]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h3&gt;&lt;img src=&quot;https://i.loli.net/2019/07/27/5d3c1676f
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>vision to language 大牛</title>
    <link href="http://yoursite.com/2019/07/26/vision-to-language-%E5%A4%A7%E7%89%9B/"/>
    <id>http://yoursite.com/2019/07/26/vision-to-language-大牛/</id>
    <published>2019-07-26T07:53:12.000Z</published>
    <updated>2019-07-26T07:54:02.603Z</updated>
    
    <content type="html"><![CDATA[<h3 id="王鑫"><a href="#王鑫" class="headerlink" title="王鑫"></a>王鑫</h3><p>Papers can be found at <a href="https://sites.cs.ucsb.edu/~xwang" target="_blank" rel="noopener">https://sites.cs.ucsb.edu/~xwang</a><br>Email: <a href="mailto:xwang@cs.ucsb.edu" target="_blank" rel="noopener">xwang@cs.ucsb.edu</a></p><h4 id="video-captioning-via-hierarchical-reinforcement-learning"><a href="#video-captioning-via-hierarchical-reinforcement-learning" class="headerlink" title="video captioning via hierarchical  reinforcement learning"></a>video captioning via hierarchical  reinforcement learning</h4><ol><li>强化学习</li><li>加入音频信号</li></ol><h4 id="zero-shot-video-captioning"><a href="#zero-shot-video-captioning" class="headerlink" title="zero-shot video captioning"></a>zero-shot video captioning</h4><ul><li>Topic-Aware Mixture of Experts (TAMoE)  <h4 id="evaluation"><a href="#evaluation" class="headerlink" title="evaluation"></a>evaluation</h4></li></ul><ol><li><p>如何去评判，本身就是一个问题，当前的评价指标并不是那么合理</p></li><li><p>human evaluation是一个必要的评测方法，尤其是对于生成story的</p></li></ol><h4 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h4><ul><li>利用强化学习直接对指标进行优化，很可能会造成，指标上去了，但是生成的句子语义并不好。所以提出了下篇论文</li></ul><ol><li>Adversarial REward Learning (AREL)</li></ol><h4 id="Connecting-Language-and-Vision-to-Actions"><a href="#Connecting-Language-and-Vision-to-Actions" class="headerlink" title="Connecting Language and Vision to Actions"></a>Connecting Language and Vision to Actions</h4><ul><li>Look Before You Leap: Model-based RL</li><li>Reinforced Cross-Modal Matching (RCM)</li></ul><h3 id="吴琦"><a href="#吴琦" class="headerlink" title="吴琦"></a>吴琦</h3><h4 id="从-Vision-到-Language-再到-Action，万字漫谈三年跨域信息融合研究"><a href="#从-Vision-到-Language-再到-Action，万字漫谈三年跨域信息融合研究" class="headerlink" title="从 Vision 到 Language 再到 Action，万字漫谈三年跨域信息融合研究"></a><a href="https://mp.weixin.qq.com/s/lnoL1TpKY8HQqCMaBqWA5Q" target="_blank" rel="noopener">从 Vision 到 Language 再到 Action，万字漫谈三年跨域信息融合研究</a></h4><h4 id="一文纵览-Vision-and-Language-领域最新研究与进展"><a href="#一文纵览-Vision-and-Language-领域最新研究与进展" class="headerlink" title="一文纵览 Vision-and-Language 领域最新研究与进展"></a><a href="https://mp.weixin.qq.com/s/dyY64QrvPWbjGvJw5H51OA" target="_blank" rel="noopener">一文纵览 Vision-and-Language 领域最新研究与进展</a></h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;王鑫&quot;&gt;&lt;a href=&quot;#王鑫&quot; class=&quot;headerlink&quot; title=&quot;王鑫&quot;&gt;&lt;/a&gt;王鑫&lt;/h3&gt;&lt;p&gt;Papers can be found at &lt;a href=&quot;https://sites.cs.ucsb.edu/~xwang&quot; targ
      
    
    </summary>
    
      <category term="视频描述" scheme="http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="视频描述" scheme="http://yoursite.com/tags/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>CMU课程上新：Neural Networks for NLP</title>
    <link href="http://yoursite.com/2019/07/26/CMU%E8%AF%BE%E7%A8%8B%E4%B8%8A%E6%96%B0%EF%BC%9ANeural-Networks-for-NLP/"/>
    <id>http://yoursite.com/2019/07/26/CMU课程上新：Neural-Networks-for-NLP/</id>
    <published>2019-07-26T07:35:41.000Z</published>
    <updated>2019-07-26T07:36:42.594Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CMU课程上新：Neural-Networks-for-NLP（18年视频课件放出）"><a href="#CMU课程上新：Neural-Networks-for-NLP（18年视频课件放出）" class="headerlink" title="CMU课程上新：Neural Networks for NLP（18年视频课件放出）"></a>CMU课程上新：Neural Networks for NLP（18年视频课件放出）</h1><p>今天文摘菌给大家推荐一门非常棒的课程《Neural Networks for NLP》。这个课程首先简要介绍一下神经网络的基本知识，然后课程的大部讲分如何将神经网络应用于自然语言处理。</p><p>课程中的每一节都会介绍自然语言中的特定问题和现象，并描述建模的难点，当然，并也会绍几个解决这些问题的模型。</p><p>总的来说，这个“神课”会涉及用神经网络建模过程中所使用的各种技术，包括如何处理结构化句子，如何处理大数据，以及半监督和无监督学习，结构化预测和多语言建模等等。</p><p>注意，修读本门课程需要有一定的自然语言处理的知识储备，按照课程的要求，就是应该上过《17-711，NLP算法》。</p><p>2018年的课程视频已经公开，无法上外网的同学，国内也有热心的小伙伴将课程搬到了国内的B站，通过下面的链接可以打开哟</p><p><a href="https://www.bilibili.com/video/av31156700/" target="_blank" rel="noopener">https://www.bilibili.com/video/av31156700/</a></p><p>19年的春季新课程新增了ELMo/BERT上下文词表示、模型可解释性等内容，PyTorch/DyNet代码示例。</p><p>19年课程的课程目录等详细信息，可以去课程主页去查看哟~~文摘菌在下面给大家简单介绍一下这门课程的师资以及作业等情况。</p><p><a href="https://phontron.com/class/nn4nlp2019/schedule.html" target="_blank" rel="noopener">https://phontron.com/class/nn4nlp2019/schedule.html</a></p><p><strong>师资力量</strong></p><p><img src="https://image.jiqizhixin.com/uploads/editor/6e56a929-53db-4396-bd4d-e42e2bd166cd/640.png" alt="img"></p><p>本课程有两位主讲教师，分别是：Graham Neubig、Antonios Anastasopoulos。其中Graham Neubig是卡内基梅陇大学的教授，主要研究自然语言处理，他对机器学习非常感兴趣。Antonios Anastasopoulos我是圣母大学的在读博士，目前David Chiang自然语言处理技术组的成员。专注于“濒危”语言的机器翻译和语音识别。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/db527776-dbf8-4611-a415-1c94e99db32e/640.png" alt="img"></p><p>除了上课的教师外，还有一大批助教来解答同学们的疑惑。从助教安排，我们可以看出，课程在尽量做到有问必答。</p><h2 id="作业介绍及资料公开"><a href="#作业介绍及资料公开" class="headerlink" title="作业介绍及资料公开"></a>作业介绍及资料公开</h2><p>在课程的官网上，对课程的每一次作业都做了详细的说明，包括评分要求，完成作业的条件等等。除此之外还给出了作业示例。</p><p><img src="https://image.jiqizhixin.com/uploads/editor/0cf289f4-3ea8-43ad-b318-c97703e921ac/640.png" alt="img"></p><p>课程官网也贴心的给大家准备好了每一次讲课的PPT，在上课之前，大家多多预习哟~</p><p>PPT下载地址：<a href="https://phontron.com/class/nn4nlp2019/schedule.html" target="_blank" rel="noopener">https://phontron.com/class/nn4nlp2019/schedule.html</a></p><p>此次课程，初步是线下课程。请大家关注大数据文摘，如果后期有视频放出，文摘菌也一定会为大家更新的。</p><p>最后，再次给出课程主页：<a href="https://phontron.com/class/nn4nlp2019/schedule.html" target="_blank" rel="noopener">https://phontron.com/class/nn4nlp2019/schedule.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CMU课程上新：Neural-Networks-for-NLP（18年视频课件放出）&quot;&gt;&lt;a href=&quot;#CMU课程上新：Neural-Networks-for-NLP（18年视频课件放出）&quot; class=&quot;headerlink&quot; title=&quot;CMU课程上新：
      
    
    </summary>
    
      <category term="课程" scheme="http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B/"/>
    
    
      <category term="课程" scheme="http://yoursite.com/tags/%E8%AF%BE%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Learning Video Representations from Correspondence Proposals</title>
    <link href="http://yoursite.com/2019/07/26/Learning-Video-Representations-from-Correspondence-Proposals/"/>
    <id>http://yoursite.com/2019/07/26/Learning-Video-Representations-from-Correspondence-Proposals/</id>
    <published>2019-07-26T01:55:31.000Z</published>
    <updated>2019-07-26T05:00:03.785Z</updated>
    
    <content type="html"><![CDATA[<h3 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h3><ul><li>与Non-local 类似，都是在现有CNN（2D， 3D）模型中加入一个设计的模块</li></ul><h3 id="CPNet-介绍"><a href="#CPNet-介绍" class="headerlink" title="CPNet 介绍"></a>CPNet 介绍</h3><p>（1）在CNN的某一层，得到了<code>T*H*W*d</code>的特征，这<code>T*H*W</code>个特征，是经过conv来的，即一个特征，返回到原图对应的是一个块（区域）的特征。   </p><p>（2）类似于graph 中的邻接矩阵的操作，计算这个<code>T*H*W</code>个节点之间的相似性，相似性近的前K个（且不在同一帧），认为他们之间存在对应关系，即找到了一个区域对应到其他帧的对应区域。   </p><p>（3）将原区域，与对应区域的特征，与他们之间的位置关系，输入到MLP中，得到了一个更新的特征。对每个对应区域都采取这样的操作，得到K个特征。取max，得到了一个鲁棒的特征（可以去掉不是对应块区域的特征，即去掉噪声）。   </p><img src="https://i.loli.net/2019/07/26/5d3a7afee6b0178187.png" alt="搜狗截图20190726120054.png" title="搜狗截图20190726120054.png"><ul><li>是不是跟Non-Local很像，==CP Module就是融合了相似区域的特征，对原区域的特征进行更新。==</li></ul><h3 id="Non-local-vs-CPNet"><a href="#Non-local-vs-CPNet" class="headerlink" title="Non-local  vs   CPNet"></a>Non-local  vs   CPNet</h3><ul><li><p>在toy dataset （figure4）上设计了toy model（两层 CNN）,将现有的三个SOTA model以及自己设计的CPNet上进行试验</p></li><li><p>可以看到 I3D，ARTNet ，TRN三个模型的效果都不是很好</p></li><li><p>ARTNet ，TRN 是由于只使用了两个卷积层，不能捕捉长范围的运动信息</p></li><li><p>Non-local 可以捕捉长范围的运动信息，但是为什么效果还是不好：==NL block 没有加进去位置信息==（作者这么说的原因，就是因为在他们的CP module中有position information）</p></li></ul><img src="https://i.loli.net/2019/07/26/5d3a6c44659ae30922.png" alt="搜狗截图20190726102546.png" title="搜狗截图20190726102546.png"><h3 id="CPNet-可以退化成这三个model-没看懂-之后可以结合代码再深入分析"><a href="#CPNet-可以退化成这三个model-没看懂-之后可以结合代码再深入分析" class="headerlink" title="CPNet 可以退化成这三个model: ( 没看懂 ,之后可以结合代码再深入分析)"></a>CPNet 可以退化成这三个model: ( 没看懂 ,之后可以结合代码再深入分析)</h3><img src="https://i.loli.net/2019/07/26/5d3a889e2ce0b69830.png" alt="搜狗截图20190726125811.png" title="搜狗截图20190726125811.png">]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;我的理解&quot;&gt;&lt;a href=&quot;#我的理解&quot; class=&quot;headerlink&quot; title=&quot;我的理解&quot;&gt;&lt;/a&gt;我的理解&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;与Non-local 类似，都是在现有CNN（2D， 3D）模型中加入一个设计的模块&lt;/li&gt;
&lt;/ul&gt;
&lt;h
      
    
    </summary>
    
      <category term="行为识别" scheme="http://yoursite.com/categories/%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB/"/>
    
    
      <category term="行为识别" scheme="http://yoursite.com/tags/%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>Memory-Attended Recurrent Network for Video Captioning</title>
    <link href="http://yoursite.com/2019/07/25/Memory-Attended-Recurrent-Network-for-Video-Captioning/"/>
    <id>http://yoursite.com/2019/07/25/Memory-Attended-Recurrent-Network-for-Video-Captioning/</id>
    <published>2019-07-25T08:28:58.000Z</published>
    <updated>2019-08-11T14:20:15.009Z</updated>
    
    <content type="html"><![CDATA[<h3 id="词频"><a href="#词频" class="headerlink" title="词频"></a>词频</h3><p>“&gt; =3”的保留</p><p>MSR-VTT :11K   MSVD:4K</p><h3 id="Attention-Decoder"><a href="#Attention-Decoder" class="headerlink" title="Attention Decoder"></a>Attention Decoder</h3><ul><li><p>采用SA-LSTM的结构</p></li><li><p>细节：</p><ul><li><p>==共享attention==<br>由于需要对frames_feature ==(L帧)==  与 C3D_feature ==（L帧 -&gt; L/16个特征向量）== 都进行attention，这里进行了共享attention，好处：   <br>  </p><p>（1）将2D 和 3D 特征映射到相似的特征空间  <br></p><p>（2）像是一种正则化，减少了参数，避免过拟合  <br>  </p></li><li><p>降维<br>将2D 和 3D 的2048维度的特征，降维到512</p></li></ul></li></ul><h3 id="Attended-Memory-Decoder"><a href="#Attended-Memory-Decoder" class="headerlink" title="Attended Memory Decoder"></a>Attended Memory Decoder</h3><ul><li><p>当前模型的不足：</p><ul><li>现有的模型在生成word的时候，只依赖于当前video的信息，而不能依赖于那些，出现过该单词的其他video的信息</li><li>生成下一个单词，仅依赖于video信息和当前单词，没有建模相邻两个单词之间的兼容性（没看懂）</li></ul></li><li><p>具体的memeory设计详见论文</p></li></ul><h3 id="Attention-Coherent-Loss-AC-Loss"><a href="#Attention-Coherent-Loss-AC-Loss" class="headerlink" title="Attention-Coherent Loss (AC Loss)"></a>Attention-Coherent Loss (AC Loss)</h3><ul><li>将C3D 输入的L帧作为1个time interval,希望对一个time interval 中的frames feature 的attention系数值相近</li><li>仅对frames_features 的attention 系数，计算这样的一个loss</li></ul><img src="https://i.loli.net/2019/07/25/5d397582d36f640160.png" alt="搜狗截图20190725172350.png" title="搜狗截图20190725172350.png">]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;词频&quot;&gt;&lt;a href=&quot;#词频&quot; class=&quot;headerlink&quot; title=&quot;词频&quot;&gt;&lt;/a&gt;词频&lt;/h3&gt;&lt;p&gt;“&amp;gt; =3”的保留&lt;/p&gt;
&lt;p&gt;MSR-VTT :11K   MSVD:4K&lt;/p&gt;
&lt;h3 id=&quot;Attention-Decod
      
    
    </summary>
    
    
  </entry>
  
</feed>
