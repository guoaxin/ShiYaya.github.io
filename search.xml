<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>R-Drop: Regularized Dropout for Neural Networks</title>
      <link href="2021/07/05/R-Drop-Regularized-Dropout-for-Neural-Networks/"/>
      <url>2021/07/05/R-Drop-Regularized-Dropout-for-Neural-Networks/</url>
      
        <content type="html"><![CDATA[<p>来源：<a href="https://mp.weixin.qq.com/s/IvhGbFEMotpKJIUPExUklg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/IvhGbFEMotpKJIUPExUklg</a></p><p>关注 NLP 新进展的读者，想必对四月份发布的 SimCSE [1] 印象颇深，它通过简单的“Dropout 两次”来构造正样本进行对比学习，达到了无监督语义相似度任务的全面 SOTA。无独有偶，最近的论文《R-Drop: Regularized Dropout for Neural Networks》提出了 R-Drop，它将“Dropout两次”的思想用到了有监督任务中，每个实验结果几乎都取得了明显的提升。此外，笔者在自己的实验还发现，它在半监督任务上也能有不俗的表现。</p><p>小小的“Dropout两次”，居然跑出了“五项全能”的感觉，不得不令人惊讶。本文来介绍一下 R-Drop，并分享一下笔者对它背后原理的思考。</p><p><img src="https://i.loli.net/2021/07/05/fCjDeTwZyztI3s6.png" alt="image-20210705111612040" style="zoom:50%;"></p><p><strong>论文标题：</strong>R-Drop: Regularized Dropout for Neural Networks</p><p><strong>论文链接：</strong><a href="https://arxiv.org/abs/2106.14448" target="_blank" rel="noopener">https://arxiv.org/abs/2106.14448</a></p><p><strong>代码链接：</strong><a href="https://github.com/dropreg/R-Drop" target="_blank" rel="noopener">https://github.com/dropreg/R-Drop</a></p><h2 id="1-SimCSE"><a href="#1-SimCSE" class="headerlink" title="1. SimCSE"></a>1. SimCSE</h2><p>《中文任务还是 SOTA 吗？我们给 SimCSE 补充了一些实验》[1] 中，我们已经对 SimCSE 进行了介绍。简单来说，SimCSE 是 NLP 的一种对比学习方案，对比学习的标准流程是同一个样本通过不同的数据扩增手段得到的结果视为正样本对，而 batch 内的所有其他样本视为负样本，然后就是通过 loss 来缩小正样本的距离、拉大负样本的距离了。</p><p>所以难度主要集中在数据扩增手段上。对于 NLP 来说，我们很难人工构建保证语义不变的数据扩增，所以 SimCSE 干脆不人工进行数据扩增，而是通过“Dropout 两次”的方式来得到同一个输入的不同特征向量，并将它们视为正样本对。奇怪的是，这个简单的“Dropout 两次”构造正样本，看上去是一种“无可奈何”的妥协选择，但消融实验却发现它几乎优于所有其他数据扩增方法，令人惊讶之余又让人感叹“大道至简”。</p><p><img src="https://i.loli.net/2021/07/05/Z8UKsXLWCAyp3dO.png" alt="image-20210705112154737" style="zoom:50%;"></p><p>在实现上，SimCSE 也相当简单，所谓“Dropout 两次”，只需要将样本重复地输入到模型，然后计算相应的 loss 就行了，如上图所示。由于 Dropout 本身的随机性，每个样本的 Dropout 模式都是不一样的，所以只要单纯地重复样本，就可以实现“Dropout 两次”的效果。</p><h2 id="2-R-Drop"><a href="#2-R-Drop" class="headerlink" title="2. R-Drop"></a>2. R-Drop</h2><p>从结果上来看，SimCSE 就是希望 Dropout对模型结果不会有太大影响，也就是模型输出对 Dropout 是鲁棒的。所以很明显，“Dropout 两次”这种思想是可以推广到一般任务的，这就是 R-Drop（Regularized Dropout）。</p><h3 id="2-1-分类问题"><a href="#2-1-分类问题" class="headerlink" title="2.1 分类问题"></a>2.1 分类问题</h3><p>在笔者看来，R-Drop 跟 SimCSE 是高度相关的，甚至 R-Drop 应该是受到了 SimCSE 启发的，不过 R-Drop 论文并没有引用 SimCSE，所以这就比较迷了。</p><p><img src="https://i.loli.net/2021/07/05/PkRl8QUn2uC1W5j.png" alt="image-20210705112225164" style="zoom: 80%;"></p><p>具体来说，以分类问题为例，训练数据为 ，模型为 ，每个样本的 loss 一般是交叉熵：</p><p>$\mathcal{L}_{i}=-\log P_{\theta}\left(y_{i} \mid x_{i}\right)$</p><p>在“Dropout 两次”的情况下，其实我们可以认为样本已经通过了两个略有不同的模型，我们分别记为 和 。这时候 R-Drop 的 loss 分为两部分，一部分是常规的交叉熵：</p><p>$\mathcal{L}_{i}^{(C E)}=-\log P_{\theta}^{(1)}\left(y_{i} \mid x_{i}\right)-\log P_{\theta}^{(2)}\left(y_{i} \mid x_{i}\right)$</p><p>另一部分则是两个模型之间的对称 KL 散度，它希望不同 Dropout 的模型输出尽可能一致：</p><p>$\mathcal{L}_{i}^{(K L)}=\frac{1}{2}\left[K L\left(P_{\theta}^{(2)}\left(y \mid x_{i}\right) | P_{\theta}^{(1)}\left(y \mid x_{i}\right)\right)+K L\left(P_{\theta}^{(1)}\left(y \mid x_{i}\right) | P_{\theta}^{(2)}\left(y \mid x_{i}\right)\right)\right]$</p><p>最终 loss 就是两个 loss 的加权和：</p><p>$\mathcal{L}_{i}=\mathcal{L}_{i}^{(C E)}+\alpha \mathcal{L}_{i}^{(K L)}$</p><p>也就是说，它在常规交叉熵的基础上，加了一项强化模型鲁棒性正则项。</p><h3 id="2-2-一般形式"><a href="#2-2-一般形式" class="headerlink" title="2.2 一般形式"></a>2.2 一般形式</h3><p>可能有些读者会问非分类问题应该将 KL 项替换为什么，事实上原论文并没有在非分类问题上进行实验，不过这里可以补充一下。我们可以留意到：</p><p>$-\log P_{\theta}\left(y_{i} \mid x_{i}\right)=K L\left(\right.$ one_hot $\left.\left(y_{i}\right) | P_{\theta}\left(y \mid x_{i}\right)\right)$</p><p>所以，上述 只不过是 KL 散度的反复使用，它的一般形式是：</p><p>$\mathcal{L}_{i}=\mathcal{D}\left(y_{i}, f_{\theta}^{(1)}\left(x_{i}\right)\right)+\mathcal{D}\left(y_{i}, f_{\theta}^{(2)}\left(x_{i}\right)\right)+\frac{\alpha}{2}\left[\mathcal{D}\left(f_{\theta}^{(2)}\left(x_{i}\right), f_{\theta}^{(1)}\left(x_{i}\right)\right)+\mathcal{D}\left(f_{\theta}^{(1)}\left(x_{i}\right), f_{\theta}^{(2)}\left(x_{i}\right)\right)\right]$</p><p>因此对于非分类问题，我们将 换成适当的度量（而不是 KL 散度）即可。</p><h2 id="3-实验效果"><a href="#3-实验效果" class="headerlink" title="3. 实验效果"></a>3. 实验效果</h2><p>我们先来看看 R-Drop 的实验结果。</p><p>R-Drop 的主要超参有三个：batch_size、 和 Dropout 概率。batch_size 一 般取决于我们的算力，对个人来说调整空间不大；原论文的 从 都有，笔者自己的实验中，则取了 ，也没细调。至于 Dropout的概率，跟笔者在《中文任务还是 SOTA 吗？我们给 SimCSE  补充了一些实验》[1] 所选的一样，设为 0.3 效果比较好。</p><h3 id="3-1-论文报告"><a href="#3-1-论文报告" class="headerlink" title="3.1 论文报告"></a>3.1 论文报告</h3><p>说实话，原论文所报告的 R-Drop 的效果是相当让人惊艳的，这也是笔者不得不要介绍一波 R-Drop 的主要原因。原论文在 NLU、NLG、CV 的分类等多种任务上都对 R-Drop 做了对比实验，大部分实验效果都称得上“明显提升”。</p><p>官方实现：<a href="https://github.com/dropreg/R-Drop" target="_blank" rel="noopener">https://github.com/dropreg/R-Drop</a></p><p>下面截图一部分实验结果：</p><p><img src="https://i.loli.net/2021/07/05/Ff3U5wNhAv9kEjz.png" alt="image-20210705112718166" style="zoom:50%;"></p><p>▲ R-Drop在机器翻译任务上的效果</p><p><img src="https://i.loli.net/2021/07/05/tVo7znGCKvUDN5m.png" alt="image-20210705112750407" style="zoom: 50%;"></p><p>▲ R-Drop在GLUE任务上的效果</p><p>特别地，在机器翻译任务上，简单的“Transformer + R-Drop”超过了其他更加复杂方法的效果：</p><p><img src="https://i.loli.net/2021/07/05/gm1UzqLS9pB5KIy.png" alt="image-20210705112819739" style="zoom: 50%;"></p><p>▲ 机器翻译任务上不同方法的对比</p><p>论文还包括自动摘要、语言模型、图像分类等实验，以及关于超参数的一些消融实验，大家仔细看原论文就好。总的来说，R-Drop 的这份“成绩单”，确实足以让人为之点赞了。</p><h3 id="3-2-个人尝试"><a href="#3-2-个人尝试" class="headerlink" title="3.2 个人尝试"></a>3.2 个人尝试</h3><p>当然，笔者坚持的观点是“没有在中文测试过的模型是没有灵魂的”，一般情况下笔者都是在中文任务上亲自尝试过之后，才会写作分享。</p><p>个人实现：<a href="https://github.com/bojone/r-drop" target="_blank" rel="noopener">https://github.com/bojone/r-drop</a></p><p>有中文监督任务上，笔者实验了两个文本分类任务（CLUE 榜单的 IFLYTEK 和 TNEWS）。</p><p><img src="https://i.loli.net/2021/07/05/6WdlENPmujJQh8y.png" alt="image-20210705112907796" style="zoom:50%;"></p><p>和一个文本生成任务（CSL 标题生成，参考 <a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247504805&amp;idx=2&amp;sn=e0e149112e3318bf65d309e32db356b2&amp;chksm=96ea0c25a19d8533ec3d623547b415a58072b353037ac4cd95c68d9dc67f6eb48370684dc6dd&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Seq2Seq 中 Exposure Bias 现象的浅析与对策</a>）：</p><p><img src="https://i.loli.net/2021/07/05/epuUgz8yJSC6jxY.png" alt="image-20210705112924382" style="zoom:50%;"></p><p>可以看到，R-Drop 的结果足以 PK 在<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247504686&amp;idx=2&amp;sn=087dc7e98ede7960b3baacb20d55ce40&amp;chksm=96ea0caea19d85b8dccc52f07763b82f4aaffda6dd4cbdd784e682f13de6a11c6ced5b8df96f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">对抗训练浅谈：意义、方法和思考（附Keras 实现</a>）中介绍的著名正则化手段“对抗训练”和“梯度惩罚”了。</p><h3 id="3-3-实现要点"><a href="#3-3-实现要点" class="headerlink" title="3.3 实现要点"></a>3.3 实现要点</h3><p>相比于对抗学习等复杂正则化方法，R-Drop 的实现难度可谓是相当低了，这里以 bert4keras 为例，简单介绍一下如何将一个普通的训练脚本改为带 Dropout 的模式。</p><p>首先，是数据生成部分，改动如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">data_generator</span><span class="params">(DataGenerator)</span>:</span></span><br><span class="line">    <span class="string">"""数据生成器</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self, random=False)</span>:</span></span><br><span class="line">        batch_token_ids, batch_segment_ids, batch_labels = [], [], []</span><br><span class="line">        <span class="keyword">for</span> is_end, (text, label) <span class="keyword">in</span> self.sample(random):</span><br><span class="line">            token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)</span><br><span class="line">            <span class="comment"># batch_token_ids.append(token_ids)</span></span><br><span class="line">            <span class="comment"># batch_segment_ids.append(segment_ids)</span></span><br><span class="line">            <span class="comment"># batch_labels.append([label])</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">                batch_token_ids.append(token_ids)</span><br><span class="line">                batch_segment_ids.append(segment_ids)</span><br><span class="line">                batch_labels.append([label])</span><br><span class="line">            <span class="comment"># if len(batch_token_ids) == self.batch_size or is_end:</span></span><br><span class="line">            <span class="keyword">if</span> len(batch_token_ids) == self.batch_size * <span class="number">2</span> <span class="keyword">or</span> is_end:</span><br><span class="line">                batch_token_ids = sequence_padding(batch_token_ids)</span><br><span class="line">                batch_segment_ids = sequence_padding(batch_segment_ids)</span><br><span class="line">                batch_labels = sequence_padding(batch_labels)</span><br><span class="line">                <span class="keyword">yield</span> [batch_token_ids, batch_segment_ids], batch_labels</span><br><span class="line">                batch_token_ids, batch_segment_ids, batch_labels = [], [], []</span><br></pre></td></tr></table></figure><p>然后，自定义一个新 loss：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.losses <span class="keyword">import</span> kullback_leibler_divergence <span class="keyword">as</span> kld</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">categorical_crossentropy_with_rdrop</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="string">"""配合上述生成器的R-Drop Loss</span></span><br><span class="line"><span class="string">    其实loss_kl的除以4，是为了在数量上对齐公式描述结果。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    loss_ce = K.categorical_crossentropy(y_true, y_pred)  <span class="comment"># 原来的loss</span></span><br><span class="line">    loss_kl = kld(y_pred[::<span class="number">2</span>], y_pred[<span class="number">1</span>::<span class="number">2</span>]) + kld(y_pred[<span class="number">1</span>::<span class="number">2</span>], y_pred[::<span class="number">2</span>])</span><br><span class="line">    <span class="keyword">return</span> K.mean(loss_ce) + K.mean(loss_kl) / <span class="number">4</span> * alpha</span><br></pre></td></tr></table></figure><p>最后把模型的 Dropout 打开，并用这个 data_generator 和 categorical_crossentropy_with_rdrop 来训练模型就行了。</p><h2 id="4-个人理解"><a href="#4-个人理解" class="headerlink" title="4. 个人理解"></a>4. 个人理解</h2><p>看完了让人赏心悦目的实验结果后，我们来啃一下理论。原论文提供了对 R-Drop 的一个理论分析，大致意思是 R-Drop 会促进参数的同化，从而起到正则化作用。不过个人感觉这个解释并不直观，而且还不够本质。下面笔者试图提供 R-Drop 的另外几个角度的理解。</p><h3 id="4-1-一致性"><a href="#4-1-一致性" class="headerlink" title="4.1 一致性"></a>4.1 一致性</h3><p>R-Dropout 可以看成是 Dropout 的改进，那 Dropout 有什么问题呢？其实 Dropout 是典型的训练和预测不一致的方法。具体来说，Dropout 在训练阶段往（某些层的）输入加上了乘性噪声，使得模型从 变成了 ，其中 的每个元素有 p 的概率为 0，剩下 1-p 的概率为 1/(1-p)，训练目标就是：</p><p>$\mathbb{E}_{(x, y) \sim \mathcal{D}} \mathbb{E}_{\varepsilon}\left[l\left(y, f_{\theta}(x, \varepsilon)\right)\right]$</p><p>这样训练之后，我们应该用哪个模型预测最好呢？不确定，但如果损失函数是 距离的话，那么我们可以推出最佳预测模型应该是：</p><p>$\mathbb{E}_{\varepsilon}\left[f_{\theta}(x, \varepsilon)\right]$</p><p><strong>推导：</strong>如果用 损失，此时单个样本的损失是：</p><p>$\mathbb{E}_{\varepsilon}\left[\left|y-f_{\theta}(x, \varepsilon)\right|^{2}\right]=|y|^{2}-2\left\langle y, \mathbb{E}_{\varepsilon}\left[f_{\theta}(x, \varepsilon)\right]\right\rangle+\mathbb{E}_{\varepsilon}\left[\left|f_{\theta}(x, \varepsilon)\right|^{2}\right]$</p><p>注意，现在我们的问题是“模型训练完后应该用什么函数来预测”，所以 是常数，y 才是要优化的变量，这只不过是一个二次函数的最小值问题，容易解得 时损失函数最小。</p><p>我们假定这个结果能泛化到一般情况。上式告诉我们，带 Dropout 的模型的正确步骤是“模型融合”：</p><p>对同一个输入多次传入模型中（模型不关闭 Dropout），然后把多次的预测结果平均值作为最终的预测结果。</p><p>但我们一般情况下的预测方式显然不是这样的，而是直接关闭 Dropout 进行确定性的预测，这等价于预测模型由“模型平均”变成了“权重平均”：</p><p>$f_{\theta}\left(x, \mathbb{E}_{\varepsilon}[\varepsilon]\right)=f_{\theta}(x, 1)=f_{\theta}(x)$</p><p>这里的 1 指的是全 1 向量。所以，我们训练的是不同 Dropout 的融合模型，预测的时候用的是关闭 Dropout 的单模型，两者未必等价，这就是 Dropout 的训练预测不一致问题。</p><p>现在，我们就不难理解 R-Drop 了，它通过增加一个正则项，来强化模型对 Dropout 的鲁棒性，使得不同的 Dropout 下模型的输出基本一致，因此能降低这种不一致性，促进“模型平均”与“权重平均”的相似性，从而使得简单关闭 Dropout 的效果等价于多 Dropout 模型融合的结果，提升模型最终性能。</p><h3 id="4-2-连续性"><a href="#4-2-连续性" class="headerlink" title="4.2 连续性"></a>4.2 连续性</h3><p>本文开头就提到 R-Drop 与 SimCSE 的相似性，事实上它还跟另外一个方法相当相似，那便是“虚拟对抗训练（Virtual Adversarial Training，VAT）”。（不过 R-Drop 也没引 VAT，难道就只有笔者觉得像吗？？）</p><p>关于 VAT 的介绍，大家可以参考笔者之前的文章<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247507771&amp;idx=1&amp;sn=b92559c624c7c58e77a11c89bec5e99a&amp;chksm=96ea00bba19d89adb05dd98193e7371d43f090fcaf6be401e01c0109beee7920e84cb13274c9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练</a>。简单来说，VAT 也是通过一个正则项，使得模型对扰动更加鲁棒，增强模型本身的连续性（小的变化不至于对结果产生大的影响）。它们不同的地方在于加扰动的方式，VAT 只把扰动加入到输入中，并且通过对抗的思想提升扰动的针对性；R-Drop 的扰动则可以施加到模型的每一层中，并且扰动是随机的。</p><p>有读者可能想到了，VAT 可是主打半监督训练的，那是不是意味着 R-Drop 也可以做半监督训练？这部分原论文并没有实验，是笔者自己做的实验，答案是确实可以，跟 VAT 类似，R-Drop 新增的 KL 散度项是不需要标签的，因此可以无监督训练，混合起来就是半监督了，效果也还不错。下面是笔者的实验结果：</p><p><img src="https://i.loli.net/2021/07/05/UT8bI3qG54D19wt.png" alt="image-20210705113103693" style="zoom:50%;"></p><p>可以看到，R-Drop 的半监督效果完全不逊色于 VAT，而且它实现比 VAT 简单，速度也比 VAT 快！看来 VAT 有望退休了～ 直觉上来看，虽然 R-Drop 的扰动是随机的，但是 R-Drop 的扰动更多，所以它造成的扰动也会放大，也可能比得上 VAT 经过对抗优化的扰动，所以 R-Drop 能够不逊色于 VAT。</p><h3 id="4-3-非目标类"><a href="#4-3-非目标类" class="headerlink" title="4.3 非目标类"></a>4.3 非目标类</h3><p>一个比较直接的疑问是，如果我的模型够复杂，单靠交叉熵这一项，不能使得模型对 Dropout 鲁棒吗？KL 散度那一项造成了什么直接的区别？</p><p>事实上，还真的不能。要注意的是，交叉熵的训练目标主要是：让目标类的得分大于非目标类的得分，这样模型就能正确地把目标类预测出来了（参考<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247505145&amp;idx=1&amp;sn=42467a5475b64d3031a46594261600d2&amp;chksm=96ea0b79a19d826fb7e040968bbe928daf528d38e63d916b05cbc60a5d350755e619abc2aff8&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">将“softmax+交叉熵”推广到多标签分类问题</a>）。也就是说，如果只有交叉熵这一项，模型的训练结果顶多是：</p><blockquote><p>不同的 Dropout 下，目标类的得分都大于非目标类的得分。</p></blockquote><p>而不能做到：</p><blockquote><p>不同的 Dropout 下，每个类的得分一致。</p></blockquote><p>所以也就没有解决训练预测不一致的问题。从公式上来看，交叉熵（2）只跟目标类别有关，不关心非目标类的分布，假如目标类为第一个类别，那么预测结果是 [0.5, 0.2, 0.3] 或 [0.5, 0.3, 0.2]，对它来说都没区别。但对于 KL 散度项（3）来说就不一样了，每个类的得分都要参与计算，[0.5, 0.2, 0.3] 或 [0.5, 0.3, 0.2] 是有非零损失的。</p><h2 id="5-本文小结"><a href="#5-本文小结" class="headerlink" title="5. 本文小结"></a>5. 本文小结</h2><p>本文介绍了 R-Drop，它将“Dropout 两次”的思想用到了有监督任务中，每个实验结果几乎都取得了明显的提升。此外，笔者在自己的实验还发现，它在半监督任务上也能有不俗的表现。最后，分享了笔者对 R-Drop 的三个角度的思考。</p><h2 id="6-参考文献"><a href="#6-参考文献" class="headerlink" title="6. 参考文献"></a>6. 参考文献</h2><p>[1] <a href="https://kexue.fm/archives/8348" target="_blank" rel="noopener">https://kexue.fm/archives/8348</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP, ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP, ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unifying Vision-and-Language Tasks via Text Generation</title>
      <link href="2021/07/03/Unifying-Vision-and-Language-Tasks-via-Text-Generation/"/>
      <url>2021/07/03/Unifying-Vision-and-Language-Tasks-via-Text-Generation/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/j-min/VL-T5" target="_blank" rel="noopener">code</a></p><h2 id="当前跨模态预训练模型存在的问题"><a href="#当前跨模态预训练模型存在的问题" class="headerlink" title="当前跨模态预训练模型存在的问题"></a>当前跨模态预训练模型存在的问题</h2><ul><li>当前跨模态预训练模型在做下游任务时，通常都是根据特定任务设计相应的 head 和 objective。例如，a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning.</li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>为了避免复杂的设计，本文提出了一个联合框架，可以在同一个结构中学习不同的任务。<strong style="color:red;">具体地，本文将判别式任务和生成式任务都转化为 Text  Generation task。</strong>本文的这种方法达到了近似SOTA的效果。</p><p>同时本文提出的框架可以在<strong>同一个参数</strong>下进行多任务训练，这种设置下，可以实现与单独训练特定任务达到相似的性能。(yaya解释，意思是说，这个模型在多任务的设置下训练之后，可以直接拿来去做各种任务，与单独训练特定任务的效果近似)</p><p>这种范式的好处：</p><ul><li>不需要为特定任务设计特定的head</li><li>对于一个新的任务，通过对input and output 进行 text rewrite即可，而不需要增加额外的参数或者是设计新的结构和训练目标。</li><li>由于预训练任务是生成式任务，因此相比于MLM这种理解型任务，文本生成能力更强。比如，当我们回答需要非简单答案的开放式问题时，这一点尤其有帮助，在这种情况下，判别性方法只能从<strong>预定义的频繁候选者集合中</strong>回答，而我们的模型可以生成开放式的自然语言答案。</li></ul>]]></content>
      
      
      <categories>
          
          <category> croos-moal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zero-Shot Detection via Vision and Language Knowledge Distillation</title>
      <link href="2021/06/25/Zero-Shot-Detection-via-Vision-and-Language-Knowledge-Distillation/"/>
      <url>2021/06/25/Zero-Shot-Detection-via-Vision-and-Language-Knowledge-Distillation/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>理解对比学习和 SimCSE</title>
      <link href="2021/06/22/%E7%90%86%E8%A7%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%92%8C-SimCSE/"/>
      <url>2021/06/22/%E7%90%86%E8%A7%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%92%8C-SimCSE/</url>
      
        <content type="html"><![CDATA[<p>来源：<a href="https://mp.weixin.qq.com/s/12UvfXnaB4NTy54wWIFZdQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/12UvfXnaB4NTy54wWIFZdQ</a></p><p>2020 年的 Moco 和 SimCLR 等，掀起了对比学习在 CV 领域的热潮，2021 年的 SimCSE，则让 NLP 也乘上了对比学习的东风。下面就尝试用 QA 的形式挖掘其中一些细节知识点，去更好地理解对比学习和 SimCSE。</p><ul><li><p>如何去理解对比学习，它和度量学习的差别是什么？</p></li><li><p>对比学习中一般选择一个 batch 中的所有其他样本作为负例，那如果负例中有很相似的样本怎么办？</p></li><li><p>infoNCE loss 如何去理解，和 CE loss有什么区别？</p></li><li><p>对比学习的 infoNCE loss 中的温度常数的作用是什么？</p></li><li><p>SimCSE 中的 dropout mask 指的是什么，dropout rate 的大小影响的是什么？</p></li><li><p>SimCSE 无监督模式下的具体实现流程是怎样的，标签生成和 loss 计算如何实现？</p></li></ul><h2 id="1-如何去理解对比学习，它和度量学习的差别是什么？"><a href="#1-如何去理解对比学习，它和度量学习的差别是什么？" class="headerlink" title="1. 如何去理解对比学习，它和度量学习的差别是什么？"></a><strong>1. 如何去理解对比学习，它和度量学习的差别是什么？</strong></h2><p>对比学习的思想是去拉近相似的样本，推开不相似的样本，而目标是要从样本中学习到一个好的语义表示空间。</p><p>论文 [1] 给出的 “Alignment and Uniformity on the Hypersphere”，就是一个非常好的去理解对比学习的角度。</p><p>好的对比学习系统应该具备两个属性：Alignment和Uniformity（参考上图）。</p><p>所谓“Alignment”，指的是相似的例子，也就是正例，映射到单位超球面后，应该有接近的特征，也即是说，在超球面上距离比较近；</p><p>所谓“Uniformity”，指的是系统应该倾向在特征里保留尽可能多的信息，这等价于使得映射到单位超球面的特征，尽可能均匀地分布在球面上，分布得越均匀，意味着保留的信息越充分。分布均匀意味着两两有差异，也意味着各自保有独有信息，这代表信息保留充分（参考自 [2]）。</p><p>度量学习和对比学习的思想是一样的，都是去拉近相似的样本，推开不相似的样本。但是对比学习是无监督或者自监督学习方法，而度量学习一般为有监督学习方法。而且对比学习在 loss 设计时，为单正例多负例的形式，因为是无监督，数据是充足的，也就可以找到无穷的负例，但<font color="red">如何构造有效正例才是重点。</font></p><p>而度量学习多为二元组或三元组的形式，如常见的 Triplet 形式（anchor，positive，negative），Hard Negative 的挖掘对最终效果有较大的影响。</p><h2 id="2-对比学习中一般选择一个-batch-中的所有其他样本作为负例，那如果负例中有很相似的样本怎么办？"><a href="#2-对比学习中一般选择一个-batch-中的所有其他样本作为负例，那如果负例中有很相似的样本怎么办？" class="headerlink" title="2. 对比学习中一般选择一个 batch 中的所有其他样本作为负例，那如果负例中有很相似的样本怎么办？"></a>2. <strong>对比学习中一般选择一个 batch 中的所有其他样本作为负例，那如果负例中有很相似的样本怎么办？</strong></h2><p>在无监督无标注的情况下，这样的伪负例，其实是不可避免的，首先可以想到的方式是去扩大语料库，去加大 batch size，以降低 batch 训练中采样到伪负例的概率，减少它的影响。</p><p>另外，神经网络是有一定容错能力的，像伪标签方法就是一个很好的印证，但前提是错误标签数据或伪负例占较小的比例。</p><p>PS：也确有人考虑研究过这个问题，可以参考论文 [3] [4]</p><h2 id="3-infoNCE-loss-如何去理解，和-CE-loss-有什么区别？"><a href="#3-infoNCE-loss-如何去理解，和-CE-loss-有什么区别？" class="headerlink" title="3. infoNCE loss 如何去理解，和 CE loss 有什么区别？"></a>3. <strong>infoNCE loss 如何去理解，和 CE loss 有什么区别？</strong></h2><p><strong>infoNCE loss</strong> 全称 info Noise Contrastive Estimation loss，对于一个 batch 中的样本 i，它的 loss 为：</p><p><img src="https://i.loli.net/2021/06/22/Omj8YCPkqeWwyp4.png" alt="image-20210622211019708" style="zoom:50%;"></p><p>要注意的是，<font color="red">log 里面的分母叠加项是<strong>包括了分子项</strong>的。</font> 分子是正例对的相似度，分母是正例对+所有负例对的相似度，最小化 infoNCE loss，就是去最大化分子的同时最小化分母，也就是最大化正例对的相似度，最小化负例对的相似度。</p><p>上面公式直接看可能没那么清晰，可以把负号放进去，分子分母倒过来化简一下就会很明了了。</p><p><strong>CE loss</strong>，Cross Entropy loss，在输入 p 是 softmax 的输出时：</p><p><img src="https://i.loli.net/2021/06/22/YFIfqETp1S7Kg83.png" alt="image-20210622211041814" style="zoom:50%;"></p><p>在分类场景下，真实标签 y 一般为 one-hot 的形式，因此，CE loss 可以简化成（i 位置对应标签 1）：</p><p><img src="https://i.loli.net/2021/06/22/saTEDlBWg1uUNAp.png" alt="image-20210622211108963" style="zoom:50%;"></p><p>看的出来，info NCE loss 和在一定条件下简化后的 CE loss 是非常相似的，但有一个区别要注意的是：</p><p>infoNCE loss 中的 K 是 batch 的大小，是可变的，是第 i 个样本要和 batch 中的每个样本计算相似度，而 batch 里的每一个样本都会如此计算，因此上面公式只是样本 i 的 loss。</p><p>CE loss 中的 K 是分类类别数的大小，任务确定时是不变的，i 位置对应标签为 1 的位置。不过实际上，infoNCE loss 就是直接可以用 CE loss 去计算的。</p><p>注：1）info NCE loss 不同的实现方式下，它的计算方式和 K 的含义可能会有差异；2）info NCE loss 是基于 NCE loss 的，对公式推导感兴趣的可以参考 [5]。</p><h2 id="4-对比学习的-infoNCE-loss-中的温度常数-t-的作用是什么？"><a href="#4-对比学习的-infoNCE-loss-中的温度常数-t-的作用是什么？" class="headerlink" title="4. 对比学习的 infoNCE loss 中的温度常数 t 的作用是什么？"></a>4. <strong>对比学习的 infoNCE loss 中的温度常数 t 的作用是什么？</strong></h2><p>论文 [6] 给出了非常细致的分析，知乎博客 [7] 则对论文 [6] 做了细致的解读，这里摘录它的要点部分：</p><p>温度系数的作用是调节对困难样本的关注程度：<strong>越小的温度系数越关注于将本样本和最相似的困难样本分开</strong>，去得到更均匀的表示。然而困难样本往往是与本样本相似程度较高的，很多困难负样本其实是潜在的正样本，过分强迫与困难样本分开会破坏学到的潜在语义结构，因此，温度系数不能过小。</p><p>考虑两个极端情况，温度系数趋向于 0 时，对比损失退化为只关注最困难的负样本的损失函数；当温度系数趋向于无穷大时，对比损失对所有负样本都一视同仁，失去了困难样本关注的特性。</p><p>还有一个角度：</p><p>可以把不同的负样本想像成同极点电荷在不同距离处的受力情况，距离越近的点电荷受到的库伦斥力更大，而距离越远的点电荷受到的斥力越小。</p><p>对比损失中，越近的负例受到的斥力越大，具体的表现就是对应的负梯度值越大 [4]。这种性质更有利于形成在超球面均匀分布的特征。</p><p>对照着公式去理解：</p><p><img src="https://i.loli.net/2021/06/22/QTPnuCLMc6Ug1sb.png" alt="image-20210622211420543" style="zoom:33%;"></p><p>温度系数很小时，越相似也即越困难的负例，对应的 就会越大，在分母叠加项中所占的比重就会越大，对整体 loss 的影响就会越大，具体的表现就是对应的负梯度值越大 [7]。</p><p>当然，这仅仅是提供了一种定性的认识，定量的认识和推导可以参见博客 [7]。</p><h2 id="5-SimCSE-中的-dropout-mask-指的是什么，dropout-rate-的大小影响的是什么？"><a href="#5-SimCSE-中的-dropout-mask-指的是什么，dropout-rate-的大小影响的是什么？" class="headerlink" title="5. SimCSE 中的 dropout mask 指的是什么，dropout rate 的大小影响的是什么？"></a>5. <strong>SimCSE 中的 dropout mask 指的是什么，dropout rate 的大小影响的是什么？</strong></h2><p>一般而言的 mask 是对 token 级别的 mask，比如说 BERT MLM 中的 mask，batch 训练时对 padding 位的 mask 等。</p><p>SimCSE 中的 dropout mask，对于 BERT 模型本身，是一种网络模型的随机，是对网络参数 W 的 mask，起到防止过拟合的作用。</p><p>而 SimCSE 巧妙的把它作为了一种 noise，起到数据增强的作用，因为同一句话，经过带 dropout 的模型两次，得到的句向量是不一样的，但是因为是相同的句子输入，最后句向量的语义期望是相同的，因此作为正例对，让模型去拉近它们之间的距离。</p><p>在实现上，因为一个 batch 中的任意两个样本，经历的 dropout mask 都是不一样的，因此，一个句子过两次 dropout，SimCSE 源码中实际上是在一个 batch 中实现的，即 [a,a,b,b…] 作为一个 batch 去输入。</p><p>dropout rate 大小的影响，可以理解为，这个概率会对应有 dropout 的句向量相对无 dropout 句向量，在整个单位超球体中偏移的程度，因为 BERT 是多层的结构，每一层都会有 dropout，这些 noise 的累积，会让句向量在每个维度上都会有偏移的，只是 p 较小的情况下，两个向量在空间中仍较为接近，如论文所说，“keeps a steady alignment”，保证了一个稳定的对齐性。</p><h2 id="6-SimCSE-无监督模式下的具体实现流程是怎样的，标签生成和-loss-计算如何实现？"><a href="#6-SimCSE-无监督模式下的具体实现流程是怎样的，标签生成和-loss-计算如何实现？" class="headerlink" title="6. SimCSE 无监督模式下的具体实现流程是怎样的，标签生成和 loss 计算如何实现？"></a>6. <strong>SimCSE 无监督模式下的具体实现流程是怎样的，标签生成和 loss 计算如何实现？</strong></h2><p>这里用一个简单的例子和 Pytorch 代码来说明：</p><p><strong>前向句子 embedding 计算：</strong></p><p>假设初始输入一个句子集 sents = [a,b]，每一句要过两次 BERT，因此复制成 sents = [a,a,b,b]。</p><p>sents 以 batch 的形式过 BERT 等语言模型得到句向量：batch_emb = [a1,a2,b1,b2]。</p><p><strong>batch 标签生成：</strong>标签为 1 的地方是相同句子不同 embedding 对应的位置。</p><p><img src="https://i.loli.net/2021/06/22/bE15TPnoGCRvFZk.png" alt="image-20210622211637062" style="zoom: 33%;"></p><p>pytorch 中的 CE_loss，要使用一维的数字标签，上面的 one-hot 标签可转换成：[1,0,3,2]。</p><p>可以把 label 拆成两个部分：奇数部分 [1,3…] 和偶数部分 [0,2…]，交替的每个奇数在偶数前面。因此实际生成的时候，可以分别生成两个部分再 concat 并 reshape 成一维。</p><p>pytorch 中 label 的生成代码如下：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造标签</span></span><br><span class="line"><span class="keyword">batch</span>_size = <span class="keyword">batch</span>_emb.size<span class="params">(0)</span></span><br><span class="line">y_<span class="literal">true</span> = torch.cat<span class="params">([torch.arange(1,batch_size,<span class="attr">step</span>=2,<span class="attr">dtype</span>=torch.long)</span><span class="string">.unsqueeze</span><span class="params">(1)</span>,</span><br><span class="line">                    torch.arange<span class="params">(0,batch_size,<span class="attr">step</span>=2,<span class="attr">dtype</span>=torch.long)</span><span class="string">.unsqueeze</span><span class="params">(1)</span>],</span><br><span class="line">                    dim=1)<span class="string">.reshape</span><span class="params">([batch_size,])</span></span><br></pre></td></tr></table></figure><p><strong>score 和 loss计算：</strong></p><p>batch_emb 会先 norm，再计算任意两个向量之间的点积，得到向量间的余弦相似度，维度是：[batch_size, batch_size]。</p><p>但是对角线的位置，也就是自身的余弦相似度，需要 mask 掉，因为它肯定是 1，是不产生 loss 的。</p><p>然后，要除以温度系数，再进行 loss 的计算，loss_func 采用 CE loss，注意 CE loss 中是自带 softmax 计算的。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算score和loss</span></span><br><span class="line"><span class="attr">norm_emb</span> = F.normalize(batch_emb, dim=<span class="number">1</span>, p=<span class="number">2</span>)</span><br><span class="line"><span class="attr">sim_score</span> = torch.matmul(norm_emb, norm_emb.transpose(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="attr">sim_score</span> = sim_score - torch.eye(batch_size) * <span class="number">1</span>e12</span><br><span class="line"><span class="attr">sim_score</span> = sim_score * <span class="number">20</span>        <span class="comment"># 温度系数为 0.05，也就是乘以20</span></span><br><span class="line"><span class="attr">loss</span> = loss_func(sim_score, y_<span class="literal">true</span>)</span><br></pre></td></tr></table></figure><p>完整代码：</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">loss_func</span> = nn.CrossEntropyLoss()</span><br><span class="line">def simcse_loss(batch_emb):</span><br><span class="line">    <span class="string">""</span><span class="string">"用于无监督SimCSE训练的loss</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    <span class="comment"># 构造标签</span></span><br><span class="line">    <span class="attr">batch_size</span> = batch_emb.size(<span class="number">0</span>)</span><br><span class="line">    <span class="attr">y_true</span> = torch.cat([torch.arange(<span class="number">1</span>, batch_size, <span class="attr">step=2,</span> <span class="attr">dtype=torch.long).unsqueeze(1),</span></span><br><span class="line">                        torch.arange(<span class="number">0</span>, batch_size, <span class="attr">step=2,</span> <span class="attr">dtype=torch.long).unsqueeze(1)],</span></span><br><span class="line">                       <span class="attr">dim=1).reshape([batch_size,])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算score和loss</span></span><br><span class="line">    <span class="attr">norm_emb</span> = F.normalize(batch_emb, <span class="attr">dim=1,</span> <span class="attr">p=2)</span></span><br><span class="line">    <span class="attr">sim_score</span> = torch.matmul(norm_emb, norm_emb.transpose(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="attr">sim_score</span> = sim_score - torch.eye(batch_size) * <span class="number">1</span>e12</span><br><span class="line">    <span class="attr">sim_score</span> = sim_score * <span class="number">20</span></span><br><span class="line">    <span class="attr">loss</span> = loss_func(sim_score, y_true)</span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure><p>注：看过论文源码 [8] 的同学可能会发现，这个和论文源码中的实现方式不一样，论文源码是为了兼容无监督 SimCSE 和有监督 SimCSE，并兼容有 hard negative 的三句输入设计的，因此实现上有差异。</p><p>看过苏神源码 [9] 的同学也会发现，构造标签的地方不一样，那是因为 keras 的 CE loss 用的是 one-hot 标签，pytorch 用的是数字标签，但本质一样。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</p><p>[2] <a href="https://zhuanlan.zhihu.com/p/367290573" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/367290573</a></p><p>[3] Debiased Contrastive Learning</p><p>[4] ADACLR: Adaptive Contrastive Learning Of Representation By Nearest Positive Expansion</p><p>[5] <a href="https://zhuanlan.zhihu.com/p/334772391" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/334772391</a></p><p>[6] Understanding the Behaviour of Contrastive Loss</p><p>[7] <a href="https://zhuanlan.zhihu.com/p/357071960" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/357071960</a></p><p>[8] <a href="https://github.com/princeton-nlp/SimCSE" target="_blank" rel="noopener">https://github.com/princeton-nlp/SimCSE</a></p><p>[9] <a href="https://github.com/bojone/SimCSE" target="_blank" rel="noopener">https://github.com/bojone/SimCSE</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>GEM: A General Evaluation Benchmark for Multimodal Tasks</title>
      <link href="2021/06/22/GEM-A-General-Evaluation-Benchmark-for-Multimodal-Tasks/"/>
      <url>2021/06/22/GEM-A-General-Evaluation-Benchmark-for-Multimodal-Tasks/</url>
      
        <content type="html"><![CDATA[<h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><ul><li><p>类似于 GLUE，SuperGLUE 这种 language evaluation benchmark 可以评估纯语言预训练模型的能力，但是多模态方向上，多模态预训练模型的能力还没有相关的benchmark，本文提出了 <font color="red">多模态任务上 evaluation benchmark：<strong>GEM</strong>。</font></p></li><li><p>本文的多模态任务，包括 image-text <strong>GEM-I</strong>; video-text <strong>GEM-V</strong>。同时相比于当前存在的多模态数据集（例如 MSCOCO，Flickr30K，YouCook2, MSR-VTT）， <font color="red"><strong>GEM</strong> 规模上更大，且涵盖多种语言。</font></p></li><li><p>本文提供了两个多模态多语言预训练模型, <a href="https://arxiv.org/abs/2006.02635" target="_blank" rel="noopener">M3P</a> and <a href="https://arxiv.org/abs/2002.06353" target="_blank" rel="noopener">m-UniVL</a> 作为 GEM 的baseline. M3P是一个现成的多语言， image-text 预训练模型，m-UniVL是 本文对video-text预训练模型 UniVL 做的扩展</p></li></ul><h2 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h2><p>总结一下多模态预训练模型中，<strong>包含多语言</strong>的一些模型。</p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval</title>
      <link href="2021/06/18/Frozen-in-Time-A-Joint-Video-and-Image-Encoder-for-End-to-End-Retrieval-1/"/>
      <url>2021/06/18/Frozen-in-Time-A-Joint-Video-and-Image-Encoder-for-End-to-End-Retrieval-1/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VinVL: Revisiting Visual Representations in Vision-Language Models</title>
      <link href="2021/06/16/VinVL-Revisiting-Visual-Representations-in-Vision-Language-Models/"/>
      <url>2021/06/16/VinVL-Revisiting-Visual-Representations-in-Vision-Language-Models/</url>
      
        <content type="html"><![CDATA[<h2 id="本文的任务"><a href="#本文的任务" class="headerlink" title="本文的任务"></a>本文的任务</h2><p>研究如何改善 V+L 跨模态预训练模型</p><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>通过改善目标检测器模型，来改善以物体 (object) 为中心的图像表示 (image representation)</p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UniT: Multimodal Multitask Learning with a Unified Transformer</title>
      <link href="2021/06/15/UniT-Multimodal-Multitask-Learning-with-a-Unified-Transformer/"/>
      <url>2021/06/15/UniT-Multimodal-Multitask-Learning-with-a-Unified-Transformer/</url>
      
        <content type="html"><![CDATA[<p>曾几何时，多模态预训练已经不是一个新的话题，各大顶会诸多论文仿佛搭上Visual和BERT，就能成功paper+=1，VisualBERT、ViLBERT层出不穷，傻傻分不清楚……这些年NLPer在跨界上忙活的不亦乐乎，提取视觉特征后和文本词向量一同输入到万能的Transformer中，加大力度预训练，总有意想不到的SOTA。</p><p>如何在多模态的语境中更细致准确地利用Transformer强大的表达能力呢？Facebook最新的 <strong><em>Transformer is All You Need</em></strong> 也许可以给你答案。</p><p><img src="https://i.loli.net/2021/06/15/7HAQCbFuxkGpOgt.png" alt="image-20210615125803091" style="zoom:67%;"></p><p>这篇貌似标题党的文章开宗明义，针对文本+视觉的多模态任务，用好Transformer就够了，与许多前作不同，这次提出的模型一个模型可以解决多个任务：目标检测、自然语言理解、视觉问答，各个模型板块各司其职、条理清晰：<strong>视觉编码器</strong>、<strong>文本编码器</strong>、<strong>特征融合解码器</strong>，都是建立在多层Transformer之上，最后添加为每个任务设计的<strong>处理器</strong>，通过多任务训练，一举刷新了多个任务的榜单。</p><p><strong>论文题目</strong>:<br><strong><em>Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer</em></strong></p><p><strong>论文链接</strong>:<br><em><a href="https://arxiv.org/pdf/2102.10772.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2102.10772.pdf</a></em></p><p><img src="https://i.loli.net/2021/06/15/EXpkQu8UT67eYrn.png" alt="image-20210615125848323" style="zoom: 67%;"></p><h2 id="文本编码器"><a href="#文本编码器" class="headerlink" title="文本编码器"></a>文本编码器</h2><p>用Transformer提取文本特征是个老生常谈的问题，从BERT石破天惊开始，纯文本领域近乎已被Transformer蚕食殆尽，所以该文也不能免俗，直接借用BERT的结构提取文本内容，区别在于，为了解决多个任务，在文本序列前添加了一个针对不同任务的参数向量，在最后输出隐藏状态到解码器时再去掉。</p><p><img src="https://i.loli.net/2021/06/15/l5z9uxCfsK3iEkD.png" alt="image-20210615125939485" style="zoom:50%;"></p><h2 id="视觉编码器"><a href="#视觉编码器" class="headerlink" title="视觉编码器"></a>视觉编码器</h2><p>本文将Transformer强大的表达能力运用到视觉特征的提取中，由于图片像素点数量巨大，首先通过基于卷积神经网络的ResNet-50提取卷积特征，极大程度上地降低了特征数量，最终得到的feature map大小为，然后用全联接层调整单个特征的维度到，再利用多层Transformer中的注意力机制提取各个feature之间的关系，由于Transformer的输入是序列，文章将拉成一条长为的序列，另外和文本编码器类似，同样添加了与下游任务相关的。</p><p><img src="https://i.loli.net/2021/06/15/8s9PnX7Zgz3piNS.png" alt="image-20210615130014598" style="zoom:67%;"></p><p>其中是调整维度的全联接层，是多层Transformer编码器。</p><h2 id="模态融合解码器"><a href="#模态融合解码器" class="headerlink" title="模态融合解码器"></a>模态融合解码器</h2><p>多模态的关键之一就在于怎么同时利用多个模态，在本文中是通过Transformer的解码器实现的，这个解码器首先将任务相关的query做self-attention，再将结果与文本编码器和视觉编码器的结果做cross-attention，针对单一模态的任务，选取对应编码器的输出即可，针对多模态的任务，取两个编码器输出的拼接。</p><p><img src="https://i.loli.net/2021/06/15/TJHRK84Ppv9jzYU.png" alt="image-20210615130046055" style="zoom:50%;"></p><h2 id="任务处理器-task-specific-output-head"><a href="#任务处理器-task-specific-output-head" class="headerlink" title="任务处理器(task-specific output head)"></a>任务处理器(task-specific output head)</h2><p>之前多模态预训练模型往往只针对某一项任务，而本文提出的一个模型可以解决多个文本+视觉任务，与BERT可以解决多个文本任务类似，本文的模型在模态融合解码器的结果上添加为每个任务设计的处理器，这个处理器相对简单，用于从隐藏状态中提取出与特定任务相匹配的特征。</p><ul><li><strong>目标检测</strong>：添加box_head和class_head两个前馈神经网络从最后一层隐藏状态中提取特征用来确定目标位置和预测目标类型。</li></ul><p><img src="https://i.loli.net/2021/06/15/jZhuaekyDWdP46w.png" alt="image-20210615130137970"></p><ul><li><strong>自然语言理解、视觉问答</strong>：通过基于全联接层的分类模型实现，将模态融合解码器结果的第一位隐藏状态输入到两层全联接层并以GeLU作为激活函数，最后计算交叉熵损失。</li></ul><p><img src="https://i.loli.net/2021/06/15/kCJYopFwB7MKRE4.png" alt="image-20210615130146288"></p><h2 id="实验与总结"><a href="#实验与总结" class="headerlink" title="实验与总结"></a>实验与总结</h2><p>本文提出的多模态预训练模型各个板块划分明确，通过多层Transformer分别提取特征，再利用解码器机制融合特征并完成下游任务，同时借助最后一层任务相关的处理器，可以通过一个模型解决多个任务，同时也让多任务预训练成为可能，并在实验中的各个数据集上得到了论文主要进行了两部分实验：</p><h3 id="多任务学习："><a href="#多任务学习：" class="headerlink" title="多任务学习："></a>多任务学习：</h3><p>这里的多任务涉及目标检测和视觉问答两个任务，在目标检测上运用COCO和VG两个数据集，在视觉问答上运用VQAv2数据集。对比了单一任务和多任务同时训练的结果，同时对比了不同任务共用解码器的结果。</p><p><img src="C:\Users\shiyaya\AppData\Roaming\Typora\typora-user-images\image-20210615130353457.png" alt="image-20210615130353457"></p><p>从结果中我们可以看出，单纯的使用多任务训练并不一定可以提高结果，不同任务间虽然相关但是却不完全相同，这可能是任务本身差异或者数据集的特性所导致，第二行和第五行可以很明显地看出COCO上的目标检测和VQAv2的视觉问答相结合后，结果有显著的下降，然而VG上的目标检测却能够和视觉问答很好地结合，通过三个数据集上的共同训练，可以得到最高的结果。</p><h3 id="多模态学习："><a href="#多模态学习：" class="headerlink" title="多模态学习："></a>多模态学习：</h3><p>这一实验中，为了体现所提出模型能够有效解决多个多种模态的不同任务，论文作者在之前COCO、VG、VQAv2的基础上，增加了单一文本任务GLUE的几个数据集（QNLI、QQP、MNLI、SST-2）和视觉推断数据集SNLI-VE，从数据集的数量上可以看出本文模型的全能性。与本文对比的有纯文本的BERT、基于Transformer的视觉模型DETR、多模态预训练模型VisualBERT。</p><p><img src="https://i.loli.net/2021/06/15/hSUibrVRlXz8KF2.png" alt="image-20210615130226872"></p><p>仔细看各个数据集上的结果，不难看出本文提出的模型其实并不能在所有数据集多上刷出SOTA，比如COCO上逊色于DETR，SNLI-VE逊色于VisualBERT，SST-2逊色于BERT，其他数据集上都有一定的提高，但是模型却胜在一个“全”字，模型的结构十分清晰明了，各个板块的作用十分明确，同时针对不同任务的处理器也对后续多模态任务富有启发性。</p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SimCSE: Simple Contrastive Learning of Sentence Embeddings</title>
      <link href="2021/06/11/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/"/>
      <url>2021/06/11/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Non-Autoregressive Neural Machine Translation</title>
      <link href="2021/06/07/Non-Autoregressive-Neural-Machine-Translation/"/>
      <url>2021/06/07/Non-Autoregressive-Neural-Machine-Translation/</url>
      
        <content type="html"><![CDATA[<h2 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h2><p> <a href="https://homes.cs.washington.edu/~jkasai/2020-01-28/nat/" target="_blank" rel="noopener">https://homes.cs.washington.edu/~jkasai/2020-01-28/nat/</a></p><p>翻译版：<a href="https://zhuanlan.zhihu.com/p/110794460" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/110794460</a></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇博客针对当前使用<strong>非自回归</strong>方式处理<strong>机器翻译</strong>任务的相关论文进行总结。</p><h2 id="为什么要进行非自回归机器翻译？"><a href="#为什么要进行非自回归机器翻译？" class="headerlink" title="为什么要进行非自回归机器翻译？"></a>为什么要进行非自回归机器翻译？</h2><p>最近的一系列工作提出了非自回归机器翻译的方法 (NAT, <a href="https://arxiv.org/abs/1711.02281" target="_blank" rel="noopener">Gu et al. 2018</a>) 。NAT并行生成目标单词，这与标准自动回归翻译（AT）形成对比，后者可以预测以所有先前单词为条件的每个单词。虽然AT通常在相似配置下比NAT表现更好，但是NAT通过并行计算加快了推理速度。这种非自回归生成的一个非常成功的应用是Parallel WaveNet  (<a href="https://arxiv.org/abs/1711.10433" target="_blank" rel="noopener">Oord et al. 2017</a>)，将原始自回归Wavenet的速度提高了1000倍以上，并部署在了Google助手中。从NAT快速推断得到的收益可以允许在工业界的特定延迟和预算下部署更大，更深的Transformer模型。在这篇博客文章中，我将概述有关非自回归翻译的最新研究，并讨论我认为对进一步发展而言缺失或重要的内容。</p><h2 id="基本问题和可能的解决方法"><a href="#基本问题和可能的解决方法" class="headerlink" title="基本问题和可能的解决方法"></a>基本问题和可能的解决方法</h2><p>生成中的<a href="https://arxiv.org/abs/1711.02281" target="_blank" rel="noopener">多模式性(Multimodality)</a>对NAT提出了根本的挑战。我们都知道语言是高度多模态的(multimodal)。举一个小例子，英文句子<code>he is very good at Japanese</code>和<code>he speaks Japanese very well</code>是日语句子<code>彼は日本語が上手です</code>的两个有效译文。但是，看起很像的两个句子：<code>he speaks very good at Japanese</code>或<code>he is very good at very well</code>则没有任何意义。我们需要知道模型提交给它自己的两种可能的翻译是哪一种，但是在条件独立的解码中很难实现这一点。并行解码打破了条件依赖性，并经常导致输出不一致。文献中的一些工作已经提出解决NAT中Multimodality问题的方法。在这里，我对提出的方法进行了概括和分类。</p><h3 id="1-基于迭代（Iteration-based）的方法"><a href="#1-基于迭代（Iteration-based）的方法" class="headerlink" title="1. 基于迭代（Iteration-based）的方法"></a>1. 基于迭代（Iteration-based）<strong>的方法</strong></h3><p>解决并行解码问题的一种方法是迭代地优化模型输出(<a href="https://arxiv.org/abs/1802.06901" target="_blank" rel="noopener">Lee et al., 2018</a>; <a href="https://arxiv.org/abs/1904.09324" target="_blank" rel="noopener">Ghazvininejad et al., 2019</a>; <a href="https://homes.cs.washington.edu/~jkasai/2020-01-28/nat/Ghazvininejad et al., 2019" target="_blank" rel="noopener">Gu et al. 2019</a>; <a href="https://arxiv.org/pdf/2001.05136.pdf" target="_blank" rel="noopener">Kasai et al. 2020</a>)。在此框架中，我们放弃了完全可并行化的生成，而是在<strong>每次迭代中优化了先前生成的单词</strong>。由于通常我们所需的迭代次数比输出句子中单词的数量少得多，因此与自回归模型相比，迭代方法仍可以改善等待时间。这些论文均采用不同的方法进行细化，为了更清晰地理解，您可以参考<a href="https://arxiv.org/abs/1904.09324" target="_blank" rel="noopener">Ghazvininejad et al., 2019</a>  提出的一种典型的条件屏蔽语言模型（CMLM）。在给定源文本的情况下，使用目标端的BERT-style的掩蔽语言建模目标对CMLM进行训练，在推断中，我们Mask住<em>低置信度</em>的token，并在每次迭代中对其进行更新。 <a href="https://arxiv.org/pdf/2001.05136.pdf" target="_blank" rel="noopener">Kasai et al. 2020</a> 工作提出了DisCo Transformer，该Transformer可计算出这种掩盖语言建模MLM的有效替代方案。特别地，在给定其他reference token的任意子集的情况下，可以训练DisCo Transformer来预测每个输出token。可以将其视为一次模拟多个masking。我们证明了DisCo Transformer可以减少所需的迭代次数（从而减少解码时间），同时保持转换质量。</p><blockquote><p>[1] Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement</p><p>[2] Mask-Predict: Parallel Decoding of Conditional Masked Language Models. Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer</p><p>[3] Non-autoregressive Machine Translation with Disentangled Context Transformer. Jungo Kasai,  James Cross,  Marjan Ghazvininejad, Jiatao Gu</p></blockquote><h3 id="2-比NLL更好的训练目标"><a href="#2-比NLL更好的训练目标" class="headerlink" title="2. 比NLL更好的训练目标"></a>2. 比NLL更好的训练目标</h3><p>一些工作提出了对数负似然性NLL的替代损失函数。我的直觉是使用普通NLL损失进行训练无法捕捉高度多峰分布(multimodal distributions)的表征。这在某种程度上让人联想到生成对抗网络（GAN）中的对抗损失。在图像生成中，原始的L2重建损失将使模式崩溃并产生模糊的图像，当使用NLL损失训练NAT模型时，可能会发生类似情况。拟议的替代损失函数包括NAT模型与自回归教师之间的隐藏状态的距离(<a href="https://arxiv.org/abs/1909.06708" target="_blank" rel="noopener">Li et al. 2019</a>)，Ngram词袋差 (<a href="https://arxiv.org/pdf/1911.09320.pdf" target="_blank" rel="noopener">Shao et al. 2020</a>) 和辅助正则化 (<a href="https://arxiv.org/pdf/1902.10245.pdf" target="_blank" rel="noopener">Wang et al. 2019</a>). 。与基于迭代的方法相比，这一系列方法可以并行实现一次生成，但代价是性能大大降低。</p><h3 id="3-精简-部分的自回归解码"><a href="#3-精简-部分的自回归解码" class="headerlink" title="3. 精简/部分的自回归解码"></a>3. 精简/部分的自回归解码</h3><p>先前的工作还提出了将轻度或部分自回归模块整合到NAT模型中的方法。 <a href="https://arxiv.org/pdf/1803.03382.pdf" target="_blank" rel="noopener">Kaiser et al. 2018</a> 生成了较短序列的潜在变量，并在顶部进行了并行单词预测。Blockwise decoding和Insertion Transformer产生的局部自回归方式一个句子 (<a href="https://arxiv.org/abs/1811.03115" target="_blank" rel="noopener">Stern et al. 2018</a>, <a href="https://arxiv.org/abs/1902.03249" target="_blank" rel="noopener">2019</a>).  <a href="https://arxiv.org/abs/1910.11555" target="_blank" rel="noopener">Sun et al. 2019</a> 在变压器输出向量之上引入了factorized CRF层，并通过波束近似(beam approximation)进行了快速自回归解码。 <a href="https://arxiv.org/abs/1911.02215" target="_blank" rel="noopener">Ran et al. 2019</a> 引入了精简自回归源端重排序模块，以促进并行目标解码。请注意，他们还使用非自回归重排序模块显示了结果，但是性能却差得多。</p><h3 id="4-用潜在变量建模"><a href="#4-用潜在变量建模" class="headerlink" title="4. 用潜在变量建模"></a>4. 用潜在变量建模</h3><p>我们可以在此框架中解释许多模型。例如，可以将所有以预测长度(predicted length)为条件的NAT模型视为具有潜在变量的建模。但尤其是 <a href="https://arxiv.org/abs/1909.02480" target="_blank" rel="noopener">Ma et al. 2019</a>  使用生成流技术对目标句子的复杂分布进行建模。 <a href="https://arxiv.org/abs/1908.07181" target="_blank" rel="noopener">Shu et al. 2020</a> 开发出了具有确定性推论(deterministic inference)的连续潜在变量NAT模型。</p><h3 id="5-从自回归模型中蒸馏"><a href="#5-从自回归模型中蒸馏" class="headerlink" title="5. 从自回归模型中蒸馏"></a>5. 从自回归模型中蒸馏</h3><p>据我所知，几乎所有表现好的NAT模型都经过自回归模型 (e.g. <a href="https://arxiv.org/abs/1711.02281" target="_blank" rel="noopener">Gu et al. 2018</a>) 的序列级知识蒸馏(<a href="https://arxiv.org/abs/1606.07947" target="_blank" rel="noopener">Kim &amp; Rush 2016</a>）训练而成。虽然较大Transformer的蒸馏也有助于自回归机器翻译，尤其是在贪婪解码的情况下，但它的作用是是较大程度上简化NAT模型 (<a href="https://arxiv.org/pdf/2001.05136.pdf" target="_blank" rel="noopener">Kasai et al. 2020</a>)。 <a href="https://arxiv.org/abs/1911.02727" target="_blank" rel="noopener">Zhou et al. 2019</a> 研究了模型容量与蒸馏数据之间的关系，表明模型容量与蒸馏数据复杂性之间存在相关性。这表明知识蒸馏可以杀死原始数据中的某些模式，从而可以更好地训练NAT模型。</p><h2 id="悬而未决的问题和未来目标"><a href="#悬而未决的问题和未来目标" class="headerlink" title="悬而未决的问题和未来目标"></a>悬而未决的问题和未来目标</h2><p>在这里，我重点介绍了我个人好奇的非自回归机器翻译中的开放性问题。</p><ul><li><strong>我们需要蒸馏吗？</strong>蒸馏肯定是一次性的训练成本，但是如果每次更改训练数据或语言对时都必须这样做，则蒸馏成本可能会很高。我们是否可以利用原始数据获得合理的性能？</li><li><strong>我们需要预测目标长度吗？</strong>我仍然发现目标长度预测很奇怪。当前的许多NAT方法都要求目标长度预测并且以这个预测的长度作为条件。虽然长度预测为我们提供了在潜在变量空间中进行搜索的机会，但长度预测会破坏生成的灵活性。</li><li><strong>NAT可以胜过AT吗？</strong>我们已经看到，在相同的配置下，AT的性能通常优于NAT。但是，NAT可以做得更好吗？或者更实际的说，在相同的延迟预算下，NAT是否能明显胜过AT？NAT可以使用更大的配置。</li><li><strong>预训练和NAT。</strong>在非自回归机器翻译中使用大规模预训练的掩蔽语言模型MLM可能比在自回归翻译中使用更容易。NAT中的解码器（例如条件屏蔽语言模型）看起来更像BERT。</li><li><strong>训练和推理中的隔阂(bridge)。</strong>在迭代NAT框架中，<strong><strong style="color:red;">训练和推理之间经常会出现差距</strong></strong>。For example, a conditional language model (CMLM) is trained to predict masked tokens given the other <strong>gold</strong> observed tokens. 最近一项成功的尝试是对CMLM进行SMART训练 (<a href="https://arxiv.org/abs/2001.08785" target="_blank" rel="noopener">Ghazvininejad et al. 2020</a>) ，他们训练模型以从先前的预测误差中恢复。这种方法可普及到基于迭代的NAT。</li><li><strong>向结构化预测学习。</strong>NLP中已经在结构化预测（例如语法和语义解析）方面投入了很多精力。我们可以从结构化预测的方法中学习以更好地处理生成中的条件依赖性吗？训练和推理之间的上述差距是句法分析中研究的一个问题 (e.g. dynamic oracle, <a href="https://www.aclweb.org/anthology/C12-1059/" target="_blank" rel="noopener">Goldberg &amp; Nivre 2012</a>)。我怀疑从结构化预测中还会吸取更多教训。</li></ul><h2 id="ACL-2021"><a href="#ACL-2021" class="headerlink" title="ACL 2021"></a>ACL 2021</h2><p>【ACL findings】Progressive Multi-Granularity Training for Non-Autoregressive  Translation<br><strong>标题</strong>：非自回归翻译的渐进式多粒度训练</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://zhuanlan.zhihu.com/p/110794460" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/110794460</a></p><p><a href="https://homes.cs.washington.edu/~jkasai/2020-01-28/nat/" target="_blank" rel="noopener">https://homes.cs.washington.edu/~jkasai/2020-01-28/nat/</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The multimodality problem in NAT</title>
      <link href="2021/06/07/The-multimodality-problem-in-NAT/"/>
      <url>2021/06/07/The-multimodality-problem-in-NAT/</url>
      
        <content type="html"><![CDATA[<h2 id="multimodality-problem"><a href="#multimodality-problem" class="headerlink" title="multimodality problem"></a>multimodality problem</h2><p>非自回归神经机器翻译系统(NAT)通过打破自回归性，并行地生成所有目标词，大幅度地提高了推断速度。然而，现有的NAT模型由于<strong>多峰问题</strong>，与自回归神经网络机器翻译模型相比，翻译质量仍有很大差距。</p><p><strong><strong style="color:red;">什么是多峰问题</strong></strong>，举个简单的例子将汉语句子“干/得/好/！”翻译成英文，可以翻译成“Good job !”或者“Well done !”。由于<strong style="color:blue;">非自回归模型</strong>的条件独立性假设，推断时第一个词“Good”和“Well”的概率是差不多大的，如果第二个词“job”和“done”的概率也差不多大，会使得模型生成出“Good done !”或者“Well job !”这样错误的翻译。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation</p><p>Syntactically Supervised Transformers for Faster Neural Machine Translation</p><p>ICML 2021 (Oral)：Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最新机器翻译进展</title>
      <link href="2021/06/07/%E6%9C%80%E6%96%B0%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E8%BF%9B%E5%B1%95/"/>
      <url>2021/06/07/%E6%9C%80%E6%96%B0%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E8%BF%9B%E5%B1%95/</url>
      
        <content type="html"><![CDATA[<h2 id="AAAI-2021"><a href="#AAAI-2021" class="headerlink" title="AAAI  2021"></a>AAAI  2021</h2><p>在 AAAI2021 上同样涌现了许多关于机器翻译任务的研究工作，几乎所有的工作都是基于Transformer模型展开讨论。这里对机器翻译在AAAI2021上的最新研究进展进行总结：</p><h2 id="1-引入语法信息"><a href="#1-引入语法信息" class="headerlink" title="1. 引入语法信息"></a><strong>1. 引入语法信息</strong></h2><p>尽管依托于模型本身本文就能从海量数据中捕获到语言之间的映射关系，但研究人员一直在探索如何将句法、语义等先验知识有效地融入到模型中，并指导模型取得进一步的性能突破。传统的做法通常使用外部工具从训练样本中构造句法树等先验知识，之后在编码端、解码端分别融入先验知识。SyntAligner[1]采取一种自监督双语句法对齐方法，让模型在高维空间中对源语-目标语的句法结构进行精确对齐，从而最大限度地利用对齐后的句法结构之间的互信息提高翻译的性能。</p><blockquote><p>Self-supervised Bilingual Syntactic Alignment for Neural Machine Translation</p></blockquote><h2 id="2-无监督机器翻译"><a href="#2-无监督机器翻译" class="headerlink" title="2. 无监督机器翻译"></a><strong>2. 无监督机器翻译</strong></h2><p>无监督机器翻译同样是机器翻译中备受关注的研究热点。在现实世界中，除了部分富资源语言（如英语，汉语，德语，俄语，印地语等），更多的语言本身受众较小，缺乏海量的双语平行语料进行监督学习。因此，如何在这种资源匮乏，甚至零资源的条件下，学习语言之间的映射是极具挑战的。目前无监督机器翻译通常采用迭代式的back-translation。此外，利用预训练的技术手段能够有效地加快模型的收敛，提高翻译的正确性。[2]通过在构造伪数据的过程中对合成的句子进行正则化约束能够有效地改善翻译的性能。</p><blockquote><p>Empirical Regularization for Synthetic Sentence Pairs in Unsupervised Neural Machine Translation</p></blockquote><h2 id="3-多语言翻译"><a href="#3-多语言翻译" class="headerlink" title="3. 多语言翻译"></a><strong>3. 多语言翻译</strong></h2><p>伴随着机器翻译的发展，研究人员逐渐开始探索不局限于双语句对之间的翻译。多语言模型通过一个模型实现多个语种之间的翻译能够有效降低多语言翻译部署成本。同时将一种源语言翻译成多种不同的目标语言是多语言翻译最常见的场景之一。SimNMT[3]提出了一种同步交叉交互解码器，即在每个目标语生成时，可以依赖未来的信息，以及其他目标语言的历史和未来的上下文信息，充分利用语言内与语言间的信息。</p><h2 id="4-语音翻译"><a href="#4-语音翻译" class="headerlink" title="4. 语音翻译"></a><strong>4. 语音翻译</strong></h2><p>语音翻译直接将源语的语音翻译成目标语言的文本。传统的方法中，采用语音识别和机器翻译级联的方法来解决这一问题。但是具有延迟高，占用存储大，以及容易产生错误累积的问题，很多工作开始关注直接使用端到端的语音到文本的模型来解决这一问题。对于跨模态之间的语言映射，为了让单一的模型充分学习模态之间的关联信息，往往需要引入更多的跨模态和跨语言的特征，造成了沉重的负担，同时单纯的用于端到端模型的语音到文本数据较少，无法充分利用语言识别和机器翻译的数据。为了解决这些问题，COSTT[4]作为一种通用的框架同时结合了级联模型与端到端模型的优点，能够更好地利用大规模双语平行语料，在多个测试集上取得了最优的效果。</p><p>同声传译是一种实时的语言翻译场景，对翻译时延的要求更加严格。目前主流的手段是采用Wait-K策略，但仍然存在由于重复编码导致的训练慢，以及缺少对未来信息建模的问题。Future-guided Training[5]采取unidirectional Transformer方式来避免重复编码，并引入averaged embedding来满足当前词与过去词之间的信息交互。同时利用知识精炼的手段让网络充分利用未来的信息，从而达到更准确的预测。</p><h2 id="5-领域适应"><a href="#5-领域适应" class="headerlink" title="5. 领域适应"></a><strong>5. 领域适应</strong></h2><p>在神经机器翻译中，通过微调来做领域的迁移是一种常见的方法。但是，无约束的微调需要非常仔细的超参数调整，否则很容易在目标域上出现过拟合，导致在通用领域上的性能退化。PRUNE-TUNE[6]是一种基于渐变修剪的领域适应算法。它学习微小的特定于领域的子网以进行调优，通过调整它相应的子网来适应一个新的领域。有效缓解了在微调过中的过拟合和退化问题。</p><p>此外，领域适应与其他方法相结合也是研究的一个热点。元学习对于低资源神经机器翻译(NMT)的有效性已经得到了充分的验证。但是元训练的NMT系统在未见领域中的翻译性能仍然较差。Meta-Curriculum Learning[7]是一种新的面向领域适应的元课程学习方法。在元训练过程中，NMT首先从各个领域学习相似的知识，以避免早期陷入局部最优，最后学习针对不同领域学习个性化的知识，以提高模型对领域特定知识学习的鲁棒性。</p><h2 id="6-解码加速：轻量模型-非自回归解码"><a href="#6-解码加速：轻量模型-非自回归解码" class="headerlink" title="6. 解码加速：轻量模型/非自回归解码"></a><strong>6. 解码加速：轻量模型/非自回归解码</strong></h2><p>过参数化的（超大规模）模型能够有效提升神经机器翻译的性能，但是庞大的存储开销和高昂的计算复杂度使得这类模型无法直接部署到边缘设备(如手机，翻译笔，离线翻译机等)上。早期为了提高模型对未登录词的覆盖度往往使用更大的词表，同时增大了词嵌入矩阵的存储开销，以及构建词表上概率分布时对计算资源的消耗。针对该问题，Partial Vector Quantization[8]提出了一种部分矢量量化的方法，通过压缩词嵌入降低softmax层的计算复杂度，同时使用查找操作来替换softmax层中的大部分乘法运算，在保障翻译质量的同时大大减少了词嵌入矩阵的参数和softmax层的计算复杂度。</p><p>近期，深层模型在神经机器翻译中取得突破性进展，但伴随着层数的堆叠同样面临上述问题。GPKD[9]中提出一种基于群体置换的知识蒸馏方法将深层模型压缩为浅层模型，该方法可以分别应用与编码端与解码端达到模型压缩和解码加速的目的。文中探讨了一种深编码器-浅解码器的异构网络， 其既能保证翻译的准确度，同时满足工业生产的推断时延需求。此外采用子层跳跃的正则化训练方法缓解随着网络加深带来的过拟合问题。</p><p>此外，沿着减少解码端计算复杂度的研究方向，例如Averaged Attention Network（ACL2018）和Sharing Attention Network（IJCAI2019），Compressed Attention Network[10]采取压缩子层的方式，将解码器每一层中分离的多个子层压缩成一个子层，进而简化解码端的计算复杂度，达到解码加速的目的。这种方式在深编码器-浅解码器的结构上取得了进一步的加速增益。</p><p>上述的工作通过轻量化模型提高推断速度，本质上在解码过程中还是采用自回归的方式。相比之下非自回归解码同样是一种有效的解码加速手段。非自回归神经机器翻译系统(NAT)通过打破自回归性，并行地生成所有目标词，大幅度地提高了推断速度。然而，现有的NAT模型由于<strong>多峰问题</strong>，与自回归神经网络机器翻译模型相比，翻译质量仍有很大差距。<strong>什么是多峰问题</strong>，举个简单的例子将汉语句子“干/得/好/！”翻译成英文，可以翻译成“Good job !”或者“Well done !”。由于非自回归模型的条件独立性假设，推断时第一个词“Good”和“Well”的概率是差不多大的，如果第二个词“job”和“done”的概率也差不多大，会使得模型生成出“Good done !”或者“Well job !”这样错误的翻译。ReorderNAT[11]提出一个新颖的NAT框架，通过显式地建模重排序信息来指导非自回归解码。区别于传统方法，根据源语的繁衍率来构造解码端的输入，ReorderNAT在编码器和解码器中间引入了重排序机制。该机制将源语的表示按照目标语的语序进行重新组合，减少解码器对语序的再加工。</p><h2 id="7-评测方法及应用"><a href="#7-评测方法及应用" class="headerlink" title="7. 评测方法及应用"></a><strong>7. 评测方法及应用</strong></h2><p>除了针对机器翻译系统的研究外，如何有效的评估机器翻译系统的性能也是一个重要的研究方向。通常情况下我们使用BLEU作为译文质量评估的常用指标，但是在很多应用场景中，并没有可以对比的参考译文。机器翻译质量评估(QE)便是在不依赖任何参考译文的情况下预测机器翻译质量的一项任务。在QE任务中，通常使用预测器-估计器框架(Predictor-Estimator)。使用预训练的预测器作为特征提取器，再通过评估器对译文进行评估。但是预测器和估计器在训练数据和训练目标上都存在差距，这使得QE模型不能更直接地从大量平行语料库中受益。<strong>DirectQE</strong>[12]中提出了一个新框架，通过生成器在构造QE伪数据，使用额外的探测器在生成的数据上进行训练，并为QE任务设定了新的学习目标，将原本分离的过程进行整合。</p><p>同样机器翻译本身也可以作为工具应用于其他的任务。研究人员为了解决问答任务的数据稀缺问题，通过机器翻译方法来构造多语言问答数据[13]。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <strong>Self-supervised Bilingual Syntactic Alignment for Neural Machine Translation</strong></p><p>[2] Empirical Regularization for Synthetic Sentence Pairs in Unsupervised Neural Machine Translation</p><p>[3] Synchronous Interactive Decoding for Multilingual Neural Machine Translation</p><p>[4] Consecutive Decoding for Speech-to-text Translation</p><p>[5] Future-Guided Incremental Transformer for Simultaneous Translation</p><p>[6] Finding Sparse Structure for Domain Specific Neural Machine Translation</p><p>[7] Meta-Curriculum Learning for Domain Adaptation in Neural Machine Translation</p><p>[8] Accelerating Neural Machine Translation with Partial Word Embedding Compression</p><p>[9] Learning Light-Weight Translation Models from Deep Transformer</p><p>[10] An Efficient Transformer Decoder with Compressed Sub-layers</p><p>[11] <strong>Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information</strong></p><p>[12] <strong>DirectQE: Direct Pretraining for Machine Translation Quality Estimation</strong></p><p>[13] Multilingual Transfer Learning for QA Using Translation as Data Augmentation</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Masked Non-Autoregressive Image Captioning</title>
      <link href="2021/06/06/Masked-Non-Autoregressive-Image-Captioning/"/>
      <url>2021/06/06/Masked-Non-Autoregressive-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>以 Non-Autoregressive 的方式来做 Image Captioning</p><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><ul><li><p>自回归的方式做生成任务存在的问题</p><p>自回归解码导致了一些问题，如连续的错误积累、生成缓慢、不恰当的语义和缺乏多样性。 </p><p><strong>顺序解码很容易从训练数据中 copy tokens 来提高语法准确性</strong>，这很容易造成语义错误，而且在图像字幕的生成方面缺乏多样性。</p></li><li><p>非自回归的方式</p><p>非自回归解码已被提出来解决神经机器翻译（NMT）的生成速度较慢的问题。但直接应用到多模态描述生成任务上不是很直接：由于对真实目标分布的间接建模，不可避免地引入了另一个问题，被称为 “多模态问题”。</p><p>多模态问题【网络】：一种特殊的<strong>问题</strong>，其中不存在唯一的全局解决方案。可以在搜索空间周围找到多个全局优化或一个具有多个局部优化（或峰值）的全局优化。即，一对多问题。</p><p>多模态问题【本文】：完全的条件独立会导致对真实目标分布的近似度低。complete conditional independence results in the poor approximation to the true target distribution。</p></li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>本文提出 masked non-autoregressive decoding 来解决 自回归解码 和 非自回归解码 中存在的问题。</p><ul><li><p>训练阶段</p><p>对于输入句子，以几种 （K） 比例进行掩码。</p></li><li><p>在推理阶段</p><p>在推理过程中，从一个<strong>完全被掩盖</strong>的序列到一个<strong>完全没有被掩盖</strong>的序列，以一种合成的方式，分几个 （K） 阶段平行地生成字幕。</p></li></ul><p>实验证明，我们提出的模型可以更有效地保留语义内容，并可以生成更多样性的标题。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://i.loli.net/2021/06/06/GagfcOSPqskTiEW.png" alt="image-20210606152813892"></p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>使用CNN提取的特征图，或者是 使用目标检测器检测得到的 object features.</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>与传统的Tansformer 来做seq-to-seq 是一样的，只是移去了 decoder的 autoregressive mask ， 即decoder 中的每个token，是双向attention。</p><h3 id="Masked-non-autoregressive-decoding"><a href="#Masked-non-autoregressive-decoding" class="headerlink" title="Masked non-autoregressive decoding"></a>Masked non-autoregressive decoding</h3><p>如图1</p><p>训练阶段，以 K 种比例进行掩码，比如 K = [1, 0.6, 0.2],其中 k=1, 代表全部掩码</p><p>测试阶段，使用 K stage，从全部掩码到部分掩码，来一步一步的优化caption。</p><details><summary>细节-增强鲁邦性</summary>我们还以一定比例的随机词来替代 [MASK] token 或 ground-truth token。 在我们的实验中，由于标题的长度相对较短，我们只是在每个非完全屏蔽的输入序列中用一个随机的词替换一个词。 使用随机词可以增强标记的上下文表示，并通过在训练期间引入噪声标记来提高推理过程的稳健性，因为模型在推理的早期阶段很容易产生错误的标记。</details><h3 id="作者分析"><a href="#作者分析" class="headerlink" title="作者分析"></a>作者分析</h3><p>我们提供了更多的分析和讨论，关于模型在不同比例的掩蔽序列中所学习的内容，以及模型在推理过程中不同阶段的预测。 我们进一步讨论了自回归和屏蔽式非自回归解码之间的内在差异。</p><ul><li><p>在训练阶段</p><ul><li>掩码的比例大时，会输出视觉单词</li><li>掩码的比例小时，会对语法进行修正</li></ul></li><li><p>在测试阶段</p><p>在推理过程中，the masked non-autoregressive 解码过程很好地反映了模型在训练过程中所学习的内容。 在早期阶段，该模型倾向于在语言组织较差的图像中生成包含高频率（如 “a”、”on”）和突出的视觉线索（如物体、颜色名词和重要动词）的caption，而在后期阶段，该模型可以通过采用训练好的双向语言模型来选择最合适的词来连接子序列的两边，从而生成语义和语法上正确的标题。 如图1 </p></li><li><p>自回归和屏蔽式非自回归解码之间的内在差异</p><p>在推理过程中，自回归解码和掩码非自回归解码的区别在于掩码非自回归解码自然接近人类的语言生成。更具体地说，人类首先在大脑中生成视觉场景的关键词，然后选择其他词来连接不同的部分，并按照语言规则组成整个句子。这是一个<strong>先视觉再语言</strong>的生成过程，视觉信息奠定了字幕的基础，语言信息辅助以组合的方式而不是顺序的方式形成最终的字幕，这样会更好地保留有意义的语义信息。<strong>掩码非自回归解码一步生成整个句子，因此前面标记的质量不会显着影响后面的标记，这从根本上缓解了自回归解码中存在的顺序错误累积</strong>。相比之下，自回归解码是一个从左到右逐字的生成过程，因此后面步骤生成的标记在很大程度上取决于前面步骤的标记，一旦前面的标记不合适，就容易出现顺序错误累积。更糟糕的是，它只有一次机会生成整个标题，而无法调整前面不适当的标记。因此，自回归解码在保持流畅性方面相当不错，但难以准确说出图像丰富的显着语义内容。</p></li></ul><h2 id="Inference-rules"><a href="#Inference-rules" class="headerlink" title="Inference rules"></a>Inference rules</h2><ul><li><p>推理阶段每个 stage, 如何确定mask 哪些token？或者保留哪些token ?</p><p><strong>保留信息量最大的 token</strong> 并屏蔽每个阶段生成的字幕中的其他位置以生成新的屏蔽输入序列至关重要。</p><p>在本文中，我们采用了一种直接的方法。在这种方法中，在这种方法中，那些不包括在高频率的标记集中的 tokens (tf-idf的思想) ，以及具有高概率且与迄今所选标记不重复的 tokens ，被指定为高度优先保留的 tokens 。例如，在图 <a href="https://www.arxiv-vanity.com/papers/1906.00717/#S3.F1" target="_blank" rel="noopener">1中</a>，我们保留了第一阶段输出序列的 “二”、“鸭子”、“游泳” 和“水”。此外，对最后阶段生成的 caption 进行处理，选择与之前选择的 tokens 不重复的 tokens 。 </p></li><li><p>推理阶段，caption的长度是如何确定的？</p><p>关于推理过程中 length of caption 的确定, 我们首先计算训练数据中的长度的分布情况，然后从这个分布中选择一个随机的长度 T 作为标题。 随后。由T个 [MASK] token 组成的序列被送入模型，这样，一个完整的标题就可以最终被解码。 另一个选择是，我们直接为所有图像设置一个固定的序列长度。 该模型将根据长度自动强制生成粗略或精细的标题，但具有类似的语义信息。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mask-Predict: Parallel Decoding of Conditional Masked Language Models</title>
      <link href="2021/06/06/Mask-Predict-Parallel-Decoding-of-Conditional-Masked-Language-Models/"/>
      <url>2021/06/06/Mask-Predict-Parallel-Decoding-of-Conditional-Masked-Language-Models/</url>
      
        <content type="html"><![CDATA[<p>Facebook发表在EMNLP 2019上的工作</p><h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>机器翻译</p><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>非自回归的生成方式</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>这篇文章将masked language model和iterative refinement进行了结合。并提出了更精确的解码方法。</p><p>具体来说，在训练时，这篇文章采用了和masked language model一样的设定。即，随机mask掉目标语句中的一些单词作为decoder input，mask的策略也和Bert中的一致。而decoder output则是这些位置上被mask掉的原始单词。其中，与Bert对每个句子固定mask掉15%的单词不同的是，这篇文章中被mask单词的数目是在从1到句子长度的范围中随机选取的。这样做的好处会在decoding时显现出来。</p><p><img src="https://i.loli.net/2021/06/06/bBgAcQlLFVSt6Es.jpg" alt="img"></p><p>图二：Mask-Predict 解码示意图</p><p>在预测时，这篇文章提出了基于mask and predict的解码方法，是文章的主要贡献。其实在masked language model这个框架下，解码方法是水到渠成的，即每次迭代时，都在当前翻译结果上mask掉一些词，再预测这些词即可。这里有几个点需要特别考虑。一是如何决定mask掉哪些词。文中给出的解决方法是选取top k个解码时置信度最小的词，把他们mask掉再重新预测。二是如何确定mask掉多少个词，也就是如何选取k。文中给出了基于迭代次数递减的策略，即<strong>在第一次解码时，将所有位置都置为[MASK]</strong>，同时预测所有位置的词。<strong>在之后解码迭代中，按照随迭代次数线性衰减的方式确定需要mask的单词个数</strong>：</p><p>$n=N \cdot \frac{T-t}{T}$</p><p>其中N是目标语句的长度，T和t则分别是总的迭代次数和当前迭代次数。上图中展示了解码过程的一个例子。</p><p>上面提到对每个句子，训练时每个epoch确定被mask单词数目时都是随机从 [1，句子长度] 中选取得到的。由于在解码时，被mask单词的个数是线性递减的，即会从 [句子长度，1] 依次递减。因此，训练时这样选取被mask单词的数目可以增加模型的capacity，让模型能处理任意个单词被mask掉时的情况，从而更符合解码时的策略，减少bias并达到更好的效果。</p><p>在确定目标语句的长度时，这篇文章采用了与上篇文章相同的方法，即通过encoder output来预测目标语句的长度，和golden目标语句的长度作为额外的loss function单独训练。</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a><strong>Results</strong></h2><p><img src="https://i.loli.net/2021/06/06/xongXjtwZciSe42.jpg" alt="img" style="zoom:50%;"></p><p>从上表中可以看到，这篇文章在WMT14 En-De和WMT16 En-Ro上均达到了SOTA。但有一点存疑的是，由于文章中也采用了knowledge distillation来训练non-autoregressive模型，而其采用的autoregressive teacher模型均为比较强的模型（28.6 on WMT14 En-De and 34.28 on WMT16 En-Ro），没有控制变量采用和baseline相同效果的autoregressive teacher。而一般来说，teacher越强训练出的non-autoregressive模型也会越强，因此<strong>这篇文章良好的效果应该也有一部分是得益于其选择了较强的teacher</strong>。</p><p>文中也做了在WMT17 En-Zh上的实验，以及对迭代次数和mutiple length candidates的分析，这里就不一一赘述了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Iterative refinement无疑是一个改善 $O(1)$非自回归模型的良好方向，在增加了有限的复杂度（通常是 $O(10)$)的代价下，非自回归模型的翻译质量得到了大幅提高。在这个方向中， Mask-Predict这篇文章给出了一个优良的解码范例，即每次迭代并不会预测所有的单词，而是预测置信度较低的数个单词。</p><p>但这样的解码范例也可能并不是最优的。因为模型给出的概率上的置信度有时候并不会反应真正的翻译质量。如何证明/设计与翻译质量挂钩的解码策略也是一个值得思考的问题。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Improving Neural Machine Translation with Soft Template Prediction</title>
      <link href="2021/06/06/Improving-Neural-Machine-Translation-with-Soft-Template-Prediction/"/>
      <url>2021/06/06/Improving-Neural-Machine-Translation-with-Soft-Template-Prediction/</url>
      
        <content type="html"><![CDATA[<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>机器翻译</p><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>受到人类翻译过程和其他领域基于模板和基于语法的方法启发，微软亚洲研究院提出了一种使用<strong>从语法树结构中提取的模板</strong>作为<strong>软目标模板</strong>来指导翻译过程的方法 ST-NMT。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>尽管近年来神经机器翻译（NMT）取得了长足的进步，但大多数神经机器翻译模型直接从源文本生成翻译。受到人类翻译过程和其他领域基于模板和基于语法的方法的成功启发，我们提出了一种使用从语法树结构中提取的模板作为软目标模板来指导翻译过程的方法。为了学习目标句子的句法结构，我们采用语法分析树来生成候选模板，并将模板信息融合到编码器-解码器框架中，同时利用模板和源文本生成翻译。实验表明，我们的模型在四个数据集上明显优于基线模型，证明了软目标模板的有效性。</p><h2 id="软模板预测"><a href="#软模板预测" class="headerlink" title="软模板预测"></a>软模板预测</h2><p>近年来，神经机器翻译取得了巨大的进步，但常规的神经机器翻译模型一般直接把源语言文本翻译到目标语言文本。但实际上，我们知道人类在翻译一句话时通常是先对目标文本的句型或者结构有一个大致的想法，然后再将源语言文本翻译成为目标语言文本，并不是直接逐词进行翻译。</p><p>通常我们在进行造句训练的时候，最开始老师会教给我们一些句型，例如 “sb. like doing sth; There be…” 等，然后让我们做练习。下图是一个中英翻译训练教材的内容，要求根据以下句型将中文翻译到英文。</p><p><img src="https://i.loli.net/2021/06/06/ipqb2FWnoZxsufz.png" alt="img" style="zoom:50%;"></p><p><em>图1：中英翻译训练教材内容</em></p><p>受到人类翻译过程的启示，我们提出了使用从句法分析树提取模板作为软目标语言模板来指导翻译过程的方法。为了学习目标句子的句法结构，我们采用句法分析树来生成候选模板。如图2所示，我们首先根据源语言文本预测将要用到的目标语言的模板，这里 “我喜欢打篮球”，很容易想到 “sb. like doing sth” 这个句型，随后我们根据源语言和模板来生成翻译。</p><p><img src="https://i.loli.net/2021/06/06/UnaNz1vmtAidhTQ.png" alt="img" style="zoom:50%;"></p><p><em>图2：使用软目标模板指导翻译过程示例</em></p><p>基于这一核心思想以及模板方法在机器摘要、问答和其他文本上取得的成功。我们假设目标句子的候选模板可以指导句子翻译过程。我们将这些从句法分析树中提取的模板作为软模板，由 S, NP, VP 这些具有句子结构信息和目标语言组成。这里模板是“软 (soft) ”的，因为我们<strong>并不强制要求生成的目标语言翻译一定是完全基于模板来生成</strong>，这里的模板仅仅是提供一种参考来对翻译提供一定的帮助。</p><p>为了更有效地使用软模板，我们引入了基于目标语言软模板的神经机器翻译模型 (Soft Template-based NMT，ST-NMT)，它可以使用源文本和软模板来预测最终的翻译。我们的方法可以分为两个阶段。在第一阶段，通过使用源文本和从句法分析树中提取的模板，训练一个标准的 Transformer 模型来专门预测软目标模板。在第二阶段，我们使用两种编码器，包括软目标模板编码器和源语言编码器，以对源文本和模板进行编码并生成最终翻译。</p><h2 id="目标软模板预测"><a href="#目标软模板预测" class="headerlink" title="目标软模板预测"></a>目标软模板预测</h2><p><img src="https://i.loli.net/2021/06/06/MSToYB93vDrqCwz.png" alt="img" style="zoom:50%;"></p><p><em>图3：从分析树中抽取模板</em></p><p>在此过程中，通过使用源语言 S 和模板 T 数据，我们对 P(T|X) 建模，使得我们能够根据源语言对模板进行预测。为了构造源语言-模板数据集，我们使用句法分析树来解析目标语言文本并获得树形结构。然后，我们裁剪超过一定深度的节点，并将裁剪后的子树按照原有顺序还原回去得到模板数据。通过这些操作，我们获得了源语言-模板平行训练数据，并训练了 Transformer 模型 P(T|X) 来预测软目标模板。</p><p>语法解析树可以显示整个句子的结构和语法信息，利用语法来区分终端（terminal nodes）和非终端节点（non-terminal nodes）。更确切地说，非终端节点由属于非终端节点集合 S，而终端节点属于目标语言节点集合 V。S={V, VP, NP, …, ASBR} 等代表语法成分的标记和 V={There, are, …, people} 包含目标语言单词。如 图3 所示，句子“有人在奔跑”通过语法解析树生成树形结构。在这种情况下，非终端节点集合S0={S, NP, VP, EX, VBP, NP, DT, NNS, VBG} 和终端节点集合 V0={There, are, some, people, running}。我们的模板 T={t1, t2, t3, t4} 是有序序列，由终端节点和非终端节点组成。在这种情况下，t1=There, t2=are, t3=VP, t4=NP。我们的模板是提取特定深度的子树，并使用位于子树的叶节上的终端和非终端节点来生成模板。</p><p>为了预测软目标模板，我们根据源文本和提取的模板的训练数据来训练一个Transformer 模型。Transformer 模型读取源文本，并使用束搜索预测软目标模板。然后，我们选择束搜索的前 K 个结果作为模板。</p><p><strong>选择子树的深度是一个权衡。在图3中，当深度等于1是一种特殊的情况，此时模板仅具有一个符号 S。模板 S 无法提供任何有用的信息。另一个特殊情况是当深度大于6时，“There are some people running” 此时模板只有终端节点。该模板仅包含目标语言单词，不能提供任何其他信息。而当深度等于4时，模板为 “There are VP NP”。该模板包含句子句法和结构信息，适用于我们的方法。</strong></p><p>使用 Transformer 模型 P(T|X)，我们需要构造伪训练数据(源语言文本、目标语言文本、目标软模板)，而不是通过语法分析树直接提取的模板。给定源文本 X，我们使用 P(T|X) 通过束搜索来生成排名靠前的目标语言软模板 T。最后，我们得到三元组训练数据 (源语言文本、目标语言文本、软模板）为下一阶段做准备。</p><h2 id="ST-NMT模型"><a href="#ST-NMT模型" class="headerlink" title="ST-NMT模型"></a>ST-NMT模型</h2><p>我们的模型首先通过源语言 Transformer 编码器以读取源语言序列 X=(x1, x2, x3, …, xn)并生成模板序列 T=(t1, t2, t3, …, tm) 由模板 Transformer 解码器提供。如图3所示，我们的模型使用源语言 Transformer 编码器和模板 Transformer 编码器，分别对源语言序列 X 和模板序列 T 进行编码，最终解码器生成最终翻译。我们的方法主要包括两个阶段：（1）训练数据由基于选区的解析树构造。然后，我们采用标准的 Transformer 将源文本转换为下一代软目标模板。（2）基于源文本和预测的软目标模板，我们使用两个编码器分别将两个序列编码为隐藏状态，并使用目标语言解码器生成最终翻译。</p><p><img src="https://i.loli.net/2021/06/06/Lz1C5HU7uFwle6K.png" alt="image-20210606102157632"></p><p><em>图4：ST-NMT 模型</em></p><p>给定三元组训练数据（源语言文本，目标语言文本，软模板）后，我们使用源语言文本和软模板生成目标语言文本如下公式。源语言 Transformer 编码器和软模板Transformer 编码器将输入序列 X 和由目标语言单词和非终端节点组成的模板 T 映射到隐层向量。然后，与两个编码器交互的 Transformer 解码器生成最终翻译 Y，即：</p><p><img src="https://i.loli.net/2021/06/06/qFu6TDtBbhQOXnf.png" alt="image-20210606102328094" style="zoom: 33%;"></p><p>基于源语言编码器隐藏层状态和软模板编码器隐藏层状态，目标语言 Transformer 解码器使用编码器-解码器多头注意共同使用源语言和模板信息来生成最终翻译 Y。此外，目标序列解码器使用两组注意力机制参数用于不同的编码器。解码器分别使用源语句上下文 X=(x1, …, xm)和目标模板上下文 T=(t1, …, tn)，然后我们的模型通过关注源上下文和模板上下文获得两个隐藏状态,在这里，我们使用门控单元融合了包含源语言信息的隐藏层状态和包含模板信息的隐藏层状态，如下所示：</p><p><img src="https://i.loli.net/2021/06/06/2omSktuZxNEUYeC.png" alt="image-20210606111455578" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/06/06/F4RvzfdciHe3Jr8.png" alt="image-20210606111502794" style="zoom:50%;"></p><p>与常规 NMT 相似，为了使模型能够预测目标序列，我们使用最大似然估计来更新模型参数。当我们在不使用模板 Transformer 编码器的情况下训练 P(Y|X) 时，我们仅需要优化以下损失函数:</p><p><img src="https://i.loli.net/2021/06/06/BZDHSQC19LhXlOA.png" alt="image-20210606120505997" style="zoom:50%;"></p><p>我们使用模板 Transformer 编码器训练 P(Y|X,T) 时，损失函数可以通过以下公式计算：</p><p><img src="https://i.loli.net/2021/06/06/kvnLlAHoF42XbcG.png" alt="image-20210606120523147" style="zoom:50%;"></p><p>在实践中，我们发现优化这两个目标可以使得模型更易于训练避免收到模板中噪声的干扰，并获得较高的 BLEU 分数，因为存在一些影响翻译质量的低质量模板。<strong>通过同时优化两个目标，我们可以减少某些低质量模板的影响并提高模型的稳定性。</strong>为了平衡这两个目标，我们的模型在两个目标上同时进行了迭代训练，如下所示：</p><p><img src="https://i.loli.net/2021/06/06/BrkEVRDWHZen41L.png" alt="image-20210606120604504" style="zoom:50%;"></p><h2 id="实验与分析"><a href="#实验与分析" class="headerlink" title="实验与分析"></a>实验与分析</h2><p>为了证明该方法的有效性，我们在多个数据集和多个语种进行了实验，包括 IWSLT14 德语-英语翻译任务，WMT14 英语-德语翻译任务，LDC 中英翻译任务和 ASPEC 日中翻译任务。实验表明，与基线模型相比，我们的方法取得了更好的结果，这表明软目标模板可以有效地指导翻译过程并提供积极的影响。同时我们的方法可用于不同规模、不同语种、不同领域的数据集。</p><p><img src="https://i.loli.net/2021/06/06/EfX8xaJeFYLzpDb.png" alt="image-20210606120732113" style="zoom:50%;"></p><p><em>表1：LDC 中英翻译任务结果</em></p><p><img src="https://i.loli.net/2021/06/06/19RGWj5xsK7HzXV.png" alt="image-20210606120755429" style="zoom: 50%;"></p><p><em>表2：WMT14 英语-德语翻译任务结果</em></p><p><img src="https://i.loli.net/2021/06/06/2HjICRYAgV9db5L.png" alt="image-20210606120824797" style="zoom:50%;"></p><p><em>表3：IWSLT14 德语-英语翻译任务结果</em></p><p><img src="https://i.loli.net/2021/06/06/e4hJ8gaKn3xmXwz.png" alt="image-20210606120844581" style="zoom:50%;"></p><p><em>表4：ASPEC 日中翻译任务结果</em></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这项工作中，我们提出了一种利用源文本和软模板生成翻译的机器翻译模型。我们的方法可以从子树中提取模板，该子树是从语法分析树的特定深度得到的。然后，我们使用 Transformer 模型来预测源文本的软目标模板。进一步，我们结合了源文本和模板信息来指导翻译过程。我们将软模板神经机器翻译模型（ST-NMT）与其他基线模型在多个数据集上进行比较。实验结果表明，ST-NMT 可以显著提高翻译性能。</p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li>其实也可以构造成 hard target, 强硬使其输出为对应的span</li><li>但是如果这样的话，soft target 是作为辅助信息，影响力 (&gt;=0)。如果使用强硬的，则影响力可能为&lt;0</li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval</title>
      <link href="2021/05/29/Frozen-in-Time-A-Joint-Video-and-Image-Encoder-for-End-to-End-Retrieval/"/>
      <url>2021/05/29/Frozen-in-Time-A-Joint-Video-and-Image-Encoder-for-End-to-End-Retrieval/</url>
      
        <content type="html"><![CDATA[<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p> text-to-video-retrieval</p><h2 id="当前存在的问题"><a href="#当前存在的问题" class="headerlink" title="当前存在的问题"></a>当前存在的问题</h2><ul><li>这一领域的挑战包括: (1) 视觉结构的设计和 (2) 训练数据的性质，因为现有的大规模视频-文本训练数据集，如HowTo100M，是有噪声的，因此只有通过大量的计算才能实现有竞争力的性能。</li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><ul><li>【训练使用的数据集】针对上述提到的问题，本文提出了一个端到端的可训练模型，该模型利用大规模的图像描述和视频描述数据集。该模型可以灵活的在图像-文本，视频-文本数据集上进行训练，可以是单独的方式或者是组合的方式。</li><li>【模型结构】本文提出的模型是对最近的ViT和Timesformer架构的改编和扩展，由空间和时间上的 attention 组成。</li><li>【训练方案】使用课程学习的训练方式（由易到难），开始时将图像视为视频的 “冻结 “快照，然后在视频数据集上训练时逐渐学会关注越来越多的时域上下文。</li></ul>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[T5] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
      <link href="2021/04/28/T5-Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer/"/>
      <url>2021/04/28/T5-Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer/</url>
      
        <content type="html"><![CDATA[<p>转载：<a href="https://zhuanlan.zhihu.com/p/88438851" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/88438851</a></p><p>对于 T5 这篇论文，<em>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</em>，无疑也是类似的论文。它的意义不在烧了多少钱，也不在屠了多少榜（砸钱就能砸出来），其中 idea 创新也不大，它最重要作用是给<strong style="color:red;"><strong>整个 NLP 预训练模型领域提供了一个通用框架</strong></strong>，把所有任务都转化成一种形式，正如论文里所说的</p><blockquote><p>introducing a unified framework that converts every language problem into a text-to-text format.</p></blockquote><p>之后未来做 NLP 实验时，可能就不再是自己怎么调一些模型了，而是无论什么任务，直接拿来一个超大预训练模型，然后<strong style="color:red;"><strong>主要工作就变成了怎么把任务转换成合适的文本输入输出</strong></strong>，于是我们就成了带引号的”数据科学家“。而且可以用于多种任务，而模型对这些任务的区分只是根据你构建的输入输出形式，其实这让我想起 Jeff Dean 在某次谈话中谈到的谷歌未来方向，想做一个超级模型，什么任务都能直接处理，而它内部可以是稀疏的，或者可以局部 Distill，来对单独任务进行处理。</p><p>关于论文，作者们做了很多实验。将近七十个实验，这也是大家吐槽财大气粗的原因，太有冲击力了，小家小业的话估计跑里面个小实验就够呛了。</p><p>正因为如此多实验，所以才对预训练模型中的大量技巧获得了一个较公平的比对和分析，但这也使得整篇论文长度巨长，读起来头晕。不是 idea 的冲击，而都是些琐碎细节，看了几大段后发现，还是看图表一目了然。</p><p>这里就简单介绍一下里面做了哪些实验，之后各取所需回看论文。</p><h2 id="Why-Text-to-Text？"><a href="#Why-Text-to-Text？" class="headerlink" title="Why Text-to-Text？"></a>Why Text-to-Text？</h2><p>首先为什么叫 T5 模型，因为是 <strong>Transfer Text-to-Text Transformer</strong> 的简写，和 XLNet 一样也不在芝麻街玩了，也有说法是吐槽谷歌 <strong>T5 Level</strong>（高级软件工程师）。</p><p>Transfer 来自 Transfer Learning，预训练模型大体在这范畴，Transformer 也不必多说，那么 Text-to-Text 是什么呢。那就是作者在这提出的一个统一框架，靠着大力出奇迹，<strong>将所有 NLP 任务都转化成 Text-to-Text （文本到文本）任务</strong>。</p><p><img src="https://i.loli.net/2021/04/28/zSwycrjd2qkPGas.png" alt="image-20210428195223812"></p><p>举几个例子就明白了，比如英德翻译，只需将训练数据集的输入部分前加上“translate English to German（给我从英语翻译成德语）” 就行。假设需要翻译”That is good”，那么先转换成 “translate English to German：That is good.” 输入模型，之后就可以直接输出德语翻译 “Das ist gut.”</p><p>再比如情感分类任务，输入”sentiment：This movie is terrible!”，前面直接加上 “sentiment：”，然后就能输出结果“negative（负面）”。</p><p>最神奇的是，对于需要输出连续值的 STS-B（文本语义相似度任务），居然也是直接输出文本，而不是加个连续值输出头。以每 0.2 为间隔，从 1 到 5 分之间分成 21 个值作为输出分类任务。比如上图中，输出 3.8 其实不是数值，而是一串文本，之所以能进行这样的操作，应该完全赖于 T5 模型强大的容量。</p><p>通过这样的方式就能将 NLP 任务都转换成 Text-to-Text 形式，也就可以<strong>用同样的模型，同样的损失函数，同样的训练过程，同样的解码过程来完成所有 NLP 任务</strong>。其实这个思想之前 GPT2 论文里有提，上斯坦福 cs224n 时 Socher 讲的 The Natural Language Decathlon 也有提。</p><h2 id="Data：C4-（Bomb-）"><a href="#Data：C4-（Bomb-）" class="headerlink" title="Data：C4 （Bomb!）"></a>Data：C4 （Bomb!）</h2><p>作者从 Common Crawl（一个公开的网页存档数据集，每个月大概抓取 20TB 文本数据） 里清出了 750 GB 的训练数据，然后取名为 ” Colossal Clean Crawled Corpus （超大型干净爬取数据）“，简称 C4，论作者取名之恶趣味。</p><p>大概清理过程如下：</p><ul><li>只保留结尾是正常符号的行；</li><li>删除任何包含不好的词的页面，具体词表参考<strong><a href="https://link.zhihu.com/?target=https%3A//github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words" target="_blank" rel="noopener">List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words</a></strong>库（笔者按：宝藏库，到里面转了一圈，看了看熟悉的几门语言，瞬间涨了不少新姿势 ）；</li><li>包含 Javascript 词的行全去掉；</li><li>包含编程语言中常用大括号的页面；</li><li>任何包含”lorem ipsum（用于排版测试）“的页面；</li><li>连续三句话重复出现情况，保留一个。</li></ul><h2 id="Architecture：The-Best-One"><a href="#Architecture：The-Best-One" class="headerlink" title="Architecture：The Best One"></a><strong>Architecture：The Best One</strong></h2><p>首先作者们先对预训练模型中的多种模型架构（Transformer）进行了比对，最主要的模型架构可以分成下面三种。</p><p><img src="C:\Users\shiyaya\AppData\Roaming\Typora\typora-user-images\image-20210428195326843.png" alt="image-20210428195326843"></p><p>第一种，<strong>Encoder-Decoder 型</strong>，即 Seq2Seq 常用模型，分成 Encoder 和 Decoder 两部分，对于 Encoder 部分，输入可以看到全体，之后结果输给 Decoder，而 Decoder 因为输出方式只能看到之前的。此架构代表是 MASS（今年WMT的胜者），而 BERT 可以看作是其中 Encoder 部分。</p><p>第二种， 相当于上面的 <strong>Decoder 部分</strong>，当前时间步只能看到之前时间步信息。典型代表是 GPT2 还有最近 CTRL 这样的。</p><p>第三种，<strong>Prefix LM（Language Model） 型</strong>，可看作是上面 Encoder 和 Decoder 的融合体，一部分如 Encoder 一样能看到全体信息，一部分如 Decoder 一样只能看到过去信息。最近开源的 UniLM 便是此结构。</p><p>上面这些模型架构都是 Transformer 构成，之所以有这些变换，主要是<strong>对其中注意力机制的 Mask 操作</strong>。</p><p><img src="C:\Users\shiyaya\AppData\Roaming\Typora\typora-user-images\image-20210428195656637.png" alt="image-20210428195656637"></p><p>通过实验作者们发现，在提出的这个 Text-to-Text 架构中，Encoder-Decoder 模型效果最好。于是乎，就把它定为 T5 模型，因此<strong>所谓的 T5 模型其实就是个 Transformer 的 Encoder-Decoder 模型</strong>。</p><p>之后是对预训练目标的大范围探索，具体做了哪些实验，下面这张图就能一目了然。</p><p><img src="https://i.loli.net/2021/04/28/HGA86geR3s5JoOl.png" alt="image-20210428200712939"></p><p>总共从四方面来进行比较。</p><p>第一个方面，<strong>高层次方法（自监督的预训练方法）对比</strong>，总共三种方式。</p><ol><li><strong>语言模型式</strong>，就是 GPT-2 那种方式，从左到右预测；</li><li><strong>BERT-style 式</strong>，就是像 BERT 一样将一部分给破坏掉，然后还原出来；</li><li><strong>Deshuffling （顺序还原）式</strong>，就是将文本打乱，然后还原出来。</li></ol><p><img src="https://i.loli.net/2021/04/28/La7uURigVxpHzdG.png" alt="image-20210428200943309"></p><p>其中发现 Bert-style 最好，进入下一轮。</p><p>第二方面，对文本一部分进行<strong>破坏时的策略</strong>，也分三种方法。</p><ol><li><strong>Mask 法</strong>，如现在大多模型的做法，将被破坏 token 换成特殊符如 [M]；</li><li><strong style="color:red;"><strong>replace span（小段替换）法</strong>，可以把它当作是把上面 Mask 法中相邻 [M] 都合成了一个特殊符，每一小段替换一个特殊符，提高计算效率；</strong></li><li><strong>Drop 法</strong>，没有替换操作，直接随机丢弃一些字符。</li></ol><p><img src="https://i.loli.net/2021/04/28/swF3Ab4uBetv1Nf.png" alt="image-20210428201451717"></p><p>此轮获胜的是 <strong>Replace Span 法</strong>（如下图），类似做法如 SpanBERT 也证明了有效性。</p><p><img src="https://i.loli.net/2021/04/28/9gAnaOWSlmMbxCF.png" alt="image-20210428195311177"></p><p>当当当，进入下一轮。</p><p>第三方面，到底该<strong>对文本百分之多少进行破坏</strong>呢，挑了 4 个值，10%，15%，25%，50%，最后发现 BERT 的 <strong>15%</strong> 就很 ok了。这时不得不感叹 BERT 作者 Devlin 这个技术老司机直觉的厉害。</p><p>接着进入更细节，第四方面，因为 Replace Span 需要决定<strong>对大概多长的小段进行破坏</strong>，于是对不同长度进行探索，2，3，5，10 这四个值，最后发现 <strong>3</strong> 结果最好。</p><p>终于获得了完整的 T5 模型，还有它的训练方法。</p><p><img src="C:\Users\shiyaya\AppData\Roaming\Typora\typora-user-images\image-20210428201927674.png" alt="image-20210428201927674"></p><ul><li>Transformer Encoder-Decoder 模型；</li><li>BERT-style 式的破坏方法；</li><li>Replace Span 的破坏策略；</li><li>15 %的破坏比；</li><li>3 的破坏时小段长度。</li></ul><p>到此基本上 T5 预训练就大致说完了，之后是些细碎探索。</p><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><p>接着作者们拿着 C4 数据集做了各种实验，比如说从里面分出各种类型的数据集，单独训练 T5 模型，之后看在下游任务的表现，发现一些情况<strong>领域内的预训练数据可以增强下游任务</strong>（想当然的）。而 C4 完整数据集因为数据太多太杂，可能反而不如这种领域内较少数据集。</p><p>还有从 C4 中抽出不同量数据做实验，发现<strong>数据少时，模型会记住数据所以之后表现会比较差</strong>（这个也是想当然）。</p><p><img src="https://i.loli.net/2021/04/28/8bgrwBkAYaWQTJp.png" alt="image-20210428202002738" style="zoom:67%;"></p><h2 id="Training：Multi-Task-Learning"><a href="#Training：Multi-Task-Learning" class="headerlink" title="Training：Multi-Task Learning"></a><strong>Training：Multi-Task Learning</strong></h2><p>作者们之后又针对 MTDNN 给 T5 做了一系列类似训练，在一堆监督和非监督数据上进行预训练。</p><p>结果发现，只要<strong>混合训练比例调得OK，和前面说的非监督预训练性能差不多</strong>。</p><h2 id="Scaling：bigger-is-better"><a href="#Scaling：bigger-is-better" class="headerlink" title="Scaling：bigger is better?"></a><strong>Scaling：bigger is better?</strong></h2><p>接着又做了当放大模型某方面规模的相关实验，分别是增大模型，增大数据，还有在一定资源限制下的集成。</p><p>结论是，当<strong>这些因素放大时对性能都有提高，但其中大模型是最必要的</strong>。</p><h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a><strong>Models</strong></h2><p>最后就是结合上面所有实验结果，训练了不同规模几个模型，由小到大：</p><ul><li>Small，Encoder 和 Decoder 都只有 6 层，隐维度 512，8 头；</li><li>Base，相当于 Encoder 和 Decoder 都用 BERT-base；</li><li>Large，Encoder 和 Decoder 都用 BERT-large 设置，除了层数只用 12 层；</li><li>3B（Billion）和11B，层数都用 24 层，不同的是其中头数量和前向层的维度。</li></ul><p>11B 的模型最后在 GLUE，SuperGLUE，SQuAD，还有 CNN/DM 上取得了 SOTA，而 WMT 则没有。看了性能表之后，我猜想之所以会有 3B 和 11B 模型出现，主要是为了刷榜。看表就能发现。</p><p><img src="https://i.loli.net/2021/04/28/JF4X6LzSgpxho93.jpg" alt="img"></p><p>比如说 GLUE，到 3B 时效果还并不是 SOTA，大概和 RoBERTa 评分差不多都是 88.5，而把模型加到 11B 才打破 ALBERT 的记录。然后其他实验结果也都差不多，3B 时还都不是 SOTA，而是靠 11B 硬拉上去的。除了 WMT 翻译任务，可能感觉差距太大，要拿 SOTA 代价过大，所以就没有再往上提。根据这几个模型的对比，可以发现<strong>即使是容量提到 11B，性能提升的间隔还是没有变缓</strong>，<strong>因此我认为再往上加容量还是有提升空间</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Straightforward Framework For Video Retrieval Using CLIP</title>
      <link href="2021/04/20/A-Straightforward-Framework-For-Video-Retrieval-Using-CLIP/"/>
      <url>2021/04/20/A-Straightforward-Framework-For-Video-Retrieval-Using-CLIP/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/Deferf/CLIP_Video_Representation" target="_blank" rel="noopener">code</a></p><h2 id="本文的任务"><a href="#本文的任务" class="headerlink" title="本文的任务"></a>本文的任务</h2><p>Zero-shot Video-Text Retrieval</p><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>CLIP model 是用互联网上<strong>大规模</strong>的image-text pairs 训练出来的一个多模态检索模型，其在 image text 检索上获得了非常优异的性能，且适合于zero-shot 的场景。</p><p>在本文中，作者尝试使用clip model 来处理 video-text pair, 并且仍然以  <strong style="color:red;">zero-shot</strong> 的方式。</p><ul><li>使用clip的 image encoder 来 获取frame feature, 然后使用 平均聚合，获取video-level feature.</li><li><p>使用 clip 的 text encoder 来获取 text feature。</p></li><li><p>使用cosine similarity 来计算 video-text sim</p></li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>TVR: Text Video Retrieval</p><p>VTR: Video Text Retrieval</p><h3 id="MSR-VTT"><a href="#MSR-VTT" class="headerlink" title="MSR-VTT"></a>MSR-VTT</h3><p><img src="https://i.loli.net/2021/04/23/pKQ6Y8msudJSgah.png" alt="image-20210423101035581" style="zoom: 67%;"></p><h3 id="MSVD"><a href="#MSVD" class="headerlink" title="MSVD"></a>MSVD</h3><p><img src="https://i.loli.net/2021/04/23/3jQUnoyTuNWBvPs.png" alt="image-20210423101137713" style="zoom:67%;"></p><h3 id="LSMDC"><a href="#LSMDC" class="headerlink" title="LSMDC"></a>LSMDC</h3><p><img src="https://i.loli.net/2021/04/23/bhjOL9yDcdunm7W.png" alt="image-20210423101208808" style="zoom:67%;"></p><h2 id="others"><a href="#others" class="headerlink" title="others"></a>others</h2><p>本文也探索了其他两种时域聚合的方式，但是不如取平均的方式好。</p><ul><li>（1） Follow【1】。将视频提取帧之后，取第30帧。由于视频每秒大约有30帧，所以，相当于取第一秒的最后一针。</li><li>（2）Follow【2】。使用 K-means， 获取K个video embedding , 计算每个embedding 与 text embedding 之间的相似度 and <strong>register each query’s minimum rank,</strong></li></ul><p>在MSR-VTT 验证集上的1000个视频-文本对上进行了实验：</p><p><img src="https://i.loli.net/2021/04/23/ZSOYgE2zj7nHpeI.png" alt="image-20210423100745024" style="zoom:67%;"></p><p>值得注意的是，在K-means方法中，结果之间没有明显的差异。 这可能是因为MSR视频的长度不超过32秒，这可能不足以在创建聚类时区分中心点。<strong style="color:red;">（<strong>yaya: 可能K-means 这种方法更加适合long-term video</strong>）</strong></p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>本文虽然创新性，没有那么强，但是在<strong>性能上是非常可观</strong>的。而且采取的是 <strong>Zero-Shot</strong> 的方式。</li><li>这种zero-shot 的方式，就可以应用到很多需要泛化性的地方。</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li><p>但是本文只是在这几个数据集上进行了测试，在其他的数据集上的性能还不可知，例如，在long-term videos 上的性能如何呢？</p></li><li><p>因为CLIP是在英文的句子上进行预训练的，当前测试的数据集仅仅是 english text，那么对于Chinese text效果如何呢？</p></li></ul><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>本文的视频是直接解帧，然后对所有的帧进行时域聚合（平均池化）？这样的采样，是不是比较密集，且是不是含有冗余？？</li></ul>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Seeing Out of tHe bOx End-to-End Pre-training for Vision-Language Representation Learning</title>
      <link href="2021/04/20/Seeing-Out-of-tHe-bOx-End-to-End-Pre-training-for-Vision-Language-Representation-Learning-1/"/>
      <url>2021/04/20/Seeing-Out-of-tHe-bOx-End-to-End-Pre-training-for-Vision-Language-Representation-Learning-1/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Chinese Video and Language via Contrastive Multimodal Pre-Training</title>
      <link href="2021/04/20/Understanding-Chinese-Video-and-Language-via-Contrastive-Multimodal-Pre-Training/"/>
      <url>2021/04/20/Understanding-Chinese-Video-and-Language-via-Contrastive-Multimodal-Pre-Training/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval</title>
      <link href="2021/04/20/CLIP4Clip-An-Empirical-Study-of-CLIP-for-End-to-End-Video-Clip-Retrieval/"/>
      <url>2021/04/20/CLIP4Clip-An-Empirical-Study-of-CLIP-for-End-to-End-Video-Clip-Retrieval/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/ArrowLuo/CLIP4Clip" target="_blank" rel="noopener">code</a></p><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>将 CLIP的知识迁移到 video-text retrieval 任务中，但是存在以下几个问题：</p><ul><li>图像的特征对于 video-text retrieval 是足够的吗？</li></ul>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CLIPScore: A Reference-free Evaluation Metric for Image Captioning</title>
      <link href="2021/04/20/CLIPScore-A-Reference-free-Evaluation-Metric-for-Image-Captioning/"/>
      <url>2021/04/20/CLIPScore-A-Reference-free-Evaluation-Metric-for-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<h2 id="1-本文研究的任务"><a href="#1-本文研究的任务" class="headerlink" title="1. 本文研究的任务"></a>1. 本文研究的任务</h2><ul><li>图像描述的评价指标</li></ul><h2 id="2-目前存在的问题"><a href="#2-目前存在的问题" class="headerlink" title="2. 目前存在的问题"></a>2. 目前存在的问题</h2><ul><li>当前图像描述的评价指标(automatic metrics )都是基于 predicted caption 与 reference 之间的文本匹配。但是人类进行评价时是直接将图片与prediction进行对比。即 automatic metrics与 human evaluation的评价方式是不一样的</li><li>reference的收集需要耗费人力，且较慢</li><li>reference是有限的，并不能覆盖所有可能的描述。因此，这些automatic metrics 更加<strong>偏向于</strong>reference中出现的单词，对于新的且正确的单词会给予错误的<strong>惩罚</strong>。</li></ul><h2 id="3-Motivation"><a href="#3-Motivation" class="headerlink" title="3. Motivation"></a>3. Motivation</h2><p>本文提出使用 clip model 来作为 图像描述的reference-free metric，基于以下两点原因：</p><ul><li><p><strong>基于在WMT上的借鉴：</strong>在多语言机器翻译任务中，提出了使用多语言模型（M-BERT), 计算 source-target 之间的相似性，作为reference free metric 来评估机器翻译。因此，可以使用image-text matching model 来计算image 与 predicted caption 之间的相似性。</p></li><li><p>CLIP，ALIGN，这种模型以 <strong>zero-shot</strong> 的方式展示出了在image-text matching task 上的优异性能。因此CLIP可以作为一个现成的(<strong>off-the-shelf</strong>) 的reference-free caption evaluation。</p></li></ul><h2 id="4-本文的贡献"><a href="#4-本文的贡献" class="headerlink" title="4. 本文的贡献"></a>4. 本文的贡献</h2><ul><li>提出一个 评估图像描述的 reference-free metric（CLIPScore, CLIP-s）。<br>另外提出了一个增强版本(RefCLIPScore,)，可以结合reference。</li><li>一个信息增益的分析实验，证明了CLIP-S 与 other metrics（B_4, Cider, Meteor， ViLBERTScore-F）是互相补充的。</li><li>实验验证CLIP-S对恶意构建的图像标题是敏感的，其中一个名词短语被换成了一个可信的（但不正确的）单词。</li><li><p>构建一个从未在网上公开发布过的图像语料库，以检验CLIP-S是否能够重建人类对从未见过的图像的判断。</p></li><li><p>最后，我们在四个与直接场景描述(direct scene description)不同的案例研究中评估CLIP-S。在两种情况下，CLIP-S效果很好。 它与<strong>Twitter</strong>上的alt-text质量评级实现了高度关联，并展示了对<strong>clipart images+captions</strong>进行推理的惊人能力。对于<strong>新闻标题</strong>的生成，基于参考的方法与人类的判断有最好的相关性。 而且，对于由<strong>社交媒体上的语言使用所激发的情感性标题</strong>，即使是基于参考的指标也是不足的。</p></li></ul><h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5. 实验"></a>5. 实验</h2><h3 id="5-1-Performance-on-a-set-of-literal-image-description-corpora"><a href="#5-1-Performance-on-a-set-of-literal-image-description-corpora" class="headerlink" title="5.1 Performance on a set of literal image description corpora"></a>5.1 Performance on a set of literal image description corpora</h3><h4 id="Caption-level-Likert-Judgments"><a href="#Caption-level-Likert-Judgments" class="headerlink" title="Caption-level Likert Judgments"></a>Caption-level Likert Judgments</h4><ul><li>BERT-S  是指 BERTScore: Evaluating text generation with BERT</li><li>BERT-S++ 是指 Improving image captioning evaluation by considering inter references variance.</li></ul><p><img src="https://i.loli.net/2021/04/21/8A2W4u7EalDmTYL.png" alt="image-20210421104120105"></p><p><img src="https://i.loli.net/2021/04/21/OUaqp5JvolduweY.png" alt="image-20210421104130443"></p><p><img src="https://i.loli.net/2021/04/21/3n72RmIfBZFUcza.png" alt="image-20210421104218527"></p><h4 id="Pairwise-Ranking-on-Pascal-50S"><a href="#Pairwise-Ranking-on-Pascal-50S" class="headerlink" title="Pairwise Ranking on Pascal-50S"></a>Pairwise Ranking on Pascal-50S</h4><p><img src="https://i.loli.net/2021/04/21/ck1xFipoPAEf74m.png" alt="image-20210421104150925"></p><h4 id="System-level-Correlation-for-MSCOCO"><a href="#System-level-Correlation-for-MSCOCO" class="headerlink" title="System-level Correlation for MSCOCO."></a>System-level Correlation for MSCOCO.</h4><p>该文章作者提出，该数据其实仅有12个数据，数据量太少，用来评估相关性，其实是不准确的。但是还是做了这方面的实验。</p><p>在M1 和 M2 上，CLIP-S achieves Spearman $\rho_{M 1} / \rho_{M 2}=.59 / .63$ and RefCLIP-S achieves $\rho_{M 1} / \rho_{M 2}=.69 / .74$ </p><h3 id="5-2-Sensitivity-of-CLIP-S-to-hallucination"><a href="#5-2-Sensitivity-of-CLIP-S-to-hallucination" class="headerlink" title="5.2 Sensitivity of CLIP-S to hallucination"></a>5.2 Sensitivity of CLIP-S to hallucination</h3><p>先前的研究（Object hallucination in image captioning）表明，人类在给human打分时，会更加偏向于caption是否正确，而不是caption是否足够具体(全面)。因此，了解评价指标是否以及如何处理包含不正确的 “幻觉 “(incorrect “hallucinations,”)的图像描述，例如，对未描述的物体的引用，是很重要的。</p><p>本文使用FOIL数据集（FOIL it! find one mismatch between image and language caption），来测试CLIPScore对检测描述中潜在的微妙不准确的细节有多敏感。这个语料库由MSCOCO的reference进行修改后组成，其中有一个名词短语被对抗性地换掉，以使caption不正确，例如，将 “摩托车 “换成 “自行车”。</p><p><img src="https://i.loli.net/2021/04/21/Kqsab4TN7lC3OJX.png" alt="image-20210421111246841"></p><h3 id="5-3-Sensitivity-of-CLIP-S-to-memorization"><a href="#5-3-Sensitivity-of-CLIP-S-to-memorization" class="headerlink" title="5.3  Sensitivity of CLIP-S to memorization"></a>5.3  Sensitivity of CLIP-S to memorization</h3><p>以上数据集的成功表现，可能存在一种风险，即在评估时使用的数据已经在预训练时看到过。可能是在CLIP的预训练数据中包含了 COCO，Flickr 这种数据。为了消除这个疑虑，本文收集了250图片，这些图片没有上传到网上。然后使用Microsoft Azure Cognitive Services 和 Discriminability objective for training descriptive captions 这两个工具为每张图片生成两个预测的描述，然后，组织3个author去分析，哪个预测更好。</p><p>CLIP-S 实验了86%的 accuracy（人类的一致性92%）。</p><h3 id="5-4-评价指标之间的冗余和互补"><a href="#5-4-评价指标之间的冗余和互补" class="headerlink" title="5.4 评价指标之间的冗余和互补"></a>5.4 评价指标之间的冗余和互补</h3><p><img src="https://i.loli.net/2021/04/21/B1OvP7LW5ycr4u2.png" alt="image-20210421114103630"></p><p>6个常用的metrics (B_1, B_4, Meteor, Rouge, Cider, Spice)</p><p>4个新的metrics (TIGER, BERTScore, ViLBERTScore-F, and CLIP-S/RefCLIPS)</p><p>在这10个评价指标上，执行greedy前向搜索，开始时为一个空集，选择最有信息量的metric 添加到集合中来。关于信息增益的考量：根据线性回归模型，一个指标与人类判断的additional (增加)相关性有多大。（yaya: 当前步，引入一个metric到集合中来，可以带来多少相关性的提升）</p><p><strong>yaya: 具体实现细节不是很清楚，可以等代码开源之后看看</strong></p><h2 id="6-Other-Captioning-Domains"><a href="#6-Other-Captioning-Domains" class="headerlink" title="6. Other Captioning Domains"></a>6. Other Captioning Domains</h2><ul><li><p>Alt-Text ratings from Twitter</p></li><li><p>Abstract-50S</p></li><li>Personality Captions</li><li>News Image Captioning</li></ul><h2 id="7-Future-Work"><a href="#7-Future-Work" class="headerlink" title="7. Future Work"></a>7. Future Work</h2><ul><li><p>consider CLIP-S as a reinforcement learning reward for literal caption generators<br><strong>以clipscore 作为reward来做强化学习，提升captioning tasks</strong></p></li><li><p>explore whether a small amount of labelled human rating data could help CLIP-S adapt to domains where it struggles, e.g., engagingness prediction.</p><p><strong>用human ratings data来提升CLIP-S 作为评价指标的性能，从而提升对于一些目前它比较困难的领域。</strong></p></li></ul><h2 id="8-yaya"><a href="#8-yaya" class="headerlink" title="8. yaya"></a>8. yaya</h2><font color="red">由于clip的预训练过程对文本添加了前缀，类似”A photo of"。因此，在本文CLIPScore 中，对candidate captions也添加了前缀：“A photo depicts"</font>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval</title>
      <link href="2021/04/20/TVR-A-Large-Scale-Dataset-for-Video-Subtitle-Moment-Retrieval/"/>
      <url>2021/04/20/TVR-A-Large-Scale-Dataset-for-Video-Subtitle-Moment-Retrieval/</url>
      
        <content type="html"><![CDATA[<p>本文新提出了一个数据集 Video Corpus Moment Retrieval (VCMR)。</p><p>该数据集与以前数据集不同的点有：</p><ul><li>人类标注的query，不仅依据视频(视觉信息)，还根据subtitles(文本信息)</li><li>选取的moment不是对video进行均匀采样得到的(DiDeMo dataset)，而是自由选取的，这样可以更加关注于重要的moment</li><li>人类标注的query不依赖上下文信息，不会出现first, then这种描述（ActivityNet Captions），使得标注的数据更加适合检索任务。</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Seeing Out of tHe bOx End-to-End Pre-training for Vision-Language Representation Learning</title>
      <link href="2021/04/19/Seeing-Out-of-tHe-bOx-End-to-End-Pre-training-for-Vision-Language-Representation-Learning/"/>
      <url>2021/04/19/Seeing-Out-of-tHe-bOx-End-to-End-Pre-training-for-Vision-Language-Representation-Learning/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Fill-in-the-Blank as a Challenging Video Understanding Evaluation Framework</title>
      <link href="2021/04/13/Fill-in-the-Blank-as-a-Challenging-Video-Understanding-Evaluation-Framework/"/>
      <url>2021/04/13/Fill-in-the-Blank-as-a-Challenging-Video-Understanding-Evaluation-Framework/</url>
      
        <content type="html"><![CDATA[<h2 id="本文研究的任务"><a href="#本文研究的任务" class="headerlink" title="本文研究的任务"></a>本文研究的任务</h2><p>本文提出了一个新的任务，给定一段视频和一个带有blank的文本，目标是对blank进行补充。该任务的目的是测试系统（模型）对视频的理解。</p><p>In this task, given a video and a blanked caption, the model needs to generate the missing phrase.</p><h2 id="目前存在的问题"><a href="#目前存在的问题" class="headerlink" title="目前存在的问题"></a>目前存在的问题</h2><p>迄今为止，有关语言知识的视频理解的工作主要解决了两个任务：（1）<strong>使用多项选择题的方式来做视频VQA任务</strong>，当前模型的性能一般表现都相对较好，这是因为多项选择的方式，使得候选答案是容易获得的；（2）<strong>视频描述任务</strong>，通常会以开放式评估框架来对生成的caption进行评估，但是这种评估方式存在缺陷。比如，如果prediction的形式与reference有所不同，则prediction可能被认为是不正确的。</p><p>因此，在本文中，提出了一个fill-in-the-blanks任务来作为 视频理解评估框架，以解决以上的问题。这种任务形式，更加符合现实生活中，是没有预先给定多项选择的。</p><h2 id="fill-in-the-blank-task-的优势"><a href="#fill-in-the-blank-task-的优势" class="headerlink" title="fill-in-the-blank task 的优势"></a>fill-in-the-blank task 的优势</h2><ul><li>相比于 多项选择的video VQA 更加符合现实场景</li><li>不会遇到 video captioning 评估存在偏差的问题</li><li>fill-in-the-blank data 是可以自动生成的，因此训练数据的规模是可以无限增大的</li></ul><h2 id="构建数据集"><a href="#构建数据集" class="headerlink" title="构建数据集"></a>构建数据集</h2><p>我们通过两个步骤来构建这个 “填空 “视频数据集。 (1)数据生成，我们编制了大量的视频-字幕对，并有选择地挖去某些<strong>名词</strong>（multi-word spans）。 (2)数据标注，AMT workers 为这些空白提供<strong>额外</strong>的有效答案。（收集这种附加注释的主要原因是为了说明语言的自然多样性，并为每个空白有多个备选答案。）</p><p><strong>数据来源：</strong>we generate our training, validation, and test data starting with the VATEX v1.1 training set, a random subset of size 1,000 from the validation set, and a random subset of size 1,000 from the test set, respectively.</p><p><img src="https://i.loli.net/2021/04/13/dHsFYELJyvQunef.png" alt="image-20210413180750120" style="zoom: 50%;"></p><h2 id="本文设计的方法"><a href="#本文设计的方法" class="headerlink" title="本文设计的方法"></a>本文设计的方法</h2><p>本文设计了两种encoder: 早期融合encoder 和 后期融合encoder</p><p><img src="https://i.loli.net/2021/04/13/pJkgYWhMsncd7TN.png" alt="image-20210413183537583"></p><p>考虑到 <strong>T5</strong> 可以填充可变长度的blanks的能力，因此使用 <strong>T5</strong> 来初始化本文的多模态模型。</p><p>对于early fusion model: 使用T5-base weights。</p><p>对于late fusion model: 使用 T5-base 来初始化 text encoder 和 decoder。使用两个一层transformer 来作为video encoder 和 multimodal transformer encoder。</p><p>特殊字符：the special token <strong><extra_id_0></extra_id_0></strong> is used to represent the blanked phrase, and <strong>&lt;\s&gt;</strong> is used to separate the text and video sequences。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>本文提出了两个baseline:</p><ul><li>使用最频繁的answer作为预测：“a man”</li><li>Text-based Transformer (T5): 使用text-only model来测试该文构建数据集的language bias</li></ul><p>评估指标（Evaluation Metrics）</p><ul><li><p>exact match： we count a generated text string as correct if it has at least one string-level match among the provided annotations.</p></li><li><p>token-level F1：we compute the token overlap (true positives) between the generated text string and each annotation, normalize by the sum of the true positives and average of the false negatives/positives, then compute the maximum across all annotations. </p></li></ul><h3 id="本文数据集的-langauge-bias如何"><a href="#本文数据集的-langauge-bias如何" class="headerlink" title="本文数据集的 langauge bias如何"></a>本文数据集的 langauge bias如何</h3><ul><li>T5 fine-tuned 结果与 下面 multimodal models的结果相差不多，可以看到其实语言偏置还是很大的。</li></ul><p><img src="https://i.loli.net/2021/04/13/Pmf9nKMXdWvqAN8.png" alt="image-20210413185634092"></p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li><p>(1) 仅对 noun phrase 挖空，(2) 只挖了一个空，都是对任务的简化，使得该任务变得简单。</p></li><li><p>文中图表说明，最频繁的名词是 man, person。这是这个数据集的 language bias。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</title>
      <link href="2021/04/06/Exploiting-Cloze-Questions-for-Few-Shot-Text-Classification-and-Natural-Language-Inference/"/>
      <url>2021/04/06/Exploiting-Cloze-Questions-for-Few-Shot-Text-Classification-and-Natural-Language-Inference/</url>
      
        <content type="html"><![CDATA[<h2 id="1-存在的问题"><a href="#1-存在的问题" class="headerlink" title="1. 存在的问题"></a>1. 存在的问题</h2><p>当前的语言模型都依赖大量的数据，但是在现实中，大量的标注数据是很难获取的(cost) 。因此，在少样本下的训练方式需要被研究。</p><h2 id="2-本文的点"><a href="#2-本文的点" class="headerlink" title="2. 本文的点"></a>2. 本文的点</h2><ul><li>本文出发点：将任务描述 （task description）组合到模型的输入中，会使得任务变得简单。</li><li><p>通过本文提出的Pattern-Exploiting Training (PET)，<strong>将输入样本转为cloze-style phrases。并以半监督的方式来训练</strong>。具体的分为以下三步：</p><ul><li>(1) 首先设计了几种pattern, 对于不同的pattern，在小的训练集上以cloze-style的方式进行训练。</li><li>(2) 使用(1)中训练出来的多个模型的集成，来为未标记样本添加soft labels。</li><li>(3) 使用(2) 中<font color="red">扩充的伪标签数据 Finetune 一个<strong>常规</strong></font> 的（非 MLM 的）模型。</li></ul></li><li><p>在多语言的不同任务集上，本文表明，给定少量到中等数量的标记示例，本文提出的PET的表现大幅优于无监督方法、监督训练和强半监督基线。</p></li></ul><h2 id="3-PET"><a href="#3-PET" class="headerlink" title="3. PET"></a>3. PET</h2><p>We define a <strong>pattern</strong> to be a function $P$ that takes $\mathrm{x}$ as input and outputs a phrase or sentence $P(\mathbf{x}) \in V^{*}$ that contains exactly one mask token, i.e., its output can be viewed as a cloze question. </p><p>We define a <strong>verbalizer</strong> as an injective function $v: \mathcal{L} \rightarrow V$ that maps each label to a word from $M$ ‘s vocabulary. We refer to $(P, v)$ as a pattern-verbalizer pair (PVP).</p><h3 id="3-1-PVP-Training-and-Inference"><a href="#3-1-PVP-Training-and-Inference" class="headerlink" title="3.1 PVP Training and Inference"></a>3.1 PVP Training and Inference</h3>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> few-shot learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> few-shot learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GPT Understands, Too</title>
      <link href="2021/04/06/GPT-Understands-Too/"/>
      <url>2021/04/06/GPT-Understands-Too/</url>
      
        <content type="html"><![CDATA[<p>转载：苏剑林. (Apr. 03, 2021). 《P-tuning：自动构建模版，释放语言模型潜能 》[Blog post]. Retrieved from <a href="https://kexue.fm/archives/8295" target="_blank" rel="noopener">https://kexue.fm/archives/8295</a></p><h2 id="1-P-tuning：自动构建模版，释放语言模型潜能"><a href="#1-P-tuning：自动构建模版，释放语言模型潜能" class="headerlink" title="1. P-tuning：自动构建模版，释放语言模型潜能"></a>1. P-tuning：自动构建模版，释放语言模型潜能</h2><p>在之前的文章<a href="https://kexue.fm/archives/7764" target="_blank" rel="noopener">《必须要GPT3吗？不，BERT的MLM模型也能小样本学习》</a>中，我们介绍了一种名为Pattern-Exploiting Training（PET）的方法，它通过人工构建的模版与BERT的MLM模型结合，能够起到非常好的零样本、小样本乃至半监督学习效果，而且该思路比较优雅漂亮，因为它将预训练任务和下游任务统一起来了。然而，人工构建这样的模版有时候也是比较困难的，而且不同的模版效果差别也很大，如果能够通过少量样本来自动构建模版，也是非常有价值的。</p><p>最近Arxiv上的论文<a href="https://arxiv.org/abs/2103.10385" target="_blank" rel="noopener">《GPT Understands, Too》</a>提出了名为P-tuning的方法，成功地实现了模版的自动构建。不仅如此，借助P-tuning，GPT在SuperGLUE上的成绩首次超过了同等级别的BERT模型，这颠覆了一直以来“GPT不擅长NLU”的结论，也是该论文命名的缘由。</p><h3 id="1-1-yaya-与本文相关的文献"><a href="#1-1-yaya-与本文相关的文献" class="headerlink" title="1.1 yaya-与本文相关的文献"></a>1.1 yaya-与本文相关的文献</h3><p>All NLP Tasks Are Generation Tasks: A General Pretraining Framework<br>GPT Understands, Too<br>How Many Data Points is a Prompt Worth<br>It’s Not Just Size That Matters:  Small Language Models Are Also Few-Shot Learners</p><h2 id="2-什么是模版-Pattern"><a href="#2-什么是模版-Pattern" class="headerlink" title="2. 什么是模版(Pattern)"></a>2. 什么是模版(Pattern)</h2><p>所谓PET，主要的思想是借助由自然语言构成的模版（英文常称Pattern或Prompt），将下游任务也转化为一个完形填空任务，这样就可以用BERT的MLM模型来进行预测了。比如下图中通过条件前缀来实现情感分类和主题分类的例子：</p><p><img src="https://i.loli.net/2021/04/06/Pov8T9D2a7Xur41.png" alt="通过特定模版将情感分类转换为MLM任务" style="zoom:33%;"></p><p>以上，通过特定模版将情感分类转换为MLM任务</p><p><img src="https://i.loli.net/2021/04/06/QyFxiS7RWTNGMK9.png" alt="通过特定模版将新闻分类转换为MLM任务" style="zoom:33%;"></p><p>以上，通过特定模版将新闻分类转换为MLM任务</p><p>当然，这种方案也不是只有MLM模型可行，用GPT这样的单向语言模型（LM）其实也很简单：</p><p><img src="https://i.loli.net/2021/04/06/GBAsIiEjOdvuSKp.png" alt="通过特定模版将情感分类转换为LM任务" style="zoom:33%;"></p><p>以上，通过特定模版将情感分类转换为LM任务</p><p><img src="https://i.loli.net/2021/04/06/lXsUG8bhfc4amLn.png" alt="通过特定模版将新闻分类转换为LM任务" style="zoom:33%;"></p><p>以上，通过特定模版将新闻分类转换为LM任务</p><p>不过由于语言模型是从左往右解码的，因此预测部分只能放在句末了（但还可以往补充前缀说明，只不过预测部分放在最后）。</p><p>某种意义上来说，这些模版属于语言模型的“探针”，我们可以通过模版来抽取语言模型的特定知识，从而做到不错的零样本效果，而配合少量标注样本，可以进一步提升效果，这些在<a href="https://kexue.fm/archives/7764" target="_blank" rel="noopener">《必须要GPT3吗？不，BERT的MLM模型也能小样本学习》</a>中已经比较详细讨论过了。</p><p>然而，前面已经说了，对于某些任务而言，<strong style="color:red;"><strong>人工构建模版并不是那么容易的事情</strong></strong>，模型的优劣我们也不好把握，而不同模型之间的效果差别可能很大，在这种情况下，人工标注一些样本可能比构建模版还要轻松得多。<strong style="color:red;">所以，如何根据已有的标注样本来自动构建模版，便成了一个值得研究的问题了。</strong></p><h2 id="3-P-tuning"><a href="#3-P-tuning" class="headerlink" title="3. P-tuning"></a>3. P-tuning</h2><p>P-tuning重新审视了关于模版的定义，放弃了“模版由自然语言构成”这一常规要求，从而将模版的构建转化为连续参数优化问题，虽然简单，但却有效。</p><h3 id="3-1-模版的反思"><a href="#3-1-模版的反思" class="headerlink" title="3.1 模版的反思"></a>3.1 模版的反思</h3><p>首先，我们来想一下“<strong style="color:red;">什么是模版</strong>“”。直观来看，模版就是由自然语言构成的前缀/后缀，通过这些模版我们使得下游任务跟预训练任务一致，这样才能更加充分地利用原始预训练模型，起到更好的零样本、小样本学习效果。</p><p><strong style="color:red;">等等，我们真的在乎模版是不是“自然语言”构成的吗？</strong></p><p>并不是。本质上来说，我们并不关心模版长什么样，<strong style="color:red;">我们只需要知道模版由哪些token组成，该插入到哪里，插入后能不能完成我们的下游任务，输出的候选空间是什么。</strong>模版是不是自然语言组成的，对我们根本没影响，“自然语言”的要求，只是为了更好地实现“一致性”，但不是必须的。于是，P-tuning考虑了如下形式的模版：</p><p><img src="https://i.loli.net/2021/04/06/SNYaqglG1LvJsMu.png" alt="img" style="zoom:33%;"></p><p>以上，<strong><strong style="color:blue;">P-tuning直接使用[unused*]的token来构建模版，不关心模版的自然语言性</strong></strong></p><p>这里的[u1]～[u6]，代表BERT词表里边的<strong style="color:red;">[unused1]～[unused6]</strong>，也就是用几个从未见过的token来构成模板，这里的token数目是一个超参数，放在前面还是后面也可以调整。接着，为了让“模版”发挥作用，我们<strong style="color:red;">用标注数据来求出这个模板</strong>。</p><h3 id="3-2-如何去优化"><a href="#3-2-如何去优化" class="headerlink" title="3.2 如何去优化"></a>3.2 如何去优化</h3><p>这时候，根据标注数据量的多少，我们又分两种情况讨论。</p><p><strong>第一种，标注数据比较少。</strong>这种情况下，我们固定整个模型的权重，只优化[unused1]～[unused6]这几个token的Embedding，换句话说，其实我们就是要学6个新的Embedding，使得它起到了模版的作用。这样一来，因为模型权重几乎都被固定住了，训练起来很快，而且因为要学习的参数很少，因此哪怕标注样本很少，也能把模版学出来，不容易过拟合。</p><p><strong>第二种，标注数据很充足。</strong>这时候如果还按照第一种的方案来，就会出现欠拟合的情况，因为只有6个token的可优化参数实在是太少了。因此，我们可以放开所有权重微调，原论文在SuperGLUE上的实验就是这样做的。读者可能会想：这样跟直接加个全连接微调有什么区别？原论文的结果是这样做效果更好，可能还是因为跟预训练任务更一致了吧。</p><p><img src="https://i.loli.net/2021/04/06/bJ2oPygslG3MhCN.png" alt="P-tuning在SuperGLUE上的表现"></p><p>以上，P-tuning在SuperGLUE上的表现</p><p>此外，在上面的例子中，目标token 如 “很”、“体育”是认为选定的，那么它们可不可以也用[unused$<em>$] 的token代替呢？答案是可以，但也分两种情况考虑：1、在标注数据比较少的时候，人工来选定适当的目标token效果往往更好些；2、在标注数据很充足的情况下，目标token用[unused$</em>$]效果更好些，因为这时候模型的优化空间更大一些。</p><h3 id="3-3-增强相关性"><a href="#3-3-增强相关性" class="headerlink" title="3.3 增强相关性"></a>3.3 增强相关性</h3><p>在原论文中，P-tuning并不是随机初始化几个新token然后直接训练的，而是通过一个小型的LSTM模型把这几个Embedding算出来，并且将这个LSTM模型设为可学习的。这样多绕了一步有什么好处呢？原论文大概的意思是：<strong>LSTM出现的token表示相关性更强，某种程度上来说更像“自然语言”（因为自然语言的token之间不是独立的），此外还能防止局部最优</strong>。我在Github上进一步向作者确认了一下（参考<a href="https://github.com/THUDM/P-tuning/issues/5" target="_blank" rel="noopener">这里</a>），效果上的差别是通过LSTM多绕一步的方法可以使得模型收敛更快、效果更优。</p><p>然而，这样多了一个LSTM，总感觉有些别扭，而且实现上也略微有点麻烦。按照作者的意思，LSTM是为了帮助模版的几个token（某种程度上）更贴近自然语言，但这并不一定要用LSTM生成，而且就算用LSTM生成也不一定达到这一点。<strong>笔者认为，更自然的方法是在训练下游任务的时候，不仅仅预测下游任务的目标token（前面例子中的“很”、“新闻”），还应该同时做其他token的预测。</strong></p><p>比如，<strong>如果是MLM模型，那么也随机mask掉其他的一些token来预测；如果是LM模型，则预测完整的序列，而不单单是目标词</strong>。这样做的理由是：因为我们的MLM/LM都是经过自然语言预训练的，所以我们（迷之自信地）认为能够很好完成重构的序列必然也是接近于自然语言的，因此这样增加训练目标，也能起到让模型更贴近自然语言的效果。经过笔者的测试，<strong>加上这样辅助目标，相比单纯优化下游任务的目标，确实提升了效果。</strong></p><h2 id="4-实验与效果"><a href="#4-实验与效果" class="headerlink" title="4. 实验与效果"></a>4. 实验与效果</h2><p>所谓“talk is cheap, show me the code”，又到了喜闻乐见的实验时间了。这里分享一下P-tuning的实验结果，其中还包括笔者对P-tuning的实现思路，以及笔者在中文任务上的实验结果。</p><h3 id="4-1-停止的梯度"><a href="#4-1-停止的梯度" class="headerlink" title="4.1 停止的梯度"></a>4.1 停止的梯度</h3><p>怎么实现上述的P-tuning算法比较好呢？如果是放开所有权重训练，那自然是简单的，跟普通的BERT微调没有什么区别。关键是在小样本场景下，如何实现“只优化几个token”呢？</p><p>当然，实现的方法也不少，比如为那几个要优化的token重新构建一个Embedding层，然后拼接到BERT的Embedding层中，然后训练的时候只放开新Embedding层的权重。但这样写对原来模型的改动还是蛮大的，最好的方法是尽可能少改动代码，让使用者几乎无感。为此，笔者构思了一种用<code>stop_gradient</code>简单修改<code>Embedding</code>层的方案，大体上是将<code>Embedding</code>层修改如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PtuningEmbedding</span><span class="params">(Embedding)</span>:</span></span><br><span class="line">    <span class="string">"""新定义Embedding层，只优化部分Token</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, mode=<span class="string">'embedding'</span>)</span>:</span></span><br><span class="line">        embeddings = self.embeddings</span><br><span class="line">        embeddings_sg = K.stop_gradient(embeddings)</span><br><span class="line">        mask = np.zeros((K.int_shape(embeddings)[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line">        mask[<span class="number">1</span>:<span class="number">9</span>] += <span class="number">1</span>  <span class="comment"># 只优化id为1～8的token</span></span><br><span class="line">        self.embeddings = embeddings * mask + embeddings_sg * (<span class="number">1</span> - mask)</span><br><span class="line">        <span class="keyword">return</span> super(PtuningEmbedding, self).call(inputs, mode)</span><br></pre></td></tr></table></figure><p>变量经过<code>stop_gradient</code>算子后，在反向传播的时候梯度为0，但是前向传播不变，因此在上述代码中，前向传播的结果不会有变化，但是反向传播求梯度的时候，梯度不为0的token由<code>mask</code>变量控制，其余token的梯度都为零，因此就实现了只更新部分token。</p><p>完整代码可见：</p><blockquote><p><strong>Github：<a href="https://github.com/bojone/P-tuning" target="_blank" rel="noopener">https://github.com/bojone/P-tuning</a></strong></p></blockquote><p>对了，原论文也开源了代码：</p><blockquote><p><strong>Github：<a href="https://github.com/THUDM/P-tuning" target="_blank" rel="noopener">https://github.com/THUDM/P-tuning</a></strong></p></blockquote><h3 id="4-2-测试与效果"><a href="#4-2-测试与效果" class="headerlink" title="4.2 测试与效果"></a>4.2 测试与效果</h3><p>前面已经分享了原作者在SuperGLUE上的实验结果，显示出如果配合P-tuning，那么：<strong style="color:red;">1、GPT、BERT的效果相比直接finetune都有所提升</strong>；<strong style="color:red;">2、GPT的效果还能超过了BERT。</strong>这表明GPT不仅有NLG的能力，也有NLU能力，可谓是把GPT的潜能充分“压榨”出来了，当然BERT配合P-tuning也有提升，说明P-tuning对语言模型潜能的释放是较为通用的。</p><p>原论文的实验比较丰富，建议读者仔细阅读原论文，相信会收获颇多。特别指出的是原论文的Table 2最后一列，当预训练模型足够大的时候，我们的设备可能无法finetune整个模型，而P-tuning可以选择只优化几个Token的参数，因为优化所需要的显存和算力都会大大减少，所以<strong>P-tuning实则上给了我们一种在有限算力下调用大型预训练模型的思路</strong>。</p><p><img src="https://i.loli.net/2021/04/06/BMTubQ4AvSJperz.png" alt="P-tuning在各个体量的语言模型下的效果" style="zoom: 50%;"></p><p>以上，P-tuning在各个体量的语言模型下的效果</p><p>当然，笔者一直以来的观点是“没有在中文上测试过的算法是没有灵魂的”，因此笔者也在中文任务上简单测试了，测试任务跟<a href="https://kexue.fm/archives/7764" target="_blank" rel="noopener">《必须要GPT3吗？不，BERT的MLM模型也能小样本学习》</a>一致，都是情感分类的小样本学习，测试模型包括BERT和GPT，两者的候选模版分别如下图：</p><p><img src="https://i.loli.net/2021/04/06/SJfYPAulG4Lt7km.png" alt="笔者在中文情感分类上使用的“BERT+P-tuning”模版" style="zoom:33%;"></p><p>以上，笔者在中文情感分类上使用的“BERT+P-tuning”模版</p><p><img src="https://i.loli.net/2021/04/06/DV5OrLdygc3wxfn.png" alt="笔者在中文情感分类上使用的“GPT+P-tuning”模版" style="zoom:33%;"></p><p>以上，笔者在中文情感分类上使用的“GPT+P-tuning”模版</p><p>注意，对于LM模型，前缀的引入非常重要，只引入后缀时效果会明显变差；而对于MLM模型，前缀的效果通常也优于后缀。总的效果如下表：</p><p><img src="https://i.loli.net/2021/04/06/3N8TOk1YsmpSbHc.png" alt="image-20210406111755122" style="zoom:50%;"></p><p>其中“小样本”只用到了“少量标注样本”，“无监督”则用到了“大量无标注样本”，“半监督”则用到了“少量标注样本+大量无标注样本”，“P-tuning”都是小样本，PET的几个任务报告的是最优的人工模版的结果，其实还有更差的人工模版。从小样本角度来看，P-tuning确实取得了最优的小样本学习效果；从模版构建的角度来看，P-tuning确实也比人工构建的模版要好得多；从模型角度看，P-tuning确实可以将GPT的分类性能发挥到跟BERT相近，从而揭示了GPT也有很强的NLU能力的事实。</p><h2 id="5-进一步理解"><a href="#5-进一步理解" class="headerlink" title="5. 进一步理解"></a>5. 进一步理解</h2><p>这一节将会介绍笔者对P-tuning的进一步思考，以求从多个维度来理解P-tuning。</p><h3 id="5-1-离散-vs-连续"><a href="#5-1-离散-vs-连续" class="headerlink" title="5.1 离散 vs 连续"></a>5.1 离散 vs 连续</h3><p>在P-tuning之前，也已经有一些在做模版的自动构建，如<a href="https://arxiv.org/abs/1911.12543" target="_blank" rel="noopener">《How Can We Know What Language Models Know?》</a>、<a href="https://arxiv.org/abs/2010.15980" target="_blank" rel="noopener">《AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts》</a>等，但<strong>它们搜索的都是在离散空间下搜索的自然语言模版</strong>，所以效果有所限制，并没有取得特别突出的结果。</p><p>相反，P-tuning放弃了“模版由自然语言构成”这一要求，从而将其变成了可以简单梯度下降求解的连续参数问题，效果还更好。同时，这一改动意味着P-tuning突出了模版的本质——即模版的关键在于它是怎么用的，不在于它由什么构成——给人一种去芜存菁、眼前一亮额的感觉，确实值得点赞。</p><h3 id="5-2-Adapter"><a href="#5-2-Adapter" class="headerlink" title="5.2 Adapter"></a>5.2 Adapter</h3><p>我们还可以从Adapter的角度来理解P-tuning。BERT出来后不久，Google在论文<a href="https://arxiv.org/abs/1902.00751" target="_blank" rel="noopener">《Parameter-Efﬁcient Transfer Learning for NLP》</a>中提出了一种名为Adapter的微调方式，它并不是直接微调整个模型，而是固定住BERT原始权重，然后在BERT的基础上添加一些残差模块，只优化这些残差模块，由于残差模块的参数更少，因此微调成本更低。Adapter的思路实际上来源于CV的<a href="https://arxiv.org/abs/1705.08045" target="_blank" rel="noopener">《Learning multiple visual domains with residual adapters》</a>，不过这两年似乎很少看到了，也许是因为它虽然提高了训练速度，但是预测速度却降低了，精度往往还有所损失。</p><p><strong>在P-tuning中，如果我们不将新插入的token视为“模版”，是将它视为模型的一部分，那么实际上P-tuning也是一种类似Adapter的做法，<strong style="color:red;">同样是固定原模型的权重，然后插入一些新的可优化参数</strong>，<strong style="color:purple;">同样是只优化这些新参数，只不过这时候新参数插入的是Embedding层</strong>。因此，从这个角度看，P-tuning与Adapter有颇多异曲同工之处。</strong></p><h3 id="5-3-为什么有效"><a href="#5-3-为什么有效" class="headerlink" title="5.3 为什么有效"></a>5.3 为什么有效</h3><p>然后，还有一个值得思考的问题：为什么P-tuning会更好？比如全量数据下，大家都是放开所有权重，P-tuning的方法依然比直接finetune要好，为啥呢？</p><p>事实上，提出这个问题的读者，应该是对BERT加个全连接层的直接finetune做法“习以为常”了。很明显，不管是PET还是P-tuning，它们其实都更接近预训练任务，而加个全连接层的做法，其实还没那么接近预训练任务，所以某种程度上来说，P-tuning有效更加“显然”，反而是加个全连接层微调为什么会有效才是值得疑问的。</p><p>去年有篇论文<a href="https://arxiv.org/abs/2010.03648" target="_blank" rel="noopener">《A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks》</a>试图回答这个问题，大致的论证顺序是：</p><blockquote><p>1、预训练模型是某种语言模型任务；</p><p>2、下游任务可以表示为该种语言模型的某个特殊情形；</p><p>3、当输出空间有限的时候，它又近似于加一个全连接层；</p><p>4、所以加一个全连接层微调是有效的。</p></blockquote><p>可以看到，该论文的假设主要是第2点，其实就是直接假设了下游任务可以表达为类似PET的形式，然后才去证明的。所以这进一步说明了，PET、P-tuning等才是更自然的使用预训练模型的方式，加全连接直接finetune的做法其实只是它们的推论罢了，也就是说，PET、P-tuning才是返璞归真、回归本质的方案，所以它们更有效。</p><h2 id="6-简单的总结"><a href="#6-简单的总结" class="headerlink" title="6. 简单的总结"></a>6. 简单的总结</h2><p>本文介绍了P-tuning，它是一种模版的自动构建方法，而通过模版我们可以从语言模型中抽取知识，完成零样本、小样本等学习任务，并且效果往往还更好。借助P-tuning，GPT也能实现优秀的NLU效果，在SuperGLUE上的表现甚至超过了BERT。除此之外，P-tuning还一种在有限算力下调用大型预训练模型的有效方案。</p><h1 id="智源"><a href="#智源" class="headerlink" title="智源"></a>智源</h1><p><img src="https://i.loli.net/2021/05/31/x3uXTm5QU47IKZF.png" alt="image-20210531133837040" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/5tbaTcxr8FCivUL.png" alt="image-20210531133920627" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/NWXjCMk4zsIVubE.png" alt="image-20210531134146305" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/7j3lW4vrfcVIwgU.png" alt="image-20210531134313255" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/VKDJZYjpRd63UWP.png" alt="image-20210531134449585" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/5gzWlk74Z18Exf3.png" alt="image-20210531134511330" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/8FK6wd1Ez9pouLC.png" alt="image-20210531134639415" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/vqU6M3CftRNSbh2.png" alt="image-20210531134820215" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/Wn2Qw4A1bOZLXpI.png" alt="image-20210531134929428" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/qlV85o9kDLza6Fj.png" alt="image-20210531134943431" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/Ya8I5cBEC7OmURA.png" alt="image-20210531135052454" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/pRGmzBlJ6DiW7Uf.png" alt="image-20210531135156520" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/TEhUFRkCWa3nX6Z.png" alt="image-20210531135528640" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/65buBKrIaRnPgJX.png" alt="image-20210531135651006" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/rAc7YBSuMXk12se.png" alt="image-20210531135716122" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/kd1PwMIaQHWoNRA.png" alt="image-20210531135826044" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/Ley1OMGqgDaJpsF.png" alt="image-20210531140027073" style="zoom: 50%;"></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[COOT] Cooperative Hierarchical Transformer for Video-Text Representation Learning</title>
      <link href="2021/04/03/COOT-Cooperative-Hierarchical-Transformer-for-Video-Text-Representation-Learning/"/>
      <url>2021/04/03/COOT-Cooperative-Hierarchical-Transformer-for-Video-Text-Representation-Learning/</url>
      
        <content type="html"><![CDATA[<p>这篇文章发表在 NeurIPS 2020，同时做 video-text retrieval and video captioning task.</p><p><img src="https://i.loli.net/2021/04/03/c8m4XTZtS6NE7hJ.png" alt="image-20210403102956324" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/04/04/IXfubaGzPV21sEt.png" alt="image-20210404124021644" style="zoom:50%;"></p><h2 id="本文的方法"><a href="#本文的方法" class="headerlink" title="本文的方法"></a>本文的方法</h2><p><font color="red"><strong>To model intra-level cooperation</strong></font>, we introduce <strong>an attention-aware feature aggregation layer (Attention-FA)</strong> to focus on temporal interactions between low-level entities (Figure 1-Attention-FA).<br>This component replaces traditional sequence representation aggregation methods in transformers such as using a [CLS] token [11, 14, 15, 19] or mean pooling [25] with an attention-aware fusion. It leverages temporal context to encourage important entities to contribute more to the final representation of a sequence of frames or words.</p><p><font color="red"><strong>For the inter-level cooperation</strong></font>, we introduce <strong>a contextual attention module(Contextual transformer)</strong>, which enforces the network to highlight semantics relevant to the general context of the video and to suppress the irrelevant semantics. This is done by modeling the interaction between low-level (clips-sentences) and high-level entities (global contexts), as shown in Figure 1-green region.</p><p>In addition to this architectural contributions, we introduce <strong style="color:red;">a new cross-modal cycle-consistency loss</strong> to enforce interaction between modalities and encourage the semantic alignment between them in the learned common space. We show that enforcing two domains to produce consistent representations leads to substantially improved semantic alignment.</p><h3 id="An-attention-aware-feature-aggregation-layer-Attention-FA"><a href="#An-attention-aware-feature-aggregation-layer-Attention-FA" class="headerlink" title="An attention-aware feature aggregation layer (Attention-FA)"></a>An attention-aware feature aggregation layer (Attention-FA)</h3><ul><li>简单来说，就是以自身（sentence/clip）特征计算一个权重，然后权重求和得到（paragraph/video）特征。</li></ul><p><img src="https://i.loli.net/2021/04/04/PtqSgLAi2Ky6Fc7.png" alt="image-20210404143442057" style="zoom:50%;"></p><h3 id="cross-modal-cycle-consistency-loss-lt-strong-gt"><a href="#cross-modal-cycle-consistency-loss-lt-strong-gt" class="headerlink" title="cross-modal cycle-consistency loss&lt;/strong&gt;"></a>cross-modal cycle-consistency loss&lt;/strong&gt;</h3><p>sentences of  a paragraph, clip of a video</p><p>给定一个第$i$ 个 sentence embedding， 计算其与 clips 之间的关系 $\alpha_i$，然后由$\alpha_i$权重求和计算得到soft nearest neighbor.</p><p>由该soft nearest neighbor, 计算其与 sentences 之间的关系 $\beta_j$, 然后由$\beta_j$计算 soft location: $\mu=\sum_{j=1}^{m} \beta_{j} j$</p><p>The sentence embedding  is semantically cycle consistent if and only if it cycles back to the original location.</p><p>Our objective is the distance between the source location $i$ and the soft destination location $\mu .$</p><script type="math/tex; mode=display">\ell_{C M C}=\|i-\mu\|^{2}</script><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ul><li><p>探究本文提出的 the attention-aware feature aggregation (AF), the Contextual Transformer (CoT), and the cross-modal<br>cycle-consistency loss (CMC) 的作用</p><ul><li>对比 COOT+ CLS/AVG/MAX  和 COOT+AFA, 可以发现，AFA是有效的</li><li>对比 COOT+AFA X X  和 COOT+AFA √ X, 可以发现，CMC是有效的，但是效果其实没有那么明显</li><li>对比 COOT+AFA √ X 和  COOT+AFA √ √，可以发现，CoT是有效的</li></ul><p><img src="https://i.loli.net/2021/04/04/NlfV3giYjohSDbn.png" alt="image-20210404141458907" style="zoom:50%;"></p></li></ul><ul><li><p>在ActivityNet-caption dataset 上与最新方法对比</p><p><img src="https://i.loli.net/2021/04/04/1C26nJMLsBlwdEP.png" alt="image-20210404142959158" style="zoom:50%;"></p></li><li><p>在 YouCook2 dataset  上与最新方法对比</p><ul><li>利用howto100m进行预训练是有用的。</li></ul><p><img src="https://i.loli.net/2021/04/04/bmDwJnpIBXPHA1j.png" alt="image-20210404143035311" style="zoom:50%;"></p></li><li><p>验证本模型学到的特征包含了有意义的信息，对于其他任务(e.g., video captioning )也是有效的。</p><p>在 MART [2] 上使用了本文提出的特征，实验结果如下：</p><p><img src="https://i.loli.net/2021/04/04/Em3ILM27ueTbCOa.png" alt="image-20210404145023633" style="zoom: 50%;"></p><p><img src="https://i.loli.net/2021/04/04/i71VYerR8gZSWCj.png" alt="image-20210404144942155" style="zoom:50%;"></p></li></ul><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li>对我的启发： the attention-aware feature aggregation (AF)是有用的，可以借鉴。</li><li>本文不仅使用了普通的ranking loss, 本文提出的循环一致损失，还使用了【1】提出的 一个损失</li><li>本文仅在 activitynet-caption 和 youcook2 上做了实验。</li><li>To apply the cycle-consistency loss, we found that sampling 1 clip per video and 1 sentence per paragraph works best.<br>这句话，是什么意思？？？？</li><li>本文的方法可能更加适合于 paragraph-video 的匹配任务。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]  Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In ECCV, 2018.</p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cross-Batch Memory for Embedding Learning</title>
      <link href="2021/04/02/Cross-Batch-Memory-for-Embedding-Learning/"/>
      <url>2021/04/02/Cross-Batch-Memory-for-Embedding-Learning/</url>
      
        <content type="html"><![CDATA[<p>转载自：<a href="https://zhuanlan.zhihu.com/p/139187724" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/139187724</a></p><p>转载的原因：</p><ul><li>此篇论文与 MoCo 的思路非常的相似</li><li>其实度量学习的思想也是可以用于 跨模态检索任务中的，毕竟跨模态检索做的也是度量任务。<br>目前看到有两篇论文已经这样做了：<ul><li>HiT Hierarchical Transformer with Momentum Contrast for Video-Text Retrieval</li><li>Memory Enhanced Embedding Learning for Cross-Modal Video-Text Retrieval</li></ul></li></ul><p>码隆科技今年在 CVPR 多有斩获。在大会论文接受率仅有 22%、被称为“十年来最难的一届”的情况下，有两篇论文入选 CVPR 2020，其中本文 Cross-Batch Memory for Embedding Learning为口头报告论文（Oral）。口头报告论文是 CVPR 官方核定的含金量最高的论文，以 CVPR 2019 为例，投稿的 5160 篇论文中，仅有约 5.6% 获评口头报告论文。同时，作为以技术为先的人工智能企业，码隆科技和往年一样，是 CVPR 2020 的金牌赞助商。</p><p>本篇论文提出了 XBM 方法，能够用极小的代价，提供巨量的样本对，为 pair-based 的深度度量学习方法取得巨大的效果提升。这种提升难例挖掘效果的方式突破了过去两个传统思路：加权和聚类，并且效果也更加简单、直接，很好地解决了深度度量学习的痛点。XBM 在多个国际通用的图像搜索标准数据库上（比如 SOP、In-Shop 和 VehicleID 等)，取得了目前最好的结果。</p><p><strong>我们的算法研究员也在知乎上针对这篇论文撰写了一版更为通俗有趣的解读文章。感兴趣的话，可以点击下方直达。</strong></p><p><a href="https://zhuanlan.zhihu.com/p/136522363" target="_blank" rel="noopener">王珣：跨越时空的难样本挖掘zhuanlan.zhihu.com<img src="https://i.loli.net/2021/04/02/xsgfovFm5TVaCyq.jpg" alt="图标"></a></p><p><strong>论文</strong>：Cross-Batch Memory for Embedding Learning</p><p><strong>地址</strong>：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1912.06798" target="_blank" rel="noopener">https://arxiv.org/abs/1912.06798</a></p><h2 id="背景和动机"><a href="#背景和动机" class="headerlink" title="背景和动机"></a><strong>背景和动机</strong></h2><p>难例挖掘是深度度量学习领域中的核心问题，最近有颇多研究都通过改进采样或者加权方案来解决这一难题，目前主要两种思路：第一种思路是在 mini-batch 内下功夫，对于 mini-batch 内的样本对，从各种角度去衡量其难度，然后给予难样本对更高权重，比如 N-pairs、Lifted Struture Loss、MS Loss 使用的就是此种方案。第二种思路是在 mini-batch 的生成做文章，比如 HTL、Divide and Conquer，他们的做法虽然看上去各有不同，但是整体思路有异曲同工之处。大致思路都是对整个数据集进行聚类，每次生成 mini-batch 不是从整个数据集去采样，而是从一个子集，或者说一个聚类小簇中去采样。这样一来，由于采样范围本身更加集中，生成的 mini-batch 中难例的比例自然也会更大，某种程度上也能解决问题。然而，无论是第一种方法的额外注重难样本，还是第二种方法的小范围采样，他们的难例的挖掘能力其实依然有一个天花板——那就是 mini-batch 的大小。这个 mini-batch 的大小决定了在模型中单次迭代更新中，可以利用的样本对的总量。因此，即使是很精细的采样加权方法，在 mini-batch 大小有限的情况下，也很难有顶级的表现。我们在三个标准图像检索数据库上进行了实验，基于三种标准的 pair-based 方法，我们发现随着 mini-batch 变大，效果（Recall@1）大幅提升。实验结果如下图：</p><p><img src="https://i.loli.net/2021/04/02/XnEsAeKYZ78Sak3.png" alt="image-20210402165727364"></p><p>可以看出，随着 mini-batch 的增大，效果有显著提升。但是，在实际工业应用中 mini-batch 越大，训练所需要的 GPU 或 TPU 就越多，即使计算资源有充分保证，在多机多卡的训练过程中，如何在工程上保证通信的效率也是一个有挑战的问题。</p><h2 id="特征偏移"><a href="#特征偏移" class="headerlink" title="特征偏移"></a><strong>特征偏移</strong></h2><p>由此，我们希望另辟蹊径，得以在 mini-batch 有限的情况下，也能获得充足的难例样本对。首先，必须突破深度度量学习一直以来的一个思维局限——仅在对当前 mini-batch里的样本对两两比较，形成样本对。以此我们引入了 XBM（Cross-batch Memory）这一方法来突破局限，跨越时空进行难例挖掘，把过去的 mini-batch 的样本提取的特征也拿过来与当前 mini-batch 作比较，构建样本对。</p><p><img src="https://i.loli.net/2021/04/02/gzBpr4V6MLxbPCd.jpg" alt="img"></p><p>我们将样本特征随着模型训练的偏移量，称之为特征偏移（Feature Drift）。从上图我们发现，在训练的一开始，模型还没有稳定，特征剧烈变化，每过 100 次迭代，特征偏移大约 0.7 以上。但是，随着训练的进行，模型逐步稳定，特征的偏移也变小。我们称这个现象为慢偏移（Slow Drift），这是我们可以利用的一点。</p><h2 id="方法梗概"><a href="#方法梗概" class="headerlink" title="方法梗概"></a><strong>方法梗概</strong></h2><p>我们发现，虽然在训练的前 3K iterations，mini-batch 过去的提取特征与当前模型偏差很大，但是，随着训练时间的延长，过去的迭代里所提取过的特征，逐渐展示为当前模型的一个有效近似。我们要做的不过是把这些特征给存下来，每个特征不过是 128 个 float 的数组，即便我们存下了过去 100 个 mini-batch 的特征，不过是6400个（假设 batch size = 64）float 数组，所需要不过是几十 MB 的显存。而它带来的好处是显而易见的，我们能够组成的样本对的个数是仅仅利用当前 mini-batch 的一百倍。即便这些特征不能高精准地反映当前模型的信息，但是只要特征偏移在合理的范围内，这种数量上带来的好处，可以完全补足这种误差带来的副作用。具体来看，我们的 XBM 的方法架构大致如下：</p><p><img src="https://i.loli.net/2021/04/02/ixCLyrXDOHwck8u.jpg" alt="img"></p><p>伪代码如下：</p><p><img src="https://i.loli.net/2021/04/02/yWbPMmZtnerAHV3.jpg" alt="img"></p><p>我们的 XBM 从结构上非常简单清晰。我们先训练一个 epoch 左右，等待特征偏移变小。然后，我们使用 XBM：一个特征队列去记忆过去 mini-batch 的特征，每次迭代都会把当前 mini-batch 提取出来的新鲜特征加入队列，并把最旧的特征踢出队列，从而保证 XBM 里的特征尽量是最新的。每次去构建样本队列的时候，我们将当前 mini-batch 和 XBM 里的所有特征都进行配对比较，从而形成了巨量的样本对。如果说 XBM 存储了过去 100 个 mini-batch，那么其所产生的样本对就是基于 mini-batch 方法的 100 倍。不难发现，XBM 其实直接和过去基于样本对的方法结合，只需要把原来的 mini-batch 内的样本对换成当前 mini-batch 和 XBM 的特征构成的样本对就可以了。所以，我们通过 XBM 这种存储特征的机制，能够让不同时序的 mini-batch 的特征成功配对。</p><h2 id="消融实验一"><a href="#消融实验一" class="headerlink" title="消融实验一"></a><strong>消融实验一</strong></h2><p><img src="https://i.loli.net/2021/04/02/Be4GvZMLxiCbEyt.jpg" alt="img"></p><p>首先，我们在三个常用的检索数据集，和三个基于样本对的深度学习的方法上，使用 XBM 进行测试，同时控制其他的设置全部不变。我们发现，XBM 带来的效果很明显。尤其是在最基本的对比损失（Contrastive Loss）上，可以看到，本来这个方法只利用 mini-batch 内的样本时，其效果并不显著，但是 XBM 带来了显著的效果提升。在三个数据集， Recall@1 都至少提升 10 个点，尤其是 VehicleID 数据集的最大（Large）测试集，效果提升了 22 个点，从 70.0 到 92.5。</p><h2 id="消融实验二"><a href="#消融实验二" class="headerlink" title="消融实验二"></a><strong>消融实验二</strong></h2><p><img src="https://i.loli.net/2021/04/02/VD6UHdejXizlyTf.jpg" alt="img"></p><p>关于 mini-batch 的大小对结果的影响， 从上图可发现三点：</p><p><strong>1.</strong> 无论是否使用 XBM，mini-batch 越大，效果越好；</p><p><strong>2.</strong> XBM 方法即便是使用很小的 batch (16)， 也比没有 XBM 使用大的 batch (256) 效果好；</p><p><strong>3.</strong> 由于 XBM 本身可以提供正样本对，所以可以不一定要用 PK sampler 来生成 mini-batch，而是可以直接使用原始的 shuffle sampler，效果相似。</p><h2 id="计算资源消耗"><a href="#计算资源消耗" class="headerlink" title="计算资源消耗"></a><strong>计算资源消耗</strong></h2><p>下图我们展示了在 SOP 上训练 XBM 时的计算资源消耗，即便把整个训练集（50K+）的特征都加载到 XBM，不过需要 0.2GB 的显存；而如果是使用增大 batch 的方法，会额外需要 15GB 显存，是 XBM 的 80 倍，但是效果的提升比 XBM 差很多。毕竟 XBM 仅仅需要存特征，特征也是直过去的 mini-batch 的前向计算的结果，计算资源的需求很小。</p><p><img src="https://i.loli.net/2021/04/02/ATGEJLXR9UjWCN6.jpg" alt="img"></p><h2 id="对比-SOTA"><a href="#对比-SOTA" class="headerlink" title="对比 SOTA"></a><strong>对比 SOTA</strong></h2><p>与最近的深度度量学习方法对比，我们在四个检索数据库上效果均大幅提升，这里仅列出 VehicleID 的效果，其他数据集的效果见原论文。</p><p><img src="https://i.loli.net/2021/04/02/veWF2knBIuEogwj.jpg" alt="img"></p><p>简单来说，不同于部分文章中使用更好的网络结构，更大的输出维度，或者更大的 mini-batch 来提升效果，强行 SOTA。我们列出 XBM 在 64 的 mini-batch 在不同的主干网络下及各种维度下的效果，其效果一直是最好的。</p><p><strong>另外，我们的方法也将整理开源，地址在：</strong></p><p><strong><a href="https://link.zhihu.com/?target=https%3A//github.com/MalongTech/research-xbm" target="_blank" rel="noopener">https://github.com/MalongTech/research-xbm</a></strong></p><p><strong>论文地址在：</strong></p><p><strong><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1912.06798" target="_blank" rel="noopener">https://arxiv.org/abs/1912.06798</a></strong></p><p><strong>期待与更多研究者共同学习交流。</strong></p><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a><strong>可视化</strong></h2><p><img src="https://i.loli.net/2021/04/02/TIxiYh9LDWnUq4s.jpg" alt="img"></p><p>更多可视化见论文补充材料，有更多实例说明效果。</p><h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a><strong>总结与展望</strong></h2><p>第一，本文提出的 XBM 方法能够记住过去的特征，使得模型的训练不再仅仅依靠当前 mini-batch 的样本，而是可以跨越时空进行样本配对。从而用极小的代价，提供了巨量的样本对，为 pair-based 的深度度量学习方法取得了巨大的效果提升。这种提升难例挖掘效果的方式，也是突破了过去两个传统思路：加权和聚类，并且效果也更加简单、直接，很好地解决了深度度量学习的痛点。</p><p>第二，其实 Memory 机制并不是本文原创，但是用 Memory 来做难例挖掘是一个全新的尝试。同样在 CVPR 2020 获得 Oral，也是由 Kaiming He 作为一作的 MoCo 也是这种思路。<font color="red"><strong>本文的方法其实可以认为是 MoCo 在 ｍ=0 的特例</strong></font>，Kaiming 通过动量更新 key encoder，可以直接控制住特征偏移。作者认为，这种方法还会在很多任务带来提升，不局限于 Kaiming 的自监督表示学习，以及此前我们一直关注研究的度量学习（或者说监督表示学习）。</p><p>第三，在本文中，虽然 XBM 在所有的 pair-based 的方法都有提升，但是明显在对比损失（Contrastive Loss）上提升最大，具体原因待考。另外，我们也把在无监督表示上表现很好的 infoNCE 方法用到了深度度量学习，但效果并不显著。作者认为这两个问题的答案是同一个，且有值得深究的价值，希望在后续研究中进行进一步跟进探索。</p>]]></content>
      
      
      
        <tags>
            
            <tag> metric learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HiT Hierarchical Transformer with Momentum Contrast for Video-Text Retrieval</title>
      <link href="2021/04/02/HiT-Hierarchical-Transformer-with-Momentum-Contrast-for-Video-Text-Retrieval/"/>
      <url>2021/04/02/HiT-Hierarchical-Transformer-with-Momentum-Contrast-for-Video-Text-Retrieval/</url>
      
        <content type="html"><![CDATA[<h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><ul><li><p>当前基于 transformer architecture 的 跨模态检索，仅仅使用最后一层的embeddings。 不同的层有不同的特诊特性，却没有被利用。</p></li><li><p>端到端的训练机制限制了负样本的交互仅能在一个mini-batch 中被执行（负样本的数量受到限制）。</p></li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><ul><li>提出了一个新颖的hierarchical transformer 来做 video-text retrieval。在feature-level（第一层） 和 semantic-level（最后一层） 来实现分层跨模态对比匹配。</li><li>受到MOCO的启发，在跨模态学习中使用Momentum Cross-modal Contrast（<strong>MCC</strong>），以使能 大规模的负样本交互，这种方式对生成更加精确且有判别力的表达是有贡献的。</li></ul><p>本文的 momentum encoder 与 论文“Memory Enhanced Embedding Learning forCross-Modal Video-Text Retrieval”的思想很一致</p><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><p><img src="https://i.loli.net/2021/04/02/uTMwEnhAfIVgixm.png" alt="image-20210402161043011"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ul><li><p>在各个数据集上的实验效果</p><p><img src="https://i.loli.net/2021/04/02/PrfJab2l3kLcxmg.png" alt="image-20210402164510842" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/04/02/rps6KOJ31wBnitX.png" alt="image-20210402164527524" style="zoom:50%;"></p></li><li><p>使用不同维度下的MCC的效果对比</p><p><img src="https://i.loli.net/2021/04/02/25nEX687HBZSqVN.png" alt="image-20210402164404377" style="zoom:50%;"></p></li><li><p>InfoNCE loss vs Triplet Ranking loss</p><p><img src="https://i.loli.net/2021/04/02/4LPx5Nh1nIAgaGf.png" alt="image-20210402164121908" style="zoom:50%;"></p></li></ul><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><p>值得注意的是本文使用的是 dual encoding的方法，</p><p>对 video feature 和 sentence feature 分别使用各自的transformer获得特征。</p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Memory Enhanced Embedding Learning for Cross-Modal Video-Text Retrieval</title>
      <link href="2021/04/02/Memory-Enhanced-Embedding-Learning-for-Cross-Modal-Video-Text-Retrieval/"/>
      <url>2021/04/02/Memory-Enhanced-Embedding-Learning-for-Cross-Modal-Video-Text-Retrieval/</url>
      
        <content type="html"><![CDATA[<h2 id="本文研究的任务"><a href="#本文研究的任务" class="headerlink" title="本文研究的任务"></a>本文研究的任务</h2><ul><li>视频文本跨模态检索</li></ul><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p>本文提出的问题其实也是度量学习中的一个关键问题，对于一个anchor, 为了在一个mini-batch中看到更多的负样本，一个简单的方法是扩大batch size，计算量就提升了。</p><p>针对此问题，有人提出了 memory bank, 将整个训练集中的样本特征预先提取出来，存储起来，每个epoch更新一次。Memory Bank 虽然可以保留足够多的样本进行 negative采样，但是其中存储的图像特征都是过去的 encoder 编码的特征。由于 Memory Bank 容量很大，导致了采样的特征具有不一致性（是由不同的encoder产生的）。</p><h3 id="MOCO"><a href="#MOCO" class="headerlink" title="MOCO"></a>MOCO</h3><p>鉴于此，MoCo 使用一个队列来存储和采样 negative 样本，队列中存储多个<strong>近期</strong>用于训练的 batch 的特征向量。队列容量要远小于 Memory Bank，但可以远大于 batch 的容量。本文提出了将队列作为一个动态的，而非静态的 Memory Bank。队列在不断的进行更新，新的训练batch入队列后，最老的训练batch 出队列。这里入队列的并不是图像本身，而是图像特征。</p><p>训练时, Anchor 样本记为 $x^{q}$ (query), 经过 encoder 网络 $f_{q}$ 进行编码得到 $q=f_{q}\left(x^{q}\right)$<br>o 随后从队列中采样了 $K+1$ 个样本 $\left\{k^{0}, \ldots, k^{K}\right\}$ 作为key。这些 key 是用不同的队列 encoder 网络 $f_{k}$ 进行编码得到的。由于 $f_{k}$ 的变化非常缓慢, 因此虽然 $\left\{k^{0}, \ldots, k^{K}\right\}$ 是通 过不同 encoder 编码的，编码器导致的差异会非常小。具体的，本文使用一种 Momentum 更新 的方式来更新 $f_{k},$ 其参数是对 Query Encoder 的平滑拷贝：<br>$\theta_{k}=m \theta_{k}+(1-m) \theta_{q}$<br>其中 $m=0.999,$ 表示 $\theta_{k}$ 呈现一种缓慢的变化。这种更新方式类似于 Q-learning 中 target-Q冈络更新。</p><p>下图形式化的表示了三种结构, end-to-end, memory-bank和MoCo的区别。MoCo的特点 是: $\quad(1)$ 用于 negative 采样的队列是动态的 (2) 用于 negative 样本特征提取的编码器与用于 query提取的编码器不一致，是一种Momentum更新的关系。而在memory-bank中两个编码器 是一致的。 (3) 与Memory Bank类似, NCE 产生的损失并不影响 $f_{k},$ 只影响 $f_{q}$ 。原因是 $x^{k}$ 样本一般是 $x^{q}$ 样本的几倍, 更新 $f_{k}$ 的代价很大（例如在 end-to-end中 $),$ 因此 Memory-Bank 和 MoCo 都不更新 $f_{k}$ 。</p><p><strong>在实验中，大的动量（例如0.999）往往效果好于小的动量（例如0.9），意味着缓慢变化的key encoder是利用好队列的关键所在。</strong></p><p><img src="https://i.loli.net/2021/04/02/ymTscqK8Q5Fpkua.png" alt="image-20210402122243715"></p><h2 id="本文的方法"><a href="#本文的方法" class="headerlink" title="本文的方法"></a>本文的方法</h2><ul><li>基于此思想，本文提出了用于跨模态检索任务中的 mementum cross-modal memory bank。</li></ul><h2 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h2><p>相关领域的知识是可以互相借鉴的。</p><p>跨模态检索领域，是可以从<strong>对比学习的无监督视觉表达学习</strong>，<strong>度量学习</strong>，等任务中借鉴的。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>MOCO： <strong>Momentum</strong> contrast for unsupervised visual representation learning</p><p><strong>InfoNCE</strong>: Representation learning with <strong>contrastive</strong> predictive coding.</p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers</title>
      <link href="2021/04/01/Thinking-Fast-and-Slow-Efficient-Text-to-Visual-Retrieval-with-Transformers/"/>
      <url>2021/04/01/Thinking-Fast-and-Slow-Efficient-Text-to-Visual-Retrieval-with-Transformers/</url>
      
        <content type="html"><![CDATA[<p>发表在CVPR 2021</p><p>这篇论文与(ICLR 2021) Support-set bottlenecks for video-text representation learning都结合了 captioning task</p><h2 id="研究任务"><a href="#研究任务" class="headerlink" title="研究任务"></a>研究任务</h2><p>提高跨模态检索的推理速度</p><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p>目前做 cross-modal retrieval task 的主流模型可以分为两类：（1）dual encoding，速度快，但是准确率低；（2）cross-attention model, 准确率高，但是速度慢。</p><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>保证模型准确率的同时，提高推理速度</p><ul><li><p>提高模型准确率，采取了两个点：（1）使用CNN特征上采样来获取细粒度视觉特征；（2）使用captioning loss来代替 对比学习损失来做检索任务。如下图</p><p><img src="https://i.loli.net/2021/04/01/VtX5mQn8T2LcwMs.png" alt="image-20210401205905230"></p></li><li><p>在提高模型推理速度上，采取了两个点：(1) 使用 teacher-student 方法，以 slow cross-attention model 作为教师，以 fast dual-encoding model 作为学生；（2）先使用 fast model检索，选取top-k，然后使用slow model 进行 re-rank。如下图</p><p><img src="https://i.loli.net/2021/04/01/bh9IRmOBQWlZKCg.png" alt="image-20210401205835827" style="zoom:50%;"></p></li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>VirTex: Learning Visual Representations from Textual Annotations</p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> real-time </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
            <tag> real-time </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Baby Talk</title>
      <link href="2021/03/26/Neural-Baby-Talk/"/>
      <url>2021/03/26/Neural-Baby-Talk/</url>
      
        <content type="html"><![CDATA[<h2 id="当前存在的问题"><a href="#当前存在的问题" class="headerlink" title="当前存在的问题"></a>当前存在的问题</h2><p>当前的模型缺少 visual grounding，比如，不能将生成的named concept 与 image 中的pixels相关联起来。</p><p>模型的关注点与人类的关注点不同，并且倾向于从训练数据中copy captionings，即，得到 Hallucination Objects。 </p><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>本文提出了两步走策略来处理图像描述任务：（1）sentence template generation （2）slot filling with object detectors。</p><ul><li>First generatesa sentence ‘template’ with slot locations explicitly tied tospecific  image  regions.</li><li>These  slots  are  then  filled  in  by visual  concepts  identified  in  the  regions  by  object  detectors.</li></ul><p>整个结构是端到端可微分的。</p>]]></content>
      
      
      <categories>
          
          <category> image captioning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> image captioning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Non-Autoregressive Image Captioning with Counterfactuals-Critical Multi-Agent Learning</title>
      <link href="2021/03/26/Non-Autoregressive-Image-Captioning-with-Counterfactuals-Critical-Multi-Agent-Learning/"/>
      <url>2021/03/26/Non-Autoregressive-Image-Captioning-with-Counterfactuals-Critical-Multi-Agent-Learning/</url>
      
        <content type="html"><![CDATA[<h2 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h2><p>图像描述任务</p><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p>目前的图像描述任务采用自回归的方式来生成序列，如下图中上部分表示，下一个单词的生成需要依据先前生成的单词。</p><p>这样的方式会导致：high inference latency, which is sometimes unaffordable for real-time industrial applications</p><p><img src="https://i.loli.net/2021/03/26/9biELph3vmCSzY1.png" alt="image-20210326112017198" style="zoom:50%;"></p><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><ul><li>本文提出使用非自回归的方式来生成，如上图中的下部分。但是，直接采用最朴素的方式来自回归的生成句子，并用交叉熵损失来优化，会导致生成的句子中单词之间的交互较少，几乎没有。进而导致生成的句子一致性较差。</li><li>因此，本文提出将 非自回归图像描述任务作为一个多智能体强化学习系统来学习。target sequence 中的位置被看做是智能体，智能体之间联合起来，一起最大化整个句子的质量。</li><li>具体地<ul><li>以当前非自回归方式生成的序列与GT caption比较得到整个句子的得分，以sentence-level的得分来作为每个智能体的得分(reward)，</li><li>但是这种方式会导致每个智能体的奖惩力度相同，很难学习。因此本文提出了counterfactual baseline。即将该单词去掉，计算剩下句子的得分，作为该agent的reward。</li><li>由于每个agent的action空间是整个词汇表（非常大），因此，仅仅考虑最大概率的k个action。</li></ul></li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://i.loli.net/2021/03/26/9trOPQS1pekXFGW.png" alt="image-20210326112702445" style="zoom: 67%;"></p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><p>没有细读论文，但是大体上是使用，多智能体强化学习来做非自回归图像描述任务。</p><p>这种以非自回归的方式来做text generation task的出发点就是自回归的方式推理速度慢。</p><p>但是非自回归的方式又会导致句子的一致性较差，因此，需要考虑一些方法来弥补这一不足。考虑的方式即为论文的创新点。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Human-like Controllable Image Captioning with Verb-specific Semantic Roles</title>
      <link href="2021/03/25/Human-like-Controllable-Image-Captioning-with-Verb-specific-Semantic-Roles/"/>
      <url>2021/03/25/Human-like-Controllable-Image-Captioning-with-Verb-specific-Semantic-Roles/</url>
      
        <content type="html"><![CDATA[<h2 id="本文研究的内容"><a href="#本文研究的内容" class="headerlink" title="本文研究的内容"></a>本文研究的内容</h2><p>Controllable Image Captioning</p><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p>目前存在的objective control signals忽视了理想控制信号的两个必不可少的特征：</p><ul><li>Event-compatible</li></ul>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> image captioning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
            <tag> image captioning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[调研] non-autoregressive neural machine translation</title>
      <link href="2021/03/24/%E8%B0%83%E7%A0%94-non-autoregressive-neural-machine-translation/"/>
      <url>2021/03/24/%E8%B0%83%E7%A0%94-non-autoregressive-neural-machine-translation/</url>
      
        <content type="html"><![CDATA[<p>本篇对基于非自回归的机器翻译任务进行调研</p><h2 id="First-paper"><a href="#First-paper" class="headerlink" title="First paper"></a><strong style="color:blue;">First paper</strong></h2><p>Gu, J.; Bradbury, J.; Xiong, C.; Li, V. O.; and Socher, R. 2018. <strong>Non-autoregressive neural machine translation.</strong> In ICLR.</p><h2 id="Enhancing-the-decoder-inputs"><a href="#Enhancing-the-decoder-inputs" class="headerlink" title="Enhancing the decoder inputs"></a><strong style="color:blue;">Enhancing the decoder inputs</strong></h2><p>Bingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang Lin, and Xu Sun. 2019. <strong>Imitation learning for nonautoregressive neural machine translation.</strong> In ACL.</p><p>Wang, Y.; Tian, F.; He, D.; Qin, T.; Zhai, C.; and Liu, T.-Y. 2019b. <strong>Non-autoregressive machine translation with auxiliary regularization.</strong> In AAAI.</p><p>Guo, J.; Tan, X.; He, D.; Qin, T.; Xu, L.; and Liu, T.-Y. 2019. <strong>Non-autoregressive neural machine translation with enhanced decoder input.</strong> In AAAI.</p><p>Lee, J.; Mansimov, E.; and Cho, K. 2018. <strong>Deterministic nonautoregressive neural sequence modeling by iterative refinement.</strong> In EMNLP.</p><h2 id="Modeling-the-dependencies-among-target-outputs"><a href="#Modeling-the-dependencies-among-target-outputs" class="headerlink" title="Modeling the dependencies among target outputs"></a><strong style="color:blue;">Modeling the dependencies among target outputs</strong></h2><p>Ghazvininejad, M.; Levy, O.; Liu, Y.; and Zettlemoyer, L. 2019. <strong>Mask-predict: Parallel decoding of conditional masked language models.</strong> In EMNLP-IJCNLP.</p><p>Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, and Lei Li. 2020. <strong>Glancing transformer for non-autoregressive neural machine translation.</strong> arXiv preprint arXiv:2008.07905.</p><p>Gu, J.;Wang, C.; and Zhao, J. 2019. <strong>Levenshtein transformer.</strong> In Advances in NIPS.</p><p>Mansimov, E.; Wang, A.; and Cho, K. 2019. <strong>A generalized framework of sequence generation with application to undirected sequence models.</strong> arXiv preprint arXiv:1905.12790 .</p><ul><li><p><strong style="color:blue;">model the target-side dependencies in the latent space</strong></p><p>Non-Autoregressive Translation by Learning Target Categorical Codes. NAACL-2021</p><p><a href="https://mp.weixin.qq.com/s/qDN1ROXjkVI6wdTHa18YlA" target="_blank" rel="noopener">NAACL2021论文：基于隐式类别建模的非自回归式翻译</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Retrieve Fast, Rerank] Smart Cooperative and Joint Approaches for Improved Cross-Modal Retrieval</title>
      <link href="2021/03/23/Retrieve-Fast-Rerank-Smart-Cooperative-and-Joint-Approaches-for-Improved-Cross-Modal-Retrieval/"/>
      <url>2021/03/23/Retrieve-Fast-Rerank-Smart-Cooperative-and-Joint-Approaches-for-Improved-Cross-Modal-Retrieval/</url>
      
        <content type="html"><![CDATA[<h2 id="现存的问题"><a href="#现存的问题" class="headerlink" title="现存的问题"></a>现存的问题</h2><ul><li>本文主要研究基于预训练跨模态模型<ul><li>pretrained from scratch and less scalable</li><li>huge retrieval latency</li></ul></li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><ul><li>为了同时提高模型<strong>性能</strong>与<strong>效率</strong>。提出了一个新颖的微调框架，可以将任意预训练的 text-image 多模态模型转化为一个有效的检索模型。</li><li>该框架基于对检索和重新排序进行协作，该方法结合了：（1）双胞胎网络分别对语料库的所有项目进行编码，从而实现有效的初始检索； 2）交叉编码器组件，用于对检索到的小项目集进行更细微（即更智能）的排名。 </li><li>我们还建议通过共享权重共同微调这两个分量，从而产生一个参数更有效的模型。</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Large-Scale Zero-Shot Image Classification from Rich and Diverse Textual Descriptions</title>
      <link href="2021/03/22/Large-Scale-Zero-Shot-Image-Classification-from-Rich-and-Diverse-Textual-Descriptions/"/>
      <url>2021/03/22/Large-Scale-Zero-Shot-Image-Classification-from-Rich-and-Diverse-Textual-Descriptions/</url>
      
        <content type="html"><![CDATA[<h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><ul><li><p>【目前常用的benchmark都比较小，且常常使用手工标注的类别属性作为补充数据。但是这种类型的补充数据，由于是手工标注的因此很难扩展到大规模数据集上（eg: ImageNet）。因此，目前在ImageNet 这样的大规模数据集上的零样本学习性能比较差】</p><p>ZSL benchmarks mostly cover either a very <strong>small</strong> or narrow set of classes, where <strong>human-made class attributes</strong> are often used as auxiliary data. Unfortunately, on ImageNet, where such attributes are not available, the performance is still very low.</p></li><li><p>【小规模数据集提供的类别有限，有可能训练集，测试集都是动物类别，无法评估模型在一个新颖类别上的性能，比如汽车。因此对于零样本学习任务，有必要发展一个大规模的数据集】</p><p>The large-scale setup enables us to study the main challenges of a more realistic and practical zero-shot image classification scenario and study the generalization of models to novel groups of classes (e.g., animal species in general), not only  individual classes (e.g., specific animal species).</p></li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><ul><li><p>不对算法进行改进，而是研究 补充数据类型（auxiliary data type）对性能的影响。为 ImageNet class 收集对应的 wikipedia article，作为text descriptions。本文是第一个在大规模数据集上使用文本描述的。</p></li><li><p>在 ImageNet mp500 测试集上，使用本文提供的wikipedia文本描述作为补充数据，性能上取得了很大的提高，比以往的方法都好。</p></li><li>以前的小数据集受到类别有限的限制，无法评估在新颖类别上的泛化性。在本文提出的数据集上，证明了当前的ZSL model 的泛化性是比较差的。</li><li></li></ul>]]></content>
      
      
      <categories>
          
          <category> Image Classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Image Classification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[MDMMT] Multidomain Multimodal Transformer for Video Retrieval</title>
      <link href="2021/03/22/MDMMT-Multidomain-Multimodal-Transformer-for-Video-Retrieval/"/>
      <url>2021/03/22/MDMMT-Multidomain-Multimodal-Transformer-for-Video-Retrieval/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>How Many Data Points is a PromptWorth?</title>
      <link href="2021/03/20/How-Many-Data-Points-is-a-PromptWorth/"/>
      <url>2021/03/20/How-Many-Data-Points-is-a-PromptWorth/</url>
      
        <content type="html"><![CDATA[<p>转载：<a href="https://mp.weixin.qq.com/s/vX6o4lJKP4ajVfGSw8-tqg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/vX6o4lJKP4ajVfGSw8-tqg</a></p><blockquote><p>今天介绍的是一篇 NAACL’21 新鲜出炉的工作！NAACL 上周四出的结果，这篇工作本周一上传 arxiv，周二被王苏小哥哥发现，周三拜读了一下，今天就来和大家分享啦！！</p></blockquote><p>给大家提个问题：如果训练样本只有几百条，这时候我们该怎么办呢？</p><p>传统的 RNN 在这个样本大小下很难被训练好，自然地，我们会想到使用预训练模型，在其基础上进行 finetune。具体来讲，就是将预训练模型作为模型的底层，在上面添加与当前任务特点相关的网络结构。这样就引入了预训练的知识，对当前任务能产生很大的帮助。</p><p><img src="https://i.loli.net/2021/03/21/QodMJ9GWgv6fcUy.png" alt="微信截图_20210321123106" style="zoom:33%;"></p><p>除了预训练的知识，是不是还有其他的信息我们没有用上呢？近年来，越来越多的人在使用另一种 finetune 方法，即<strong>结合具体场景，设计新的 finetune 任务形式，从而将与当前任务相关的提示信息（prompt）引入模型</strong>。我们大名鼎鼎的 GPT 系列就是这么干的。比如我们拿 GPT3 做 QA 的 finetune，直接喂给他一串“<em>Question：问题内容 Answer：</em>”，剩下的答案部分就让 GPT3 自己填完。</p><p><img src="https://i.loli.net/2021/03/21/s8DHwgmNJY7Ryvk.png" alt="image-20210321123155512" style="zoom: 33%;"></p><p>这类 finetune 技巧虽然陆续被使用，但并没有人论证：<strong>这种做法相比于传统的 finetune 方法，真的能带来提升吗</strong>？如果答案是肯定的，<strong>那么究竟能提升多少呢（能否量化这种提升）？</strong></p><p>今天这篇来自 Huggingface 的文章就填补了上述两个问题的答案。他们通过大量实验证明：<strong>引入提示信息和多标注几百条数据带来的性能提升是相当的</strong>！所以，下次老板只给少量样本，就要你 finetune 模型——不要慌！我们今天又多学了一个 trick！</p><p><strong>论文题目</strong>:<br><strong><em>How Many Data Points is a Prompt Worth?</em></strong></p><p><strong>论文链接</strong>:<br><em><a href="https://arxiv.org/abs/2103.08493" target="_blank" rel="noopener">https://arxiv.org/abs/2103.08493</a></em></p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>前文提到，这一类 finetune 是将任务对应的输入改写成新的完形填空格式，让模型预测 <mask> 部分的词，作为抽取任务的答案或者生成任务的结果。这种方法不需要改变模型结构、没有额外的参数，简直不要太方便！</mask></p><h3 id="引入描述集合"><a href="#引入描述集合" class="headerlink" title="引入描述集合"></a>引入描述集合</h3><p>本文对这类方法进行了进一步简化：不要求 <mask> 处生成任意的文本，而是只需要完成类似于有选项的完形填空任务。这里的选项是固定的几个词，我们称之为描述集合（verbalizer），不同任务会有不同的描述集合。</mask></p><p>比如，对于判断题的阅读理解任务，就可以将阅读文本、问题和 <mask> 拼接，让预训练模型直接预测 <mask> 属于描述集合 {yes, no} 中的哪一种描述：</mask></mask></p><blockquote><p>小明天天码代码码到天明 [SEP] <strong>小明有女朋友吗？</strong> <mask></mask></p></blockquote><p>其中前半部分是阅读文本，后面<strong>加粗</strong>的部分是问题。模型只需要判断 <mask> 属于描述集合 {yes, no} 中的哪一种。</mask></p><p>可能读到这里，大家会疑惑：直接拼起来搞一个 True / False 的二分类不就好了嘛，何必让模型填空呢？嘿嘿，这恰好是作者的用意：通过让模型填空，<strong>模型可以习得描述集合中标签文本的语义信息</strong>。</p><h3 id="引入提示信息"><a href="#引入提示信息" class="headerlink" title="引入提示信息"></a>引入提示信息</h3><p>直接拼接是最朴素的，但这能让模型知道自己在做什么任务嘛？为此，作者引入了<strong>提示信息</strong>（prompt）。</p><p>还是判断题的阅读理解任务，对文章 和问题 ，作者将他们与一些固定的词进行整合，以此输入模型，让模型预测 <mask> 。作者提出了三种整合方式：</mask></p><p><img src="https://i.loli.net/2021/03/21/QEPwMG7bfI2UzNH.png" alt="image-20210321123632246" style="zoom: 33%;"></p><p>没错，就是这么简单！这些固定的词作为提示信息，让模型了解当前在做的任务；同时，提示词文本的含义也对于模型的理解产生了一定的帮助。</p><p>除了单选阅读理解，这篇文章还关注了文本蕴含、多选阅读理解、指代销歧等共六个任务。对于不同的任务，有不同的提示信息与输入格式：</p><p>对于文本蕴含任务，可以将前提 (premise, ) 与假设 (hyphothesis, ) 通过提示信息整合，作者提出了两种整合方式：</p><p><img src="https://i.loli.net/2021/03/21/k1ul7icFX69IKnL.png" alt="image-20210321123241288" style="zoom: 33%;"></p><p>这样就只需要让模型预测 <mask> 属于描述集合 {yes, no, maybe} 中的哪一种，以此判断前提能否支撑假设。</mask></p><p>对于指代销歧任务，可以将句子 、带标记的介词 与名词 通过提示信息整合：</p><p><img src="https://i.loli.net/2021/03/21/sRZIGzn2Pg76mpd.png" alt="image-20210321123253166" style="zoom: 33%;"></p><p>这样就只需要让模型预测 <mask> ，以此判断介词是否指代名词。这里的描述集合是不受限制的，即让模型在 <mask> 处预测指代的名词 。</mask></mask></p><p>其他任务也采用类似的整合方式，感兴趣可以参考原文～</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者发现，这种使用提示信息的整合方式，在低资源的情况下对模型性能有非常大的提升！</p><p>比如在阅读理解任务的 BoolQ 数据集上，作者将使用提示信息整合的 finetune 方法与增加一层分类层的 finetune 方法进行了对比。下图是在使用不同数量的样本训练时，模型准确率的对比。</p><p><img src="https://i.loli.net/2021/03/21/31cqsF5Q7VoSeLX.png" alt="image-20210321123431996" style="zoom:50%;"></p><p>可以发现，在数据量比较小的时候，使用提示信息整合的 finetune 方法（黄色）比增加一层分类层的 finetune 方法（紫色）有更好的表现。</p><p>在某些任务上，这种表现的提升是惊人的：</p><p><img src="https://i.loli.net/2021/03/21/WCo9uUakm1rOH8N.png" alt="image-20210321123504144" style="zoom:50%;"></p><p>这是在指代销歧任务的 WSC 数据集上的实验结果。在水平方向看，<strong>仅使用 25 个样本，就达到传统 fintune 方法使用 300 个样本才能达到的效果！</strong></p><p>此外，作者还进行了一系列的消融实验，得到一些有意思的结论：</p><ol><li>模型通过预测 <mask> 属于描述集合中的哪种，以此完成任务。如果将这里改为不带语义的单纯的分类，性能也会有所下降。</mask></li><li>作者为每个任务都提供了多种整合提示信息的方式，但是发现，不同方式的区别对性能影响甚微。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇文章对基于提示信息的 finetune 方法在进行了大量实验，证明了这类方法在低资源的情况下性能大幅优于传统方法。这种 finetune 的思路应该是可以应用于各类 NLP 下游任务的。尤其是低资源场景下，应该会非常有帮助。如果老板真的只给几百条数据让训练模型，这样的方法说不定就有奇效！</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> few-shot learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> few-shot learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[It’s Not Just Size That Matters] Small Language Models Are Also Few-Shot Learners</title>
      <link href="2021/03/20/It%E2%80%99s-Not-Just-Size-That-Matters-Small-Language-Models-Are-Also-Few-Shot-Learners/"/>
      <url>2021/03/20/It%E2%80%99s-Not-Just-Size-That-Matters-Small-Language-Models-Are-Also-Few-Shot-Learners/</url>
      
        <content type="html"><![CDATA[<p>转载：<a href="https://www.sohu.com/a/422484297_500659" target="_blank" rel="noopener">https://www.sohu.com/a/422484297_500659</a></p><p>显然，这标题对标的就是 GPT-3，于是笔者饶有兴趣地点进去看看是谁这么有勇气挑战 GPT-3，又是怎样的小模型能挑战 GPT-3？经过阅读，原来作者提出通过适当的构造， <strong>用 BERT 的 MLM 模型</strong>也可以做小样本学习，看完之后颇有一种“原来还可以这样做”的恍然大悟感。在此与大家分享一下。</p><h2 id="冉冉升起的MLM"><a href="#冉冉升起的MLM" class="headerlink" title="冉冉升起的MLM"></a><strong>冉冉升起的MLM</strong></h2><p>MLM，全称“Masked Language Model”，可以翻译为“掩码语言模型”，实际上就是一个完形填空任务，随机 Mask 掉文本中的某些字词，然后要模型去预测被 Mask 的字词，示意图如下：</p><p><img src="https://i.loli.net/2021/03/20/izGapk4S6ZRqgFI.png" alt="img" style="zoom:50%;"></p><p>▲ BERT的MLM模型简单示意图</p><p>其中被 Mask 掉的部分，可以是直接随机选择的 Token，也可以是随机选择连续的能组成一整个词的 Token，后者称为 WWM（Whole Word Masking）。</p><p>开始，MLM 仅被视为 BERT 的一个预训练任务，训练完了就可以扔掉的那种，因此有一些开源的模型干脆没保留 MLM 部分的权重，比如 brightmart 版 [3] 和 clue 版 [4] 的 RoBERTa，而哈工大开源的 RoBERTa-wwm-ext-large [5]则不知道出于什么原因随机初始化了 MLM 部分的权重，因此如果要复现本文后面的结果，这些版本是不可取的。</p><p>然而，随着研究的深入，研究人员发现不止 BERT 的 Encoder 很有用，预训练用的 MLM 本身也很有用。</p><p>比如论文 <strong>BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model</strong>[6]指出 MLM 可以作为一般的生成模型用，论文 <strong>Spelling Error Correction with Soft-Masked BERT</strong>[7] 则将 MLM 用于文本纠错。</p><p>笔者之前在 的实验也表明 MLM 的预训练权重也可以当作 UniLM 来用做 Seq2Seq 任务，还有一文将 MLM 的思想用于无监督分词和句法分析了。可以说 MLM 已经是大放异彩了。</p><h2 id="将任务转成完形填空"><a href="#将任务转成完形填空" class="headerlink" title="将任务转成完形填空"></a><strong>将任务转成完形填空</strong></h2><p>在本文里，我们再学习 MLM 的一个精彩应用：用于小样本学习或半监督学习，某些场景下甚至能做到零样本学习。</p><p>怎么将我们要做的任务跟 MLM 结合起来呢？很简单， <strong>给任务一个文本描述，然后转换为完形填空问题</strong>即可。举个例子，假如给定句子“这趟北京之旅我感觉很不错。”，那么我们补充个描述，构建如下的完形填空：</p><blockquote><p> <strong>__</strong>满意。这趟北京之旅我感觉很不错。</p></blockquote><p>进一步地，我们限制空位处只能填一个“很”或“不”，问题就很清晰了，就是要我们根据上下文一致性判断是否满意，如果“很”的概率大于“不”的概率，说明是正面情感倾向，否则就是负面的，这样我们就将<strong>情感分类问题</strong>转换为一个完形填空问题了，它可以用 MLM 模型给出预测结果，而 MLM 模型的训练可以不需要监督数据，因此理论上这能够实现零样本学习了。</p><p><strong style="color:blue;">多分类问题</strong>也可以做类似转换，比如<strong>新闻主题分类</strong>，输入句子为“八个月了，终于又能在赛场上看到女排姑娘们了。”，那么就可以构建：</p><blockquote><p> 下面播报一则<strong>__</strong>新闻。八个月了，终于又能在赛场上看到女排姑娘们了。</p></blockquote><p>这样我们就将新闻主题分类也转换为完形填空问题了，一个好的 MLM 模型应当能预测出“体育”二字来。</p><p>还有一些<strong style="color:blue;">简单的推理任务</strong>也可以做这样的转换，常见的是给定两个句子<strong>，判断这两个句子是否相容</strong>，比如“我去了北京”跟“我去了上海”就是矛盾的，“我去了北京”跟“我在天安门广场”是相容的，常见的做法就是将两个句子拼接起来输入到模型做，作为一个二分类任务。如果要转换为完形填空，那该怎么构造呢？一种比较自然的构建方式是：</p><blockquote><p>我去了北京？<strong>__</strong>，我去了上海。</p><p>我去了北京？<strong>__</strong>，我在天安门广场。</p><p>其中空位之处的候选词为 是 的 不 是 。</p></blockquote><h2 id="Pattern-Exploiting-Training"><a href="#Pattern-Exploiting-Training" class="headerlink" title="Pattern-Exploiting Training"></a><strong>Pattern-Exploiting Training</strong></h2><p>读到这里，读者应该不难发现其中的规律了，就是给输入的文本增加一个前缀或者后缀描述，并且 Mask 掉某些 Token，转换为完形填空问题，这样的转换在原论文中称为 <strong>Pattern</strong>，这个转换要尽可能与原来的句子组成一句自然的话，不能过于生硬，因为预训练的 MLM 模型就是在自然语言上进行的。</p><p>显然同一个问题可以有很多不同的 Pattern，比如情感分类的例子，描述可以放最后，变成“这趟北京之旅我感觉很不错。<strong><strong>满意。”；也可以多加几个字，比如“觉得如何？</strong></strong>满意。这趟北京之旅我感觉很不错。”。</p><p>然后，我们需要构建预测 Token 的候选空间，并且建立 Token 到实际类别的映射，这在原论文中称为 <strong>Verbalizer</strong>，比如情感分类的例子，我们的候选空间是 很 不 ，映射关系是 很 正 面 不 负 面 ，候选空间与实际类别之间不一定是一一映射，比如我们还可以加入“挺”、“太”、“难”字，并且认为 很 挺 太 正 面 以 及 不 难 负 面 ，等等。</p><p>不难理解，不少 NLP 任务都有可能进行这种转换，但显然这种转换一般只适用于 <strong>候选空间有限</strong>的任务，说白了就是只用来做 <strong>选择题</strong>，常见任务的就是 <strong>文本分类</strong>。</p><p>刚才说了，同一个任务可以有多种不同的 Pattern，原论文是这样处理的：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1. </span>对于每种 Pattern，单独用训练集 Finetune一个 MLM 模型出来；</span><br><span class="line"><span class="bullet">2. </span>然后将不同 Pattern对应的模型进行集成，得到融合模型；</span><br><span class="line"><span class="bullet">3. </span>用融合模型预测未标注数据的伪标签；</span><br><span class="line"><span class="bullet">4. </span>用伪标签数据 Finetune 一个常规的（非 MLM 的）模型。</span><br></pre></td></tr></table></figure><p>具体的集成方式大家自己看论文就行，这不是重点。这种训练模式被称为 <strong>Pattern-Exploiting Training（PET）</strong>，它首先出现在论文 <strong>Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</strong>。</p><p><strong style="color:blue;">yaya: 与 这篇论文思想很类似：How Many Data Points is a PromptWorth? （arXiv: 2103.08493v1）</strong></p><p>本文要介绍的这篇论文则进一步肯定和完善了 Pattern-Exploiting Training 的价值和结果，并整合了多任务学习，使得它在 SuperGLUE 榜单上的小样本学习效果超过了 GPT3。两篇论文的作者是相同的，是一脉相承的作品。</p><p><img src="https://i.loli.net/2021/03/20/dOMZNLsKykIDioQ.png" alt="img"></p><p>▲ PET在SuperGLUE上的小样本学习的结果</p><p>不过要吐槽一个点是，上图中 PET 的 223M 参数，所用的模型是 ALBERT-xxlarge-v2，事实上称 ALBERT 为“小模型”是一种很耍流氓的行为，因为它前向计算的速度并没有得到任何提升。ALBERT-xxlarge 共有 12 层，层与层之间参数是共享的，就前向计算而言，它应该等价于约 2700M（12 倍）参数的 GPT 才对。</p><h2 id="中文实践，检验效果"><a href="#中文实践，检验效果" class="headerlink" title="中文实践，检验效果"></a><strong>中文实践，检验效果</strong></h2><p>要真正确认一个方法或模型的价值，看论文的实验表格是不够的，论文给出的实验结果谁都不好说能否复现，其次就算英文上能复现也不代表中文上有价值，因此最实际的还是亲自动手做实验验证。下面是笔者的实验代码，供读者参考：</p><p>Github 地址：</p><p><a href="https://github.com/bojone/Pattern-Exploiting-Training" target="_blank" rel="noopener">https://github.com/bojone/Pattern-Exploiting-Training</a></p><p>我们将从以下几个角度来探讨 PET 的可行性：</p><p>\1. 直接利用现成的 MLM 模型效果如何？ <strong>（零样本学习1）</strong></p><p>\2. 用“大量无标签数据”微调现成的 MLM 模型效果如何？ <strong>（零样本学习2）</strong></p><p>\3. 用“小量标签数据”微调现成的 MLM 模型效果如何？ <strong>（小样本学习）</strong></p><p>\4. 用“小量标签数据+大量无标签数据”微调现成的MLM模型效果如何？ <strong>（半监督学习）</strong></p><p>下面主要给出 <strong>情感二分类</strong>的实验结果。另外还有一个新闻主题的多分类，代码也放到 Github 了，其结果是类似的，就不重复陈述了。</p><h3 id="4-1-零样本学习1"><a href="#4-1-零样本学习1" class="headerlink" title="4.1 零样本学习1"></a><strong>4.1 零样本学习1</strong></h3><p>这里主要探索的是给输入文本补上对应的 Pattern 后，直接基于现成的 MLM 模型进行预测，预测的准确率。由于构建模型的整个过程都不涉及到标签数据监督训练，因此这算是一种“零样本学习”。我们需要比较的是不同 Pattern、不同 MLM 模型上的效果：</p><p>下面是实验的几个 Pattern，其中空位处候选词语都为“很”和“不”：</p><p>P1：____满意。这趟北京之旅我感觉很不错。</p><p>P2：这趟北京之旅我感觉很不错。____满意。</p><p>P3：____好。这趟北京之旅我感觉很不错。</p><p>P4：____理想。这趟北京之旅我感觉很不错。</p><p>P5：感觉如何？____满意。这趟北京之旅我感觉很不错。</p><p>至于 MLM 模型，则是下面几个：</p><p>M1：Google 开源的中文版 BERT Base：</p><p><a href="https://github.com/google-research/bert" target="_blank" rel="noopener">https://github.com/google-research/bert</a></p><p>M2：哈工大开源的 RoBERTa-wwm-ext Base：</p><p><a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">https://github.com/ymcui/Chinese-BERT-wwm</a></p><p>M3：腾讯 UER 开源的 BERT Base：</p><p><a href="https://share.weiyun.com/5QOzPqq" target="_blank" rel="noopener">https://share.weiyun.com/5QOzPqq</a></p><p>M4：腾讯 UER 开源的 BERT Large：</p><p><a href="https://share.weiyun.com/5G90sMJ" target="_blank" rel="noopener">https://share.weiyun.com/5G90sMJ</a></p><p>实验结果如下表（验证集/测试集）：</p><p><img src="https://i.loli.net/2021/03/20/8Yg2qH5CXoFPkcx.png" alt="img"></p><p>可以观察到，不同的 Pattern、不同的预训练模型之间还是有一定的差异的，整体而言 Large 版本的效果要明显好于 Base 版本的模型，说明像 GPT 到 GPT2 再到 GPT3 一样，还是把模型做得更大会更好。</p><p>此外，这还有可能说明实际上 MLM 还没有被充分训练好，或许是因为 BERT 这种 Mask 掉一部分的训练方式过于低效了，可能用 修改 Transformer 结构，设计一个更快更好的 MLM 模型 一文提到的改进版 MLM 会更好。</p><h3 id="4-2-零样本学习2"><a href="#4-2-零样本学习2" class="headerlink" title="4.2 零样本学习2"></a><strong>4.2 零样本学习2</strong></h3><p>看完上述结果，读者可能会想到：如果我用领域内的数据继续预训练 MLM 模型，那么能不能提升效果呢？答案是：能！下面是我们的实验结果，算力有限，我们只在 RoBERTa-wwm-ext（上述的 M2，继续预训练后的模型我们称为 M2+ 无监督）的基础上做了比较：</p><p><img src="https://i.loli.net/2021/03/20/Y8CdHbW1cjqByrD.png" alt="img"></p><p>要注意的是，这里我们只是用领域内的数据继续做 MLM 训练，这个过程是无监督的，也不需要标注信号，因此也算是“零样本学习”。同时，从到目前为止的结果我们可以看出，给输入本文加入“前缀”的效果比“后缀”更有优势一些。</p><h3 id="4-3-小样本学习"><a href="#4-3-小样本学习" class="headerlink" title="4.3 小样本学习"></a><strong>4.3 小样本学习</strong></h3><p>刚才我们讨论了无标签数据继续预训练 MLM 的提升，如果回到 PET 的目标场景，直接用小量的标签数据配合特定的 Pattern 训练 MLM 又如何呢？</p><p>这也就是真正的“小样本学习”训练了，这里我们保留约 200 个标注样本，构造样本的时候，我们先给每个句子补上 Pattern，除了 Pattern 自带的 Mask 位置之外，我们还随机 Mask 其他一部分，以增强对模型的正则。最终实验结果如下：</p><p><img src="https://i.loli.net/2021/03/20/qVFgDbyLQrXpZUt.png" alt="img"></p><p>结论就是除了“后缀式”的 P2 之外，其它结果都差不多，这进一步说明了“前缀式”的 Pattern 会比“后缀式”更有竞争力一些。在效果上，直接用同样的数据用常规的方法去微调一个 BERT 模型，大概的结果是 88.93 左右，所以基于 “MLP+Pattern” 的小样本学习方法可能带来轻微的性能提升。</p><h3 id="4-4-半监督学习"><a href="#4-4-半监督学习" class="headerlink" title="4.4 半监督学习"></a><strong>4.4 半监督学习</strong></h3><p>无监督的零样本学习和有监督的小样本学习都说完了，自然就轮到把标注数据和非标注数据都结合起来的“半监督学习”了。还是同样的任务，标注数据和非标注数据的比例大约是 1:99，标注数据带 Pattern，非标注数据不带 Pattern，大家都 Mask 掉一部分 Token 进行 MLM 预训练，最终测出来的效果如下：</p><p><img src="https://i.loli.net/2021/03/20/4QKcUYDtgAmeGqM.png" alt="img"></p><p>还是同样的，“后缀”明显比“前缀”差，“前缀”的效果差不多。具体效果上，则是肯定了额外的无标注数据也是有作用的。</p><p>直觉上来看，“前缀”比“后缀”要好，大体上是因为“前缀”的 Mask 位置比较固定，微弱的监督信号得以叠加增强？但这也不能解释为什么零样本学习的情况下也是“前缀”更好，估计还跟模型的学习难度有关系，可能句子前面部分的规律更加明显，相对来说更加容易学一些，所以前面部分就学习得更加充分？这一切都还只是猜测。</p><h3 id="4-5-汇总与结论"><a href="#4-5-汇总与结论" class="headerlink" title="4.5 汇总与结论"></a><strong>4.5 汇总与结论</strong></h3><p>将上述结果汇总如下：</p><p><img src="https://i.loli.net/2021/03/20/cpGqLZ2twR5z4di.png" alt="img"></p><p>读者还可以对比我们之前在文章 泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练 中用虚拟对抗训练（VAT）做半监督学习的结果，可以看到不管是零样本学习、小样本学习还是半监督学习，基于 MLM 模型的方式都能媲美基于 VAT 的半监督学习的结果。</p><p>我们在做短新闻多分类实验时的结果也是相似的。因此，这说明了 MLM 模型确实也可以作为一个优秀的零样本/小样本/半监督学习器来使用。</p><p>当然，基于 MLM 模型的缺点还是有的，比如 MLM 所使用的独立假设限制了它对更长文本的预测能力（说白了空位处的文字不能太长），以及无法预测不定长的答案也约束了它的场景（所以当前只能用于做选择题）。我们期待有更强的 MLM 模型出现，那时候就有可能在所有任务上都能与 GPT3 一较高下了。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文介绍了 BERT 的 MLM 模型的一个新颖应用：配合特定的描述将任务转化为完形填空，利用 MLM 模型做零样本学习、小样本学习和半监督学习。</p><p>在原论文的 SuperGLUE 实验里边，它能达到媲美 GPT3 的效果，而笔者也在中文任务上做了一些实验，进一步肯定了该思路的有效性。整个思路颇为别致，给人一种“原来还可以这样做”的恍然大悟感，推荐大家学习一下。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] <a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">https://arxiv.org/abs/2005.14165</a></p><p>[2] <a href="https://arxiv.org/abs/2009.07118" target="_blank" rel="noopener">https://arxiv.org/abs/2009.07118</a></p><p>[3] <a href="https://github.com/brightmart/roberta_zh" target="_blank" rel="noopener">https://github.com/brightmart/roberta_zh</a></p><p>[4] <a href="https://github.com/CLUEbenchmark/CLUEPretrainedModels" target="_blank" rel="noopener">https://github.com/CLUEbenchmark/CLUEPretrainedModels</a></p><p>[5] <a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">https://github.com/ymcui/Chinese-BERT-wwm</a></p><p>[6] <a href="https://arxiv.org/abs/1902.04094" target="_blank" rel="noopener">https://arxiv.org/abs/1902.04094</a></p><p>[7] <a href="https://kexue.fm/archives/7661" target="_blank" rel="noopener">https://kexue.fm/archives/7661</a></p><p>[8] <a href="https://arxiv.org/abs/2001.07676" target="_blank" rel="noopener">https://arxiv.org/abs/2001.07676</a></p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><p>不懂的问题：</p><ul><li><p>mask 一个 Span, 多个空位然后逐词预测？？</p></li><li><p>在 [MASK] 位置 预测空间是多大？整个vocabulary ??</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> few-shot </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> few-shot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[All NLP Tasks Are Generation Tasks] A General Pretraining Framework</title>
      <link href="2021/03/20/All-NLP-Tasks-Are-Generation-Tasks-A-General-Pretraining-Framework/"/>
      <url>2021/03/20/All-NLP-Tasks-Are-Generation-Tasks-A-General-Pretraining-Framework/</url>
      
        <content type="html"><![CDATA[<h2 id="yaya-总结"><a href="#yaya-总结" class="headerlink" title="yaya 总结"></a>yaya 总结</h2><ul><li><p>模型结构上</p><p><strong style="color:red;">GLM 采用的是Transformer Model 中decoder 结构，而BERT采用的是 encoder 结构</strong></p></li><li><p>预训练任务上：<br>提出了两个 multi short span recover (benefit for NLU) and  a single longer span recover (benefit for NLG) 的预训练任务。</p><p>与这篇做 text filling 论文很相似：Enabling Language Models to Fill in the Blanks</p></li><li><p>下游任务的微调：</p><p>受到[1] [2] 的启发， 将分类任务转化为文本生成的填空任务。</p></li></ul><h2 id="1-存在的问题"><a href="#1-存在的问题" class="headerlink" title="1. 存在的问题"></a>1. 存在的问题</h2><p>目前基于预训练 的语言模型大致分为三类：</p><ul><li>autoregressive models (e.g.,GPT) 擅长长文本生成</li><li>autoencoding models (e.g., BERT) 擅长理解型任务，分类任务</li><li>encoder-decoder models (e.g., T5) 擅长基于条件的文本生成任务，比如 text summarize</li></ul><p>但是目前还未存在一个预训练框架可以在这三种任务上同时表现出优异的性能。这给模型的开发和选择带来了不便。</p><p>下表总结了不同的预训练框架可以处理的任务：</p><p><img src="https://i.loli.net/2021/03/20/ldKAem71a86xBcq.png" alt="image-20210320100920079" style="zoom: 25%;"></p><p>先前的工作试图通过多任务学习将各自的 objective 结合起来，从而统一不同的框架。但是，自回归和自编码的 objective 在本质上是不同的，简单的结合不能够充分的揭示所有框架的优势。</p><h2 id="2-本文的点"><a href="#2-本文的点" class="headerlink" title="2. 本文的点"></a>2. 本文的点</h2><h3 id="2-1-新颖的预训练框架GLM"><a href="#2-1-新颖的预训练框架GLM" class="headerlink" title="2.1 新颖的预训练框架GLM"></a>2.1 新颖的预训练框架GLM</h3><p>本文提出了一个新颖的预训练框架GLM（General Language Model）来解决这个问题。如图1。</p><p><img src="https://i.loli.net/2021/03/20/VrzfCnOFQi27NRU.png" alt="image-20210320103954828" style="zoom: 33%;"></p><ul><li>本文的预训练模型GLM基于autoregressive blank-filling（预训练方案），<strong style="color:red;">遵循自动编码(auto-encoding)的思想，我们从输入文本中随机消除了令牌的连续跨度。并遵循自回归预训练(auto-regressive)的思想训练模型以重建跨度。</strong></li><li>为了在一个框架中同时学习双向和单向的注意力机制，本文将文本分成两部分，未掩码的部分可以互相关注。掩码的部分不可以关注后续的掩码的token。</li><li>本文还提出了一个 2D位置编码技术，来指示inter- and intra- span position information。</li><li></li></ul><p>因此，本文的框架 GLM在预训练过程中，可以同时学习上下文表达和自回归生成。</p><h3 id="2-2-多任务预训练方案"><a href="#2-2-多任务预训练方案" class="headerlink" title="2.2 多任务预训练方案"></a>2.2 多任务预训练方案</h3><p>为了使本文的预训练模型更加适合文本生成任务，本文也研究了一个多任务预训练的设置：（1）采样多个short spans, 目标是重构masked spans，该预训练任务对下游NLU任务有益处（2）采样单个 longer span，目标是回复该单个spans。该预训练任务对下游NLG任务有益处。</p><p>这种多任务预训练方案，在理解型任务，条件生成任务和具有共享参数的语言建模任务方面均有改善。</p><h3 id="2-3-pretrain-finetune-consistency"><a href="#2-3-pretrain-finetune-consistency" class="headerlink" title="2.3 pretrain-finetune consistency"></a>2.3 pretrain-finetune consistency</h3><p>在下游任务微调GLM时，受到以下两篇文章[1] [2] 的启发，构建为blank-filling generation的形式。每个任务都与一个人工制作的完形填空问题相关联，并且该模型可以预测完形填空的答案。例如，情感分类任务被重新构造为一个 <strong>“[SENTENCE]. It’s really __ ”.</strong> 这种格式的填空任务。对于”good” or “bad” 的预测暗示了情感是积极地还是消极地。</p><p>在这种格式下，GLM 在预训练和微调的一致中受益。因为<strong style="color:blue;">预训练和微调都涉及到以给定上下文来生成文本的方式来训练模型</strong>。因此，GLM相比于BERT-like models 更适合下游分类任务。<strong style="color:red;"><strong>yaya：这里的因此，好像不能推断出来</strong></strong></p><h2 id="3-贡献"><a href="#3-贡献" class="headerlink" title="3. 贡献"></a>3. 贡献</h2><p>本文的结构有三个主要的优势：</p><ul><li>在一个预训练模型上，可以在三种任务上都表现的很好。</li><li>由于 <strong style="color:blue;">pretrain-finetune consistency</strong>，在分类任务上，本文提出的模型相比 BERT-like models 性能更加优异。</li><li>可以自然的处理 <strong style="color:red;">variable-length blank filling</strong>，这对很多下游任务是很重要的。</li></ul><h2 id="4-Method"><a href="#4-Method" class="headerlink" title="4. Method"></a>4. Method</h2><h3 id="4-1-Model-Architecture"><a href="#4-1-Model-Architecture" class="headerlink" title="4.1 Model Architecture"></a>4.1 Model Architecture</h3><p>本文提出的结构 GLM 与 BERT很相似。Following Megatron-LM, 对BERT的结构做了两点改动。（1）rearrange the order of layer normalization and the residual connection。（2）replace the feed-forward network for token prediction with a linear layer。</p><h3 id="4-2-Autoregressive-Blank-Infilling"><a href="#4-2-Autoregressive-Blank-Infilling" class="headerlink" title="4.2 Autoregressive Blank Infilling"></a>4.2 Autoregressive Blank Infilling</h3><p>通过优化 autoregressive blank infilling 任务对GLM进行训练。</p><p>给定 an input text $\boldsymbol{x}=\left[x_{1}, \cdots, x_{n}\right]$，多个被采样 text spans {$s_{1},…,s_{m}$} ，每个span $s_{i}$ 是一系列连续的tokens $\left[s_{i, 1}, \cdots, s_{i, l_{i}}\right]$。text spans 的数量和长度取决于预训练目标（将会在下文中被介绍）。</p><p>该模型以自回归的方式从损坏的文本中预测 span 中丢失的 tokens，这意味着在预测 span 中丢失的 tokens，模型可以访问损坏的文本<em>和</em>先前预测的spans。为了充分捕捉不同span之间的相互依存关系，我们随机地排列span的顺序。yaya: 以下公式中 $\boldsymbol{s}_{\boldsymbol{z}_{&lt;i}}$ 被随机排列了，并不是按照其在句子中的顺序。</p><p>预训练目标为：$\max _{\theta} \mathbb{E}_{\boldsymbol{z} \sim Z_{m}}\left[\sum_{i=1}^{m} \log p_{\theta}\left(\boldsymbol{s}_{z_{i}} \mid \boldsymbol{x}_{\text {corrupt }}, \boldsymbol{s}_{\boldsymbol{z}_{&lt;i}}\right)\right]$</p><p>该任务与SpanBERT 的区别在于 <strong>模型不知道span中丢失tokens 的数量</strong>。</p><p>具体来说，我们使用以下技巧实现了自动回归的空白填充任务。输入令牌分为两部分。A部分包含损坏的文本$x_{corrupt}$ ,其中采样的text span 被替换为 [MASK] 令牌。B部分由masked span 中的tokens 组成。A部分中的 tokens 可以 attend to A中的所有tokens ，但不能 attend to B中的任何tokens 。B部分中的tokens 可以 attend to A中的tokens 及其在B中的先行词，但不能attend to B中的任何后续位置。</p><p><strong style="color:red;">与原始Transformer 模型中的decoder 相似</strong>，span中的tokens被补充两个特殊token [START]和 [END]，以这种方式，本文提出的模型可以同时学习（1）一个双向encoder（PART A）和（2）一个单向decoder (PART B)。如下图2所示。</p><p><img src="https://i.loli.net/2021/03/21/i4AEV3J2psDv1aT.png" alt="image-20210321135425429"></p><h4 id="4-2-1-2D-Positional-Encoding"><a href="#4-2-1-2D-Positional-Encoding" class="headerlink" title="4.2.1 2D Positional Encoding"></a>4.2.1 2D Positional Encoding</h4><p>每个 token 都使用两个 position ids 进行编码。</p><p>第一个 position id 代表corrupted text 中的位置。对于B中的token，它是对应的[MASK] token的position。</p><p>第二位置id表示intra-span position。对于A中的令牌，第二个位置ID为0。对于B中的令牌，范围为1到span的长度。</p><p>这两个 position ids  通过两个单独的 embedding table 投影到两个位置向量中，并添加到 input embeddings 中。</p><h3 id="4-3-Pre-Training-Objectives"><a href="#4-3-Pre-Training-Objectives" class="headerlink" title="4.3 Pre-Training Objectives"></a>4.3 Pre-Training Objectives</h3><p>采样：the masked spans make up 15% of the original tokens.</p><p>span leagth:  drawn from a Poisson distribution with $\lambda$= 3</p><p>与其他BERT样式的模型类似，GLM 对 short spans 进行掩码，适用于NLU任务。 但是，我们对单个预训练模型可以同时处理NLU和text generation 感兴趣。</p><p>我们进一步研究了<em>多任务预训练</em>设置，第二个目标：生成更长文本。并与GLM联合优化。具体来说，我们采样了a single span 覆盖原始文本长度的50％–100％。跨度长度是从均匀分布中采样的。以与原始目标相同的方式定义新目标。唯一的区别是只有一个跨度，但跨度更长。</p><h3 id="4-4-Finetuning-GLM"><a href="#4-4-Finetuning-GLM" class="headerlink" title="4.4 Finetuning GLM"></a>4.4 Finetuning GLM</h3><h4 id="NLU-task"><a href="#NLU-task" class="headerlink" title="NLU task"></a>NLU task</h4><p><strong>对于NLU 任务，以前的PLMs 存在预训练-微调目标不一致的问题</strong>， 具体解释如下：</p><p>先前的方法处理NLU任务，通常采用将预训练模型得到的representation送入一个线性分类层中来预测答案。对于token classification: 使用 token representation；对于 sentence classification：使用 [CLS] token representation。但是对于预训练任务采用的是cloze filling task 。这就导致了预训练-微调目标不一致的问题。</p><p>本文中，将NLU中的分类任务定义为 blank filling task。</p><p>给定一个标注样本（$x$, y），经由一个包含了 single mask token 的 pattern。</p><p>将输入文本 $x$ 映射成一个 cloze question $c(x)$ 。情感分类任务被重新构造为一个 <strong>“[SENTENCE]. It’s really [MASK] ”.</strong>  </p><p>标签 $y$ 也映射为填空问题的答案，称为 verbalizer $v(y)$ 。在情感分类任务中消极和积极被映射到单词“好”或“坏”。</p><p>Therefore, the conditional probability of $y$ given $\boldsymbol{x}$ is</p><script type="math/tex; mode=display">p(y \mid \boldsymbol{x})=\frac{p(v(y) \mid c(\boldsymbol{x}))}{\sum_{y^{\prime} \in \mathcal{Y}} p\left(v\left(y^{\prime}\right) \mid c(\boldsymbol{x})\right)}</script><p>where $\mathcal{Y}$ is the label set. Then we can finetune GLM with the cross entropy loss.</p><h4 id="NLG-task"><a href="#NLG-task" class="headerlink" title="NLG task"></a>NLG task</h4><p>对于文本生成任务，可以直接的将GLM作为一个自回归模型来使用。</p><p>给定的上下文构成了输入的A部分，其中有一个[MASK]结尾的令牌。然后，GLM自动在B部分中生成文本。我们可以将预训练的GLM直接应用于无条件生成，也可以在下游的有条件生成任务上微调GLM。</p><p><img src="https://i.loli.net/2021/03/21/k3E8uCchorg5LO9.png" alt="image-20210321151303074" style="zoom:80%;"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] [It’s Not Just Size That Matters] Small Language Models Are Also Few-Shot Learners</p><p>[2] Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</p><h1 id="智源"><a href="#智源" class="headerlink" title="智源"></a>智源</h1><p><img src="https://i.loli.net/2021/05/31/5iPEkVlSW6JOX1R.png" alt="image-20210531141056347" style="zoom:67%;"></p><p><img src="https://i.loli.net/2021/05/31/aeMzL5sy28A6Oj1.png" alt="image-20210531141431038" style="zoom: 67%;"></p><p><img src="https://i.loli.net/2021/05/31/q7yzipXa1RTUoZe.png" alt="image-20210531141522110" style="zoom: 67%;"></p><p><img src="https://i.loli.net/2021/05/31/JcnGQjioAmLXDkg.png" alt="image-20210531141536365" style="zoom: 67%;"></p><p><img src="https://i.loli.net/2021/05/31/PnBrkNesYhzFRMi.png" alt="image-20210531141619653" style="zoom: 67%;"></p><p><img src="https://i.loli.net/2021/05/31/x8XETlYPb2od45A.png" alt="image-20210531141830670" style="zoom:67%;"></p><p><img src="https://i.loli.net/2021/05/31/ABFUgp1xrDv8nRL.png" alt="image-20210531141846681" style="zoom:67%;"></p><p><img src="https://i.loli.net/2021/05/31/tYlRNrnOvUE4BTZ.png" alt="image-20210531142047997" style="zoom:67%;"></p><p><img src="https://i.loli.net/2021/05/31/ywNpmPQY387eD2k.png" alt="image-20210531142137420" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/05/31/RETjgnHZM896ASN.png" alt="image-20210531142213537" style="zoom:67%;"></p><p><img src="https://i.loli.net/2021/05/31/zjlOPUJpVcFHQBk.png" alt="image-20210531142354926" style="zoom:67%;"></p><p><img src="https://i.loli.net/2021/05/31/bPFnzMH5SONEtgU.png" alt="image-20210531142440078" style="zoom:67%;"></p><p><img src="C:\Users\shiyaya\AppData\Roaming\Typora\typora-user-images\image-20210531142521384.png" alt="image-20210531142521384" style="zoom:67%;"></p><p><img src="https://i.loli.net/2021/05/31/ZkH6OSD7cCzMFTw.png" alt="image-20210531142733913" style="zoom:67%;"></p><p><img src="https://i.loli.net/2021/05/31/AdwjNaiIEux45p3.png" alt="image-20210531142944070" style="zoom:67%;"></p><p><img src="https://i.loli.net/2021/05/31/SHE4RONT9M6fo1J.png" alt="image-20210531143059560" style="zoom: 67%;"></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer</title>
      <link href="2021/03/19/Transformer/"/>
      <url>2021/03/19/Transformer/</url>
      
        <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/03/19/DrhRUENTcwXPo89.png" alt="image-20210319185913348" style="zoom:50%;"></p><p>Transformer模型中采用了 encoer-decoder 架构。论文中encoder层由6个encoder堆叠在一起，decoder层也一样。</p><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>对于encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。</p><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>decoder也包含encoder提到的两层网络，但是在这两层中间还有一层attention层，帮助当前节点获取到当前需要关注的重点内容。</p><p>Decoder 和 Encoder的结构差不多，但是多了一个attention的sub-layer，这里先明确一下decoder的输入输出和解码过程：</p><ul><li>输出：对应 $i$ 位置的输出词的概率分布</li><li>输入：encoder的输出 与 对应  $i-1$ 位置decoder的输出。所以中间的attention不是self-attention，它的<strong style="color:blue;">K，V来自encoder</strong>，<strong style="color:blue;">Q来自上一位置decoder的输出</strong></li><li><p>解码：这里要注意一下，训练和预测是不一样的。在训练时，解码是一次全部decode出来，用上一步的ground truth来预测（mask矩阵也会改动，让解码时看不到未来的token）；而预测时，因为没有ground truth了，需要一个个预测。</p><p>为了确保按照生成顺序：从左到右，使用sequence mask。</p><p>sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。</p><p>那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。</p></li></ul><p><img src="https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156846899939997439.gif" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>On Semantic Similarity in Video Retrieval</title>
      <link href="2021/03/19/On-Semantic-Similarity-in-Video-Retrieval/"/>
      <url>2021/03/19/On-Semantic-Similarity-in-Video-Retrieval/</url>
      
        <content type="html"><![CDATA[<h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><ul><li>当前的检索任务是目标实例进行检索（ target instance-based retrieval）（IVR），即，给定一个query caption，仅一个 origami video 被认为是正确的检索结果。但，实际上，数据集中的许多视频can be similar to the point of being identical。检索此类视频的顺序不应影响方法的评估。</li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>本文提出 Semantic Similarity Video Retrieval（SVR），相比于 normal IVR 的不同点是：</p><ul><li>对于1个video，允许有多个captions与其 的相似度为1</li><li>如果 $sim(x_i, y_j)$ = $sim(x_i, j_k)$，$x_i$ 为 video set 中的一个video。则表示这两个caption被认为是与该video有相等的相关度。可以以任意的顺序来检索，并且不能被evaluation metric 惩罚。</li></ul><h2 id="Proxy-Measures-for-Semantic-Similarity"><a href="#Proxy-Measures-for-Semantic-Similarity" class="headerlink" title="Proxy Measures for Semantic Similarity"></a>Proxy Measures for Semantic Similarity</h2><p>video $x_i$ 与 caption $y_{i}$ 是ground truth pair。</p><p>定义video 与 other captions 的语义相似度为：corresponding caption 与 other captions 之间的语义相似度。</p><p>$S_{S}\left(x_{i}, y_{j}\right)=\left\{\begin{array}{ll}1 &amp; i==j \\ S^{\prime}\left(y_{i}, y_{j}\right) &amp; \text { otherwise }\end{array}\right.$</p><p>关于 $S^{\prime}$ ，本文使用了四种方式来度量文本之间的语义相似度：bag of words, part-of- speech knowledge, synset similarity and the METEOR metric。</p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p><img src="https://i.loli.net/2021/03/22/oMS76vcp2HX5DgK.png" alt="image-20210322144739237" style="zoom:50%;"></p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>the log-ratio loss :</p><blockquote><p>Tao Qin, Tie-Yan Liu, and Hang Li. A general approximation framework for direct optimization of information retrieval measures. Information retrieval, 2010. </p></blockquote><p>nDCG loss</p><blockquote><p>Sungyeon Kim, Minkyo Seo, Ivan Laptev, Minsu Cho, and Suha Kwak. Deep metric learning beyond binary supervision. In CVPR, 2019. </p></blockquote>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Non-Autoregressive Coarse-to-Fine Video Captioning</title>
      <link href="2021/03/19/Non-Autoregressive-Coarse-to-Fine-Video-Captioning/"/>
      <url>2021/03/19/Non-Autoregressive-Coarse-to-Fine-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<p>发表在AAAI 2021</p><h2 id="现在存在的问题"><a href="#现在存在的问题" class="headerlink" title="现在存在的问题"></a>现在存在的问题</h2><ul><li>由于自回归解码（autoregressive decoding）导致的 <strong>slow inference speed</strong> </li><li>由于对视觉单词的训练不充分，而使得模型更加<strong>偏向于生成泛化性句子</strong>，缺乏细节和多样性。</li><li>此外，具有误差累积倾向的模型产生令人满意的字幕是具有挑战性的。 因此，还需要一种<strong>支持单词修改的灵活解码范例</strong>（decoding paradigm）。</li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p><img src="https://i.loli.net/2021/03/22/Kyegni76dZ4kCrH.png" alt="image-20210322164100678" style="zoom:33%;"></p><ul><li>我们提出了一种具有粗略至精细 （coarse-to-fine）字幕过程的基于非自回归解码（nonautoregressive<br>decoding）的模型</li><li><p><strong>For achieving inference speedup</strong>：employ a bi-directional self-attention based network as our language model for achieving inference speedup</p></li><li><p><strong>For improving caption quality：</strong>propose an alternative paradigm to decompose the captioning procedure into two stages,</p></li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://i.loli.net/2021/06/06/K7UPsboZR35kDA8.png" alt="image-20210606171106991" style="zoom:50%;"></p><p>（1）使用 <code>visual word generation</code>模块，来生成视觉单词，并以此来生成 a coarse-grained sentence “template”。（2）然后 使用合适的单词来 fill the blanks in the “template” 。（3）<strong>迭代 </strong>mask并重新考虑语言模型最没有信心的一些不适当的词，以确保句子的流畅性或捕捉更多的相关细节。</p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>使用 pre-trained 2D/3D CNNs 提取视觉特征。</p><h3 id="Length-Predictor"><a href="#Length-Predictor" class="headerlink" title="Length Predictor"></a>Length Predictor</h3><p>具体看论文</p><p><img src="https://i.loli.net/2021/06/06/NXBbIJYkq3F1lnC.png" alt="image-20210606171252873" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/06/06/EtCnDsgSXVTJ1Px.png" alt="image-20210606171304819" style="zoom:50%;"></p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>To obtain a non-autoregressive decoder, we adopt one-layer decoder of Transformer。(1) 移去 causal mask 使用双向attention。（2）图2中的虚线部分。</p><p>做了两处修改：</p><ul><li>按照MLM的方式，对 GT sentence, 进行随机掩码，然后补全。</li></ul><p><img src="https://i.loli.net/2021/06/06/CpGFWMm39Tohwas.png" alt="image-20210606165623746" style="zoom:50%;"></p><h3 id="Visual-Word-Generation"><a href="#Visual-Word-Generation" class="headerlink" title="Visual Word Generation"></a>Visual Word Generation</h3><p>使用上文提到的decoder 来作为 visual word generation.</p><p>给定一个长度为 $N$ 的 ground-truth sentence $Y^{<em>}$ ， 首先构建一个相对应的 <em>*target sequence</em></em> $Y^{\text {vis }}=\left\{y_{n}^{\text {vis }}\right\}_{n=1}^{N}$ as follows:</p><script type="math/tex; mode=display">y_{n}^{v i s}=\left\{\begin{array}{ll}y_{n}^{*} & \text { if } \operatorname{POS}\left(y_{n}^{*}\right) \in\{\text { noun, verb }\} \\{[\text { mask }]} & \text { otherwise }\end{array}\right.</script><p>where POS(·) denotes the part-of-speech of a word. </p><p>the decoder 在没有任何单词信息的提示下来强制预测  $Y^{\text {vis }}$, i.e., $Y_{o b s}^{v i s}=\varnothing_{[v i s]}$ (a sequence of the same special token $[v i s]$ in practice). </p><p>因此 visual word generation的损失函数如下：</p><p><img src="https://i.loli.net/2021/06/06/ZGmOiCVTnUFj3Ww.png" alt="image-20210606171720504" style="zoom:50%;"></p><script type="math/tex; mode=display">\mathcal{L}_{v i s}=-\sum_{y \in Y^{v i s}} \log p_{\theta}\left(y \mid \varnothing_{[v i s]}, R\right)</script><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p><img src="https://i.loli.net/2021/06/06/mZIgPbAlh3doQkt.png" alt="image-20210606171854039" style="zoom:50%;"></p><h3 id="测试（Coarse-to-Fine-Captioning）"><a href="#测试（Coarse-to-Fine-Captioning）" class="headerlink" title="测试（Coarse-to-Fine Captioning）"></a>测试（Coarse-to-Fine Captioning）</h3><ul><li><p>第一步： 使用 visual word generation, 生成一个粗粒度的模板： “template” Y(0)</p></li><li><p>第二步：本文引入三种解码算法，来生成细粒度的描述：Mask-Predict (MP) (Ghazvininejad et al. 2019), Easy-First (EF) and Left-to-Right (L2R)</p><blockquote><p>Ghazvininejad, M.; Levy, O.; Liu, Y.; and Zettlemoyer, L. 2019. <strong>Mask-predict: Parallel decoding of conditional masked language models</strong>. In EMNLP-IJCNLP.</p><p><a href="https://shiyaya.github.io/2021/06/06/Mask-Predict-Parallel-Decoding-of-Conditional-Masked-Language-Models/" target="_blank" rel="noopener">https://shiyaya.github.io/2021/06/06/Mask-Predict-Parallel-Decoding-of-Conditional-Masked-Language-Models/</a></p></blockquote></li></ul><p><img src="https://i.loli.net/2021/06/06/ZvdTOPBbFp1e5GJ.png" alt="image-20210606184525396" style="zoom:50%;"></p><h2 id="Other-non-autoregressive"><a href="#Other-non-autoregressive" class="headerlink" title="Other non-autoregressive"></a>Other non-autoregressive</h2><ul><li>Masked Non-Autoregressive Image Captioning</li><li>Non-Autoregressive Image Captioning with Counterfactuals-Critical Multi-Agent Learning</li></ul><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li>本篇论文中存在的问题：训练和测试不一致，训练时给定的observed tokens 是GT，而在测试时，给定的</li></ul><p>observed tokens 是预测得到的。</p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> video captioning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
            <tag> video-captioning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Less is More] CLIPBERT for Video-and-Language Learning via Sparse Sampling</title>
      <link href="2021/03/18/Less-is-More-CLIPBERT-for-Video-and-Language-Learning-via-Sparse-Sampling/"/>
      <url>2021/03/18/Less-is-More-CLIPBERT-for-Video-and-Language-Learning-via-Sparse-Sampling/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> end-to-end </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
            <tag> end-to-end </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[LightningDOT] Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval</title>
      <link href="2021/03/18/LightningDOT-Pre-training-Visual-Semantic-Embeddings-for-Real-Time-Image-Text-Retrieval/"/>
      <url>2021/03/18/LightningDOT-Pre-training-Visual-Semantic-Embeddings-for-Real-Time-Image-Text-Retrieval/</url>
      
        <content type="html"><![CDATA[<h2 id="1-任务"><a href="#1-任务" class="headerlink" title="1. 任务"></a>1. 任务</h2><p>本文发表在 NAACL 2021，本文要研究的内容是如何提高 <strong>Image-text retrieval 任务的计算效率。</strong></p><h2 id="2-存在的问题"><a href="#2-存在的问题" class="headerlink" title="2. 存在的问题"></a>2. 存在的问题</h2><p>基于预训练的跨模态模型取得了很好的进展，但是在测试阶段存在<strong>推理速度慢</strong>的问题。 主要是由于Transformer 结构中的cross-modal attention 造成的巨大的计算消耗。 这种延迟以及计算消耗使其很难在实际中应用。</p><p>下图可视化了近年来 ITR task 的研究进展，（a） 早期，使用CNN和RNN分别提取视觉和语言特征，然后使用dot-product 来计算similarity。 （b）后来有人提出使用faster-RCNN 和 RNN 分别提取两个模态的特征，使用使用cross-attention，最后再计算相似性。（c）随着BERT的发展，有人使用BERT扩展出 V+L BERT 模型。(d) 由于cross-modal attention 是耗时的，因此，本文中提出去掉cross-modal 这个模块。</p><p><img src="https://i.loli.net/2021/03/18/fdU3lxwpkZo8yuG.png" alt="image-20210318143100485" style="zoom:50%;"></p><h2 id="3-本文的点"><a href="#3-本文的点" class="headerlink" title="3. 本文的点"></a>3. 本文的点</h2><ul><li>本文希望可以重新回归到 <strong>dot-product</strong> 这个简单的操作。本文中使用dot product 来做多模态融合，而不是使用计算量的self-attention。同时，为了利用有效的多模态嵌入学习，本文在两个encoder上都使用 [CLS] token。</li><li><p>通过消除模态之间耗时的交叉注意力，该模型可以在推理过程中学习视觉语义嵌入而无需在每个图像-文本对之间进行广泛匹配。此外，通过消除对图像-文本对的实时计算的依赖，我们可以一次<strong>离线地独立地</strong>计算所有图像和文本嵌入，并将这些嵌入重新用作新查询的<strong>缓存索引</strong>。</p></li><li><p>LightningDOT通过预先训练<strong>三个新颖的学习目标</strong>：Visual-embedding fused MLM (namely VMLM), Semantic-embedding  fused MRM (namely SMRM) and a cross-modal retrieval objective (namely CMR).</p><p>前两个预训练任务（VMLM 和 SMRM）是为了确保跨模态信息可以被获取到。CMR是为了鼓励模型在预训练阶段获得多模态融合。</p></li><li><p>重新排名（re-ranking）机制</p></li></ul><p>Note: 本文不是从模型压缩的角度来解决问题。</p><h2 id="4-贡献"><a href="#4-贡献" class="headerlink" title="4. 贡献"></a>4. 贡献</h2><p>提出了一个简单有效的方法，在不牺牲accuracy 的情况下，LightningDOT 可以数千倍的加速推理时间。</p><p>我们的工作是在基于预训练视觉语义嵌入，实现低延迟的实时跨模式检索的第一个已知工作。</p><h2 id="5-Method"><a href="#5-Method" class="headerlink" title="5. Method"></a>5. Method</h2><p><img src="https://i.loli.net/2021/03/18/kASuJa8xc6hYFne.png" alt="image-20210318175408610" style="zoom: 67%;"></p><p>在本节中，我们介绍LightningDOT框架，该框架由两个深层的Transformer作为图像和语言编码器。我们首先介绍三个预训练任务，然后介绍从<strong>离线特征提取</strong>到<strong>在线即时检索</strong>的 <strong>推理流程（inference pipline）</strong>。</p><p>图像编码器得到region features: $f_{\theta_{V}}(\mathbf{v})=\mathbf{h}=\left\{\mathbf{h}_{0}, \ldots, \mathbf{h}_{N}\right\}\left(\mathbf{h}_{j} \in \mathbb{R}^{d}\right)$</p><p>语言编码器得到token representations: $f_{\theta_{L}}(\mathbf{w})=\mathbf{z}=\left\{\mathbf{z}_{0}, \ldots, \mathbf{z}_{T}\right\}\left(\mathbf{z}_{j} \in \mathbb{R}^{d}\right)$</p><p>regard the output [CLS] embedding <strong><strong style="color:red;">$h_0$</strong> as global image representation</strong>, and <strong><strong style="color:red;">$z_0$</strong>as global text representation</strong></p><h3 id="5-1-Model-Pre-training"><a href="#5-1-Model-Pre-training" class="headerlink" title="5.1 Model Pre-training"></a>5.1 Model Pre-training</h3><h4 id="Visual-embedding-Fused-Masked-Language-Modeling-VMLM"><a href="#Visual-embedding-Fused-Masked-Language-Modeling-VMLM" class="headerlink" title="Visual-embedding Fused Masked Language Modeling (VMLM)"></a>Visual-embedding Fused Masked Language Modeling (VMLM)</h4><p>设有M个 masked tokens</p><p>对于 sentence $t$ and image $i$ ， The loss function of VMLM can be formulated as:</p><p>$\mathcal{L}_{\mathrm{VMLM}}(t, i)=-\log P_{\theta}\left(\mathbf{w}_{\mathbf{m}} \mid \mathbf{w}_{\backslash \mathbf{m}}, i\right)$<br>$=-\frac{1}{M} \sum_{k=1}^{M} \log P_{\theta_{\mathrm{mlm}}}\left(\mathbf{w}_{\mathbf{m}_{k}} \mid \mathbf{z}_{\mathbf{m}_{k}}+\mathbf{h}_{0}\right)$</p><p>其中 $z$ 是 hidden state。</p><p>Note： 这里的 +$h_0$ 是显式的加和，而不是使用cross-modal attention.</p><h4 id="Semantic-embedding-Fused-Masked-Region-Modeling-SMRM"><a href="#Semantic-embedding-Fused-Masked-Region-Modeling-SMRM" class="headerlink" title="Semantic-embedding Fused Masked Region Modeling (SMRM)"></a>Semantic-embedding Fused Masked Region Modeling (SMRM)</h4><p>$\mathcal{L}_{\mathrm{SMRM}}(i, t)=\mathcal{D}_{\theta_{\mathrm{mrm}}}\left(\mathbf{v}_{\mathbf{m}}, f_{\theta_{V}}\left(\mathbf{v}_{\backslash \mathbf{m}}\right), t\right)$<br>$=\frac{1}{M} \sum_{k=1}^{M} \mathcal{D}_{\theta_{\mathrm{mrm}}}\left(\mathbf{v}_{\mathbf{m}_{k}}, \mathbf{h}_{\mathbf{m}_{k}}+\mathbf{z}_{0}\right)$</p><p>这里的 $\mathcal{D}_{\theta_{\mathrm{mrm}}}$ 代表两个损失，一个是使用L2 distance 的 掩码区域特征回归，另外一个是用KL散度的掩码区域分类。</p><h4 id="Cross-modal-Retrieval-Objective-CMR"><a href="#Cross-modal-Retrieval-Objective-CMR" class="headerlink" title="Cross-modal Retrieval Objective (CMR)"></a>Cross-modal Retrieval Objective (CMR)</h4><p>The similarity score between query t and image i is defined as:</p><p>$S(t, i)=\left\langle\mathbf{z}_{0}, \mathbf{h}_{0}\right\rangle$</p><p>损失函数：</p><p>$\mathcal{L}_{\mathrm{IR}}^{(t)}=-\log \frac{e^{S\left(t, i_{1}\right)}}{\sum_{k=1}^{n} e^{S\left(t, i_{k}\right)}}$</p><p>$\mathcal{L}_{\mathrm{TR}}^{(i)}=-\log \frac{e^{S\left(i, t_{1}\right)}}{\sum_{k=1}^{n} e^{S\left(i, t_{k}\right)}}$</p><p>$\mathcal{L}_{\mathrm{CMR}}(B)=\frac{1}{2 n} \sum_{k=1}^{n} \mathcal{L}_{\mathrm{TR}}^{\left(i_{k}\right)}+\mathcal{L}_{\mathrm{IR}}^{\left(t_{k}\right)}$</p><h3 id="5-2-Real-time-Inference"><a href="#5-2-Real-time-Inference" class="headerlink" title="5.2 Real-time Inference"></a>5.2 Real-time Inference</h3><p>以text-to-image retrieval 作为样例来介绍 real-time inference pipline：</p><p>（1）离线图片特征提取与编码；（2）text query 在线检索；（3）使用top-retrieval images 做在线重拍</p><h4 id="Offline-Feature-Extraction"><a href="#Offline-Feature-Extraction" class="headerlink" title="Offline Feature Extraction"></a>Offline Feature Extraction</h4><p>首先使用 image encoder 来处理数据集中的所有图片，并存储其 global image representation 进入索引的内存中供以后使用。</p><p>整个image-to-index 过程，包括 faster rcnn 提取特征 以及 image transformer encoder 都是离线处理的。</p><h4 id="Online-Retrieval"><a href="#Online-Retrieval" class="headerlink" title="Online Retrieval"></a>Online Retrieval</h4><p>对于 text query, 使用language encoder 提取特征，然后依次计算与每个图片的相似度。图片将会被排序。实际中，人们感兴趣的是前top-k 检索结果。</p><p>使用FAISS来优化检索。</p><p>类似地，对于文本检索，可以通过简单地为所有句子预先计算嵌入并使用图像作为查询来应用相同的体系结构</p><h4 id="Re-ranking"><a href="#Re-ranking" class="headerlink" title="Re-ranking"></a>Re-ranking</h4><p>为了进一步提高检索结果，本文通过采用可选的<strong>重新排名模型</strong>提出了一种两阶段方法。</p><p>第一阶段，使用LightingDOT来检索 top-M images(or texts)。</p><p>第二阶段，使用一个性能更好的检索模型（通常比较慢）来重新排序从第一阶段检索到的 top-M pairs.</p><p>实验证明，可以同时从性能和效率两方面受益。</p><h2 id="6-Experiments"><a href="#6-Experiments" class="headerlink" title="6. Experiments"></a>6. Experiments</h2><h3 id="6-1-Results-on-Flickr30K-and-COCO"><a href="#6-1-Results-on-Flickr30K-and-COCO" class="headerlink" title="6.1 Results on Flickr30K and COCO"></a>6.1 Results on Flickr30K and COCO</h3><p><img src="https://i.loli.net/2021/03/18/TpcoUY7Zjel39dM.png" alt="image-20210318175824380" style="zoom: 50%;"></p><ul><li><p>在仅使用一阶段排序的情况下：</p><ul><li>相比于不使用预训练的模型，性能上有显著提升 CAAN (SOTA method with cross-attention）</li><li>与使用预训练的模型相比，UNITER，性能上仅下降了一点，但是速度上有600/1900倍的提升(Flickr30K/COCO)</li></ul></li><li><p>使用两阶段排序：</p><ul><li>性能上相比于一阶段有提升，同时比单纯的UNITER模型有 46-95倍速度的提升，</li></ul></li></ul><h3 id="6-2-Speed-amp-Space-Improvement"><a href="#6-2-Speed-amp-Space-Improvement" class="headerlink" title="6.2 Speed &amp; Space Improvement"></a>6.2 Speed &amp; Space Improvement</h3><ul><li><p>检索图像，比较推理速度差异</p><p>以 UNITER_base 作为比较对象。</p><p>SCAN，是一个不使用预训练的模型，但是采用了cross-modal attention.</p><p><img src="https://i.loli.net/2021/03/18/umlFqkgLfyKQ6Cx.png" alt="image-20210318180604083" style="zoom: 33%;"></p></li><li><p>扩大搜索池，性能仍然很好</p><p><img src="https://i.loli.net/2021/03/18/jelLQRdcFwWpaEU.png" alt="image-20210318180918374"></p></li></ul><h3 id="6-3-Ablation-Studies"><a href="#6-3-Ablation-Studies" class="headerlink" title="6.3 Ablation Studies"></a>6.3 Ablation Studies</h3><ul><li><p>观察各个模块的作用</p><p>(1) 【R-CNN only】不使用 image encoder, 直接使用 faster rcnn 提取的特征</p><p>(2)【 “+Image Encoder”】</p><p>(3)【+PT】 MLM+MRM+CMR 上预训练， 注意本文采用的预训练方案是 VMLM+SMRM+CMR</p><p><img src="https://i.loli.net/2021/03/18/ewAYK8M6idSBvEb.png" alt="image-20210318181810013" style="zoom: 33%;"></p><p><strong style="color:blue;">yaya: 其实，本文提出的预训练任务带来的提升并不明显。</strong></p></li><li><p>观察各个预训练任务的作用</p><p><img src="https://i.loli.net/2021/03/18/OkTcr9IVbxYBPgu.png" alt="image-20210318182135676" style="zoom: 50%;"></p><p>预训练任务对于本文提出的模型是有提升的，但是，提升的显著性似乎没有那么大。</p><p><strong style="color:blue;">yaya: 奇怪，为什么 这个 PT(ALL) 与 上个表Table 4 中的LightingDOT结果 不一致呢都？都是在Flickr30k validation上的结果</strong></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> Image-Text Retrieval </category>
          
          <category> real time </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
            <tag> Image-Text Retrieval </tag>
            
            <tag> real time </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Improving Translation Robustness with Visual Cues and Error Correction</title>
      <link href="2021/03/17/Improving-Translation-Robustness-with-Visual-Cues-and-Error-Correction/"/>
      <url>2021/03/17/Improving-Translation-Robustness-with-Visual-Cues-and-Error-Correction/</url>
      
        <content type="html"><![CDATA[<h2 id="本文研究的任务"><a href="#本文研究的任务" class="headerlink" title="本文研究的任务"></a>本文研究的任务</h2><p>多模态机器翻译任务中对噪声样本的鲁棒性。</p><p>神经机器翻译模型对输入噪声很脆弱。当前的鲁棒性技术大多使模型<strong>适应</strong>现有的嘈杂文本，但是这些模型通常在<strong>遇到看不见的噪声</strong>时会失效，并且在clean  text 上的性能会下降（即相比于那些普通的模型，使用噪声样本来扩充数据的模型，其在clean text 上的性能会下降）。</p><h2 id="本文提出的点"><a href="#本文提出的点" class="headerlink" title="本文提出的点"></a>本文提出的点</h2><p>（1） 模型上：引入了<strong><em>视觉上下文</em></strong>的概念，以提高针对嘈杂文本的翻译鲁棒性。</p><p>（2）多任务：通过<strong>将纠错作为辅助任务</strong>来提出一种新的<strong><em>纠错训练</em>方案</strong>，以进一步提高鲁棒性。</p><p>实验证明，在 English-French and English-German 翻译任务上，（1）对于训练中遇到的噪声以及未遇到的噪声都有很好的鲁棒性。（2）同时保持了在 clean text 上的翻译质量。</p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>不是重点来做 MMT model 的，略过</li></ul>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> image-guided MT </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal,image-guided MT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[VisualSparta] Sparse Transformer Fragment-level Matching for Large-scale Text-to-Image Search</title>
      <link href="2021/03/16/VisualSparta-Sparse-Transformer-Fragment-level-Matching-for-Large-scale-Text-to-Image-Search/"/>
      <url>2021/03/16/VisualSparta-Sparse-Transformer-Fragment-level-Matching-for-Large-scale-Text-to-Image-Search/</url>
      
        <content type="html"><![CDATA[<h2 id="1-任务"><a href="#1-任务" class="headerlink" title="1. 任务"></a>1. 任务</h2><p>本文是做跨模态检索问题。文本到图像的检索是多模态信息检索中的一项基本任务，即在给定文本查询的情况下从大型且未标记的图像数据集中检索相关图像。</p><h2 id="2-存在的问题"><a href="#2-存在的问题" class="headerlink" title="2. 存在的问题"></a>2. 存在的问题</h2><p>图文检索问题上存在两个核心挑战：<strong style="color:red;">准确率以及速度</strong>。</p><h2 id="3-本文提出的点"><a href="#3-本文提出的点" class="headerlink" title="3. 本文提出的点"></a>3. 本文提出的点</h2><p>在本文中，提出了基于transformer 的 VisualSparta 模型，这是一种新颖的文本到图像检索模型，该模型在准确性和效率上都比现有模型显著提高。</p><p>本文提出的模型关注点在于两点：</p><p>（1）准确率，学习query tokens 与 image regions之间的细粒度关系，以丰富跨模态理解。</p><p>（2）有效性，独立的学习query 和 answer（image）的特征表示，从而使得模型可以<strong><strong style="color:red;">离线的</strong>索引所有的candidate images</strong>。整个VisualSparta 模型可以作为一个经典的反向索引（Inverted index）搜索引擎，以实现高效搜索。</p><h2 id="4-本文的贡献"><a href="#4-本文的贡献" class="headerlink" title="4. 本文的贡献"></a>4. 本文的贡献</h2><p>1) 性能优势：提出了一个新的基于片段交互（fragment-level interaction）的图文检索模型，并取得了SOTA的性能；</p><p>2) 速度优势：相比于标准的向量搜索，VisualSparta 有391x 速度提升。且实验证明，由于VisualSparta 可以有效的进行<strong>反向索引</strong> ，因此对于更大的数据集，速度优势会更加的明显，</p><p>3) 第一：VisualSparta 是<strong>第一个</strong>可以在大规模数据集上实现<strong>实时搜索</strong>的，基于transformer的 text-to-image retrieval model，并且实现了显著的性能提升。本文的方法证明了large pretrained model 也可以占用<strong>较少的内存和较少的计算时间</strong>。</p><p>4) 对当前存在的 text-to-image retrieval models 进行了 accuracy-latency comparisons。</p><h2 id="5-Method"><a href="#5-Method" class="headerlink" title="5. Method"></a>5. Method</h2><p>本文提出了 <strong>VisualSparta retriever</strong>, a fragment-level transformer-based model for efficient text-image matching.</p><p><img src="https://i.loli.net/2021/03/16/ZTKckhILpCEBv19.png" alt="image-20210316192344412"></p><h3 id="5-1-Query-representation"><a href="#5-1-Query-representation" class="headerlink" title="5.1 Query representation"></a>5.1 Query representation</h3><p>在检索中，<strong style="color:red;">query 的处理是一个在线操作</strong>。需要很好的考虑query 编码的效率。以前的方法，使用bi-RNN来处理 query sentence，为每个token获得上下文特征表示。</p><p>本文中，不采用序列处理的方式。丢掉query中的顺序信息，仅仅使用预训练的word embedding 来表征每个token。<strong>这种方法可以使得每个token的特征表达损失独立的，与上下文无关的</strong>。同时这种方式对于高效的indexing and inference 是必要的。</p><p>a query is represented as $\hat{w}=\left\{\hat{w}_{1}, \ldots, \hat{w}_{m}\right\}$</p><h3 id="5-2-Visual-Representation"><a href="#5-2-Visual-Representation" class="headerlink" title="5.2 Visual Representation"></a>5.2 Visual Representation</h3><p>相比于 query 需要实时在线处理，answer candidates 可以在 query 到来之前离线编制索引 (indexed offline)。因此，answer candidates 的处理可以更加丰富和复杂。因此，本文 follow OSCAR的工作，对于answer candidates 本文提取其上下文特征。</p><p>具体的看上图.</p><p>$H_{\text {image }} \in \mathbb{R}^{(n+k) \times d_{H}}$ is the final contextualized representation for one answer.</p><h3 id="5-3-Scoring-Function"><a href="#5-3-Scoring-Function" class="headerlink" title="5.3 Scoring Function"></a>5.3 Scoring Function</h3><p>第一个等式：学习 image element 和 每个query token 之间的fragment-level 交互。</p><p>$y_{i} =\max _{j \in[1, n+k]}\left(\hat{w}_{i}^{T} h_{j}\right) $              <strong>（equation 10）</strong></p><p>第二个等式：经过一个 ReLu 和 可训练的bias来得到sparse embedding。</p><p>$ \phi\left(y_{i}\right) =\operatorname{ReLU}\left(y_{i}+b\right) $              <strong>（equation 11）</strong></p><p>第三个等式：对于所有的分数求和，并为了抑制过大的分数，使用log operation</p><p>$ f(q, v) =\sum_{i=0}^{m} \log \left(\phi\left(y_{i}\right)+1\right) $             <strong>（equation 12）</strong></p><h3 id="5-4-Retriever-Training"><a href="#5-4-Retriever-Training" class="headerlink" title="5.4 Retriever Training"></a>5.4 Retriever Training</h3><p>最小化如下目标：</p><p>$J=f\left(q, v^{+}\right)-\log \sum_{k \in K^{-}} e^{\left.f\left(q, v_{k}\right)\right)}$</p><p><strong style="color:blue;"><strong>yaya: 这个损失函数其实与正常的NCE损失不同</strong></strong></p><p>负样本的选择：从相同batch 中的其他image samples作为负样本。</p><p><strong>而且本文发现，相比于一些复杂的负样本选择策略（比如，使用有相近标签的相似图像作为负样本），这种负样本的选择策略是简单有效地，效果相当。</strong></p><p><strong style="color:blue;">yaya: 为什么这种选择策略比复杂的策略是有效的？？是不是在不同的场合，应该使用不同的策略呢？？</strong></p><h3 id="5-5-Efficient-Indexing-and-Inference"><a href="#5-5-Efficient-Indexing-and-Inference" class="headerlink" title="5.5 Efficient Indexing and Inference"></a>5.5 Efficient Indexing and Inference</h3><p><strong style="color:red;">real-time inference</strong></p><p>定义 testing query 为 $q=\left[w_{0}, \ldots w_{m}\right]$</p><p>the <strong>ranking score</strong> between $q$ and an image is （利用5.3 中第二个等式得到的 sparse embedding）:</p><p>​    $\operatorname{CACHE}(w, v)=\log ($ sparse embedding $) \quad w \in W $             <strong>（equation 14）</strong></p><p>​    $f(q, v)=\sum_{i=1}^{m} \operatorname{CACHE}\left(w_{i}, v\right)$             <strong>（equation 15）</strong></p><p>由于query term embedding 不是基于上下文得到。因此，可以预先计算 vocabulary $W$ 中每个<strong>term</strong> $w$  与 每个 image candidates 之间的 ranking feature $\phi(w, v)$，<strong style="color:red;">生成的分数 is cached during indexing</strong>，如等式14 所示。得到了一个一个  <strong style="color:red;">$N_{vocab}*M_{images} $的矩阵</strong></p><p><strong>during inference time，最终的分数可以经过 O(1)的查询和一个简单的求和运算得到，如 公式15所示。</strong></p><p><strong style="color:red;">Inverted Index</strong></p><p>更加重要的是，以上的计算可以经由一个 Inverted Index 来高效的实施。 Inverted Index 是现代搜索引擎的基础数据结构，如图1所示。</p><blockquote><p><a href="https://zh.wikipedia.org/wiki/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95</a></p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><ul><li><p>使用图像描述数据集作为本文text-to-image model 的数据来源。<strong>benchmark: MSCOCO; Flickr 30K</strong></p></li><li><p>for large-scale efficiency experiments: 由于目前不存在大规模的图像描述数据集。</p><p>因此，we <strong>manually design 113K and 1M datasets</strong> for testing the inference speed of different models in the large-scale setting.   对于这两个数据集，我们只关注于speed comparison。在数据上的模型性能忽略不比较。</p><p>The 113K dataset refers to the MSCOCO training set。</p><p>The 1M dataset we design consists of 1 million images randomly sampled from the MSCOCO training set.</p><p>所有的 <strong>efficiency test  experiments</strong> 都是在MSCOCO 1K and 5k test splits 再加上这113k 和 1M 数据上进行的。</p></li></ul><h3 id="Recall-Performance"><a href="#Recall-Performance" class="headerlink" title="Recall Performance"></a>Recall Performance</h3><p><img src="https://i.loli.net/2021/03/17/8dVeNz7bngQEwKl.png" alt="image-20210317120703074" style="zoom:50%;"></p><h3 id="Speed-Performance"><a href="#Speed-Performance" class="headerlink" title="Speed Performance"></a>Speed Performance</h3><p>三个模型使用相同的Faster-rcnn image region features。下表中没有考虑这部分时间。</p><p><img src="https://i.loli.net/2021/03/17/GfApECsXomRWHPq.png" alt="image-20210317120800787" style="zoom:50%;"></p><p>（1）在不同size的数据集下，本文提出的模型的速度远高于另外两个模型（一个使用dual encoding, 另一个使用transformer model）</p><p>（2）Table 2 also reveals that as the number of images increases, <strong>the performance drop is much slower</strong> when comparing VisualSparta with other two methods.</p><h3 id="Speed-Accuracy-Flexibility"><a href="#Speed-Accuracy-Flexibility" class="headerlink" title="Speed-Accuracy Flexibility"></a>Speed-Accuracy Flexibility</h3><p>在 Efficient Indexing and Inference 这一节，得到了一个  <strong style="color:red;">$N_{vocab}*M_{images} $的矩阵</strong>， 对于每个image, 与 N个words 计算出了weights, 可以挑选出 top-K， 这样更新为一个  <strong style="color:red;">$K_{words}*M_{images} $的矩阵</strong>，K 越小，检索效率越高。</p><p><img src="https://i.loli.net/2021/03/17/nkhj8yC6dPxugfw.png" alt="image-20210317132243925" style="zoom: 67%;"></p><p><img src="https://i.loli.net/2021/03/17/FmkRlfDp4WqTzOV.png" alt="image-20210317132318637" style="zoom: 50%;"></p><h3 id="Implementation-details"><a href="#Implementation-details" class="headerlink" title="Implementation details"></a>Implementation details</h3><ul><li>image encoder 的初始权重 从 Oscar-base model （12 layers and 768 hidden dimensions）中获得。</li><li>the query embedding， 使用Oscar-base word embedding的参数作为初始权重</li></ul><h2 id="可以查看的其他文献"><a href="#可以查看的其他文献" class="headerlink" title="可以查看的其他文献"></a>可以查看的其他文献</h2><p>本文受到此篇论文的启发: <strong>Sparta: Efficient open-domain question answering via sparse transformer matching retrieval.</strong></p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a><strong>yaya</strong></h2><ul><li><p>对于输入的消融实现，如果不提供 object label ？</p><p><strong>本文没有做这个实验。</strong></p></li><li><p>实验结果与 transformer-based retrieval model 的对比， eg: Oscar, Unicoder-VL 等</p><p><strong>本文没有做对比，只是与不基于pre-trained models 进行了对比。</strong></p><p><strong>但是实际上，本文的实验效果在准确率上，是不如那些基于预训练模型的。</strong></p></li><li><p>使用了 transformer 结构，那么本文的学习率是如何设计的？先warm up吗？？</p><p><strong>本文学习率为 1e-5， bs=20, 没有对学习率的变化进行说明。</strong></p></li><li><p>本文发现，相比于一些复杂的负样本选择策略（比如，使用有相近标签的相似图像作为负样本），这种负样本的选择策略是简单有效地，效果相当。</p><p><strong>对于这部分，论文中并没有相关的解释与实验数据说明</strong></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> real time </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
            <tag> real time </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Slot Filling</title>
      <link href="2021/03/15/Slot-Filling/"/>
      <url>2021/03/15/Slot-Filling/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Bridging the Gap between Training and Inference for Neural Machine Translation</title>
      <link href="2021/03/15/Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation/"/>
      <url>2021/03/15/Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation/</url>
      
        <content type="html"><![CDATA[<h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>本文是ACL 2019 的 最佳长文奖。</p><p><strong style="color:red;">论文主要解决神经网络在翻译领域，训练和测试时所用的上文信息不同造成的偏差问题。</strong></p><p><strong>论文提出了新的训练方法，而非新的模型。读完之后，发现这种方法适用于许多领域的训练-测试不匹配的问题，如：阅读理解、语言模型。</strong></p><h2 id="2-存在的问题"><a href="#2-存在的问题" class="headerlink" title="2. 存在的问题"></a>2. 存在的问题</h2><p>传统的神经机器翻译有两个问题：</p><ul><li><strong>exposure bias</strong> （训练和测试时所用的上文信息不同的问题）</li><li><strong>overcorrection</strong>（过度矫正）</li></ul><h3 id="2-1-exposure-bias"><a href="#2-1-exposure-bias" class="headerlink" title="2.1 exposure bias"></a>2.1 exposure bias</h3><p>那么，什么叫<code>训练和测试时所用的上文信息不同的问题</code>呢？</p><p>训练时, 无论上一步模型的预测输出是什么，在当前步decoder模型的输入都是ground truth word的，即：模型的输入都是正确的，如：<strong>are</strong> 。</p><p><img src="https://i.loli.net/2021/03/14/tMrJqWR1huDFHQ4.png" alt="image-20210314192737439" style="zoom:33%;"></p><p>在测试时，由于没有正确答案，所以用模型预测的上一个字的结果作为输入，如：is、 you 。</p><p>这就导致了在测试时，<strong>如果在某个地方预测错，那么之后模型的输入都是错误的</strong>，这就造成了错误会一直累积；或许模型在某个地方所预测的是另一种翻译的词，但是在训练时没有碰到过这种情况，所以模型无法进行处理。</p><p>这种偏差叫做<code>exposure bias</code>。</p><h3 id="2-2-overcorrection"><a href="#2-2-overcorrection" class="headerlink" title="2.2 overcorrection"></a>2.2 overcorrection</h3><p>训练翻译模型时，还会碰到另一个问题：<strong>overcorrection</strong>（过度矫正）</p><p>什么意思呢？</p><p><img src="https://i.loli.net/2021/03/15/k8USC1drMGB6Jwh.png" alt="img"></p><p>当模型在第三个位置预测出‘abide’时，为了让这句话的loss最小，模型之后会预测 with the rule，但是 abide with the rule 是错误的；正确的应该是 abide by the rule。</p><p>注解: abide 与 by 搭配，而不是与with 搭配。</p><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h2><p><img src="https://i.loli.net/2021/03/14/QLSZEk4dqNln6jV.png" alt="image-20210314193828727" style="zoom:50%;"></p><p>为了消除或减轻train阶段和infer阶段的差别, 论文提出从真实的词 $y_{t-1}^{*}$ 和预测的词 $y_{t-1}^{\text {oracle }}$ 中抽样, decoder根据抽样的词来预测下一个词 $y_{t}$ 。使用论文提出的方法, 在时间步 $t$ 预测 $y_{t}$ 分为三步:</p><p><img src="https://i.loli.net/2021/04/08/lZw4sXbeunLGBfq.png" alt="image-20210408132802746"></p><h3 id="3-1-Oracle-Word-Selection"><a href="#3-1-Oracle-Word-Selection" class="headerlink" title="3.1 Oracle Word Selection"></a>3.1 Oracle Word Selection</h3><p>传统的方法中， decoder会根据上一个时间步真实的 $y_{t-1}^{*}$ 来预测 $y_{t}$ 。</p><p>为了消除train阶段的infer阶段的 差别，可以从预测的词中选择oracle word $y_{t-1}^{\text {oracle }}$ 来代替 $y_{t-1^{\circ}}^{*}$ </p><p>一种方法是每个时间步采用词级别的 greedy search来生成oracle word, 称为word-level oracle(WO)。另一种方法是采用beam-search, 扩大搜索空间, 用句子级的衡量指标(如：BLEU)对beam-search的结果进行排序，称为sentence-level oracle(SO).</p><h4 id="3-1-1-Word-Level-Oracle"><a href="#3-1-1-Word-Level-Oracle" class="headerlink" title="3.1.1 Word Level Oracle"></a>3.1.1 <strong>Word Level Oracle</strong></h4><p><img src="https://i.loli.net/2021/03/14/oJlrVILE6FgkcyM.png" alt="image-20210314200245535" style="zoom: 33%;"></p><p><img src="https://i.loli.net/2021/03/14/wc7Ql4js39KBCXg.png" alt="image-20210314200310765" style="zoom:33%;"></p><p>选择 $y_{t-1}^{\text {oracle }}$ 最简单直观的方法是, 在时间步$t$-1 , 选择公式 $P_{t-1}$ 中概率最高的词作为 $y_{t-1}^{\text {oracle }},$ 如Fig.2所 示。 为了获得更健壮的 $y_{t-1}^{\text {oracle }}$, 更好地选择是使用<a href="https://www.cnblogs.com/initial-h/p/9468974.html" target="_blank" rel="noopener">gumbel max技术</a>来从分类分布中进行抽样, 如 Fig.3所示。<br>具体地讲, 将gumbel noise <strong style="color:blue;">$\eta$</strong> 作为正则化项加到decoder的预测概率分布上，进而再做softmax操作。</p><p>$\eta=-\log (-\log u)$<br>$\tilde{o}_{j-1}=\left(o_{j-1}+\eta\right) / \tau$<br>$\tilde{P}_{j-1}=\operatorname{softmax}\left(\tilde{o}_{j-1}\right)$</p><p>其中变量 $u \sim U(0,1)$ 服从均匀分布。 $\tau$ 为温度系数, 当 $\tau \rightarrow 0$ 时， 公式(8)的softmax()逐渐相当于<br>$\operatorname{argmax}()$ 函数 $;$ 当 $\tau \rightarrow \infty$ 时, $\operatorname{softmax}()$ 函数逐渐相当于均匀分布。</p><p>则 $y_{t-1}^{\text {oracle }}$ 为：$y_{j-1}^{\text {oracle }}=\operatorname{argmax}\left(\tilde{P}_{j-1}\right)$</p><p>需要注意的是gumbel noise $\eta$ 只用来选择oracle word，而不会影响train阶段的目标函数。</p><h4 id="3-1-2-Sentence-Level-Oracle"><a href="#3-1-2-Sentence-Level-Oracle" class="headerlink" title="3.1.2 Sentence Level Oracle"></a>3.1.2 <strong>Sentence Level Oracle</strong></h4><p>在每一次训练前，模型先用 beam search找到最好的 k 个候选翻译，然后将这 k 句话与正确答案计算 BLEU得分，取得分最高的当作备选句子。</p><p>有了备选句子后怎么办？比如，模型现在要预测第四个词，那么模型的输入是第三个词，这第三个词可以是正确译文的第三个词（传统做法）、可以是模型所预测的第三个词（Word Level）、也可以是这句备选句子的第三个词（Sentence-Level）。</p><p>现在有一个问题： 如果备选句子的长度与答案的长度不一样怎么办，这样备选句子与ground truth不是一一对应的了，那么这样的替换就没有意义了，因为我们希望这个词和对应答案的词是意思相近的或者是近义词。</p><p>作者给出了办法：</p><p>beam search在生成句子时，直到模型预测出结尾符<eos>才结束。</eos></p><p>假设ground truth的长度是 n ：</p><p>1、若模型在 n 之前就预测出<eos>结尾符，那么，我们选择概率第二的作为预测词。</eos></p><p>2、若模型在 n 时没有预测出<eos>结尾符，那么，我们选择<eos>结尾符，并使用它的概率。</eos></eos></p><p>作者的思路就是这样，然后就是最小化每一个字与<strong>ground truth</strong>对应字的负似然对数。</p><p>是与<strong>原始的ground truth</strong>的词计算loss！！！ 而不是与替换了的词，这个替换只发生在模型的输入。</p><h3 id="3-2-Sampling-with-Decay"><a href="#3-2-Sampling-with-Decay" class="headerlink" title="3.2 Sampling with Decay"></a>3.2 Sampling with Decay</h3><p>在train阶段刚开始时，抽中真实的词 $y_{j-1}^{*}$ 的概率比较大，随着模型逐渐收敛，抽中预测的词 $y_{j-1}^{\text {oracle }}$ 的概率变大，让模型有能力处理”过度纠正的问题”。</p><p>在训练的初始阶段, 如果过多地选择 $y_{t-1}^{\text {oracle }},$ 会导致模型收敘速度慢; 在训练的后期阶段，如果过多地选择 $y_{t-1}^{*},$ 会导致模型在train阶段没有学习到如何处理infer阶段的差别。 </p><p>因此，好的选择是：在训练的初始阶段， 更大概率地选择 $y_{t-1}^{*}$ 来加快模型收敛，当模型逐渐收敛后, 以更大概率选择 $y_{t-1}^{\text {oracle }},$ 来让模型学习到如何处理infer阶段的差别以及让模型有能力处理”过度纠正的问题”。从数学表示上，概率 $p$ 先大后逐渐衰减，$p$ 随着训练轮数 $e$ 的增大而逐渐变小。</p><p>$p=\frac{\mu}{\mu+\exp \left(\frac{e}{\mu}\right)}$</p><p><img src="https://i.loli.net/2021/03/15/H6U7Dc5wP8KAsNG.png" alt="image-20210315173055286" style="zoom: 67%;"></p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><p>本文主要的两点贡献：</p><p>(1) word level oracle selection</p><p>(2) sampling with decay</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://zhuanlan.zhihu.com/p/76227765" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76227765</a></p><p><a href="https://spring-quan.github.io/2019/08/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ABridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation%E3%80%8B/" target="_blank" rel="noopener">https://spring-quan.github.io/2019/08/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ABridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation%E3%80%8B/</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Beam Search</title>
      <link href="2021/03/15/Beam-Search/"/>
      <url>2021/03/15/Beam-Search/</url>
      
        <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. <strong>前言</strong></h2><p>自然语言处理任务中，如机器翻译、对话、文本摘要等，都涉及到序列生成。文本序列生成解码过程中有用到 greedy search、维特比算法、beam search等。</p><h2 id="2-Beam-Search-介绍"><a href="#2-Beam-Search-介绍" class="headerlink" title="2. Beam Search 介绍"></a>2. Beam Search 介绍</h2><p>beam search尝试在广度优先基础上进行进行搜索空间的优化（类似于剪枝）达到减少内存消耗的目的。</p><ul><li><strong>算法过程</strong></li></ul><p>定义词表大小是V，beam size是 B，序列长度是L。</p><p>假设V=100，B=3：</p><ol><li><p>生成第1个词时，选择概率最大的3个词（假设是a，b，c），即从100个中选了前3个；</p></li><li><p>生成第2个词时，将当前序列a/b/c分别与词表中的 100个词组合，得到 3*100个序列，从中选 3个概率最大的，作为当前序列（假设现在是am，bq，as）；</p></li><li><p>持续上述过程，直到结束。最终输出3个得分最高的。</p></li></ol><ul><li><strong>算法复杂度</strong> O(B×V×L)</li></ul><p>在第2步，要计算 B×V次。序列长度是L，生成长度为L的序列，计算 B×V×L 次。</p><h2 id="3-算法评价"><a href="#3-算法评价" class="headerlink" title="3. 算法评价"></a>3. 算法评价</h2><ul><li><strong>优点</strong></li></ul><p>(1) 减少计算开销。相对于广度优先搜索，广搜每次都要保留所有可能的结果，复杂度是  $O(V^L)$指数级。</p><ul><li><strong>缺点（第3部分详细讲）</strong></li></ul><p>(1) 数据下溢</p><p>(2) 倾向于生成短的序列</p><p>(3) 单一性问题</p><ul><li><strong>Beam size 设置</strong></li></ul><p>(1) B越大</p><p>优点：可考虑的选择越多，能找到的句子越好</p><p>缺点：计算代价更大，速度越慢，内存消耗越大</p><p>(2) B越小</p><p>优点：计算代价小，速度快，内存占用越小</p><p>缺点：可考虑的选择变少，结果没那么好</p><h2 id="4-问题解决"><a href="#4-问题解决" class="headerlink" title="4. 问题解决"></a>4. 问题解决</h2><h3 id="4-1-数据下溢"><a href="#4-1-数据下溢" class="headerlink" title="4.1 数据下溢"></a>4.1 数据下溢</h3><p>求序列概率的时候，序列概率是多个条件概率的乘积$P(y^{<1>} y^{<2>} \ldots y^{T_{y}})=P(y^{<1>} \mid x) P(y^{<2>} \mid x, y^{<1>}) \ldots P(y^{T_{y}} \mid x, y^{<1>} \ldots, y^{T_{y}-1})$.</1></1></2></1></2></1></p><p>每个概率都小于1甚至远远小于1，很多概率相乘起来，会得到很小很小的数字，会造成数据下溢，即数值太小，计算机的浮点表示不能精确储存。</p><p><strong>解决</strong>：<strong>将最大化的乘积式取对数</strong>，由 $\log M^{*} N=\log M+\log N$ 公式可得，上述需要最大化的王积式可以转化为:</p><p><img src="https://i.loli.net/2021/04/07/8EfxMgsC43zmqSb.png" alt="image-20210407205434424" style="zoom: 50%;"></p><p>即乘积的log变成了log的求和，最大化这个log的求和值能够得到同样的结果，并且不会出现 数值下溢和四舍五入。</p><h3 id="4-2-倾向于生成短的序列"><a href="#4-2-倾向于生成短的序列" class="headerlink" title="4.2 倾向于生成短的序列"></a>4.2 倾向于生成短的序列</h3><p>生成的句子序列越长，对数概率相加的结果就越小（越为负值）, 所以倾向于生成短序列。 对序列长度进行惩罚，降低生成短序列的倾向。</p><p><strong>解决方法：</strong> 对数概率相加的结果, 除以序列长度 $L$ 。<br>实践中，通常采用更柔和的方法, 在 $L$ 上加上指数 $a \in(0,1),$ 即 $L^{a},$ 例如 $a=0.7$ 。如果 $a=1, \quad L^{a}=L$ 就相当于完全用长度来归一化; 如果 $a=0, \quad L^{a}=1$ 就相当于完全没有 归一化, $a \in(0,1)$ 就是在完全归一化和没有归一化之间。</p><p>或者更加复杂一点：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">norm</span> = self.opt.beam_search_norm</span><br><span class="line"><span class="attr">candidate_logprob</span> = (beam_logprobs_sum[q] * t ** norm + local_logprob) / ((t+<span class="number">1</span>) ** norm)</span><br></pre></td></tr></table></figure><h3 id="4-3-单一性问题"><a href="#4-3-单一性问题" class="headerlink" title="4.3 单一性问题"></a><strong>4.3 单一性问题</strong></h3><p>beam search 有一个大问题是输出的 $B$ 个句子的差异性很小，无法体现语言的多样性（比如文本摘要、机器翻译的生成文本，往往有不止一种表述方式）。</p><p><strong>解决方法：</strong> 分组 加入相似性惩罚。diverse beam search 来自<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.02424.pdf" target="_blank" rel="noopener">论文</a></p><p>具体如下：选择 Beam size 为 $B$，然后将其分为 $G$组，每一组就有 $B/G$个beam。每个单独的组内跟 beam search很像，不断延展序列。同时通过引入一个dissimilarity 项来保证组与组之间有差异。</p><p><img src="https://i.loli.net/2021/03/15/VQDxijZcGtzlEph.png" alt="image-20210315161115581" style="zoom: 50%;"></p><p>如上图所示，B = 6, G=3，每一组的beam width为2。</p><p>组内与 beam search 很像：从t-1到 t 时刻，不断的减少搜索空间（如同beam search一样）。</p><p>组间差异：对于t=4时刻，我们先对第一组输出y（t=4），然后我们开始对第二组输出y（t=4），但是第二组y（t=4）的score不仅取决于第二组之前的y（t=3），也取决于其与第一组的相似程度。以此类推，在t=4时刻对于第三组的输出，我们从上图可以看到其score的打分标准。这儿对于其 dissimilarity 项的计算采用的办法是 hamming diversity，这个理解起来很简单，比如这个时刻可能输出的词在上面的组出现过，我们就对这个词的分数-1，如果这个时刻可能输出的词在上面组没有出现过，我们就对这个词的分数不惩罚。</p><ul><li><strong>DBS算法：</strong></li></ul><p><img src="https://i.loli.net/2021/03/15/DdS1XPZvoEptVgw.png" alt="image-20210315161139107" style="zoom: 50%;"></p><p>DBS算法</p><ul><li><strong>附：</strong>很多论文里有对 beam search的改进，主要是针对生成序列的<strong>多样性</strong>的。多样性问题，在对话里很常见。</li></ul><h2 id="5-其他相关问题："><a href="#5-其他相关问题：" class="headerlink" title="5. 其他相关问题："></a><strong>5. 其他相关问题</strong>：</h2><h3 id="5-1-训练的时候需要-Beam-Search-吗？"><a href="#5-1-训练的时候需要-Beam-Search-吗？" class="headerlink" title="5.1 训练的时候需要 Beam Search 吗？"></a><strong>5.1 训练的时候需要 Beam Search 吗？</strong></h3><p>不需要。因为训练的时候知道每一步的正确答案，没必要进行这样的搜索。</p><h3 id="5-2-为什么不用贪心搜索"><a href="#5-2-为什么不用贪心搜索" class="headerlink" title="5.2 为什么不用贪心搜索?"></a>5.2 为什么不用贪心搜索?</h3><p>贪心搜索相当于 Beam Search 中 B=1的情况，每次只选择概率最大的词，容易陷入局部最优，但我们真正需要的是一个序列，我们希望整个序列的概率最大。</p><h3 id="5-3-维特比算法"><a href="#5-3-维特比算法" class="headerlink" title="5.3 维特比算法"></a>5.3 维特比算法</h3><p>维特比算法是用动态规划的思想。简单来说就是：从开始状态之后每走一步，就记录下<strong>到达该状态的所有路径的概率最大值</strong>，然后以此最大值为基准继续向后推进。显然，如果这个最大值都不能使该状态成为最大似然状态路径上的结点的话，那些小于它的概率值（以及对应的路径）就更没有可能了。</p><p>Beam Search与Viterbi算法虽然都是解空间的剪枝算法，但它们的思路是不同的。Beam Search是对状态迁移的路径进行剪枝，而 Viterbi 算法是合并不同路径到达同一状态的概率值，用最大值作为对该状态的充分估计值，从而在后续计算中，忽略历史信息（这种以偏概全也就是所谓的Markov性），以达到剪枝的目的。<br>从状态转移图的角度来说，Beam Search是空间剪枝，而Viterbi算法是时间剪枝。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</title>
      <link href="2021/03/15/Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering/"/>
      <url>2021/03/15/Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering/</url>
      
        <content type="html"><![CDATA[<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>人类视觉系统存在两种attention机制。Top-down attention由当前任务所决定，我们会根据当前任务（即VQA中的问题），聚焦于与任务紧密相关的部分。Bottom-up attention指的是我们会被显著的、突出的、新奇的事物给吸引。</p><p>以前的方法用到的visual attention mechanisms大都属于top-down类型，即取问题作为输入，建模attention分布，然后作用于CNN提取的图像特征（image features）。然而，这种方法的attention作用的图像对应于下图的左图，没有考虑图片的内容。对于人类来说，注意力会更加集中于图片的目标或其他显著区域，所以作者引进Bottom-up attention机制，如下图的右图所示，attention作用于object proposal。</p><p><img src="https://i.loli.net/2021/03/15/p8WFURqkGw6iIfC.png" alt="image-20210315150105258" style="zoom:50%;"></p><h2 id="Basic-idea"><a href="#Basic-idea" class="headerlink" title="Basic idea"></a>Basic idea</h2><p>Bottom-Up注意力机制: 即基于目标（objects）或显著区域（salient image regions）来计算attention。具体来说，bottom-up机制基于Faster R-CNN，得到图片中每个目标或显著区域的特征向量（feature vector）表示。</p><p>Top-Down机制: 取question作为输入，建模特征权重（feature weightings）或者说attention分布。</p><h2 id="概括："><a href="#概括：" class="headerlink" title="概括："></a>概括：</h2><p><strong>（1）Bottom-Up</strong><br>使用Faster R-CNN 中的R-CNN来得到object feature。<br><strong>（2）Top-Down Attention</strong><br>得到了该层的隐层状态，并与object features  中的每一个<strong>v<sub>i</sub></strong>来计算一个attention 系数。<br><strong>（3）对object features 进行attention 权重求和</strong><br>得到image feature<br><strong>（4）Decoder：language LSTM</strong><br>输出预测单词</p><p><img src="https://i.loli.net/2021/03/15/IcK4HYAsGwjzPqv.jpg" alt="img" style="zoom: 67%;"><br><img src="https://i.loli.net/2021/03/15/lqZx5tngfd9r2Gs.jpg" alt="img" style="zoom:50%;"></p><h2 id="Bottom-Up"><a href="#Bottom-Up" class="headerlink" title="Bottom-Up"></a>Bottom-Up</h2><ul><li><strong>主要介绍一下Faster R-CNN 的训练过程</strong><br>（1）首先Resnet-101 是在ImageNet上预训练的<br>（2）Faster R-CNN在MS COCO上进行预训练<br>rpn 的score classification loss，bbox regression loss<br>r-cnn 的score classification loss，bbox regression loss<br>（3）Faster R-CNN在Visual Genome上再进行预训练<br>为了得到更好的特征表达，增加一个预测属性的输出： </li><li><strong>具体的网络：</strong><br>To predict attributes for region i, we concatenate the mean  pooled convolutional feature vi with a learned embedding  of the ground-truth object class, and feed this into an additional output layer defining a softmax distribution over each  attribute class plus a ‘no attributes’ class.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gumbel-Softmax Trick和Gumbel分布</title>
      <link href="2021/03/15/Gumbel-Softmax-Trick%E5%92%8CGumbel%E5%88%86%E5%B8%83/"/>
      <url>2021/03/15/Gumbel-Softmax-Trick%E5%92%8CGumbel%E5%88%86%E5%B8%83/</url>
      
        <content type="html"><![CDATA[<h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li><p>由于最近看到的几篇论文中都有提及到gumble softmax的操作，因此想要具体了解一下。</p></li><li><p>用到gumbel softmax 的论文包括以下几篇</p><ul><li><p>解决不可微分问题</p><p>【ICCV 2019】Learning to Assemble Neural Module Tree Networks for Visual Grounding</p><p>【arXiv: 2101.12059v1】VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs</p><p>【arXiv: 2103.08862】Gumbel-Attention for Multi-modal Machine Translation</p><blockquote><p><strong>Categorical reparameterization with gumbel-softmax.</strong>  ICLR 2017</p></blockquote></li><li><p>在概率分布上添加gumble noise，再从新的概率分布上以概率检索样本</p><p> Bridging the Gap between Training and Inference for Neural Machine Translation</p><blockquote><p><strong>A* sampling.</strong> NIPS 2017</p></blockquote></li></ul></li></ul><p>来源:  <a href="https://www.cnblogs.com/initial-h/p/9468974.html" target="_blank" rel="noopener">https://www.cnblogs.com/initial-h/p/9468974.html</a></p><p>之前看MADDPG论文的时候，作者提到在离散的信息交流环境中，使用了Gumbel-Softmax estimator。于是去搜了一下，发现该技巧应用甚广，如深度学习中的各种GAN、强化学习中的A2C和MADDPG算法等等。只要涉及在离散分布上运用重参数技巧时(re-parameterization)，都可以试试Gumbel-Softmax Trick。</p><p>  这篇文章是学习以下链接之后的个人理解，内容也基本出于此，需要深入理解的可以自取。</p><ul><li><a href="http://amid.fish/humble-gumbel" target="_blank" rel="noopener">The Humble Gumbel Distribution</a></li><li><a href="https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/" target="_blank" rel="noopener">The Gumbel-Max Trick for Discrete Distributions</a></li><li><a href="https://casmls.github.io/general/2017/02/01/GumbelSoftmax.html" target="_blank" rel="noopener">The Gumbel-Softmax Trick for Inference of Discrete Variables</a></li><li><a href="https://www.zhihu.com/question/62631725/answer/201338234" target="_blank" rel="noopener">如何理解Gumbel-Max trick？</a></li></ul><p>  这篇文章从直观感觉讲起，先讲Gumbel-Softmax Trick用在哪里及如何运用，再编程感受Gumbel分布的效果，最后讨论数学证明。</p><h2 id="一、Gumbel-Softmax-Trick用在哪里"><a href="#一、Gumbel-Softmax-Trick用在哪里" class="headerlink" title="一、Gumbel-Softmax Trick用在哪里"></a>一、Gumbel-Softmax Trick用在哪里</h2><h3 id="问题来源"><a href="#问题来源" class="headerlink" title="问题来源"></a>问题来源</h3><p>  通常在强化学习中，如果动作空间是离散的，比如上、下、左、右四个动作，通常的做法是网络输出一个四维的one-hot向量(不考虑空动作)，分别代表四个动作。比如 [1,0,0,0] 代表上，[0,1,0,0] 代表下等等。而具体取哪个动作呢，就根据输出的每个维度的大小，选择值最大的作为输出动作, 即argmax(v)。</p><p>  例如网络输出的四维向量为v=[−20,10,9.6,6.2]，第二个维度取到最大值10，那么输出的动作就是[0,1,0,0]，也就是下，这和多类别的分类任务是一个道理。但是这种取法有个问题是不能计算梯度，也就不能更新网络。通常的做法是加softmax函数，把向量归一化，这样既能计算梯度，同时值的大小还能表示概率的含义。softmax函数定义：$\sigma\left(z_{i}\right)=\frac{e^{z_{i}}}{\sum_{j=1}^{K} e^{z_{j}}}$</p><p>  那么将v=[−20,10,9.6,6.2]通过softmax函数后有σ(v)=[0,0.591,0.396,0.013]，这样做不会改变动作或者说类别的选取，同时softmax倾向于让最大值的概率显著大于其他值，比如这里10和9.6经过softmax放缩之后变成了0.591和0.396，6.2对应的概率更是变成了0.013，这有利于把网络训成一个one-hot输出的形式，这种方式在分类问题中是常用方法。</p><p>  但是这么做还有一个问题，这个表示概率的向量σ(v)=[0,0.591,0.396,0.013]并没有真正显示出概率的含义，因为一旦某个值最大，就选择相应的动作或者分类。比如σ(v)=[0,0.591,0.396,0.013]和σ(v)=[0,0.9,0.1,0]在类别选取的结果看来没有任何差别，都是选择第二个类别，但是从概率意义上讲差别是巨大的。所以需要一种方法不仅选出动作，而且遵从概率的含义。</p><p>  很直接的方法是依概率分布采样就完事了，比如直接用<code>np.random.choice</code>函数依照概率生成样本值，这样概率就有意义了。这样做确实可以，但是又有一个问题冒了出来：这种方式怎么计算梯度？不能计算梯度怎么用BP的方式更新网络？</p><p>  这时重参数(re-parameterization)技巧解决了这个问题，<a href="https://casmls.github.io/general/2017/02/01/GumbelSoftmax.html" target="_blank" rel="noopener">这里</a>有详尽的解释，不过比较晦涩。简单来说重参数技巧的一个用处是把采样的步骤移出计算图，这样整个图就可以计算梯度BP更新了。之前我一直在想分类任务直接softmax之后BP更新不就完事了吗，为什么非得采样。后来看了VAE和GAN之后明白，还有很多需要采样训练的任务。这里举简单的VAE(变分自编码器)的例子说明需要采样训练的任务以及重参数技巧，详细内容来自<a href="https://www.bilibili.com/video/av20165127" target="_blank" rel="noopener">视频</a>和<a href="http://kvfrans.com/variational-autoencoders-explained/" target="_blank" rel="noopener">博客</a>。</p><h3 id="Re-parameterization-Trick"><a href="#Re-parameterization-Trick" class="headerlink" title="Re-parameterization Trick"></a>Re-parameterization Trick</h3><p>  最原始的自编码器通常长这样：</p><p><img src="C:\Users\shiyaya\Desktop\1428973-20180813165000500-1207992534.jpg" alt="img" style="zoom: 67%;"></p><p>  左右两边是端到端的出入输出网络，中间的绿色是提取的特征向量，这是一种直接从图片提取特征的方式。<br>  而VAE长这样:</p><p><img src="https://i.loli.net/2021/03/18/yO5L9kCMPhrwJub.png" alt="image-20210318135149420"></p><p>  VAE的想法是不直接用网络去提取特征向量，而是提取这张图像的分布特征，也就把绿色的特征向量替换为分布的参数向量，比如说均值和标准差。然后需要decode图像的时候，就从encode出来的分布中采样得到特征向量样本，用这个样本去重建图像，这时怎么计算梯度的问题就出现了。<br>  重参数技巧可以解决这个问题，它长下面这样:</p><p><img src="https://i.loli.net/2021/03/18/27e1EIhVFfWyJKi.png" alt="img" style="zoom:67%;"></p><p>假设图中的 $x$ 和 $\phi$ 表示VAE中的均值和标准差向量, 它们是确定性的节点。而需要输出的样本 $z$ 是带有随机性的节点， 重参数就是把带有随机性的 $z$ 变成确定性的节点, 同时随机性用另一个输入节点 $\epsilon$ 代替。<br>例如，这里用正态分布采样, 原本从均值为 $x$ 和标准差为 $\phi$ 的正态分布 $N\left(x, \phi^{2}\right)$ 中采样得到 $z_{\circ}$ 现在将其转化成从标准正态分布 $N(0,1)$ 中采样得到 $\epsilon$ , 再计算得到 $z=x+\epsilon \cdot \phi_{\circ}$ 这样一来, 采样的过程移出了计算图, 整张计算图就可以计算梯度进行更新了，而新加的 $\epsilon$ 的输入分支不 做更新，只当成一个没有权重变化的输入。</p><p>到这里，需要采样训练的任务实例以及重参数技巧基本有个概念了。</p><h3 id="Gumbel-Softmax-Trick"><a href="#Gumbel-Softmax-Trick" class="headerlink" title="Gumbel-Softmax Trick"></a>Gumbel-Softmax Trick</h3><p>VAE的例子是一个连续分布(正态分布)的重参数，离散分布的情况也一样，首先需要可以采样，使得离散的概率分布有意义而不是只取概率最大的值，其次需要可以计算梯度。那么怎么做到的，具体操作如下：</p><p>对于n维概率向量$\pi$, 对$\pi$对应的离散随机变量 $x_{\pi}$ 添加Gumbel噪声，再取样$x_{\pi}=\arg \max \left(\log \left(\pi_{i}\right)+G_{i}\right)$<br>其中, $G_{i}$ 是独立同分布的标准Gumbel分布的随机变量，标准Gumbel分布的CDF为$F(x)=e^{-e^{-x}}$</p><p>这就是<strong style="color:red;">Gumbel-Max trick</strong>。可以看到由于这中间有一个argmax操作，这是不可导的。所以用softmax函数代替之，也就是<strong style="color:red;">Gumbel-Softmax Trick</strong>，而$G_{i}$ 可以通过Gumbel分布求逆从均匀分布生成，即 $G_{i}=-\log \left(-\log \left(U_{i}\right)\right), U_{i} \sim U(0,1)$</p><p>算法流程如下：</p><ul><li><p>对于网络输出的一个 $n$ 维向量 $v$ （predict logits）, 生成 $n$ 个服从均匀分布 $U(0,1)$ 的独立样本 $\epsilon_{1}, \ldots, \epsilon_{n}$</p></li><li><p>通过 $G_{i}=-\log \left(-\log \left(\epsilon_{i}\right)\right)$ 计算得到 $G_{i}$</p></li><li><p>对应相加得到新的值向量 $v^{\prime}=\left[v_{1}+G_{1}, v_{2}+G_{2}, \ldots, v_{n}+G_{n}\right]$</p></li><li><p>通过Softmax函数</p></li></ul><script type="math/tex; mode=display">\sigma_{\tau}\left(v_{i}^{\prime}\right)=\frac{e^{v_{i}^{\prime} / \tau}}{\sum_{j=1}^{n} e^{v_{j}^{\prime} / \tau}}</script><p>计算概率大小得到最终的类别。其中 $\tau$ 是温度参数。</p><p>temperature控制着softmax的soft程度，温度越高，生成的分布越平滑（接近这里的均匀分布）；温度越低，生成的分布越接近离散的one-hot分布（argmax）。因此，<strong>训练时可以逐渐降低温度，以逐步逼近真实的离散分布。</strong></p><p><strong>yaya: 其实这里的 gumbel softmax 不是针对网络输出$v$ 进行一个离散采样，而是对 $v$ 添加噪声之后，再取softmax，而后得到新的概率分布，而这个新的概率分布也不是去argmax, 而是对于每个 $v_i$ 重新分配一个概率权重。其优点在于可以调控 temperature， 当温度低时，近似做了argmax 采样</strong></p><p>直观上来说，Gumbel-Softmax就是在原来的输出上加入了一个噪声，对于强化学习来说，在选择动作之前加一个Gumbel扰动，相当于增加了探索度，感觉上是合理的，而同时他又能保证采样是对原分布的逼近。对于深度学习的任务来说，添加随机性去模拟分布的样本生成，也是合情合理的。</p>]]></content>
      
      
      <categories>
          
          <category> 杂类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Object Relational Graph with Teacher-Recommended Learning for Video Captioning</title>
      <link href="2021/03/13/Object-Relational-Graph-with-Teacher-Recommended-Learning-for-Video-Captioning/"/>
      <url>2021/03/13/Object-Relational-Graph-with-Teacher-Recommended-Learning-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> video captioning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal,video captioning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[VATEX Captioning Challenge 2019] Multi-modal Information Fusion and Multi-stage Training Strategy for Video Captioning</title>
      <link href="2021/03/13/VATEX-Captioning-Challenge-2019-Multi-modal-Information-Fusion-and-Multi-stage-Training-Strategy-for-Video-Captioning/"/>
      <url>2021/03/13/VATEX-Captioning-Challenge-2019-Multi-modal-Information-Fusion-and-Multi-stage-Training-Strategy-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> video captioning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal,video captioning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多模态人工智能</title>
      <link href="2021/03/12/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
      <url>2021/03/12/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/</url>
      
        <content type="html"><![CDATA[<p>多模态：视觉，语音，自然语言</p><p><img src="https://i.loli.net/2021/03/12/8ObjKzRVd4UXgS3.png" alt="image-20210312202150477"></p><p><img src="https://i.loli.net/2021/03/12/IqFHGuL6ZYOKwXb.png" alt="image-20210312202209438"></p><p><img src="https://i.loli.net/2021/03/12/CsF8TR3m9UlS54N.png" alt="image-20210312202229192"></p><p><img src="https://i.loli.net/2021/03/12/HbPD8yaB3GU7IAk.png" alt="image-20210312202108780"></p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering</title>
      <link href="2021/03/12/Learning-to-Contrast-the-Counterfactual-Samples-for-Robust-Visual-Question-Answering/"/>
      <url>2021/03/12/Learning-to-Contrast-the-Counterfactual-Samples-for-Robust-Visual-Question-Answering/</url>
      
        <content type="html"><![CDATA[<p>转自：<a href="https://blog.csdn.net/weixin_45347379/article/details/112182143" target="_blank" rel="noopener">https://blog.csdn.net/weixin_45347379/article/details/112182143</a></p><p>学习对比反事实样本，以实现稳健的视觉问答<br>Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering<br>在阅读本文之前，一定要阅读论文：Counterfactual Samples Synthesizing for Robust Visual Question Answering（简称CSS）</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="https://i.loli.net/2021/03/12/gODk8QFCNWaUsVP.png" alt="image-20210312175656070"></p><p>文章的方法主要包括三个部分：（1）一个基本的VQA模型。（2）一个事实和反事实样本合成（CSS）模块。（3）一个对比学习（CL）目标。</p><h4 id="第一部分和第二部分"><a href="#第一部分和第二部分" class="headerlink" title="第一部分和第二部分"></a><strong>第一部分和第二部分</strong></h4><p>属于CSS已经实现的，主要作用在于：</p><p>（1）并通过多分类的方法预测答案，并产生图中右上方基本VQAloss。</p><p><img src="https://i.loli.net/2021/03/12/z3Q52bActwqMhov.png" alt="在这里插入图片描述"></p><p>（2）得到（I, I+, I-）和（Q, Q+, Q-），</p><p><img src="https://i.loli.net/2021/03/12/din9DctNLV1EORo.png" alt="在这里插入图片描述"></p><h4 id="第三部分"><a href="#第三部分" class="headerlink" title="第三部分"></a><strong>第三部分</strong></h4><p>以（I, I+, I-）为例，将（I, I+, I-）和Q喂给VQA模型，分别产生原始样本的嵌入mm（V, Q）作为anchor（a），事实样本的嵌入mm(V+, Q)作为positive（p），反事实样本嵌入mm(V-, Q)作为negati（n）<br>利用余弦相似度作为评分函数，对正样本输出高值，对负样本输出低值，公式如下：</p><p><img src="https://i.loli.net/2021/03/12/XIjKWu7szqi3A4w.png" alt="在这里插入图片描述"></p><p>同样的方法得到anchor和negative之间的评分s(a, n), 这就相当于图中展示的，拉近原始图像与事实区域图像的关系，推远原始图像与反事实区域的距离。<br>对比损失定义为：（这就是图片下方得到的Contrastive loss）</p><p><img src="https://i.loli.net/2021/03/12/CY4OJ7PZmD9Eaud.png" alt="在这里插入图片描述"></p><p>最后，这种对比损失与基础分类损失的加权总和弥补了整体损失：</p><p><img src="https://i.loli.net/2021/03/12/8GL3BxlSEpAg951.png" alt="在这里插入图片描述"></p><p>虽然文章说，<strong style="color:red;">这种方法能够使模型学习他们之间的关系，并从更有因果关系的方面预测正确答案。</strong>但是，个人感觉如果仅仅使以上方法，并不能从理论上提高模型的能力。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p><img src="https://i.loli.net/2021/03/12/wAmbOSxRU97dN12.png" alt="在这里插入图片描述"></p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>看了本文博客之后，没有看原文，个人任务这种方法有限，</li><li>可能模型的设计上，是有新意的，使用对比学习来增强VQA模型的性能，但是往往自己做的时候会收效甚微</li></ul>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> VQA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal,VQA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Semantic Equivalent Adversarial Data Augmentation for Visual Question Answering</title>
      <link href="2021/03/12/Semantic-Equivalent-Adversarial-Data-Augmentation-for-Visual-Question-Answering/"/>
      <url>2021/03/12/Semantic-Equivalent-Adversarial-Data-Augmentation-for-Visual-Question-Answering/</url>
      
        <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>受到深度学习的快速发展，VQA 近年来取得了非常成功的进展。数据增强是深度学习中的一个有用的技巧，但是，目前很少有工作关注于VQA任务的数据增强。</p><p>对于image side: 一些简单的数据增强操作不能直接应用到VQA这一场景下，比如，rotation and flipping 等操作，都可能导致<image, question, answer> 这一结构的正确性遭到破坏。</image,></p><p>对于text side (eg: questions) , it is challenging to come up with <strong>generalized rules for language transformation.</strong> 另外，有一类任务是Visual Question Generation，根据image和 answer来生成问题，但是生成的问题常常是有语法错误的，而且，他们在同一个目标数据集上进行学习，生成的数据与原始数据的分布是一致的，因此，<strong>若使用这种方案来做数据扩充，难以解决过拟合问题</strong>(通常训练数据和测试数据不是同一个分布)。</p><p>在本文中，不直接对image或者是question进行操作，而是对images 和 questions生成对抗样本作为数据增强。增强的样本不会改变image的原始语义，也不会改变questions中的semantic meaning。对抗性示例是经过<strong>策略</strong>修改的样本，可以成功地欺骗深层模型以做出不正确的预测。这种修改是难以察觉的，<strong>它在使对抗性示例的基础分布远离原始数据的同时保持了数据的语义。</strong>本文是第一个同时对image 和 text进行数据扩充的方法（已有的方法只是单独对一方面进行数据扩充）。</p><p>进而，使用本文方法产生的<strong>数据增强样本</strong>和<strong>对抗训练</strong>来训练经典的VQA model (BUTD) 。</p><p>实验结果证明，不仅可以提高 VQAv2的整体性能，而且相比于baseline还可以有效抵抗对抗攻击。</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="https://i.loli.net/2021/03/12/rmyqUFXQewT2PBK.png" alt="image-20210312165449352" style="zoom:50%;"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="何时加入对抗样本"><a href="#何时加入对抗样本" class="headerlink" title="何时加入对抗样本"></a>何时加入对抗样本</h4><ul><li><p>本文发现，将干净样本和对抗样本进行混合，然后<strong>从头到尾</strong>的训练，这种方案不会在干净样本上收敛。因此本文只在特定的训练时期对样本进行混合，最后使用干净样本进行微调。</p><p>本文实验中max-epoch=25.</p><p><img src="https://i.loli.net/2021/03/12/YOblV6WT4rtMkSf.png" alt="image-20210312171853940" style="zoom: 50%;"></p><p>本文的解释：与干净样本相比，对抗样本与其有不同的分布。如果把提升模型在VQA任务上的性能作为我们的主要目标，那么模型在干净样本上的拟合能力需要<strong>在结束</strong>的时候to be retrieved。而<strong>在开始</strong>时，模型需要warm up，此时不适合加入对抗样本。因此在中间阶段加入融合对抗样本的训练。</p></li><li><p>实验证明本文提出的方法不仅可以提高在干净样本上的VQA任务的性能，还能提高<strong>在对抗样本上的鲁棒性</strong>。</p></li></ul><h4 id="相比于baseline还可以有效抵抗对抗攻击"><a href="#相比于baseline还可以有效抵抗对抗攻击" class="headerlink" title="相比于baseline还可以有效抵抗对抗攻击"></a>相比于baseline还可以有效抵抗对抗攻击</h4><p><img src="https://i.loli.net/2021/03/12/np4eUgXwkz2rK5M.png" alt="image-20210312172242341" style="zoom:50%;"></p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li><strong>何时加入对抗样本</strong> 这个实验告诉我们：一般情况，我们提出一种数据增强方案，通常会从头到尾的使用，但是未必是好的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> VQA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal,VQA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Behind the Scene] Revealing the Secrets of Pre-trained Vision-and-Language Models</title>
      <link href="2021/03/05/Behind-the-Scene-Revealing-the-Secrets-of-Pre-trained-Vision-and-Language-Models/"/>
      <url>2021/03/05/Behind-the-Scene-Revealing-the-Secrets-of-Pre-trained-Vision-and-Language-Models/</url>
      
        <content type="html"><![CDATA[<h3 id="本文研究的任务"><a href="#本文研究的任务" class="headerlink" title="本文研究的任务"></a>本文研究的任务</h3><p>最近Transformer-based 大规模预训练模型推动了 多模态任务的发展，比如，ViL-BERT，LXMERT and UNITER。然而，对于使它们取得成功的<strong>内部机制</strong>知之甚少。为了揭示内部机制，本文提出了VALUE（Vision-And-Language Understanding Evaluation），一组精心设计的<strong>探测任务</strong>（probing task, eg: Visual Coreference Resolution, Visual Relation Detection），可推广到标准的预训练V + L模型， 破译多模式预训练的内部运作方式（例如，在各个attention heads 中获得的隐性知识，通过上下文化多模式嵌入学习的 inherent cross-modal alignment）。</p><h3 id="本文的实验发现"><a href="#本文的实验发现" class="headerlink" title="本文的实验发现"></a>本文的实验发现</h3><p>经由这些探测任务，通过对每个原型模型体系结构的广泛分析，我们的主要观察结果是：（i）预训练的模型在推理过程中表现出对文本而不是图像的关注。 （ii）存在专门为捕获cross-modal interactions 而设计的 a subset of attetion heads（iii）在多个预训练模型中学习的注意力矩阵显示出图像区域和文本单词之间的<strong>潜在对齐</strong>，表现出<strong>一致的模式</strong>。 （iv）绘制的注意力模式(attention patern)揭示了图像区域之间的视觉可解释的关系。 （v）纯粹的语言知识也被有效地编码在注意力集中。 这些宝贵的见解可指导未来的工作，以设计更好的模型架构和多模式预训练的目标。</p><h3 id="1-What-is-the-correlation-between-multimodal-fusion-and-the-number-of-layers-in-pre-trained-models"><a href="#1-What-is-the-correlation-between-multimodal-fusion-and-the-number-of-layers-in-pre-trained-models" class="headerlink" title="1. What is the correlation between multimodal fusion and the number of layers in pre-trained models?"></a>1. What is the correlation between multimodal fusion and the number of layers in pre-trained models?</h3><p>可视化每层的embeddings (regions and tokens), 并使用 k-means算法来聚类（k=2），然后measure the difference between the formed clusters and ground-truth visual/textual clusters via Normalized Mutual Information (NMI), an unsupervised metric for evaluating differences between clusters.</p><ul><li>ground-truth visual/textual clusters, 是什么？？？？</li></ul><p><img src="https://i.loli.net/2021/04/04/r24Dx6oI7qfG8dB.png" alt="image-20210404171353836" style="zoom:50%;"></p><p><strong>结论</strong>： For (a) Multimodal Fusion Degree, clustering analysis between image and text representations shows that in single-stream models like UNITER, as the network layers go deeper, the fusion between two modalities becomes more intertwined. However, the opposite phenomenon is observed in two-stream models like LXMERT.</p><h3 id="2-Which-modality-plays-a-more-important-role-that-drives-the-pre-trained-model-to-make-final-predictions"><a href="#2-Which-modality-plays-a-more-important-role-that-drives-the-pre-trained-model-to-make-final-predictions" class="headerlink" title="2. Which modality plays a more important role that drives the pre-trained model to make final predictions?"></a>2. Which modality plays a more important role that drives the pre-trained model to make final predictions?</h3><p><strong>结论：</strong>For (b) Modality Importance, by analyzing the attention trace of the [CLS] token, which is commonly considered as containing the intended fused multimodal information and often used as the input signal for downstream tasks, we nd that the nal predictions tend to depend more on textual input rather than visual input.</p><h3 id="3-What-knowledge-is-encoded-in-pre-trained-models-that-supports-cross-modal-interaction-and-alignment"><a href="#3-What-knowledge-is-encoded-in-pre-trained-models-that-supports-cross-modal-interaction-and-alignment" class="headerlink" title="3. What knowledge is encoded in pre-trained models that supports cross-modal interaction and alignment?"></a>3. What knowledge is encoded in pre-trained models that supports cross-modal interaction and alignment?</h3><p><strong>结论：</strong>For (c) Cross-modal Interaction, we propose a Visual Coreference Resolution task to probe its encoded knowledge.</p><h3 id="4-What-knowledge-has-been-learned-for-image-to-image-intra-modal-interaction-i-e-visual-relations"><a href="#4-What-knowledge-has-been-learned-for-image-to-image-intra-modal-interaction-i-e-visual-relations" class="headerlink" title="4. What knowledge has been learned for image-to-image (intra-modal) interaction (i.e., visual relations)?"></a>4. What knowledge has been learned for image-to-image (intra-modal) interaction (i.e., visual relations)?</h3><p>For (d) Image-to-image Interaction, we conduct analysis via Visual Relation Detection between two image regions.</p><h3 id="5-Compared-with-BERT-do-pre-trained-V-L-models-eectively-encode-linguistic-knowledge-for-text-to-text-intra-modal-interaction"><a href="#5-Compared-with-BERT-do-pre-trained-V-L-models-eectively-encode-linguistic-knowledge-for-text-to-text-intra-modal-interaction" class="headerlink" title="5. Compared with BERT, do pre-trained V+L models eectively encode linguistic knowledge for text-to-text (intra-modal) interaction?"></a>5. Compared with BERT, do pre-trained V+L models eectively encode linguistic knowledge for text-to-text (intra-modal) interaction?</h3><p>结论：For (e) Text-to-text Interaction, we evaluate the linguistic knowledge encoded in each layer of the tested model with SentEval tookit [8], and compare with the original BERT [9]. Experiments show that both single- and two-stream models, especially the former, can well capture cross-modal alignment, visual relations, and linguistic knowledge.</p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[COCO-LM] Correcting and Contrasting Text Sequences for Language Model Pretraining</title>
      <link href="2021/03/04/COCO-LM-Correcting-and-Contrasting-Text-Sequences-for-Language-Model-Pretraining/"/>
      <url>2021/03/04/COCO-LM-Correcting-and-Contrasting-Text-Sequences-for-Language-Model-Pretraining/</url>
      
        <content type="html"><![CDATA[<p>转自：<a href="https://zhuanlan.zhihu.com/p/353624306" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/353624306</a></p><p>该篇文章2021年2月16日上传，提出了一种新的预训练模型的框架，个人认为<strong>COCO-LM结合了许多当下比较新进的思想，在后bert时代，一定程度上突破了对BERT模型传统的预训练方法</strong>。</p><p>We present COCO-LM, a new self-supervised learning framework that pretrains Language Models by COrrecting challenging errors and COntrasting text sequences. COCO-LM employs an auxiliary language model to mask-and-predict tokens in original text sequences. It creates more challenging pretraining inputs, where noises are sampled based on their likelihood in the auxiliary language model. COCO-LM then pretrains with two tasks: <strong style="color:blue;">The first task, corrective language modeling</strong>, learns to correct the auxiliary model’s corruptions by recovering the original tokens. <strong style="color:blue;">The second task, sequence contrastive learning</strong>, ensures that the language model generates sequence representations that are invariant to noises and transformations. In our experiments on the GLUE and SQuAD benchmarks, COCO-LM outperforms recent pretraining approaches in various pretraining settings and few-shot evaluations, with higher pretraining efficiency. Our analyses reveal that COCO-LM’s advantages come from its challenging training signals, more contextualized token representations, and regularized sequence representations.</p><p><img src="https://i.loli.net/2021/03/04/xOA857rdWJktsEY.png" alt="FireShot Capture 018 -  - arxiv.org"></p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>在标准语言模型预训练框架内，可以观察到PLM在下游任务上的the empirical performance 仅随着参数大小和预训练成本的指数增长而线性提高，<strong>这是不可持续的</strong>，因为PLM已达到数万亿个参数。</p><p>最近的研究揭示了现有预训练框架的某些固有局限性，这些局限性可能导致这种亚线性效率（sublinear efficiency）。【1】一个挑战是，<strong>使用随机更改的文本</strong>（<em>例如</em>，randomly masked tokens）进行预训练会<strong>产生许多非信息性信号</strong>，经过一定程度的预训练后它们不再有用。【2】另外一个挑战是，在 token level 进行预训练不会在 sequence level 上显式学习语言语义，并且在预训练过程中，<strong>Transformers可能无法有效地推广到 higher level 的语义 。</strong></p><p>在本文中，我们旨在通过一个<strong>新的自我监督学习框架</strong>COCO-LM来克服这些限制。该框架通过使用 more challenging noises 来 COrrecting and COntrasting text sequences，进而预训练语言模型。</p><p>【1】leverages an auxiliary language model，to corrupt text sequences by <strong>sampling more contextually plausible noises</strong> from its masked language modeling (MLM) probability. COCO-LM revives a language modeling task, corrective language modeling (CLM), which pretrains the Transformer to <strong>not only detect the challenging noises in the corrupted texts, but also correct them via a multi-task setting.</strong></p><p>【2】To improve the learning of sequence level semantics, COCOLM introduces a sequence level pretraining task, sequence contrastive learning (SCL), that uses contrastive learning to enforce the pretraining model to <strong>align the corrupted text sequence and its cropped original sequence close</strong> in the representation space, while away from other random sequences.</p><h3 id="COCO-LM框架延续了ELECTRA预训练模型的思想"><a href="#COCO-LM框架延续了ELECTRA预训练模型的思想" class="headerlink" title="COCO-LM框架延续了ELECTRA预训练模型的思想"></a>COCO-LM框架延续了ELECTRA预训练模型的思想</h3><p>ELECTRA预训练模型主要应用了GAN对抗神经网络的思想，不了解的小伙伴们可以参考一些其他资料，这里我简单说一下我的理解。</p><p>GAN对抗神经网络在CV领域上应用比较成熟，在CV的应用上GAN主要包括两个神经网络模型：一个是生成式模型G，一个是判别式模型D。生成式模型的作用是通过随机噪声生成和原始样本相似的数据（注意这里是通过随机噪声），判别式模型的作用是判断给定的实例是真实实例还是人为伪造的（也就是生成式模型所生成的）。那么这里就包含了对抗的思想，即生成式模型的目的是能够生成欺骗判别式模型的实例，判别式模型的目的是判别给定的实例是否是人为伪造的。</p><p>ELECTRA当中引用了这样的“对抗”思想，将判别式模型引入到了模型的预训练之中。像BERT、ROBERTA、XLNET等等预训练模型都属于生成式模型，在输入上用 [MASK] 遮蔽掉部分 tokens，再训练一个模型以重建出原始的 tokens。而ELECTRA预训练模型使用了判别式模型，其效果也出乎意料的好。</p><blockquote><p>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</p></blockquote><p>ELECTRA模型的主要思想也是包括了两个神经网络模型：一个生成式模型G，一个生成式模型D。生成式模型G是MLM（Masked Language Model）模型，给定一个真实样例（GAN的生成式模型给定的是随机噪声），用 [MASK] 遮蔽掉部分 tokens，生成替换的tokens；判别式模型D判断输入中每个 token 是否是由生成器生成。其过程如图所示：</p><p><img src="https://i.loli.net/2021/03/04/V7kIJGSo3L1m5ua.png" alt="image-20210304111201636" style="zoom:50%;"></p><p>通过实验表明这种新的预训练任务比 MLM 更高效，该任务定义于全部的输入 tokens，而非仅仅被遮蔽掉的那一部分小小的输入子集。</p><p>在COCO-LM模型中<strong>Corrective Language Modeling (CLM)</strong>也延续了这样的思想。</p><h3 id="COCO-LM模型引入了对比学习的思想"><a href="#COCO-LM模型引入了对比学习的思想" class="headerlink" title="COCO-LM模型引入了对比学习的思想"></a>COCO-LM模型引入了对比学习的思想</h3><p>我认为是非常非常棒的创新点。最近刚好再看对比学习的相关paper，更多的是在CV领域中使用了对比学习，而COCO-LM刚好将对比学习带入到了NLP领域中。</p><p>什么是对比学习呢？</p><p>对比学习是一种自监督的学习方法。其主要思想我的理解是，把正样本距离拉近，正样本与负样本距离拉远。对比学习的例子如下：</p><ul><li>给每个例子绘制两个独立的增强函数</li><li>使用两种增强机制，为每个示例生成两个互相关联的视图</li><li>让相关视图互相吸引，同时排斥其他示例</li></ul><p><img src="https://i.loli.net/2021/03/04/A3wEY1gPnTxIOaQ.jpg" alt="SimCLR论文解读- 知乎" style="zoom:33%;"></p><p>如上图，（Z1，Z2），（Z3，Z4）…（Z2n-1，Z2n）这些可以看作正例对，而Z1可以与除Z1、Z2的任何实例组成负例对，如（Z1，Z3）（Z1，Z4）等等。那么这样一个实例X，在一个大小为N的batch里便可以产生一个正例，以及N-1个负例，那么这个 loss 就可以看做是一个 N 分类问题，实际上就是一个交叉熵，由此可以进行网络模型的训练。</p><p>以上是我认为COCO-LM框架比较出色的地方，框架的一些细节还需要进一步的理解，之后会进一步的更新，欢迎知乎各位巨佬一起讨论~</p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li><p>在预训练任务中，引入了对抗扰动，且通过对比学习的思想来训练。</p></li><li><p>这种扰动+对比学习的思想，在vision-text pretraining model 中是否有使用？</p></li><li>本篇的idea 是如何来的，motivation 是什么？</li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Machine Translation with universal Visual Representation</title>
      <link href="2021/03/03/Neural-Machine-Translation-with-universal-Visual-Representation/"/>
      <url>2021/03/03/Neural-Machine-Translation-with-universal-Visual-Representation/</url>
      
        <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>为了降低Multi-modal NMT对有图像标注的翻译数据集的依赖，本文提出通过建立Topic-image Lookup Table的方式更高效地利用已有图像文本数据，并且在训练和测试NMT的时候通过Image Retrieval的方式获得图像信息，从而在更大规模的数据上训练Multi-modal NMT。</p><p>通过Retrieval的方式来扩充数据的工作其实有很多，比如这篇：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1904.02331" target="_blank" rel="noopener">Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation</a>。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>长期以来，机器翻译都只涉及到文本之间的转换，但实际上，人的感知功能可以是“多模态”的。</p><p><strong>本文提出一种通用的视觉表征，将图片信息融合到机器翻译模型中。</strong></p><p>使用这种视觉知识融合方法，<strong>不需要额外的<strong style="color:red;">双语-图片</strong>标注数据，模型就能够在多个数据集上取得显著的效果提升。</strong></p><h3 id="多模态与机器翻译"><a href="#多模态与机器翻译" class="headerlink" title="多模态与机器翻译"></a><strong>多模态与机器翻译</strong></h3><p>机器翻译是两种语言间的转换，比如“A dog is playing in the snow”翻译为中文就是“小狗在雪地里玩耍”。</p><p>但人类理解世界不只是用文字，还有视觉、听觉等感知能力；并且<strong style="color:blue;">翻译的过程需要保持“语义”不变</strong>。比如下面的图：</p><p><img src="https://i.loli.net/2021/03/03/31QcdLfykOoiCEX.jpg" alt="img" style="zoom:50%;"></p><p>讲中文的人会说“小狗在雪地里玩耍”，而讲英文的人会说“A dog is playing in the snow”。也就是说，人们对客观世界的本质认知是相同的，只是“方法”不同，体现在语言上，就是语法上的差异。</p><p>为此，我们可以假设<strong style="color:blue;">在机器翻译模型中，融入这种“客观的世界知识”，比如把图片信息加入，以此期望增强翻译能力。</strong>同时考虑文本和图片，这就是一种多模态。</p><p>然而，过去的翻译-图片研究大都需要大量的双语-图片标注数据，这在数据上成为一个研究的瓶颈。本文针对这种情况，<strong>提出“通用的视觉表示”，<strong style="color:red;">仅用单语-图片标注数据</strong>，就能显著提高机器翻译的效果。</strong></p><p>本文的方法<strong>在数据集EN-RO，EN-DE，EN-FR上均有约一个BLEU值的提高</strong>，这说明了本方法的有效性。</p><h3 id="本文贡献"><a href="#本文贡献" class="headerlink" title="本文贡献"></a>本文贡献</h3><ul><li>提出一种通用的视觉表示方法，无需双语-图片标注语料；</li><li>该方法<strong>可以在只有文本的数据集上使用</strong>；</li><li>实验证明了该方法效果提升的一致性。</li></ul><h3 id="通用视觉表示"><a href="#通用视觉表示" class="headerlink" title="通用视觉表示"></a><strong>通用视觉表示</strong></h3><p>首先我们有一个单语-图片数据集 $\mathcal{S}=\{X, E\}$，也就是，其中的每条数据都是一张图片 $e$ 和对图片的描述 $X_{e}=\left\{x_{1}, \cdots, x_{I}\right\},$ 把其中的停用词去掉后得到了 $X_{e}^{\prime}=\left\{x_{1}^{\prime}, \cdots, x_{J}^{\prime}\right\}$ 。</p><p>然后, 对 $X_{e}^{\prime}$ 中的每个词 $x_{j},$ 计算它在整个数据集 $\mathcal{S}$ 中的TF-IDF值， 然后取 $X_{e}^{\prime}$ 中TF-IDF值最大的前 $w$个词作为这个图片 $e$ 的主题词 $T_{e}$, 也就是和图片最相关的 $w$ 个词。</p><p>这样一来，<strong>每个图片e都有它主题词</strong> $T_{e},$ 同时，<strong>每个词都有可能同时是多个图片的主题词</strong>。我们可以把这看成一个 “主题词-图片” 查询表，输入一个词 $t$ ，就可以在表中查询以 $t$ 为主题的所有图片 $E_{t}=\left\{e_{1}, \cdots, e_{n}\right\}$。</p><p>那么，现在输入一个句子，我们就可以按照同样的步骤：</p><p>1.去除停用词；</p><p>2.计算每个词的TF-IDF；</p><p>3.取前$w$个TF-IDF最高的词；</p><p>4.在查询表中找到所有对应的图片；</p><p>5.按照出现次数的多少排序，取出前$m$个出现次数最多的图片（因为多个词可能对应同一个图片），得到集合 $G$</p><p>现在，这个图片集合 $G$ 就可以认为是和输入句子对应的视觉信息，可以用它去增强翻译效果了。下图是流程示意图：</p><p><img src="https://i.loli.net/2021/03/03/wMeWCU7jfD4NqIO.png" alt="image-20210303203731211" style="zoom: 50%;"></p><h3 id="在机器翻译中融合图片信息"><a href="#在机器翻译中融合图片信息" class="headerlink" title="在机器翻译中融合图片信息"></a><strong>在机器翻译中融合图片信息</strong></h3><p>为了把图片融合进去，我们首先用一个预训练的ResNet提取图片集$G$ 的表示，然后计算 $\bar{H}=$ Self-Attention $\left(H^{L}, K_{G}, V_{G}\right)$ 与 $H=H^{L}+\lambda \bar{H}$<br>这里, $H^{L}$ 是Transformer Encoder的最后一层, $K_{G}, V_{G}$ 是用ResNet得到的图片集的表示, $\lambda$ 使用<br>sigmoid 计算。<br>在Decoder端，直接把 $H$ 送入即可。融合步骤如下所示：</p><p><img src="https://i.loli.net/2021/03/03/SWYB4lDXt7ysVrO.png" alt="image-20210303204216912" style="zoom:50%;"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a><strong>实验</strong></h3><p>我们在三个数据集上进行实验：WMT16 En-RO, WMT14 EN-DE和WMT14 EN-FR。这三个数据集大小从小到大增加，从而在不同大小的数据集上都能验证该方法。</p><p>下表是在这三个数据集上的结果，++表示显著更优。</p><p>可以看到，和基线模型(Trans.(base/big))相比，本文的方法(+VR)在三个数据集上都能得到显著的提升，平均提升约一个BLEU值。同时，只引入了很少的参数量，这就不会使训练时间几乎不会增加。</p><p><img src="https://i.loli.net/2021/03/03/F8z9SReVPkuNdov.png" alt="image-20210303205316435" style="zoom:33%;"></p><p>下表是在数据集Multi30K上的结果，这是一个多模态数据集。可以看到，即使在多模态设置下，本文方法依旧能够取得显著结果。</p><p><img src="https://i.loli.net/2021/03/03/taQRuKN6ElJYbLg.png" alt="image-20210303205337060" style="zoom:33%;"></p><p>最后，我们来看看每个句子对应的图片集 $G$ 的大小 $m$, 和手动控制参数 $\lambda$ 的影响。下图分别是两个因素的影响结果。从图片数量来看，并不是越多的图片数量越好, 也不是越少越好,而是在 $m=5 \sim 15$ 的区间较好。这是因为， <strong>过少的图片信息不充分, 过多的图片噪声太多。</strong></p><p>参数$\lambda$控制的是图片信息融合的程度，可以看到，无论融合多少，效果都比不融合图片信息要好，这说明多模态是有效果的。</p><p>而且，手动控制它都没有模型自动学习好，这也说明模型对不同的输入句子，需要的视觉信息也是不同的。</p><p><img src="https://i.loli.net/2021/03/03/De8nVYPabyIrBwE.png" alt="image-20210303205616909" style="zoom:33%;"></p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a><strong>小结</strong></h3><p>本文提出了一种简单、有效的多模态视觉知识融合方法——首先构建从主题词到图片的查询表，然后对输入句子找到相关的图片，然后使用ResNet提取图片信息融入到机器翻译模型中。</p><p>使用这种方法，可以避免对大规模双语-图片数据的依赖。实验结果也表明，这种方法可以一致地提高翻译效果。</p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><ul><li><p>如果要翻译单语-图片数据集中没有的语言，可以怎么做？</p><p>比如$S$没有日语，我们可以用一个日语的image caption模型去自动标注每个图片的描述。</p><p>或者可以用X-日语的机器翻译得到图片翻译后的描述；或者直接用一个现有的词典，把图片的主题词直接翻译成日语。其他方法亦可。</p></li><li><p>在融合步骤，是否可以有其他的方法进行融合？</p><p>另外一个简单的方法是，把ResNet得到的图片表示和句子一起，送入Encoder，再像往常一样解码。</p></li><li><p><strong>你认为本文这种方法从逻辑上是否真的有效？为什么？</strong></p><p>见仁见智，笔者倾向于有效，但是作用不大，因为只从模型的角度难以验证图片和文本之间语义的相关性，至于效果的提升，有可能是ResNet和Aggregate的共同结果。</p><p>笔者认为，可以考虑加一个图片预测描述的任务，和翻译一起学习；再将ResNet替换为普通的CNN进行实验。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> image-guided MT </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal,image-guided MT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[MultiSubs] A Large-scale Multimodal and Multilingual Dataset</title>
      <link href="2021/03/03/MultiSubs-A-Large-scale-Multimodal-and-Multilingual-Dataset/"/>
      <url>2021/03/03/MultiSubs-A-Large-scale-Multimodal-and-Multilingual-Dataset/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="目前存在的问题"><a href="#目前存在的问题" class="headerlink" title="目前存在的问题"></a>目前存在的问题</h3><p>使用视觉信息对 language grounding 的计算模型的研究导致了许多有趣的应用，例如图像字幕，视觉问答和视觉对话。各种各样的多模态数据集（由图片和文本组成）被构建，并且用于不同的应用。大部分数据集，图像被标注了文本标签，但是没有提供应用文本或图像的上下文。</p><p>在 image captioning dataset 中，为每张图片标注了 sentence-level text。虽然这些句子为图片提供了strong concept，但是存在一个基本的缺点：每个 sentence-level text 将 image 看做一个整体，但是实际上，text 中的内容仅仅包含了image中的部分元素。这将使得很难学到视觉和文本中的元素的对应（correspondences between elements）。图像和文本之间的连接是多样的。比如，很难用单个句子描述整个图像或用单个图像说明整个句子。因此，为了了解单词和图像之间更好的基础（grounding），需要在图像和文本段之间建立<strong>更紧密的局部对应关系</strong>。</p><p>此外，文本仅限于非常特定的领域（图像描述），而图像也仅限于极少数和非常特定的对象类别或人类活动；这使得很难概括可能的现实世界场景的多样性。</p><h3 id="本文的解决办法"><a href="#本文的解决办法" class="headerlink" title="本文的解决办法"></a>本文的解决办法</h3><p>在本文中，提出了一个新的大规模 多模态和多语言的数据集 (MultiSubs)，可以促进 <strong style="color:red;"><strong>grounding words to images</strong> </strong>in the context of their corresponding sentences。如下图1。</p><p><img src="https://i.loli.net/2021/03/03/PCQZBX2DtnaFzAu.png" alt="image-20210303162030516" style="zoom: 33%;"></p><p>与以前的数据集相比，我们的基础单词不仅针对图像，而且还针对其在语言中的上下文用法，从而有可能对现实世界中的人类语言学习产生更深刻的见解。具体来说：（1）MultiSubs中的文本片段和图像具有更紧密的局部对应，便于学习文本片段及其对应的视觉表示之间的关联；（2）与图像字幕数据集相比，图像更通用，范围更广，并且不受特定域的限制；（3）每个给定的文本片段和句子都可以有多个图像；（4）文字包含类似于自由形式的真实世界文字的grammar ot syntax；（5）文本是多语言的，而不仅仅是单语言或双语的。</p><p>从电影字幕的平行语料库开始，我们提出了一种<strong>跨语言多模态消歧方法</strong>，通过利用并行多语言文本来消歧文本中单词的含义，来说明文本片段。如图2所示。</p><p><img src="https://i.loli.net/2021/03/03/wBsAvKgbT2LSk16.png" alt="image-20210303163357136"></p><p>据我们所知，目前尚未对在文本插图的上下文中对此进行探讨。我们还通过人工判断来评估数据集和 illustrated text fragments 的质量。</p><p>使用本文提出的MultiSubs 数据集，本文提出了两个不同的多模态任务：（1）A fill-inn-the-blank task：to guess a missing word from a sentence, with or without image(s) of the word as clues。（2）Lexical translation：在给定 source sentence 和与该source word 相关联的零个或多个图像的情况下，我们将带有句子上下文的 source word 翻译为外语中的target word。</p><h2 id="语料库和文本片段选择"><a href="#语料库和文本片段选择" class="headerlink" title="语料库和文本片段选择"></a>语料库和文本片段选择</h2><p><em>MultiSubs</em>基于OpenSubtitles 2016（OPUS）语料库，该语料库是从 OpenSubtitles 中获得的，涵盖了65种语言的movie subtitles。</p><blockquote><p>OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles.</p><p>OpenSubtitles: Subtitles - download movie and TV Series subtitles. <a href="http://www.opensubtitles.org/" target="_blank" rel="noopener">http://www.opensubtitles.org/</a></p></blockquote><p>本文挑选了5类电影：冒险，动画，喜剧，纪录片和家庭。大多数 subtitles 是对话性的（对话）或叙事性的（故事叙事或纪录片）。进一步将字幕过滤为仅在OPUS中与来自语料库的前30种非英语语言中的至少一种字幕对接的英语字幕的子集。这样一来，总共有45,482个电影实例<em>≈</em>38M个英语句子。对于前30种语言，电影的数量从2,354到31,168不等。</p><p>我们的目标是选择可能“在视觉上可描绘”的文本片段，并因此可以用图像进行说明。我们首先将英语字幕通过spacy  POS （en_core_web_md）来提取名词，动词，复合名词和简单形容词名词短语。这些 text frgments 的图像可成像性评分是通过MRC心理语言学数据库PaetzoldSpecia：2016通过引导获得的 ; 对于多词短语，我们将每个单词的图像可比性得分平均，为每个未见单词分配零得分。我们 retain text fragments 的可成像性得分至少为500，这是通过人工检查单词的子集来确定的。删除掉仅出现一次的 text fragments 后，输出为一组144,168个唯一候选片段（超过1600万个实例）<em>≈</em>1100万个句子。</p><h2 id="Illustration-of-text-fragments"><a href="#Illustration-of-text-fragments" class="headerlink" title="Illustration of text fragments"></a>Illustration of text fragments</h2><h3 id="Cross-lingual-sense-disambiguation"><a href="#Cross-lingual-sense-disambiguation" class="headerlink" title="Cross-lingual sense disambiguation"></a>Cross-lingual sense disambiguation</h3><p>本文提出的text illustration approach 的关键直觉是：一个带有歧义的英语句子，在另外一种语言的parallel sentence 中 可能没有歧义。</p><p>【1】<strong>Cross-lingual word alignment</strong> </p><p>在选择正确的图像 to illustrate our candidate text fragments (nouns) 时，我们尝试了多达四种<em>目标</em>语言：<strong>西班牙语</strong>（<strong>ES</strong>）和<strong>巴西葡萄牙语</strong>（<strong>PT</strong>），以及<strong>法文</strong>（<strong>FR</strong>）和<strong>德文</strong>（<strong>DE</strong>）。对于每种语言，选择字幕，以使（i）每个字幕都与英语字幕对齐；（ii）每个都至少包含一个感兴趣的名词。对于英语和每个目标语言，我们在全组平行句（不管句子中是否含有候选片段）中训练 <strong><em>fast_align</em></strong> DyerEtAl：2013，以获得在两种语言中词与词之间的对齐 。<strong>这将生成一个字典，该字典将英语名词映射到目标语言中的单词。</strong>我们对此字典进行过滤，以删除不常见的目标短语（语料库的1％以下）对。我们还将目标语言中具有相同lemmas 的单词归为一组。</p><p>【2】<strong>Sense disambiguation</strong></p><p>source -&gt; target </p><p>将名词翻译成不同的词(in the target language) 并不一定意味着它是模棱两可的。target phrases 可以简单地是指代相同概念的同义词。因此，我们进一步尝试在target side 对同义词进行分组，同时还通过查看跨多语言语料库的对齐短语来确定正确的词义。</p><p>对于 word senses，我们使用<strong>BabelNet</strong> NavigliPonzetto：2012，这是一个大型语义网络和涵盖多种语言的多语言百科全书，并统一了其他语义网络。</p><p>为了帮助我们确定给定上下文中英语名词的正确含义，我们使用目标语言中平行句子中的对齐词来消除歧义。我们计算两个查询返回的BabelNet同义词集ID之间的交集。比如 bank(english) -&gt; banco(spanish) ，如果使用英语bank 来查询将得到 financial-bank 和river-bank，如果用西班牙语banco查询将得到 financial-bank。取这两个查询结果的交集。</p><p>如果仅针对一种语言对执行上述操作，则该目标语言可能不足以消除英语术语的歧义，因为该术语在两种语言中可能是歧义的。对于紧密相关的语言（例如葡萄牙语和西班牙语）尤其如此。因此，<strong>我们建议利用<em>多种</em>目标语言，以进一步提高我们消除英语单词歧义的信心</strong>。我们的假设是，更多的语言最终将允许识别单词的正确上下文。</p><p>更具体地说，我们研究了包含<strong>多达四种目标语言</strong>的并行句子的字幕。对于每个英语短语，我们保留所有实例之间的同义词集ID之间<strong>至少有一个交集的实例</strong></p><p>【3】<strong>Image Selection</strong></p><p>构造<em>MultiSubs</em>的最后一步是为每个歧义的英语术语分配至少一个图像，and by design the term in the aligned target language(s)。由于BabelNet通常为给定的同义词集ID提供多个图像，因此我们用与该同义词集关联的所有Creative Commons images 说明该term。</p><h2 id="Human-evaluation"><a href="#Human-evaluation" class="headerlink" title="Human evaluation"></a>Human evaluation</h2><p>为了定量评估我们的automated cross-lingual sense disambiguation cleaning procedure，我们收集了人类注释，<strong>以确定<em>MultiSubs</em>中的图像是否确实对预测填空任务中的遗漏单词有用</strong>。<strong>注释还可以作为任务的人工上限</strong></p><p>我们将注释任务设置为<em>The Gap Filling Game</em>（图 <a href="https://www.arxiv-vanity.com/papers/2103.01910/#S5.F5" target="_blank" rel="noopener">5</a>）。在此游戏中，用户尝试进行<strong>三种尝试来猜测从<em>MultiSubs</em>的句子中删除的确切单词</strong>。在<strong>第一次尝试</strong>中，游戏仅显示句子（以及遗漏单词的空白）。在<strong>第二次尝试</strong>中，游戏还会为丢失的单词提供一个图像作为线索。在第三次也是<strong>最后一次尝试</strong>中，系统将显示与缺失单词关联的所有图像。在每次尝试中，如果用户输入的单词与原始单词完全匹配，则用户将获得1.0分；否则，将按预先训练的CBOW word2vec MikolovEtAl：2013之间的余弦相似度计算得出的部分分值（介于0.0和1.0之间） 预测词和原始词的嵌入。当用户输入完全匹配的内容时，或者在用尽所有三个尝试之后（以先发生者为准），每个“转”（一个句子）都会结束。第二次和第三次尝试的得分乘以<em>惩罚因子</em>（分别为0.90和0.80），以鼓励用户尽早正确猜出该单词。用户单回合的得分是所有三个尝试中的最高分，每个用户的最终累积得分是所有带注释的句子中分的总和。该最终分数确定了游戏结束时（在预定的截止日期之后）的获胜者和亚军，他们两人都分别获得了亚马逊代金券。在游戏过程中，不会为用户提供确切的“当前最高得分”表，而是会为他们提供比其当前得分更低的所有得分的所有用户所占的百分比</p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><p>visual grounding of words</p><p>language grounding</p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[The GEM Benchmark] Natural Language Generation, its Evaluation and Metrics</title>
      <link href="2021/03/01/The-GEM-Benchmark-Natural-Language-Generation-its-Evaluation-and-Metrics/"/>
      <url>2021/03/01/The-GEM-Benchmark-Natural-Language-Generation-its-Evaluation-and-Metrics/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multimodal Transformer for Multimodal Machine Translation</title>
      <link href="2021/02/26/Multimodal-Transformer-for-Multimodal-Machine-Translation/"/>
      <url>2021/02/26/Multimodal-Transformer-for-Multimodal-Machine-Translation/</url>
      
        <content type="html"><![CDATA[<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>多模态机器翻译任务，是从其他模态中引入信息（一般是静态的图像）来提高翻译质量。先前的方法没有考虑多个模态的相对重要性，它们常常平等对待文本和图像信息，并分别编码，但是这种方式，将会导致从图像中<strong>引入许多无关的信息</strong>。</p><p>在本文中，提出了一个multi-modal self-attention in Transformer 来解决上述的问题。本文提出的方法能够based on text to encode vision, 从而避免了编码图像中与文本无关的信息。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><ul><li>The focus of our work is to build a powerful encoder to incorporate the information from other modality.</li></ul><p><img src="https://i.loli.net/2021/02/26/4AnNwV85DpBkqZm.png" alt="image-20210226172540508" style="zoom:50%;"></p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> image-guided MT </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal,image-guided MT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</title>
      <link href="2021/02/26/A-Novel-Graph-based-Multi-modal-Fusion-Encoder-for-Neural-Machine-Translation/"/>
      <url>2021/02/26/A-Novel-Graph-based-Multi-modal-Fusion-Encoder-for-Neural-Machine-Translation/</url>
      
        <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>当前主流的multi-modal NMT models 不能充分利用不同模态语义单元之间的<strong>细粒度的语义对应。</strong></li><li>在本文中，提出了一个新颖的graph-based  cross-modal fusion encoder 来处理NMT task。具体地，（1）首先使用一个 unified multi-modal graph来编码input sentence and image。这种方式可以捕获到多模态语义单元（words and visual objects）之间各种语义关系。（2）使用多个 graph-based multi-modal fusion layers 来迭代的执行语义交互，以学习node representations。（3）以上获得的contextual representations 送入decoder中。</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li><p>该任务的重要性，有很多现实的应用：包括翻译多媒体新闻，Web产品信息和电影字幕。</p><blockquote><p>A visual attention grounding neural model for multimodal machine translation.</p></blockquote></li><li><p>该任务对于提高机器翻译的准确性有作用：视觉环境有助于解决歧义的多义词。</p><blockquote><p>Distilling translations with visual awareness.</p></blockquote></li></ul><p>很显然，再在 multi-modal NMT 中，如何有效的利用视觉信息是一个核心的问题。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://i.loli.net/2021/02/26/7IOXHm89QM5DNq4.png" alt="Untitled"></p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul><li><p>Multi-modal Graph</p><p><img src="https://i.loli.net/2021/02/26/w1vmXMebCR3Kt5y.png" alt="Untitled3"></p><ul><li><p>所有的单词都作为 textual nodes。使用Stanford parser来找到文本中的所有名词，然后使用 visual grounding tookit来检测 bbox，并作为visual nodes。</p><blockquote><p>visual grounding tookit: <strong>A fast and accurate one-stage approach to visual grounding</strong></p></blockquote></li><li><p>在 multi-modal graph中使用了两种类型的edges。<strong>intra-modal edge(fully-connected)</strong> and <strong>inter-modal edge(partly-connected)</strong></p></li></ul></li><li><p>Embedding Layer</p><ul><li>Before inputting the multi-modal graph into the stacked fusion layers，首先获得其初始特征。</li><li>对于textual modes， 使用word embedding 和 position embedding 的求和。</li><li>对于visual nodes，使用Faster-RCNN提取 roi pooling layer 的特征，然后使用MLP + RELU 将视觉特征映射到与文本特征相同的空间。</li></ul></li><li><p>Graph-based Multi-modal Fusion Layers</p><ul><li>On the top of embedding layer, stack multiple graph-based multimodal fusion layers to encode the above-mentioned multi-modal graph.</li><li>在每个融合层，序列地实施模态内和模态间的融合，来更新所有的节点状态。这种方式，可以使得最终的节点状态能够同时编码到相同模态和跨模态的语义信息。</li><li>由于视觉节点和文本节点是包含了不同模态信息的两种语义单元。因此，使用相同的操作，但是不同的参数（不共享）来更新它们的节点状态。</li></ul></li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul><li><p>Our decoder is similar to the conventional Transformer decoder。堆叠多个相同的层来生成 target-side hidden states，每一层由三个子层组成。</p><p>前两个子层是一个masked self-attention 和 一个encoder-decoder attention 来分别聚合target-side and source-side contexts。</p><p>最后由a position-wise fully-connected forward neural network 和 线性变化来生成next-step predict word。</p></li></ul><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li>与本文类似的模型结构有以下两篇</li></ul><blockquote><p>Multi-Modality Cross Attention Network for Image and Sentence Matching</p><p>(LXMERT) LXMERT Learning Cross-Modality Encoder Representations from Transformers</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> image-guided MT </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal,image-guided MT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Style-Content Duality of Attractiveness: Learning to Write Eye-Catching Headlines via Disentanglement</title>
      <link href="2021/02/26/The-Style-Content-Duality-of-Attractiveness-Learning-to-Write-Eye-Catching-Headlines-via-Disentanglement/"/>
      <url>2021/02/26/The-Style-Content-Duality-of-Attractiveness-Learning-to-Write-Eye-Catching-Headlines-via-Disentanglement/</url>
      
        <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/02/26/9xToFaqiXjyKLOU.png" alt="image-20210226094912200" style="zoom: 50%;"></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>抢眼的头条新闻是触发更多点击的第一个设备，在制作人和观众之间产生了相互影响。生产者可以获得更多的流量和利润，而读者可以访问优秀的文章。生成吸引人的头条新闻时，不仅要捕捉吸引人的<strong>内容</strong>，而且要遵循醒目的书面<strong>风格</strong>。</p><p>本文中，提出了一个a Disentanglement-based Attractive Headline Generator (DAHG)。该标题生成器根据有吸引力的样式来捕获有吸引力的内容的标题。具体而言，【1】我们首先设计一个解纠缠模块，将引人注目的原型标题的样式和内容划分为潜在空间，并带有两个辅助约束以确保两个空间确实被纠缠。【2】然后，潜在内容信息将用于进一步polish the document representation 并帮助捕获重要部分。【3】最后，生成器将 polished document 作为输入，以在引人注目的样式的指导下生成标题。</p><p>本文在Kuaibao dataset 上实现了最好的性能。人工评估还表明，与现有模型相比，DAHG触发的点击次数增加了22％。</p><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Headline-Generation"><a href="#Headline-Generation" class="headerlink" title="Headline Generation"></a>Headline Generation</h4><p>头条生成目前是NLP中的一个研究热点，目前大部分存在的头条生成工作仅仅关注于 summarizing the document。目前在Attractive headline generation上的研究还相对较少，目前有以下几篇。据我们所知，目前没有工作considers the style-content duality of attractiveness（考虑  吸引力的 内容-风格 二重性）。</p><blockquote><p>【1】Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning</p><p>【2】Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator. EMNLP 2019</p><p>【3】Structure Learning for Headline Generation.</p><p>【4-（not）】Hooks in the Headline: Learning to Generate Headlines with Controlled Styles</p></blockquote><h4 id="Disentanglement"><a href="#Disentanglement" class="headerlink" title="Disentanglement."></a>Disentanglement.</h4><p>现有作品集中于学习learning the disentangled representation，并且我们进一步采取了这种方法来利用这种representation来生成attractive headlines。</p>]]></content>
      
      
      <categories>
          
          <category> title </category>
          
          <category> style </category>
          
      </categories>
      
      
        <tags>
            
            <tag> title,style </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[DeepFuse] HKU’s Multimodal Machine Translation System for VMT’20</title>
      <link href="2021/02/26/DeepFuse-HKU%E2%80%99s-Multimodal-Machine-Translation-System-for-VMT%E2%80%9920/"/>
      <url>2021/02/26/DeepFuse-HKU%E2%80%99s-Multimodal-Machine-Translation-System-for-VMT%E2%80%9920/</url>
      
        <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/02/26/bTNP7oqc3nxXfMV.png" alt="image-20210226154022062" style="zoom:50%;"></p><p>VATEX： video-guided machine translation(EN-&gt;CH) Challenge</p><p>ACL 2020 workshop <a href="https://alvr-workshop.github.io/2020/index.html" target="_blank" rel="noopener">https://alvr-workshop.github.io/2020/index.html</a></p><h3 id="以前方法存在的问题"><a href="#以前方法存在的问题" class="headerlink" title="以前方法存在的问题"></a>以前方法存在的问题</h3><p>以前的image-guied Machine Translations, 在encode 阶段，往往单独对 视觉信息和语言信息进行编码。然后，在decode 阶段使用attention将视觉信息结合进来。模态之间的信息仅仅进行了浅融合。</p><h3 id="本文提出的方案"><a href="#本文提出的方案" class="headerlink" title="本文提出的方案"></a>本文提出的方案</h3><p>本文中，提出了一个 video-augmented encoder，以此，获得一个multi-modal representation 来作为decoder的输入。使用attention 机器在多个层融合了多模态的表征。</p><p>实验证明，这种深融合方法相比于之前的浅融合方法要更加的有效。</p><h3 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h3><p><img src="https://i.loli.net/2021/02/26/7qgti5pwnDkcWNZ.png" alt="image-20210226160753355" style="zoom:50%;"></p><p>本文提出的visual-augmented encoder 如图1所示，encoder 包括L=6层相同的层。</p><p>将sentence表征为一个embedding的输入序列： $\mathbf{X}=x_{1}, x_{2}, \ldots, x_{n}$</p><p>将 video 使用I3D提取clips的特征，表征为segment-level feature 的序列：$\mathbf{E}=e_{1}, e_{2}, \ldots, e_{m}$</p><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>The encoder process the input X and E as follows:</p><p>【1】input X 输入<strong>Multi-Head attention and Feed Forward</strong> 这两个sub layers， 得到 $H^L$</p><p>【2】在video-encoder attention module, 使用$H^L$ 作为 query 来选择与query 相关的 video representation。 </p><p>$\overline{\mathcal{H}}=$ Attention $\left(\mathbf{H}^{L}, \mathbf{K}_{E}, \mathbf{V}_{E}\right)$</p><p>【3】使用权重求和来得到多模态特征：</p><p>$\mathcal{H}=\mathbf{H}^{L}+\lambda \overline{\mathcal{H}}$， where，$\lambda=\operatorname{sigmoid}\left(\mathbf{W}_{\lambda} \overline{\mathcal{H}}+\mathbf{U}_{\lambda} \mathbf{H}^{L}\right)$</p><p>【4】最后的输出是：$\operatorname{LayerNorm}\left(\mathbf{H}^{L}+\mathcal{H}\right)$</p><p>【yaya】最后一步，似乎是有问题，因为，这样多模态特征，似乎就是加了两遍 $H^L$</p><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>遵循transformer 中 decoder的设计，存在L=6个相同的层。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h4><p>学习率的变化曲线：lrate $=d_{\text {model }}^{-0.5} \cdot \min \left(K^{-0.5}, K \cdot N^{-1.5}\right)$，where K is the current number of step and N=4000 is the number of warm-up steps.</p><h4 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h4><p>（1）<strong>lstm</strong>: Text-only LSTM-based encoderdecoder NMT </p><p>（2）<strong>vatex</strong>: The LSTM-based videoguided machine translation system proposed in VATEX<br>（3）<strong>Transformer</strong>: standard text-only transformer architecture proposed by “Attention is all you need.”</p><p><img src="https://i.loli.net/2021/02/26/ZD7SxmKUBMA3tIE.png" alt="image-20210226170500065" style="zoom:33%;"></p><ul><li>相比于text-only transformer 有一个显著的提高。证明了，使用深层融合来结合视觉信息的有效性。</li></ul><h3 id="收获与总结"><a href="#收获与总结" class="headerlink" title="收获与总结"></a>收获与总结</h3><ul><li><p>本文的关键是提出了使用transformer 的结构来融合文本和视觉信息。</p></li><li><p>在image-guided machine translation 任务中，也存在两篇使用co-attention 来融合两个模态信息的。如下：</p><blockquote><p>[1] (ACL 2020) Multimodal Transformer for Multimodal Machine Translation</p><p>[2] (ACL 2020) A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation 【Graph Fusion】</p></blockquote></li></ul><p>【Graph Fusion】与本文相比，encoder 的设计方式不同，但是decoder的设计是相同的。关于不同：【Graph Fusion】首先对两个模态各自self-attention 而后再根据graph edge 进行co-attention。而本文对视觉信息没有进行self-attention，仅对语言信息进行了self-attention, 在co-attention步骤中也没有显示的graph edge,而是采用了隐式的全连接。</p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> video-guided MT </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal,video-guided MT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Closer Look at the Robustness of Vision-and-Language Pre-trained Models</title>
      <link href="2021/02/24/A-Closer-Look-at-the-Robustness-of-Vision-and-Language-Pre-trained-Models/"/>
      <url>2021/02/24/A-Closer-Look-at-the-Robustness-of-Vision-and-Language-Pre-trained-Models/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>大规模的预训练多模态transformer将最新的视觉-语言任务推进到了一个新的高度。虽然在标准任务上实现了令人印象深刻的性能，但是，迄今为止，任然不清楚这些预训练模型的鲁棒性。</p><p>为了进行调查，我们针对现有的预训练模型对4种不同类型的V + L特定模型的鲁棒性进行了全面的评估：(i) Linguistic Variation; (ii) Logical Reasoning; (iii) Visual Content Manipulation; and (iv) Answer Distribution Shift. 有趣的是，by standard model finetuning，预训练的V+L模型相比于task-specific 模型展示出更好的鲁棒性。</p><p>为了<strong>进一步增强模型的鲁棒性</strong>，本文提出了<strong>MANGO</strong>，一个具有泛化性且鲁棒的方法，可以在embedding space 学习a Multimodal Adversarial Noise GeneratOr 以愚弄pre-trained V+L models。与以往针对一种特定类型的鲁棒性的研究不同，MANGO具有任务不可知性，并且可以针对各种任务（旨在评估鲁棒性的广泛方面）对预训练模型进行通用性能提升。</p><p> 全面的实验表明，MANGO在9个鲁棒性基准中有7个达到了最新水平，大大超过了现有方法。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>当前的 V+L pre-tranining model取得了很大的进展在各种 V+L tasks，但是这些benchmark 在测试集和数据集上的分布常常是相似的，textual query 几乎没有 linguistic variation, 使用干净的自然图像，而没有任何visual content manipulation。 因此，尽管这些标准基准对于通用模型评估有效，但仍<strong>缺乏明确评估模型鲁棒性的能力</strong>。（在本文中，我们不关注 adversarial robustness，<strong style="color:red;">因为目前没有可用的adversarial benchmark</strong>。因此，我们在已有的robustness benchmark上进行观测，这些benchmark 设有挑战性的设置，并且经过了人类的验证）</p><p>(i) VQA-Rephrasings[56] for <strong>linguistic variation</strong>;</p><p>(ii) VQA-LOL (Compose and Supplement) [18], VQA-Introspect [54] and GQA [25] for <strong>logical reasoning</strong>; </p><p>(iii) IV-VQA and CVVQA [2] for <strong>visual content manipulation</strong>;  </p><p>(iv) VQA-CP v2 [3] and GQA-OOD [31] for <strong>answer distribution shift</strong>.</p><h3 id="当前方法存在的问题"><a href="#当前方法存在的问题" class="headerlink" title="当前方法存在的问题"></a>当前方法存在的问题</h3><p>VILLA，在multimodal embedding 加入对抗扰动，<strong>projected gradient descent（PGD） attack training（AT）</strong> 可以在 linguistic variation and visual content manipulation 增强鲁棒性，但是在训练集和测试集之间有显著的数据分布差异时，会有收效甚微的影响甚至drop model performence。</p><h3 id="本文方法简介"><a href="#本文方法简介" class="headerlink" title="本文方法简介"></a>本文方法简介</h3><p>为了在所有方面都实现鲁棒性，本文提出了 MANGO，通过在multi-modal embedding space 引入adversarial noise来增强鲁棒性。</p><p><img src="https://i.loli.net/2021/02/24/yDSgc5JwWFHCU8h.png" alt="image-20210224120151245"></p><p>如图 figure 1a所示，不使用PGD来生成对抗扰动，而是使用一个基于可训练神经网络来学习一个adversarial noise generator。与 VILLA相同，在embedding space 加入扰动，因为本文的目标 是对抗训练的最终结果，而不是制造对抗样本。</p><p>【1】本文要学习的是一个 universial noise generator，但是在VILLA中使用的PGD方法是针对每个特定样本来生成的，本文提出的noise generator 是通用的，对输入训练样本是不加区别的。【2】而PGD的方法是<strong>耗时</strong>的，而本文提出的方法是轻量级的，不需要梯度计算中的重复迭代。同时，为了使能多样性的对抗embedding，本文进一步提出随机对image regions 和 textual tokens掩码。</p><h3 id="本文的贡献"><a href="#本文的贡献" class="headerlink" title="本文的贡献"></a>本文的贡献</h3><ul><li>第一个系统性的分析pre-trained V+L 模型的鲁棒性</li><li>提出了 MANGO，一个generic and efficient 对抗训练方法来增强 V+L model 鲁棒性</li><li>实验结果证明了本文提出方法的鲁棒性。</li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h4><p>Perturbing clean images with Gaussian noise. we use Gaussian noise augmentation as a simple baseline to investigate model robustness under V+L setting. Instead of adding noise to raw image pixels as in [52], we add perturbations directly to the embeddings.</p><h4 id="Adversarial-Noise-Generator-our"><a href="#Adversarial-Noise-Generator-our" class="headerlink" title="Adversarial Noise Generator (our)"></a><strong>Adversarial Noise Generator</strong> (our)</h4><p>Adding Gaussian noise to clean image-text pairs 可以补充训练样本。但是，随着训练的持续，模型可以逐渐的适应这种扰动，因为扰动都是从同一个 Gaussian noise distribution 中采样来的。</p><p>为了得到 harder perturbations，本文提出了一个可学习的 adversarial noise generator。对抗性噪声发生器将高斯噪声样本作为输入，通过可学习神经网络产生对抗性噪声样本。</p><p>Intuitively, to maximally fool the backbone network, 【1】we want to <strong>maximize prediction errors on these adversarially perturbed samples.</strong> 【2】In the meantime, we want the model to possess <strong>less confidence in its predictions on perturbed samples</strong> than clean samples, to promote harder adversarial examples。因此，<strong>adversarial noise generator 的目标是</strong>最大化这两个损失的求和：【1】task-speficic loss 【2】KL loss, which measures the distance between the predicted answer distribution of perturbed samples and that of clean samples.</p><p>另一方面，the trained model 旨在通过将对抗性生成的嵌入作为数据增强来最大程度地减少这两种损失。</p><p>综合上述两种方面，提出了如下的min-max game:</p><script type="math/tex; mode=display">\min _{\boldsymbol{\theta}} \max _{\boldsymbol{\phi}_{v}(\boldsymbol{v}, \boldsymbol{w}, \boldsymbol{y}) \sim \mathcal{D} \boldsymbol{\alpha} \in \mathcal{N}(\mathbf{0}, \mathbf{1})} \mathbb{E}\left[\mathcal{L}_{s t d}\left(\boldsymbol{\theta}, \boldsymbol{\phi}_{v}\right)+\beta \mathcal{R}_{k l}\left(\boldsymbol{\theta}, \boldsymbol{\phi}_{v}\right)\right]</script><p>where $\beta$ is a hyper-parameter, and</p><script type="math/tex; mode=display">\begin{array}{l}\mathcal{L}_{s t d}\left(\boldsymbol{\theta}, \boldsymbol{\phi}_{v}\right)=\mathcal{L}_{\mathrm{BCE}}\left(f_{\boldsymbol{\theta}}\left(\boldsymbol{v}+g_{\boldsymbol{\phi}_{v}}(\boldsymbol{\alpha}), \boldsymbol{w}\right), \boldsymbol{y}\right) \\\mathcal{R}_{k l}\left(\boldsymbol{\theta}, \boldsymbol{\phi}_{v}\right)=\mathcal{L}_{k l}\left(f_{\boldsymbol{\theta}}\left(\boldsymbol{v}+g_{\phi_{v}}(\boldsymbol{\alpha}), \boldsymbol{w}\right), f_{\boldsymbol{\theta}}(\boldsymbol{v}, \boldsymbol{w})\right)\end{array}</script><p>在训练时，迭代跟新 an outer loop of the backbone network and an inner loop of noise generator.</p><p>本文提出的adversarial noise generator 是轻量级的，仅仅存在 a few linear layers。相比于一个深层模型，这种轻量模型更容易陷入局部最优。因此，定期地，we replace the learned noise generator with a new one trained from scratch。每次，new  generator is trained against the latese learned parameters of the backbone.</p><h4 id="Random-Masking"><a href="#Random-Masking" class="headerlink" title="Random Masking"></a>Random Masking</h4><p>虽然 adversarial noise generator 可以产生具有挑战性，更加多样化的噪声扰动，但是不会改变训练样本的内在统计（例如，问题长度和image regions的分布）。然而，实际上，在robustness benchmark 的训练和测试集中存在这种 significant mismatch。比如，the average length of questions in VQA-LOL 测试集是VQA V2 训练集的 2-3倍。</p><p>为了补偿这种统计上的不匹配，我们建议在向图像和单词嵌入中添加对抗性噪声时，<strong>随机掩盖图像区域</strong> and <strong>随机插入[MASK]令牌</strong>。</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>进行实验分析了 pre-trained V+L model 的鲁棒性和 本文提出的MANGO framework 的有效性。本文使用UNITER作为 backbone，并将 MANGO与 UNITER和 VILLA 在9个 robustness datasets + VQA v2 dataset上进行了比较。本文在这10个 benchmark上进行研究，<strong style="color:red;"><strong>因为目前在其他任务上没有这种 robustness dataset。</strong></strong></p><p>VILLA 在预训练阶段和微调阶段都采用了 对抗训练，而本文只是在微调阶段（即针对特定任务）</p><p><img src="https://i.loli.net/2021/02/25/HwjDn2voZfWKFOk.png" alt="image-20210225095811994"></p><h3 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h3><p>investigate the adversarial robustness of pre-trained V+L models.</p><p>（在本文中，我们不关注 adversarial robustness，<strong style="color:red;">因为目前没有可用的adversarial benchmark</strong>。因此，我们在已有的robustness benchmark上进行观测，这些benchmark 设有挑战性的设置，并且经过了人类的验证）</p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> 对抗 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal,对抗 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UNIMO Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning</title>
      <link href="2021/02/23/UNIMO-Towards-Unified-Modal-Understanding-and-Generation-via-Cross-Modal-Contrastive-Learning/"/>
      <url>2021/02/23/UNIMO-Towards-Unified-Modal-Understanding-and-Generation-via-Cross-Modal-Contrastive-Learning/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VX2TEXT End-to-End Learning of Video-Based Text Generation From Multimodal Inputs</title>
      <link href="2021/02/23/VX2TEXT-End-to-End-Learning-of-Video-Based-Text-Generation-From-Multimodal-Inputs/"/>
      <url>2021/02/23/VX2TEXT-End-to-End-Learning-of-Video-Based-Text-Generation-From-Multimodal-Inputs/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li>本文提出了一个框架 for text generation from multimodal inputs consisting of video plus text, speech, or audio。</li><li><p>为了利用 transformer networks，每个模态通过一个 learnable tokenzier 首先转换为 a set of language embeddinngs。这将使得我们的方法可以在语言空间执行多模态融合，从而消除了对ad-hoc cross-modal fusion modules 的需要。</p></li><li><p>为了解决在连续输入（例如视频或音频）上tokenization 的不可微性，我们利用了一种放松方案，该方案可进行端到端训练。</p></li><li><p>进一步地，不像先前的 encoder-only models。本文提出的网络包括一个 autoregressive decoder来生成 open-ended text。同时在语言空间执行多模态融合，这使我们的方法完全具有生成性，并使其<strong>直接适用于不同的“video + $x $ to text” 问题，而无需为每个任务设计专门的网络.</strong></p></li><li><p>本文提出的框架不仅概念简单，而且效果显着。实验结果证明，our approach based on a single architecture 在三个video basedd text-generation task （captioning, question answering and audio-visual scene-aware dialog）上实现了最好的性能，而且本文提出的方法不需要任何的预训练任务。</p></li></ul><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>While this and a few other recent works [55] have leveraged decoders for text-generation from multimodal inputs, we believe <strong>we are the first</strong> to empirically demonstrate via systematic ablation the performance improvements achieved with generative learning with decoding, compared to discriminative learning applied to the same encoder model.</p><p>当前的multimodal transformer-based models inspired by the success of pretext tasks in the language domain（预训练任务）。这些工作，依赖消耗大的预训练任务。但是本文提出的VX2TEXT 可以在 unified language space 执行 跨模态融合，这不需要multimodal pretext pretraining.</p><blockquote><p>Hero: Hierarchical encoder for video language omni-representation pre-training</p><p>Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.</p><p>Videobert: A joint model for video and language representation learning, 2019.</p><p>Lxmert: Learning crossmodality encoder representations from transformers.</p><p>Unified vision-language pre-training for image captioning and vqa</p></blockquote><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>本文提出的方法可以概括为三步: (1) 利用一个 预训练的 modality-specific classifiers来为每个模态获得最可能的类别预测。(2) 将预测类别的textual names 经由本文提出的可微分tokenization scheme 嵌入到一个语义语言空间，这将使得整个系统可以端到端的训练（including the modality-specific classifiers)。（3）最终，使用一个generative encoder-decoder language model 将 多个模态的，embedding vector 映射到 free-form text，这将使得 不同形式的 ”video+$x$ to text” 问题变形为一个 sequence-to-sequence task。</p><h4 id="Differentiable-Tokenization"><a href="#Differentiable-Tokenization" class="headerlink" title="Differentiable Tokenization"></a>Differentiable Tokenization</h4><ul><li>We first leverage modality-specific classifiers trained  to predict a large set of categories over <strong>predefined language vocabularies</strong>.</li><li>虽然概念上是简单的，但是这个方法有一些缺点。第一，预训练的 modality-specific classifiers 可能不能泛化到目标数据。第二，每个分类器中选择top categories，这一操作是不可微分的，这阻止我们针对 target task 来微调modality-specific classifiers。</li><li>为了解决这些限制，本文提出了一个 differentiable tokenization scheme，这个方案可以在整个系统（modality specific classifer + sequence-to-sequence model）上进行端到端的训练。</li><li><strong>将预测类别的textual names 嵌入到一个语义语言空间</strong>：（1）对于每个模态的类别概率输出，采样top $K_m$个类别。（2）将采样的类别名称嵌入到语言空间：$\mathbf{e}_{m}^{k}=\mathbf{W}_{m}^{T} \mathbf{c}_{m}^{k}$，the embedding transformation  $\mathbf{W}_{m}$ can be initialized using a pretrained language embedding space </li></ul><h3 id="Generative-Encoder-Decoder"><a href="#Generative-Encoder-Decoder" class="headerlink" title="Generative Encoder-Decoder"></a>Generative Encoder-Decoder</h3><p>上一阶段，将不同的模态嵌入到了一个相同的语言空间，因此，现在可以使用一个<strong>text encoder</strong>来融合多模态信息。将多个模态得到的embedding vectors 组成一个长为L的序列，并结合<strong>task token</strong> 输入到 <strong>text encoder</strong>，并生成一个长为L的序列，该序列从多个模态中捕捉到了task  specific information。</p><p>将得到的新序列送入 decoder 中来做text generation。本文提出的decoder使用auto-regressive的方式。</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ul><li>使用 teacher-forcing 和 cross-entropy 来训练模型</li></ul><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul><li>大部分先前的 multimodal transformer 依赖 task-specific heads 来处理不同的任务。具体而言，为生成式任务设计的heads 通常与 判别式任务是不同的。但是，本文提出的VX2TEXT 可以同时处理这两种任务，而不需要改变结构</li><li>对于生成式任务，captioning and video dialog，使用 beam search and greedy decoding 来生成句子。</li><li>对于判别式任务，QA on TVQA，模型需要从候选答案中挑选出一个最可能的答案。在这种情况下，本文include the entire set of candidate answers as additional input to the model (using separator tokens to mark them)。然后评估每个候选答案，根据autoregressive decoder对它们输出的概率分布。</li></ul><h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><p>We use T5-base [39] as our text transformer including the text token embedding layer, the encoder and the decoder. We use pretrained weights provided in HuggingFace [50]</p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>简要总结本文有效的点</li><li>（1）提出将不同的模态，通过一个modality-specific classifier 映射到语言空间。（2）提出了一个端到端训练的模式，同时可以将 classifier 一起训练，这样解决了 迁移，泛化性不好的问题。（3）为了可以进行端到端的训练，采取了一些技术方案。we leverage the Gumbel-Softmax trick [18] and a differentiable approximation of tokenization [8].</li></ul><blockquote><p>Eric Jang, Shixiang Gu, and Ben Poole. <strong>Categorical reparameterization with gumbel-softmax.</strong>  arXiv preprint arXiv:1611.01144, 2016. <strong>ICLR 2017</strong></p><p>Yoshua Bengio, Nicholas L´eonard, and Aaron Courville. <strong>Estimating or propagating gradients through stochastic neurons for conditional computation.</strong> arXiv preprint arXiv:1308.3432, 2013.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> end-to-end </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
            <tag> end-to-end </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Adaptive Offline Quintuplet Loss for Image-Text Matching</title>
      <link href="2021/02/21/Adaptive-Offline-Quintuplet-Loss-for-Image-Text-Matching/"/>
      <url>2021/02/21/Adaptive-Offline-Quintuplet-Loss-for-Image-Text-Matching/</url>
      
        <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>现有的image-text matching 的方法一般使用<strong>在线</strong>负样本和三元组损失来训练模型。对于mini-batch中的 image 或 text anchor, 模型被训练以希望区分与anchor 相对应的 positive sample 和 most confusing negative sample。这种策略能够提高模型区分image 和 text 之间细粒度的对应或者是不对应。</p><p>但是，这种方法存在几个缺陷。（1）负样本的选择策略，给模型提供了较少的机会：从<strong>很难区分的样本</strong>中学习。（2）训练的模型从训练集到测试集的泛化性较差。（3）The penalty lacks hierarchy and adaptiveness for hard negatives with different “hardness” degrees。</p><p>在本文中，（1）我们提出了一个从整个训练集中采样 <strong>negative offline samples</strong> 的解决方法。这种方法，提供了 “harder” offline negatives than online hard negatives 让模型来区分。（2）基于 <strong>the offline hard negatives,</strong> 一个五元组损失被提出来，以提高模型的泛化性。（3）另外，提出了一个新颖的损失函数来结合 <strong>the knowledge of</strong> positives, online hard negatives and online hard negatives.</p><p>由于本文提出的方法，不是创建了一个新颖的模型，而是在采集样本与损失函数上做的改进，因此，本文在三个最好的模型上添加了本文提出的模块，并报告了实验结果，证明了本文提出方法的有效性。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li>本文使用了<strong>两轮训练</strong>来增加<strong>offline “harder” negatives</strong>。在第一轮，本文使用原始的 online triple loss来训练matching model。 然后，使用训练好的模型，对于训练集中的 image or text anchor，模型预测它们与训练中的负样本的相似性分数，并对这些负样本进行排序。在第二轮，对于mini-batch 中的每个anchor，本文从 top negative list中采取采样offline negatives. 在这个过程中，多种offline hard negative pairs被构建，这些负样本与anchor之间共享或者不共享common elements。</li><li>进一步地，本文修改了损失函数，将offline hard negative pairs的信息融合到 online triplet loss中。The complete training loss实现了对(positive pairs， offline hard negatives、online hard negatives)  <strong>分等级</strong>和<strong>自适应</strong>的惩罚，</li></ul>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
          <category> Image-Text Matching </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal,Image-Text Matching </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VMSMO Learning to Generate Multimodal Summary for Video-based News Articles</title>
      <link href="2021/02/21/VMSMO-Learning-to-Generate-Multimodal-Summary-for-Video-based-News-Articles/"/>
      <url>2021/02/21/VMSMO-Learning-to-Generate-Multimodal-Summary-for-Video-based-News-Articles/</url>
      
        <content type="html"><![CDATA[<h2 id="VMSMO-Learning-to-Generate-Multimodal-Summary-for-Video-based-News-Articles"><a href="#VMSMO-Learning-to-Generate-Multimodal-Summary-for-Video-based-News-Articles" class="headerlink" title="VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles"></a>VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles</h2><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>当前受欢迎的多媒体新闻格式是提供<strong>一个视频和相对应的文章</strong>。 这种格式广泛应用于 new media (CNN and BBC)，social media (Twitter and Weibo)。<br>在这种情况下，自动选择<strong>合适的视频封面</strong>并生成相应的<strong>文章摘要</strong>可帮助编辑人员节省时间，并使读者更有效地做出决定。</li><li>因此，在本文中，we propose the task of <strong>Video-based Multimodal Summarization with Multimodal Output</strong> (VMSMO) to tackle such a problem. </li><li>此任务中的主要挑战是使用文章的语义共同对视频的时间依赖性进行建模。</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>视频的封面应该是整个视频最显著的要素，而文本摘要应该是从原文章中提取出来的重要信息。因为视频和文章都关注于同一个报告内容中的相同事件，因此这两种信息形式在 summarizing 过程中应该是<strong>互为补充的</strong>。但是由于视频和文章是不同的模态（空间），如何充分的探索视频中的时域依赖与文章中的语义内容<strong>之间的关系</strong>仍然是一个问题。</p><p>因此，在本文中，我们提出了一个模型（DIMS）。该模型通过在过程中实施一个 dual interaction strategy 来同时summarize video and article。（1）使用 RNN 来编码 text and video. （2）设计了一个对偶交互模块（a dual interaction module）来让视频和文章相互交互。具体地，包括一个conditional self-attention mechanism 该模块可以在文章的指导下学习 local video representation。还包括一个global-attention mechanism 来学习 high level representation of video-aware article and article-aware video。（3）最后，based on fusion repersentation multimodal generator generates textual summary and cover image。</p><p>为了证明本文提出模型的有效性，本文从社会媒体网站上收集了 a large-scale news article-summary dataset associated with video-cover。实验证明，在当前广泛使用的评价标准上，DIMS可以显著的超过当前最好的baseline methods.</p><h2 id="本文的贡献"><a href="#本文的贡献" class="headerlink" title="本文的贡献"></a>本文的贡献</h2><ul><li>提出了一个任务，需要同时生成一个视频封面和一个文本摘要。</li><li>针对该任务，提出了一个模型，该模型可以同时建模视频中对的时域依赖和文章中的语义信息。</li><li>本文提出了一个大规模的数据集。在该数据集上，本文提出的方法在 automatic and human evaluation上都显著好于 baseline methods。</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><ul><li><p>Text Summarization: (1) Extractive models：从原文中提取一句话来表征整个文章的内容。（2）abstractive models：generate a summary from scratch</p></li><li><p>Multimodal Summarization：（1）结合多模态的输入，生成更高的texual summaries。（2）Multimodal<br>summarization with multimodal output 这一方向，研究的相对较少。[zhu 2018] 提出同时输出一个 textual summary 和 从6个候选中挑选出来的most relevent image。[zhu 2020] 增加了一个多模态的目标函数。</p><p>但是，在实际应用中，我们通常需要为包含数百帧的视频选择封面图。 因此，视频中帧之间的时间相关性不能通过几种静态编码方法简单地建模。</p><blockquote><p>[zhu 2018] Msmo: multimodal summarization with multimodal output. EMNLP/IJCNLP. </p><p>[zhu 2020] Multimodal summarization with guidance of multimodal reference.</p></blockquote></li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://i.loli.net/2021/02/21/KqO2uQEkZj1fyPB.png" alt="image-20210221114344434"></p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>本文从 <strong>微博</strong> 上收集了做 VMSMO task 的数据集。视频的平均时长是1分钟，帧率是25fps。对于文本，文章的平均长度是96个字，文章summary的长度是12个字。整体上，有184k 个样本被收集，180k作为训练集，2.4k作为验证集，2.4k作为测试集。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ul><li><p>compare baseline：本文提出的方法与summarization baseline 和 VQA baseline 进行比较。</p></li><li><p>evaluation metrics：<br>（1）评估生成的summary：standard full-length <strong>Rouge F1</strong>. R-1, R-2, and R-L refer to unigram, bigrams, and the longest common2 subsequence respectively.</p><p>（2）评估chosen cover frame: mean average precision （MAP）and recall at precision（$R_n@k$）。$R_n@k$用来评估是否positive sample 被排在n candidatas的前k个位置。</p></li><li><p>实验结果证明，（1）本文提出的方法相比于 baseline methods 都要好。（2）本文提出了一个联合损失，同时训练两个任务，可以看做是一个 Multi-task。在本文的实验中，探索了，如果分别训练这两个任务，是怎样的结果。从倒数第二列中可以发现，我们采用的多任务方式训练，效果会好。（3）在本文实验中探索了conditional self-attention 和 global-attention对效果的影响。从最后一列的实验结果可以发现，self-attention 模块对挑选 封面图有很大的贡献，global attention对生成文本摘要有很大的贡献。</p></li></ul><p><img src="https://i.loli.net/2021/02/21/PcFyBkTZNQlHY8m.png" alt="image-20210221154509463"></p><h2 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h2><ul><li>本文最后说到，可以结合video  script (subtitles) 来做该任务。</li></ul>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BART</title>
      <link href="2021/02/19/BART/"/>
      <url>2021/02/19/BART/</url>
      
        <content type="html"><![CDATA[<h1 id="BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and-Comprehension"><a href="#BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and-Comprehension" class="headerlink" title="BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"></a>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>本文提出了一个<strong>denoising autoencoder</strong> (BART)来预训练一个 sequence-to-sequence model。It is implemented as a sequence-to-sequence model<br>with a bidirectional encoder over corrupted text and a<br>left-to-right autoregressive decoder</li><li>BART 通过两点来训练：（1）使用一个<strong>arbitrary noising function</strong> 来对原始文本添加噪声。（2）让模型去学习重构原始文本。</li><li>BART使用<strong>一个标准的 transformer-based neural machine translation architecture</strong>。由于采用了 <strong style="color:blue;">bidirectional encoder</strong> 可以看做是对BERT的推广，由于其采用了 <strong style="color:blue;">the left-to-right decoder</strong> 也可以看做是对GPT2的推广，同时也采用了许多现在广泛使用的预训练方案。</li><li>本文对许多 noising approaches 进行评估，发现最好的方案是对原始的句子进行打乱顺序。本文采用了一个 novel in-filling scheme，<strong>使用spans of text 来代替 a single mask token</strong>。</li><li>实验结果：（1）当对文本生成进行微调之后，BART 对生成式任务的性能尤其的好。BART对理解型任务也表现很好。（2）BART与RoBERTa的性能相匹配，在 GLUE 和 SQuAD 上有可比较的training resources，在一系列  abstractive dialogue, question answering, and summarization tasks 取得了最新成果，并在ROUGE上获得了多达6个点的提升。（3）BART在机器翻译任务上，仅使用 target language 来预训练。在BLEU 指标上，获得了比 back translation system 1.1个点的提升。</li><li>消融实验：使用BART framework，采用不同的预训练方案，来确定影响了最终任务性能的关键因素是什么。</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>现存方法的一个问题：However, these methods typically focus on particular types of end tasks (e.g. span prediction, generation, etc.), limiting their applicability.<br>yaya: 现存的方法可以只限定对某一类任务有效，如只对 comprehensive tasks有效，而对 generation tasks无法使用。</li><li><p>在本文中，提出了一个可以结合 <strong style="color:red;">bidirectional</strong> and <strong style="color:blue;">auto-regressive</strong> Transformers. BART是使用<strong style="color:blue;">序列到序列</strong>模型构建的<strong style="color:red;">去噪自动编码器</strong>，BART可以适用于非常广泛的最终任务。</p></li><li><p>本文提出框架的优点：噪声的灵活性，可以将任意转换应用于原始文本，包括更改其长度。</p></li><li><p>本文对许多 noising approaches 进行评估，发现最好的方案是 shuffling the order of the<br>original sentences。本文采用了一个 novel in-filling scheme，使用spans of text 来代替 a single mask token。<br>本文提出的方法通过强制模型对更多的总体句子长度进行推理，并对输入进行更远距离的转换，从而泛化了BERT中的两个预训练任务（word masking and next sentence prediction）</p></li><li><p>BART还开辟了关于微调的新思路。本文提出了一种新的机器翻译方案，其中BART模型堆叠在其他几个transformer layers 之上。这些层经过培训，从而将外语从本质上翻译为噪声英语，可以通过BART进行传播，从而将BART用作预训练的目标方语言模型。</p></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p><strong>BART</strong> is a denoising autoencoder that maps a corrupted document to the original document it was derived from. </p><p>It is implemented as <strong>a sequence-to-sequence model</strong> with <strong>a bidirectional encoder</strong> over corrupted text and <strong>a left-to-right autoregressive decoder</strong>. For pre-training, we optimize the negative log likelihood of the original document.</p><p><img src="https://i.loli.net/2021/02/19/BCSUOQkGX5KcJTR.png" alt="image-20210219151202969" style="zoom:50%;"></p><h2 id="Pre-training-BART"><a href="#Pre-training-BART" class="headerlink" title="Pre-training BART"></a>Pre-training BART</h2><p>Unlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to apply any type of document corruption.</p><p>解释：BERT 对句子中的token进行随机掩码，采用的特定类型的 noising schemes，而 BART 可以应用任意类型的corruption。<strong>极端情况下，当原句中的所有信息都丢失时，BART可以看做是一个 language model。</strong></p><p>本文采用了 几个先前提出的变换，和几个自己新提出的变换。</p><p><img src="https://i.loli.net/2021/02/19/LxrawUIdKp8TyFX.png" alt="image-20210219152433064"></p><p>（1）Token Masking.</p><p>（2）Token Deletion. 模型必须去决定哪个在位置丢失了。</p><p>（3）Text Infilling. 采样了多个 text spans, spans lengths 从一个泊松分布（$\lambda$ = 3）中随机采样。每个span被替换成一个[MASK]。<strong>Text infilling teaches the model to predict how many tokens are missing from a span.</strong> yaya:仅仅是预测[MASK 位置处有几个tokens，而不需要预测具体点的tokens是什么]</p><p>（4）Sentence Permutation. 根据停止符，将一个document分成多个句子，然后打乱句子的顺序，</p><p>（5）Document Rotation. A token is chosen uniformly at random, and the document is rotated so that it begins with that token. This task trains the model to identify the start of the document.</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Evaluating Models&#39; Local Decision Boundaries via Contrast Sets</title>
      <link href="2021/02/19/Evaluating-Models-Local-Decision-Boundaries-via-Contrast-Sets/"/>
      <url>2021/02/19/Evaluating-Models-Local-Decision-Boundaries-via-Contrast-Sets/</url>
      
        <content type="html"><![CDATA[<p><strong>论文标题：</strong>Evaluating Models’ Local Decision Boundaries via Contrast Sets</p><p><strong>论文链接：</strong><a href="https://arxiv.org/abs/2004.02709" target="_blank" rel="noopener">https://arxiv.org/abs/2004.02709</a></p><p><strong>数据集：</strong><a href="https://allennlp.org/contrast-sets" target="_blank" rel="noopener">https://allennlp.org/contrast-sets</a></p><p><strong>Main Contribution：</strong>训练集与测试集 i.i.d 的假设使得模型很难泛化，文章提出了在原始测试集构建 contrast test set 的方法，可以真实的评估模型的语言能力。</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a><strong>Motivation</strong></h2><p>这里用了一个 toy example 和一个真实示例来表示作者想要提出并解决的问题。</p><p><strong>Toy Example</strong></p><p>考虑二维的情况，下图中的两分类问题需要一个非常复杂的 decision boundary。</p><p><img src="https://i.loli.net/2021/02/19/sfK5PyknNElIVoJ.png" alt="image-20210219175052164" style="zoom:33%;"></p><p>但是在很多情况下，由于采样的 biased，我们很可能得到如下的数据集：</p><p><img src="https://i.loli.net/2021/02/19/PBUDYm6LK7S3IN5.png" alt="image-20210219175104378" style="zoom:33%;"></p><p>网络通过一个很简单的 decision boundary 就可以将它们分类，而由于训练测试数据集独立同分布，虽然这个 decision boundary 非常差，但它在测试集表现得非常好。理想情况下，如果我们完整采样整个数据集，所有问题都迎刃而解，但这显然是很难做到的。为了正确的测试模型的能力，作者提出了对测试集做 perturbation 的方法：对测试集的每一个实例，我们生成一系列与之类似的测试样本（Contrast Set：下图中的灰色圆圈）。</p><p><img src="https://i.loli.net/2021/02/19/CdF9GOxAJe2aEv3.png" alt="image-20210219175121207" style="zoom:33%;"></p><p><strong>Complex NLP Task</strong></p><p>我们很难用图把 NLP task 中存在的问题进行描述，但是有很多工作支撑了这一观点。比较有意思的示例为在 SNLI 数据集中，表明单词”睡觉”，”电视”和”猫”几乎从来没有同时出现数据中，但是它们经常出现在 contradiction 的例子中。所以 model 很容易的学到“同时出现’睡觉’和’猫’的句子都是 contradiction sentence，并且这一分类标准工作得很好”。 </p><p>在初始数据收集过程中完全消除这些差距将是非常理想化的，在一个非常高维的空间中，语言有太多的可变性。相反，该文使用 Contrast Set 来填补测试数据中的空白，从而给出比原始数据提供的更全面的评估。</p><h2 id="Contrast-sets"><a href="#Contrast-sets" class="headerlink" title="Contrast sets"></a><strong>Contrast sets</strong></h2><p>假设我们现在为测试样本 构建 Contrast Set，有两个要点 (i) 构建样本距离与  小于某个阈值。(ii) Label 与 <strong>不一致</strong>。下图是在 NLVR2 数据集上的一些实例，在这里，句子和图像都通过一些很简单的方式进行修改（例如，通过改变句子中的一个词或找到一个相似但有区别的词），从而使输出标签发生变化。</p><p><img src="https://i.loli.net/2021/02/19/tlN7BZIVnDCsYed.png" alt="image-20210219175240236" style="zoom:50%;"></p><p>我们需要注意，contrast set 和 adversarial examples 是不一样的，对抗样本的目的是对句子/图像做 perturbation，但是保持原标签不变。 </p><p>不过文章中如何计算样本距离，阈值的确定，label 是否发生变化，都是由 expert 给出的。</p><h2 id="How-to-Create-Contrast-Sets"><a href="#How-to-Create-Contrast-Sets" class="headerlink" title="How to Create Contrast Sets"></a><strong>How to Create Contrast Sets</strong></h2><p>作者用了三个数据集来展示 Contrast Sets 的构造过程。</p><p><strong>DROP</strong></p><p>DROP 是一个阅读理解数据集，旨在涵盖对段落中的数字进行组合推理，包括过滤、排序和计数，以及进行数值运算。数据主要来自 (i) Wikipedia (ii) 美国足球联赛的描述。(iii) 人口普查结果说明。(iv) 战争摘要。作者发现数据集中存在明显的 bias，比如一旦问题是”How many…”，结果很多情况都是 2。关于事件顺序的问题通常遵循段落的线性顺序，而且大部分问题不需要理解。 </p><p>作者从三个方面改进这个数据集：</p><ul><li>关于足球联赛的问题往往需要推理和比较（比如询问两场比赛得分的差值），但是其他类型的数据很少需要推理比较，因此作者为他们提供额外的需要推理比较的问题；</li><li>将问题的部分语义颠倒，类似于 shortest 变为 longest, later 变为 earlier, How many countries 变为 which countries 等等；</li><li>改变事件发生的顺序，使得与事件顺序相关的问题推理难度增加。</li></ul><p><strong>NLVR2</strong></p><p>给模型一对图像与一个句子，判断这句话正确与否。这个数据集的特点在于 compositional reasoning，我们需要模型理解图像中的物体的属性，物体与物体的关系，物体与场景的关系。 </p><p>我们通过修改句子或用网络搜索中获得自由许可的图片替换其中一张图片来构建 NLVR2 的 Contrast Set。 </p><p>比如将句子”The leftimage contains twice the number of dogs as theright image”改为“The left image containsthree timesthe number of dogs as the right image”。或者对一个图像对，将原本 4 条狗的图像换成其他数目。也可以对一些量词比如”at least one”改为”exactly one”，或者实体”dogs”改为”cats”，或者属性”yellow”改为“red”。</p><p><strong>UD  Parsing</strong> </p><p>这是一个 dependency parsing 的数据集。作者想要通过这个数据集证明 Contrast set 不仅在 high-level 的 NLP 任务中有效，也在语义分析的任务中有效。具体方法可以查看原文。</p><p><img src="https://i.loli.net/2021/02/19/CzWXEFJs63dDHAq.png" alt="image-20210219175335245"></p><p>可以看到，再加上 Contrast Set 之后，SOTA models 的性能都有了显著的下降。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP,evaluation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>On calibration of modern neural networks</title>
      <link href="2021/01/07/On-calibration-of-modern-neural-networks/"/>
      <url>2021/01/07/On-calibration-of-modern-neural-networks/</url>
      
        <content type="html"><![CDATA[<p>On Calibration of Modern Neural Networks</p><h2 id="Calibration-一个工业价值极大，学术界却鲜有研究的问题！"><a href="#Calibration-一个工业价值极大，学术界却鲜有研究的问题！" class="headerlink" title="Calibration: 一个工业价值极大，学术界却鲜有研究的问题！"></a>Calibration: 一个工业价值极大，学术界却鲜有研究的问题！</h2><p>原创 kid丶 <a href="javascript:void(0" target="_blank" rel="noopener">夕小瑶的卖萌屋</a>;) </p><blockquote><blockquote><blockquote><blockquote><p>文 | kid丶(知乎作者)<br>编 | 夕小瑶</p></blockquote></blockquote></blockquote></blockquote><p>尽管深度学习给工业界带来了一波上线春天，但是总有很多比较难的业务，模型反复迭代后准确率依然达不到预期的产品标准，难以满足用户期望。</p><p>以下为工业界常见讨（si）论（b）场景：</p><p>R&amp;D小哥哥一顿调参输出，RoBERTa都用上了，终于将模型从80%准确率提升到了90%，但是PM小姐姐说，“不行！咱们必须要达到95%准确率才能上线！否则就是对用户和产品逼格的伤害！”</p><p>怎么办呢？</p><p>熟悉工业界上线套路的小伙伴马上就能给出答案，那就是 <strong><em>提高模型决策的阈值！</em></strong> PM小姐姐只是根据产品标准定义了模型准确率（或者说精确率，precision），但是并不在乎召回率有多高（毕竟模型只要没上线，就相当于召回率为0）。</p><p>那么基于上面的思路：假如模型的softmax输出可靠，比如二分类场景，模型softmax之后1类的输出是0.92，能表征模型有92%的把握说这是个正例，并且模型的这个把握是精准的，那么PM小姐姐说要达到95%准确率，那我们就疯狂提高模型的决策阈值就好了，这样把那些不确定性高的样本砍掉了，模型准确率自然就上来了。</p><p>然而，神经网络并不一定这么靠谱，你看模型的测试集输出的话，却常常发现模型要么以99.999的概率输出来判定正例，要么0.0001的概率输出来判定负例，基本没有样本落在0.1~0.9区间内。那么这时候上面的思路就失效了。</p><p>那么有没有办法<strong>让模型的softmax输出能真实的反映决策的置信度呢？</strong> 这个问题，就被称为Calibration问题（直译是叫“校准”）。</p><p>故事要从一篇发表于2017年的ICML顶会论文开始，目前这篇论文引用量1001。</p><p><strong>论文标题：</strong></p><p>On Calibration of Modern Neural Networks</p><p><strong>链接：</strong></p><p><a href="https://arxiv.org/pdf/1706.04599.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.04599.pdf</a></p><p>Arxiv访问慢的小伙伴可以在【<strong>夕小瑶的卖萌屋</strong>】后台回复关键词【<strong><em>0106</em></strong>】下载论文pdf~</p><h2 id="神经网络的-overconfidence"><a href="#神经网络的-overconfidence" class="headerlink" title="神经网络的 overconfidence"></a>神经网络的 overconfidence</h2><p><img src="https://i.loli.net/2021/01/07/cDzC2UkMQGL3y8I.png" alt="image-20210107121456693" style="zoom: 50%;"></p><p>首先，让咱们来思考一个普通图像分类任务。对于一张“koala”的图像，在经过神经网络后会得到 logits 输出 ，经过 softmax 层后得到对各类别的预测的后验概率，接着我们选择概率最大的类别（ koala）输出为最后的预测类别。这里，最终的预测类别 ，其对应的置信度为 。在大多情况下，我们只关心类别的预测 有多准，根本不 care 置信度是怎样的。然而，在一些实际应用场景下，置信度的度量也同样重要。例如：</p><p><img src="https://i.loli.net/2021/01/07/HILjXfdVBk7v1hw.png" alt="image-20210107121544230" style="zoom: 50%;"></p><p>如上图，对于自动驾驶中的目标识别任务，车辆的前方出现了一个人，神经网络会将其识别成塑料袋，此时输出的置信度为50%（低于阈值），则可通过其它传感器进行二次的正确识别（识别为人）。但想想看，若神经网络对塑料袋预测的置信度为90%会怎样？再例如：</p><p><img src="https://i.loli.net/2021/01/07/3CUylrwH4u6csDG.png" alt="image-20210107121606617" style="zoom:50%;"></p><p>使用 Resnet 模型简单的对一些图片任务进行训练，收敛后的模型对测试集的平均置信度高达80%-85%，然而只有将近70%的图片能被正确分对（红色代表分错，绿色代表分对）。这意味着啥？训练好的模型好像有点盲目自信，即出现 <strong>overconfidence</strong> 现象，或者可以称为模型的准确率和置信度不匹配（<strong>miscalibration</strong>）。</p><h2 id="预期校准误差（ECE）"><a href="#预期校准误差（ECE）" class="headerlink" title="预期校准误差（ECE）"></a>预期校准误差（ECE）</h2><p>直观的来看，模型的准确率应当和置信度相匹配。一个完美校准的模型可定义成如下所示：</p><p>即，模型置信度 等于概率 的条件下模型的预测 为真实标记 的概率同样也为 。因此，本文提出一个新的度量方式叫做 <strong>预期校准误差（Expected Calibrated Error, ECE）</strong> 来描述模型学习的匹配程度：很简单，其实就是将前面那个完美校准模型的等式写成差的期望的形式。我们将期望进一步展开可得到：</p><p>其中： 这里的 代表着一个个根据置信度区间划分的一个个桶（用来装样本的），如下图所示：</p><p><img src="https://i.loli.net/2021/01/07/KS6p2nRIhMdtgjv.png" alt="image-20210107121742204" style="zoom:50%;"></p><p>例如，我们将置信区间平均划分成5份，然后将样本按照其置信度挨个送到对应的桶中，分别计算每个桶中的平均置信度和准确率，两者的差值（Gap）的期望就是所定义的 <strong>ECE。</strong></p><p><strong>读到这的读者</strong>应该能逐步体会本文想干一件啥事了。本文首先引出这样一个问题，深度模型在学习过程中出现准确率和置信度的严重不匹配问题，接着提出了一个合理的评价指标来描述模型学习的匹配程度，所以接下来，它要提出方法来想办法<strong>最小化期望校准误差（ECE）。</strong></p><h2 id="什么原因导致神经网络出现准确率与置信度不匹配？"><a href="#什么原因导致神经网络出现准确率与置信度不匹配？" class="headerlink" title="什么原因导致神经网络出现准确率与置信度不匹配？"></a>什么原因导致神经网络出现准确率与置信度不匹配？</h2><p>然而 <strong>ECE</strong> 是没办法直接最小化的，因此本文尝试着做一些探索性的实验来观察啥因素会使得模型的 ECE 变大。本文分别从三个方面上去进行实验：</p><p><img src="https://i.loli.net/2021/01/07/kniAWZNqaVBwQPG.png" alt="image-20210107121834360"></p><p>▲网络复杂度对ECE的影响</p><p><strong>网络复杂度对 ECE 的影响：</strong> 首先，作者使用两个模型（LeNet和ResNet）分别对CIFAR-100数据集进行了训练，准确率分别为55.1%和69.4%，ResNet 在预测性能上完爆LeNet。然而，ResNet 置信度（右图蓝色+红色部分）的分布和准确率（右图蓝色部分）出现了严重的不匹配，导致二者的 Gap （红色部分）非常大。**注意完美校准模型的分布应当是蓝色部分刚好和对角线重合，且没有红色 Gap 部分。</p><p><img src="https://i.loli.net/2021/01/07/OQX8Ir64DqyBwdl.png" alt="image-20210107114831568" style="zoom: 33%;"></p><p>▲网络的宽度和深度对ECE的影响</p><p><strong>网络宽度和深度对 ECE 的影响：</strong> 在得知模型复杂度会影响模型的 ECE 后，作者紧接着做了网络宽度和深度对模型 ECE 和错误率（Error）的影响。可以看到，在控制变量前提下，单方面的增加网络的深度和宽度均会使得模型的 Error 降低，这是我们所期望的；然而，ECE也会同样的随着上升。<strong>换句话来说，一昧的增加模型复杂度能有效的提高模型的预测性能，但同样带来的问题是模型的 overconfidence 问题愈发严重。</strong></p><p><img src="https://i.loli.net/2021/01/07/v1S9YrfQBt5sl4O.png" alt="image-20210107114951774" style="zoom:33%;"></p><p>▲归一化和权重衰减对ECE的影响</p><p><strong>normalization 和 weight decay 对 ECE 的影响：</strong> 接着的实验也是我们为提高模型性能经常使用的 batch normalization 和 loss regularization。<strong>左图：</strong> 使用 batch normalization 会有效的提升模型的性能，但同时也会提升模型的 ECE。<strong>右图：</strong> weight decay 通常用来调节 L2 正则的权重衰减系数，随着其系数的增加相当于更多的强调模型参数 w 要尽可能的小，能有效的防止模型过拟合。<strong>该现象表明，模型越不过拟合，其ECE是越小的，也就是说模型越不会 overconfidence ；换句话说，模型对样本的拟合程度和对样本的置信度是息息相关的，拟合得越好，置信度越高，所以 ECE 越大。（个人理解，欢迎评论区指正~）</strong></p><h2 id="我们该如何对模型进行校准呢？"><a href="#我们该如何对模型进行校准呢？" class="headerlink" title="我们该如何对模型进行校准呢？"></a>我们该如何对模型进行校准呢？</h2><p><img src="https://i.loli.net/2021/01/07/8WvRUEQtXg6iseY.png" alt="image-20210107115025920" style="zoom:33%;"></p><p>作者接下来又做了一个很有意思的实验，在CIFAR-100上训练模型500个 epoch，其中在第250个 epoch 和第375个 epoch 下调节学习率，观察测试集上的 test error 和 test NLL 的变化情况。Test NLL 的定义如图中所示，它其实等价于测试集上的交叉熵。这个实验啥意思呢？我调节了一下学习率后，测试性能得到了提升，但是测试集上的交叉熵却出现了过拟合现象（出现了反常的上升现象）。<strong>有意思的点来了！</strong> 有人肯定会 argue 不是说好本文研究的是overconfidence嘛？即模型的置信度太高而准确率过低，这里对 NLL overfitting 岂不是好事，因为负对数似然上升了等价于模型的置信度的降低了。<strong>注意：这里的</strong> <strong>是对正确类上的置信度，而前面的实验是对预测类的置信度</strong> <strong>！其实认真想想，是一个意思，前面之所以 confident 很高的样本准确率很低，正是因为其在正确类别上的置信度太低导致的！！（这部分卡了很久）</strong></p><p>该结果可以表明，模型置信度和准确率的不匹配很大可能的原因来自于模型对 NLL 的过拟合导致的。所以，咋办呢？最小化 NLL 呗。</p><p><img src="https://i.loli.net/2021/01/07/gzr4juPYwMFyiSH.png" alt="image-20210107121901763"></p><p>此时，本文提出在验证集上对带 temperature 参数的 softmax 函数进行校准。即我们训练完模型后，最小化 NLL 来学习 temperature 参数，注意到对该项的优化并不会影响模型预测的准确率，只会对模型的 confidence 进行校准。最终的结果是这样的，详细可参考论文。</p><p><img src="https://i.loli.net/2021/01/07/dDNYosvc2tprZaE.png" alt="image-20210107115122188"></p><h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a><strong>讨论</strong></h2><p>上述得实验结果我觉得对很多研究领域都是很有启发意义的。</p><ol><li>模型的置信度应当是和准确率匹配的，这样的模型我觉得才是有意义的，否则以很高置信度进行很离谱的预测错误的模型会让人感觉这个模型好像什么都会、又好像什么都不会。</li><li>ECE 的指标是否能反应样本的一些性质，例如难易程度、是否为噪声等。</li><li>该文章是间接的去优化ECE的，能否有直接优化的形式，或者主动学习里面能否考虑这一点来挑选样本？</li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>End-to-End Learning of Visual Representations from Uncurated Instructional Videos</title>
      <link href="2020/12/30/End-to-End-Learning-of-Visual-Representations-from-Uncurated-Instructional-Videos/"/>
      <url>2020/12/30/End-to-End-Learning-of-Visual-Representations-from-Uncurated-Instructional-Videos/</url>
      
        <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/12/30/yfHDiZusbKvM2AY.png" alt="image-20201230171856275" style="zoom:67%;"></p><p><img src="https://i.loli.net/2020/12/30/C1HmZXKLzFlBDGM.png" alt="image-20201230171925959" style="zoom:67%;"></p><p><img src="https://i.loli.net/2020/12/30/3AudkSImHtnrwha.png" alt="image-20201230172037294"></p>]]></content>
      
      
      <categories>
          
          <category> Video Pre-trained Models </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Video Pre-trained Models </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Blank Language Model</title>
      <link href="2020/12/30/Blank-Language-Model/"/>
      <url>2020/12/30/Blank-Language-Model/</url>
      
        <content type="html"><![CDATA[<h3 id="yaya-序"><a href="#yaya-序" class="headerlink" title="yaya 序"></a>yaya 序</h3><h4 id="直接做文本填充任务的模型"><a href="#直接做文本填充任务的模型" class="headerlink" title="直接做文本填充任务的模型"></a>直接做文本填充任务的模型</h4><ul><li><p>目前已知的做文本填充的模型有ilm, BLM</p><blockquote><p><strong>ilm</strong>: <strong>(ACL 2020) Enabling Language Models to Fill in the Blanks</strong></p><p><strong>BLM</strong>:  <strong>(EMNLP 2020) Blank Language Models</strong></p><p>Text Infilling</p><p>Langsmith An Interactive Academic Text Revision System</p><p>MaskGAN Better Text Generation via Filling in the__</p><p> (ACL 2020) INSET Sentence Infilling with INter-SEntential Transformer</p></blockquote></li></ul><h4 id="直接可以做文本填充任务的模型"><a href="#直接可以做文本填充任务的模型" class="headerlink" title="直接可以做文本填充任务的模型"></a>直接可以做文本填充任务的模型</h4><ul><li><p>其次，做文本任务 对抗的一些任务，其实也可以看做文本填充</p><ul><li>AdvExpander Generating Natural Language Adversarial Examples by Expanding Text</li><li>BERT-ATTACK Adversarial Attack Against BERT Using BERT</li><li>Generate Your Counterfactuals Towards Controlled Counterfactual Generation for Text</li></ul></li><li><p>另外 padlepadle/ ERNIE 的预训练任务，不使用 sub-word 来随机掩码，而是使用 fragment来做随机掩码，其预训练模型，也是可以用来做文本填充的。但是可能性能没有那么可靠</p></li></ul><h3 id="BLM介绍"><a href="#BLM介绍" class="headerlink" title="BLM介绍"></a>BLM介绍</h3><p>来源： <a href="https://mp.weixin.qq.com/s/cVUT4FMpgqARuWf5dWY0bA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/cVUT4FMpgqARuWf5dWY0bA</a></p><p>讲者提出了填空语言模型（Blank Language Model, BLM），<strong>该模型通过动态创建和填充空白来生成序列</strong>。空白用于控制要扩展序列的那一部分，使BLM成为各种文本编辑任务的理想选择。<strong>该模型可以从单个空白或在指定位置带有空白的部分完成文本开始，迭代地确定要在空白中放入哪个单词以及是否插入新的空白，并在没有新的空白填充时停止生成。</strong>使用边缘似然的下界可以有效地训练BLM。在填充缺失文本的任务上，BLM在准确性和流利性方面均明显优于所有其他方法。在情感迁移和修复古文字的实验中，证明该框架具有广泛的应用潜力。</p><h4 id="一、动机：传统语言模型的局限性"><a href="#一、动机：传统语言模型的局限性" class="headerlink" title="一、动机：传统语言模型的局限性"></a><strong>一、动机：传统语言模型的局限性</strong></h4><p>传统的语言模型通常是从左到右对文本序列进行建模，其模式是，首先生成第一个词，然后以第一个词作为condition来生成第二个词，继而以第二个词为condition生成第三个词，如此迭代计算后一个词。</p><p>该方法的优势在于算法简单、有效。但大多情况下，并不需要从头开始生成文本，而是已有部分文本，想自动生成剩下的部分内容。比如，文本编辑，是基于已有的draft，修改文本中任意位置的内容；或是template filling，针对一些表格等具有固定格式的文件，比如医疗或者法律文件等进行填空；或是text restoration，比如一些文件可能在任意位置缺失相关内容，需要做的是复原损失部分。由于Left-to-Right Language Model仅仅考虑blanks左边的文本语境，不能很好地对这些应用进行建模。</p><p><img src="https://i.loli.net/2020/12/30/b4piu9OP6x3VINC.png" alt="image-20201230120139264" style="zoom:50%;"></p><p><strong>因此，讲者提出了Blank Language Model（BLM）</strong>，其输入形式为任意文本，blanks可存在于文本的任意位置；每一个blank可以对应任意多个单词；且BLM模型会结合上下文语境决定需要填充几个单词。BLM与Mask Language Model的区别在于，Mask Language Model的一个mask只能对应一个单词，因此如果预先不知道blank中间丢失多少个单词，就不能使用Mask Language Model来实现。</p><p><img src="https://i.loli.net/2020/12/30/uN759vob1MEhyRO.png" alt="image-20201230120251358" style="zoom:50%;"></p><h4 id="二、实现：什么是BLM"><a href="#二、实现：什么是BLM" class="headerlink" title="二、实现：什么是BLM?"></a>二、实现：什么是BLM?</h4><h4 id="1）BLM工作原理介绍"><a href="#1）BLM工作原理介绍" class="headerlink" title="1）BLM工作原理介绍"></a><strong>1）BLM工作原理介绍</strong></h4><p><strong>BLM具体是如何实现填空的呢？</strong>首先存在一个画布，有单词和blank，其中blank用来控制单词能被放置的位置；然后BLM模型在这个画布上动态进行修改，每一步都会选择一个待填充的blank。</p><p>由于每个blank可以对应任意数量的词，因此当填充这个word w之后，还要决定是不是只填一个字，或者在w左边、右边、还是两边各补上blank。由此每个blank就可以继续迭代，且被扩展成任意多个词，当没有blank留下，模型达到终止条件。其实现步骤如图3所示。</p><p><img src="https://i.loli.net/2020/12/30/41uMQY2dqSroxZD.png" alt="image-20201230120357144" style="zoom:50%;"></p><p>yaya 觉得以下这个例子更好一些</p><p><img src="https://i.loli.net/2020/12/30/1N9C5R4ecJM6ADB.png" alt="image-20201230120728774" style="zoom: 50%;"></p><p><strong>讲者举如下例子，进一步说明模型的实现步骤。</strong>原句是They also have <strong><strong> which </strong></strong>.通过从canvas中一步步选择合适的word和blank，直至句子中没有新的blank生成为止，实现句子填空。</p><p><strong>继而，讲者概括BLM的工作方式</strong>，类似于一个Grammar，如图4所示。其与传统语法<strong>Context-Free Grammar</strong>的区别在于：讲者所提出的模型在<strong>Production rules</strong>上面的概率分布是取决于模型参数和当前的<strong>context</strong>。</p><p><img src="https://i.loli.net/2020/12/30/ucYZU5Ld7T9tysv.png" alt="image-20201230120451316" style="zoom: 50%;"></p><h4 id="2）BLM框架介绍"><a href="#2）BLM框架介绍" class="headerlink" title="2）BLM框架介绍"></a>2）BLM框架介绍</h4><p>首先采用transformer模块，将input通过这个transformer以得到一系列的representation，每个representation包含有其context信息；</p><p>然后将所有blank位置的representation通过Linear project和softmax得到blanks上的distribution，选择其中一个（假设是第二个）；</p><p>进一步将被选中的blank representation投影到整个vocabulary，由此可预测一个词，比方说预测really；</p><p>最后，将blank representation和被预测词的word embedding进行拼接，都输入给一个MLP进行如图所示的四种情况分类。假设是第四种情况，得到<strong><strong> really </strong></strong>，然后将其fill到第二个Blank中，并继续这个过程直至没有新的blank生成。</p><p><img src="https://i.loli.net/2020/12/30/9jZpWxPyeuR3Cdn.png" alt="image-20201230121033747" style="zoom: 50%;"></p><p>从一个初始的blank到一个没有blank的complete text这个过程称为<strong>trajectory</strong>，包括每一步的canvas和action；每一步的action包括了choose a blank，predict a word 以及create new blanks。</p><p>如果一个句子有n个单词，那么会有n！种不同的trajectory都可以生成它，每一种trajectory对应着不同的word insertion order。如图6所示的trajectory就对应着word insertion order “3 1 10 6 2 8 4 7 5 9” 。</p><p>因此，生成一个句子x的概率就是以所有trajectory/word insertion order生成它的概率之和，公式中的Sn为所有的n排列。而给定order，就可以确定每一步的canvas和action，概率可相应分解为每一步的概率的乘积。</p><p><img src="https://i.loli.net/2020/12/30/k4ZMPjR5TmFycXA.png" alt="image-20201230130236667" style="zoom:50%;"></p><p><strong>如何高效的训练BLM呢？</strong>讲者对上述的likelihood进行估计，对等式两边取log，并借助图7中蓝色不等式可得到最终评估的loss。但同时讲者介绍到该训练方式与left-to-right language model相比并不高效，原因在于：Left-to-Right Language Model在一个pass中是同时计算了n个词的Loss，而上述公式在一个pass中只计算了一个action loss。</p><p>基于此，讲者进一步介绍到ctx,o只与word insertion order o的前t个词有关，因此可在一个pass中，将前t个order相同且在t+1时不同的trajectory进行组合，共同计算loss。同时由于EOt+2:n不会影响第t步的action和canvas，则可改写最终的loss表达，实现在一个pass中计算期望n/2个action loss。</p><p><img src="https://i.loli.net/2020/12/30/2xnYKAswB6RO47o.jpg" alt="图片"></p><p>综上所述，BLM的training的规则如图8所示。讲者通过一个实例，详细介绍了模型的训练过程，给定sentence x，先sample t，再sample order o1:t，由此可构建动态画布ctx,o，并按照图7的loss进行训练。</p><p><img src="https://i.loli.net/2020/12/30/lUAgSP1uOX9r7sH.png" alt="image-20201230130440892" style="zoom:50%;"></p><p>训练好BLM后，可采用greedy decoding或beam search来填充输入文本中的空白。需要注意的是，greedy decoding/beam search寻找的不是具有最大边缘似然p(x;θ)的sentence，而是具有最大联合似然p(x;o;θ)的sentence 和trajectory。</p><h4 id="三、实验"><a href="#三、实验" class="headerlink" title="　三、实验"></a>　三、实验</h4><p>讲者主要进行了4种不同的实验验证模型的有效性，分别为<strong>Text infilling，Ancient text restoration，Sentiment transfer以及Language modeling。</strong>下述针对<strong>Text infilling</strong>的实验进行阐述。</p><h5 id="1）Text-infilling"><a href="#1）Text-infilling" class="headerlink" title="1）Text infilling"></a>1）Text infilling</h5><p>选择Yahoo Answers（100k的文档，且最大长度的为200个单词）作为实验数据集；在该数据集上随机mask掉比例为r的tokens，将连续的masked tokens用blank代替，由此得到的数据集作为测试数据集。在评价指标方面，分别选择Accuracy和Fluency来评价模型的有效性。具体是指通过计算初始文档与模型filling后文档间的BLEU score表征Accuracy；采用经过预训练的Left-to-Right Language Model计算perplexity表征Fluency。</p><p><strong>讲者分别选择以下模型作为在Baseline models:</strong></p><p><strong>a.BERT+LM</strong></p><p>指采用BERT模型得到每一个blank的representation，并将其输入给Left-to-Right Language Model生成不同blanks对应的tokens。</p><p><strong>b.Masked Language Model with oracle length (MLM)</strong></p><p>由于模型需要知道每一个blank对应单词个数，因此给定oracle length数量的masked tokens来代替blanks；此外MLM模型同时预测masked tokens时是相互独立的，并没有考虑filling的部分之间的consistency，因此使用most-confident-first heuristic来autoregressively一个一个进行填写。</p><p><strong>c.Insertion Transformer (Stern et al., 2019)</strong></p><p>该模型支持动态单词插入，与BLM不同，该模型可在所有位置插入，因此强制模型只在指定位置进行插入，避免出现较高的失败率。</p><p><strong>d.Seq2seq-full/fill (Donahue et al., 2020)</strong></p><p>指直接输出infill之后的句子序列，或者直接输出代替blanks的tokens并用分隔符“|”划分。这两种方法具有较高的失败率。</p><p><img src="https://i.loli.net/2020/12/30/2LU8x5dlnCcOJmH.png" alt="image-20201230131800545" style="zoom:50%;"></p><p>图a)</p><p><img src="https://i.loli.net/2020/12/30/aZrSFqPjty67nXu.png" alt="image-20201230131732735" style="zoom:50%;"></p><p>图 b)</p><p>图9 Text infilling 实验结果</p><p>实验结果如图9所示，其中图a)说明当mask ratio 越来越高时，所有模型的BLUE score都会降低，同时BLM表现出最高的BLUE score；图b)中BLM与InsT模型的perplexity要低于original data的perplexity，且当mask ratio 越来越高时，模型最终的output比原始data要更加typical，则perplexity会越来越低。</p><h4 id="四、结论与展望"><a href="#四、结论与展望" class="headerlink" title="四、结论与展望"></a>四、结论与展望</h4><p>讲者提出了BLM可以用来灵活地生成文本，其生成方式是动态创建以及填充blanks。实验表明BLM模型在Text infilling，Ancient text restoration和Sentiment transfer上具有很好的表现。</p><p>BLM也具有广阔的应用前景，比如Template filling，information fusion以及assisting human writing；同时BLM也可被扩展为conditional model，可以用来edit和refine机器翻译，或者在dialogue中根据给定内容生成一句话；最后可以探究BLM跟MLM/BERT在representation的学习上的差异性。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/8ld6oicYkZZtRCNqJf5KRcmmFmYpBTS6hibiad1Gyib2x0Lt4qGWjuMCERiaGibUT5ibK0ezrHch0nMNJqDnibRgELdMRQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><p>图12 结论与展望</p><h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a><strong>参考文献：</strong></h3><p>Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. 2019. Insertion transformer: Flexible sequence generation via insertion operations. arXiv preprint arXiv:1902.03249.</p><p>Chris Donahue, Mina Lee, and Percy Liang. 2020. Enabling language models to fill in the blanks. arXiv preprint arXiv:2005.05339.</p><h3 id="论文地址："><a href="#论文地址：" class="headerlink" title="论文地址："></a><strong>论文地址：</strong></h3><p><a href="https://arxiv.org/pdf/2002.03079.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2002.03079.pdf</a></p><h3 id="Github地址："><a href="#Github地址：" class="headerlink" title="Github地址："></a><strong>Github地址：</strong></h3><p><a href="https://github.com/Varal7/blank_language_model" target="_blank" rel="noopener">https://github.com/Varal7/blank_language_model</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> Text-Filling </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像描述任务的实际应用</title>
      <link href="2020/12/24/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/"/>
      <url>2020/12/24/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h3 id="CaptionBot"><a href="#CaptionBot" class="headerlink" title="CaptionBot"></a>CaptionBot</h3><p>from: <a href="https://www.theguardian.com/technology/2016/apr/14/captionbot-microsoft-latest-ai-experiment-it-isnt-racist" target="_blank" rel="noopener">https://www.theguardian.com/technology/2016/apr/14/captionbot-microsoft-latest-ai-experiment-it-isnt-racist</a></p><p>简介：用户上传一张图片到CaptionBot Service，该系统针对该图片自动的生成一个caption。用户可以评分生成的caption 是否准确。</p><blockquote><p>The idea is that you upload a photo to the service, and it tries to automatically generate a caption that describes what the algorithm sees. You are then able to rate how accurately it has detected what was on display. It learns from the rating, and in theory, the captions get better.</p></blockquote><p>The bot, from <a href="http://go.theguardian.com/?id=114047X1572903&amp;url=https%3A%2F%2Fwww.microsoft.com%2Fcognitive-services&amp;sref=https://www.theguardian.com/technology/2016/apr/14/captionbot-microsoft-latest-ai-experiment-it-isnt-racist" target="_blank" rel="noopener">Microsoft’s Cognitive Services team</a>, is the result of <a href="http://go.theguardian.com/?id=114047X1572903&amp;url=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F264408%2FImageCaptionInWild.pdf&amp;sref=https://www.theguardian.com/technology/2016/apr/14/captionbot-microsoft-latest-ai-experiment-it-isnt-racist" target="_blank" rel="noopener">some hefty research</a> into how to model objects in photographs so that a computer can understand them. They claim that their system can recognise “a broad range of visual concepts” and also performs entity extraction so that it can recognise celebrities. </p><p><img src="https://i.loli.net/2020/12/24/kJ31yRYX5dKH64g.jpg" alt="Taylor Swift and Kanye West both identified in Microsoft’s CaptionBot app"></p><h3 id="Seeing-AI-项目"><a href="#Seeing-AI-项目" class="headerlink" title="Seeing AI 项目"></a><strong>Seeing AI</strong> 项目</h3><p>微软研究院的研究员们不仅在寻找识别图像的方法，还在为图像进行描述。这项研究结合了图像识别技术与自然语言处理技术，能帮助视障人士获得对图像的准确描述，还可能帮助那些需要图像信息却无法直接看到图像的人——比如正在开车的司机。</p><p>Seeing AI项目组中的Margaret Mitchell是一名专攻自然语言处理的研究员，也是图像描述领域顶尖的研究者之一。她说，她和同事们正在寻找方法，让计算机可以用更加人性化的方式来描述图像。例如，计算机可以将一个场景准确地描述为“一群人坐在一起”，但真人可能会将这一场景描述为“一群人坐在一起享受美好时光。”<strong>目前的挑战就是让这项技术懂得一张图像中哪些是对人们最重要、最值得描述的内容。</strong>“<strong>一张图像中有什么，和我们如何谈论一张图像可是完全不同的两回事</strong>，”Mitchell说。</p><p>微软的另一些研究员们正在努力让最新的图像识别工具提供更深入的图片解释。例如，与单纯地将图片描述为“一个男人和一个女人坐在一起”相比，对人们更有帮助的描述可能是：“奥巴马和希拉里·克林顿正在摆pose拍照”。今天人们在网上搜索图片时，绝大多数情况下搜索引擎会根据与图片相关的文字内容，从而得到美国名媛金·卡戴珊或“霉霉”泰勒·斯威夫特的照片，这些搜索结果主要依据文本内容。而微软的资深研究员张磊及郭彦东等研究员正在开发一套借助机器学习识别名人、政治家和公众人物的系统，这套系统会根据图像本身的元素，而非与图像相关的文字内容来进行图像识别。</p>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DirectQE Direct Pretraining for Machine Translation Quality Estimation</title>
      <link href="2020/12/24/DirectQE-Direct-Pretraining-for-Machine-Translation-Quality-Estimation/"/>
      <url>2020/12/24/DirectQE-Direct-Pretraining-for-Machine-Translation-Quality-Estimation/</url>
      
        <content type="html"><![CDATA[<p>AAAI2021论文：基于数据生成的机器翻译质量评估方法</p><p>来源：<a href="https://mp.weixin.qq.com/s?__biz=Mzg3ODA0NTA2OA==&amp;mid=2247484204&amp;idx=1&amp;sn=1b24bb494110fff68beedde3cb422066&amp;chksm=cf18f1cff86f78d9bdd2d883201af98c4296ec45fe8fae91509421b226ccee923ae7bd158451&amp;mpshare=1&amp;scene=1&amp;srcid=1224CwmzTjgig5X1t5fCbU0T&amp;sharer_sharetime=1608779809453&amp;sharer_shareid=ab44667880fa06ced8bfa560d1d64d36&amp;key=22efb95c8c04cdfbe43c8f83a69b76927ad01c4ab53016600f84450f07570ee6fb6b4747790c342eb1180e349f3fbc3fa8a1ab1219b80a6d725aacd79413c692d47bc3d763cd18ca87eaf3dd823648170186d2f2873bc5c2ef24ff36d3b519a54723506b60b7d4161912fd28659694c1100797e597da6bfe583dad4ef0c80802&amp;ascene=1&amp;uin=MjQwOTk2MzUwOA%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=A8kVBtMx05%2Bw41YTdHRse0I%3D&amp;pass_ticket=5UOCG2UJoSjTMMc7Gh82YYMCZ5iDMMdAZnDIpIDdHbP7%2FsA%2FS9I%2B%2FHaK1%2BTsQBXL&amp;wx_header=0" target="_blank" rel="noopener">南大NLP</a></p><h3 id="研究问题"><a href="#研究问题" class="headerlink" title="研究问题"></a>研究问题</h3><p>目前机器翻译在各个领域得到广泛应用，不同的机器翻译系统，对于同样的原文句子可能会给出不同的翻译结果（如表1所示），质量有好有坏。该如何自动评价译文的翻译质量呢？</p><div class="table-container"><table><thead><tr><th>原文</th><th>黄河之水天上来，奔流到海不复回</th></tr></thead><tbody><tr><td>翻译1</td><td>The water of the Yellow River comes from the sky, rushes to the sea and never returns</td></tr><tr><td>翻译2</td><td>The Yellow River never comes back to the sea</td></tr><tr><td>翻译3</td><td>The Yellow River comes from the sky, runs to the sea and never comes back</td></tr><tr><td>翻译4</td><td>The water of the Yellow River came from the sky and ran to the sea</td></tr><tr><td>翻译5</td><td>The water of the Yellow River comes from the sky, and the waves rush to the East China Sea and never look back</td></tr></tbody></table></div><p>表格 1：机器翻译的不同结果</p><h3 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h3><p>目前的评价指标主要有两类。一类需要依赖参考译文，比如BLEU、TER、Meteor等，主要依赖机器翻译译文和参考译文之间的匹配程度，<strong style="color:red;">但参考译文在现实应用场景下往往难以获取。</strong>第二类评价指标不需要依赖参考译文，如质量评估Quality Estimation (QE)，仅通过原文和机器译文对翻译质量进行估计。</p><p>QE的粒度有很多种，包括词级别、句子级别、短语级别、文档级别等，<strong style="color:red;">本文主要关注词级别与句子级别QE任务。</strong>词级别QE将译文中的每一个词标记为“Ok”或“Bad“；句子级别QE给每一个句子标注一个[0, 1]之间的打分（0表示很好，1表示很差），这些标记都由人工标记得到，或基于人工编辑结果得到。表2展示了一个英-中语向的QE数据示例。</p><div class="table-container"><table><thead><tr><th>原文（英）</th><th>this insubordination earned him a now famous reprimand from the King .</th></tr></thead><tbody><tr><td>机器译文（中）</td><td>这种 <strong style="color:blue;">不 服从命令</strong> 的 态度 使 他 <strong style="color:blue;">赢得 了 国王 现在 著名</strong> 的 <strong style="color:blue;">训斥</strong> .</td></tr><tr><td>词级别标记</td><td>O B B O O O O B B B B B O B B</td></tr><tr><td>句子级别标记</td><td>0.6429</td></tr></tbody></table></div><p>表格 2：QE数据示例（WMT20英-中）</p><p>早期QE任务依赖人工设计的特征，例如原文、译文中的单词数量，词频等，但是该方法的适应性较弱，效果较差。<strong style="color:red;">随后有研究者使用神经网络对QE数据进行端到端建模，取得了一定的成绩。</strong>神经网络需要大规模数据进行训练，但是<strong style="color:red;">QE数据由于需要人工进行标记，暂时规模较小</strong>（万句级别），这限制了神经网络的训练。<strong style="color:red;">目前最流行的QE模型利用知识迁移技术，从无QE标记但具有大规模（百万句级别）的平行语料中迁移QE任务所需要的知识。</strong></p><p>Predictor-Estimator是一种流行的基于知识迁移的QE框架，它是一种两阶段模型（如图一所示）。第一阶段，预测器（Predictor）将在平行语料上进行预训练，其预训练任务一般为“词预测”类型的任务。第二阶段，使用预测器提取QE句对的特征，通过评估器（Estimator）学习如何在这些特征上拟合QE标记。</p><p><img src="https://i.loli.net/2020/12/30/hLHD7IOQgznTZJd.png" alt="image-20201230101547991" style="zoom:33%;"></p><p>我们可以使用神经机器翻译模型NMT（Kim et al. 2017, Fan et al. 2018, Zhou et al. 2019）或者预训练语言模型PLM（Kepler et al. 2019, Kim et al. 2019）来作为预测器，使用LSTM模型作为评估器。</p><p><strong style="color:red;">该框架的问题在于，其两阶段之间存在差异，包括数据的差异和训练目标的差异。</strong> 数据的差异是指，<strong style="color:blue;">预测器</strong>在大规模平行语料上训练，平行语料由原文和正确译文组成；<strong style="color:blue;">评估器</strong>在QE数据上训练，句对由原文和包含错误的机器翻译译文组成。训练目标的差异是指，预测器是在做“词预测”任务；评估器是在预测词和句子的质量。那么预测器的预训练过程与目标QE任务存在差异，会导致学习不到QE任务真正需要的知识，无法充分利用大规模双语平行数据。</p><h3 id="本文提出的方法"><a href="#本文提出的方法" class="headerlink" title="本文提出的方法"></a>本文提出的方法</h3><p>QE模型的现存问题主要是，1.大规模神经网络训练参数依赖大量数据；2.数据分布及训练目标的差异可能对两阶段训练带来不利影响。</p><p>为了解决这两个问题，我们采取的改进方向是：</p><ol><li><p>使用相同/相似的数据进行预训练；</p></li><li><p>使用相同/相似的预训练目标。</p></li></ol><p>QE数据中包含一些翻译噪音，QE的训练目标需要质量标签。那么如何基于平行语料，获得带有一定噪音的数据，并且可以获得噪音数据的质量标签？</p><p>我们的解决方法是，<strong style="color:green;">首先基于平行数据训练生成器（generator）进行词改写任务；接着对平行语料进行词改写，从而引入一定量的可控噪音并利用可控噪音自动生成质量标签。最终可以将这些生成数据提供给判别器（detector）直接为QE任务进行预训练。</strong> 接下来具体介绍生成器的训练与生成过程。</p><p>首先，我们以Masked Language Model (MLM)的方式训练生成器。给定平行句对，随机的隐藏（mask）译文中某个位置的词，然后让模型预测被隐藏的词（如图2所示）。</p><p><img src="https://i.loli.net/2020/12/30/UfN9cyzTM8CZb6n.png" alt="image-20201230101626106" style="zoom: 50%;"></p><p>生成器训练结束后，我们将使用生成器对平行语料进行转化，具体分为两个步骤。</p><ol><li>生成伪造机器翻译译文。给定平行语料并隐藏译文中某个位置的词，让生成器进行预测并输出概率分布，根据概率分布采样新的词替换被隐藏词（如图3所示），即完成了对被隐藏词的改写。</li></ol><p><img src="https://i.loli.net/2020/12/30/HJvEwk2ZhzyC7KB.png" alt="image-20201230101742561" style="zoom:50%;"></p><ol><li><p>生成对应标签。</p><p>根据译文中的词是否被改写来获得词级别标签（见公式1）。</p><p>根据译文中被改写词的比例获得句子级别标签q’（见公式2）。</p><p>公式1： $o_{j}^{\prime}=\left\{\begin{array}{ll}1, &amp; \text { if } y_{j}=y_{j}^{\prime} \\ 0, &amp; \text { otherwise }\end{array}\right.$</p><p>公式2： $q^{\prime}=1-\frac{\operatorname{sum}\left(\mathbf{O}^{\prime}\right)}{\operatorname{len}\left(\mathbf{O}^{\prime}\right)}$</p></li></ol><p>通过生成器，我们能够将大规模平行语料转化为更大规模的伪造QE数据。比起平行译文，伪造的机器翻译译文在数据分布上与QE中的译文更加接近。同时，伪造QE数据针对每一个词有表示“是否由机器生成“的标签，对整个句子有表示”句子改写程度“的标签，形式上与QE数据类似。<strong style="color:green;">基于大规模伪造QE数据以及真实QE数据，我们将使用同样的训练目标，对判别器（Detector）分别在伪造数据上进行预训练、在真实数据上微调参数。最终也只需要使用判别器来做QE分数预测。</strong></p><blockquote><p>即，生成的数据，仅仅是在estimator 上提供预训练数据</p><p>即本文，并没有对 predictor做改动，是对estimator 的训练策略上做了改动</p></blockquote><p>我们对比了Predictor-Estimator框架中的两种具体实现。一种是基于NMT的QE模型，具体实现仿照QE Brain模型(Fan et al. 2018)；一种是基于PLM的QE模型，具体使用的PLM模型来自于huggingface。在本文的实现中，我们所提出的<strong>DirectQE</strong>参数量是最小的。</p><p>实验结果如表3，4，5所示，可以发现我们的模型在绝大多数情况下都是具有优势的。</p><p>表格 3：单模型结果（英-德）</p><p><img src="https://i.loli.net/2020/12/30/4OZDeJgo7CmGX5l.png" alt="image-20201230102042709"></p><p>表格 4：集成模型结果（英-德）</p><p><img src="https://i.loli.net/2020/12/24/UkC5rKy9ZoWh8Ab.png" alt="image-20201224114950396" style="zoom:33%;"></p><p>表格 5：单模型结果（英-中、英-俄）</p><p><img src="https://i.loli.net/2020/12/24/87bizODk5VYjdxQ.png" alt="image-20201224115010524" style="zoom: 25%;"></p><p>为了找出我们模型性能具体的增长点，我们按照错误词的比例划分了真实QE数据集，并评估了模型在数据每个部分的性能。如图4所示，在翻译质量存在问题时（错误词比例&gt;12.5%），DirectQE的性能更好。</p><p>图表 4：模型在不同错误比例数据上的性能对比</p><p><img src="https://i.loli.net/2020/12/30/SOFaCUxnjHIuV4X.png" alt="image-20201230101815866"></p><p>为了研究预训练使用的数据分布对QE性能的影响，我们使用基于NMT的QE模型，并且将其中训练预测器用到的平行语料替换为生成器制造的伪造机器翻译译文，其余部分均保持不变。从表6中可以看出，使用伪造译文的模型性能有所上升，说明对于QE任务而言，使用伪造译文预训练比平行译文更好。</p><p>表格 6：使用伪造译文/平行译文训练基于NMT的QE系统</p><p><img src="https://i.loli.net/2020/12/24/r2zCSgNepfovnET.png" alt="image-20201224115311979" style="zoom: 33%;"></p><p>为了研究预训练数据质量对QE性能的影响，我们测试了不同质量的数据下QE性能，这里数据质量具体指是译文质量，可以体现在替换词比例上（在相同替换策略下，替换词比例越大，译文质量越差）。从图5中可以看出，伪造译文质量太好或者太坏都不利于最终QE的性能。伪造译文质量太好（替换比例很低），句子将接近于平行语料本身，数据中几乎没有噪音；而伪造译文质量太差时，会破坏句子结构，与真实QE译文数据分布有较大差异。图5中红点表示使用随机噪音替换被隐藏词，此时的译文质量很差，可以看到QE性能也很低。</p><p>图表 5：不同伪造译文质量下的QE模型性能</p><p><img src="https://i.loli.net/2020/12/30/kSiwhoP47EzGx9Y.png" alt="image-20201230101837656" style="zoom: 33%;"></p><p>在固定规模的平行语料上，生成器每一次采样会产生不同的伪造QE数据，最终用于训练判别器的数据规模是超百万级别的，且更多样化。为了研究多样性的价值，我们使用生成器产生了固定数量的伪造QE数据，对比了在固定生成的数据上以及持续生成的数据上预训练的模型性能。结果（图6）显示，<strong style="color:red;">伪造QE数据的多样性对提升模型性能来说很重要。</strong></p><p>图6：</p><p><img src="https://i.loli.net/2020/12/30/nh5G1rxMRPpDgmO.png" alt="image-20201230101857603" style="zoom:50%;"></p><p>词级别QE任务需要判别当前词的质量，那么模型在建模当前词时，包含更多当前词的信息是有必要的。为了体现模型隐层表示含有当前词信息的多与少，我们计算了隐层表示与当前词之间的互信息（模型指判别器/预测器）。在图7中可以看到，DirectQE学到的表示中包含有更多当前词的信息。</p><p>图表 7：模型隐层表示 v.s. 当前词信息</p><p><img src="https://i.loli.net/2020/12/30/b43AOK7aHm1QklR.png" alt="image-20201230101937525" style="zoom:33%;"></p><p>假设当模型针对下游任务进行微调时，模型的隐层表示改变越小，则原始的表示更适合该下游任务。为了研究是否DirectQE可以学习到更加适合QE任务的表示，我们测试了在真实QE数据上微调前后，DirectQE和基于NMT的QE模型的隐层表示之间的相似度。表7显示，DirectQE隐层表示的相似度较高，说明DirectQE可以学到更加适合QE任务的表示。</p><p>表格 7：微调前后隐层表示变化</p><p><img src="https://i.loli.net/2020/12/30/F5c2lrWGv8wuMnP.png" alt="image-20201230101958503" style="zoom:33%;"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>大规模的QE模型需要大规模数据进行参数训练。现有的两阶段方法，由于数据和训练目标差异，无法最大化利用大规模平行语料知识。我们提出一种直接为QE模型进行预训练的框架（DirectQE）——使用生成器由平行语料得到伪QE数据，而后使用判别器，在伪QE数据上进行预训练，并且使用真实QE数据微调。我们模型的优势是，参数规模更小，模型性能更好，并且易于使用。</p><p>未来，我们将考虑使用更多样化的方式来构造伪QE数据，进一步缓解数据差异带来的影响，最大程度利用大规模语料，提升QE模型性能。</p><h3 id="可查看的参考文献"><a href="#可查看的参考文献" class="headerlink" title="可查看的参考文献"></a>可查看的参考文献</h3><ul><li><p>Predictor-Estimator: Neural Quality Estimation Based on Target Word Prediction for Machine Translation</p><p><a href="https://unbabel.github.io/OpenKiwi/cli/train_predictor_estimator.html" target="_blank" rel="noopener">https://unbabel.github.io/OpenKiwi/cli/train_predictor_estimator.html</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BERT-ATTACK Adversarial Attack Against BERT Using BERT</title>
      <link href="2020/12/01/BERT-ATTACK-Adversarial-Attack-Against-BERT-Using-BERT/"/>
      <url>2020/12/01/BERT-ATTACK-Adversarial-Attack-Against-BERT-Using-BERT/</url>
      
        <content type="html"><![CDATA[<h1 id="复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法"><a href="#复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法" class="headerlink" title="复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法"></a>复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法</h1><p>针对离散数据（例如文本）的对抗攻击比连续数据（例如图像）更具挑战性，因为很难使用基于梯度的方法生成对抗样本。当前成功的文本攻击方法通常在字符或单词级别上采用启发式替换策略，替换时难以保持语义一致性和语言流畅性。在本文中，作者提出了BERT-Attack，这是一种高质量且有效的方法，可以使用以BERT为例的MLM预训练语言模型来生成对抗性样本。作者使用BERT对抗其微调模型和其他预训练模型，以误导目标模型，使其预测错误。作者的方法在成功率和扰动百分比方面均优于最新的攻击策略，并且生成的对抗性样本很流利，并且在语义一致。而且作者的方法计算成本低，可以大规模生成。</p><p>本期AI TIME PhD直播间，我们有幸邀请到了复旦大学 NLP group2019级研究生李林阳分享他的观点。</p><p>李林阳：复旦大学 NLP group2019级研究生；导师为邱锡鹏教授；</p><hr><h2 id="一、针对文本任务的攻击"><a href="#一、针对文本任务的攻击" class="headerlink" title="一、针对文本任务的攻击"></a><strong>一、针对文本任务的攻击</strong></h2><p>尽管深度学习取得了成功，但最近的研究发现神经网络容易受到对抗样本的攻击，这些对抗样本是对原始输入进行细微扰动而制成的。尽管对抗性样本对于人而言几乎不可察觉，但是它们会误导神经网络进行错误的预测。针对对抗性攻击的学习可以提升神经网络的可靠性和健壮性，在计算机视觉领域，攻击策略及其防御措施都得到了很好的探索，但由于语言的离散性，对文本的对抗性攻击较为困难，难以保证语法流利且语义一致。</p><p><img src="https://i.loli.net/2020/12/01/S3DQcwvOEAt1uma.jpg"> </p><p> 表1 BERT-Attack方法生成样本的例子</p><p>当前对文本的成功攻击通常采用启发式规则来修改单词的字符，并用同义词替换单词。</p><p>之前的研究包括使用word embedding生成替换词；对原有句子的短语进行添加或删除；使用人工构建的规则进行词语替换。<strong>尽管上述方法取得了良好的效果，但在攻击成功率，语法正确性和语义一致性等方面，仍有很大的改进空间</strong>。此外，这些方法的替换策略通常很简单，受限于特定任务。</p><p>本文提出了一种有效且高质量的对抗样本生成方法：BERT-Attack，使用BERT作为生成器生成对抗样本。BERT-Attack的核心算法包括<strong>两个阶段</strong>：<strong>在给定输入序列中查找易受攻击的单词，然后用如BERT的生成器来生成易受攻击单词的替代词。</strong> BERT能够捕捉文本的上下文语义，因此生成的样本更为流畅且合理。作者将BERT这样的MLM语言模型用作生成器，并找到让BERT模型得到最大错误预测风险的扰动。另外，本文的方法只需要一次生成器前向，而且无需反复使用语言模型对对抗样本进行评分，速度有一定改进。表1展示了该攻击方法在几个数据集上的生成文本样例。</p><h2 id="二、BERT-ATTACK攻击方法"><a href="#二、BERT-ATTACK攻击方法" class="headerlink" title="二、BERT-ATTACK攻击方法"></a><strong>二、BERT-ATTACK攻击方法</strong></h2><p><img src="https://i.loli.net/2020/12/01/cOpDH2hWr81uzMS.png" alt="image-20201201154053279"></p><p>图1. BERT-ATTACK替换策略一步的样例</p><p>本文提出BERT-Attack，它使用原始BERT模型制作对抗性样本以对抗微调的BERT模型。对抗样本的生成包括两个步骤：（1）找出针对目标模型的易受攻击的单词，（2）用语义相似且语法正确的单词替换它们，直到成功攻击为止。具体而言：</p><p><strong>1.寻找易受攻击词(Vulnerable Words)</strong></p><p>作者给句子中的每一个词一个评分，得分与易受攻击程度呈正比，该评分按照去掉该词的句子在判别器上的输出结果的扰动程度给出。作者使用目标模型（微调的BERT或其他神经模型）的logit输出作为判别器。易受攻击词定义为序列中对最终输出logit有重要影响的单词。令表示输入语句，表示目标模型输出的正确标签y的logit，重要性得分定义为</p><p>$I_{w_{i}}=o_{y}(S)-o_{y}\left(S_{\backslash w_{i} j}\right.)$</p><p>其中，</p><p>$S_{\backslash w_{i}}=\left[w_{0} ; \ldots ; w_{i-1} ;[M A S K] ; w_{i+1} ; \ldots\right]$</p><p>就是将该词替换成“[MASK]”。然后，对降序排名，获取其中的前百分之的词组成可替换词表，记为L。</p><h2 id="2-BERT生成器的优点"><a href="#2-BERT生成器的优点" class="headerlink" title="2.BERT生成器的优点"></a><strong>2.BERT生成器的优点</strong></h2><p>找到易受攻击的单词后，将列表L中的单词一一替换，以寻找可能误导目标模型的干扰。以前的替换方法包括同义词词典，POS检查器，语义相似性检查器等。但是因为替换的时候只有词表，不考虑上下文，因此需要用传统语言模型给替换单词的句子打分。由于换一个词就得评价一次，时间成本比较高。</p><p>作者利用BERT进行单词替换，可确保所生成的句子相对流利且语法正确，还保留了大多数语义信息。此外，掩码语言模型的预测是上下文感知的，因此可以动态搜索扰动，而不是简单的同义词替换。而且针对一个词而言，仅通过一个前向即可产生候选文本，无需再用语言模型来对句子评分，提升了效率。</p><h2 id="3-替换策略"><a href="#3-替换策略" class="headerlink" title="3.替换策略"></a><strong>3.替换策略</strong></h2><p><img src="https://i.loli.net/2020/12/01/MIsY1JPptlRxGB5.png" alt="image-20201201153433807" style="zoom:50%;">图2 BERT-ATTACK替换算法</p><p>如图1所示，作者输入原句子给BERT，并根据BERT输出生成候选词。<strong style="color:red;">注意这里不用[MSAK]替换被攻击词语</strong>，其原因作者给出了如下解释：1. 有些词语替换后，和原句子几乎一样流畅但是语义可能变更。例如给定一个序列“I like the cat”，如果遮盖cat这个词，那么MLM模型很难预测原始单词cat，因为如“I like the dog”一样很流畅。2. MASK掉给定的单词后，每个候选词都需要运行一遍BERT前向，时间成本太高。</p><p>令M代表BERT模型，为原序列，是利用BERT的分词器分完词的序列，将H输入BERT中得到输出预测。使用top-K策略选择可能的替换词预测，其中K是超参数。作者遍历所有候选易攻击词表L生成替换词表。</p><p>由于BERT使用字节对编码（BPE）分词，候选词可能会被分开，因此还需要将所选单词与BERT中相应的子单词对齐。</p><p>针对未被分开的单个单词，作者使用相应的前K个预测候选逐一尝试替换，并使用NLTK过滤其中的停用词，另外对于情感分类任务候选词可能包括同义词和反义词，作者使用同义词词典过滤反义词。然后将替换完成的句子重新输入判别器，如果判别器给出与原label相反的判断那么输出该句子作为攻击句；否则，从筛选出的候选词中选择一个对logit影响最大的。</p><p>针对字词组（sub-word 应该不能翻译为字词组），由于无法直接获取其替代词，作者使用子词组合中所有词的预测中找到合适的词替代。作者首先使用MLM模型分析整个词组的易攻击程度，然后再选出词组的top-k组合。剩余过程与单个单词一致。</p><p><strong>三、实验结果</strong></p><p><strong>3.1 数据集和评价指标</strong></p><p>为了衡量所生成样本的质量，作者设计了几种评估指标：</p><p>●成功率（success rate）：攻击样本的判别器准确率。</p><p>●扰动百分比（perturbed percentage）更改文本的占比。</p><p>●每个样本的查询数量（query number per sample）一个样本生成对抗样本的需要访问判别器的次数。</p><p>●语义相似度（semantic similarity）使用通用句子编码器（Universal Sentence Encoder）评价的句子相似度。</p><p><img src="https://i.loli.net/2020/12/01/jgXqBTRfJYniZFe.png" alt="image-20201201153907478" style="zoom:50%;">表2 实验结果</p><p><strong>3.2 实验结果</strong></p><p>如表2所示，BERT-Attack方法成功欺骗了其下游的微调模型。在文本分类和自然语言推断任务中，经过微调的BERT均无法正确地对生成的对抗样本进行分类，攻击后的平均准确度低于10％。同时，扰动百分比小于10％，明显小于以前的工作，BERT-Attack方法更有效且更不易察觉。查询数量也要少得多。</p><p>另外可以观察到，由于扰动百分比非常低，因此通常更容易攻击评论分类任务。BERT-Attack仅替换少数几个单词就可能误导判别器。由于平均序列长度相对较长，因此判别器倾向于仅按序列中的几个词进行判断，这不是人类预测的自然方式。因此，这些关键字的干扰将导致目标模型的预测不正确，从而揭示了该模型的脆弱性。</p><p><strong>3.3人工验证</strong></p><p>为了进一步评估生成的对抗性样本，作者人工评估了流利性，语法以及语义保留方面生成的样本的质量。</p><p><img src="https://i.loli.net/2020/12/01/oDVS914hYZWJNje.png" alt="image-20201201153955115" style="zoom:50%;"></p><p>作者要求三名标注人员对生成的对抗性样本和原始序列的混合句子的语法正确性进行评分（1-5分），然后将原始文本和对抗文本混在一起进行人工预测。在IMDB和MNLI数据集中，作者分别选择100个原始样本和对抗样本验证。对于IMDB，将多数类作为人类预测标签，对于MNLI，则使用标注人员之间的平均分数。从表2中可以看出，对抗性样本的语义分数和语法分数接近原始样本。MNLI<strong>任务数据长且更加复杂（存在句子对（sentence pair）之间，重复出现的词汇较多，而基于替换的对抗样本则破坏了这种相同词汇的对应关系</strong>），使标注人员难以正确预测，因此其准确性要比简单的句子分类任务低。作者同样做了大量消融实验，实验结果表明该对抗方法生成的样本迁徙性强，生成速度快。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>在这项工作中，作者提出了一种高质量有效的BERT-Attack方法，以使用BERT<strong>掩蔽语言模型（masked-LM）生成对抗性样本。</strong>实验结果表明，该方法在保持最小扰动的同时，取得了较高的成功率。然而，从屏蔽语言模型生成的候选者有时可能是反义词或与原始单词无关，从而导致语义损失。因此，增强语言模型以生成更多与语义相关的扰动可能是将来完善BERT-Attack的一种可能解决方案。</p><hr><blockquote><p>整理：李键铨<br>排版：杨梦蒗<br>审稿：李林阳</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</title>
      <link href="2020/10/28/Beyond-Accuracy-Behavioral-Testing-of-NLP-Models-with-CheckList/"/>
      <url>2020/10/28/Beyond-Accuracy-Behavioral-Testing-of-NLP-Models-with-CheckList/</url>
      
        <content type="html"><![CDATA[<p>转载：<a href="https://dy.163.com/article/FH8NB44C0511DPVD.html" target="_blank" rel="noopener">https://dy.163.com/article/FH8NB44C0511DPVD.html</a></p><p>现在，ACL2020各个奖项都已悉数公布，对此AI科技评论做了详细报道。其中，最受人瞩目的当属最佳论文奖，今年该奖项由微软团队的 《Beyond Accuracy: Behavioral Testing of NLP Models with CheckList》一举拿下。</p><p>　　小编看到论文题目的第一眼就觉得哪些有些不对，于是赶紧通读了一下文章，嗯~确实不太对，这貌似和之前我们熟悉的NLP“大力出奇迹”的模型套路不太一样啊？</p><p>　　那么这篇论文到底讲了什么呢，又何以摘得桂冠呢？</p><p>　　论文解读以外，我们进一步对论文的第二作者吴彤霜进行了专访，以更深入地了解最佳论文团队背后的工作。</p><h3 id="论文方法一览"><a href="#论文方法一览" class="headerlink" title="论文方法一览"></a><strong>论文方法一览</strong></h3><p>　　我们从论文的题目入手来了解一下这篇论文在讲什么。</p><p>　　首先是”Beyond Accuracy”：这是在说要超越Accuracy，这里Accuracy说的是NLP模型在各大数据集和任务上跑出的准确率，也即是性能的一种度量。</p><p>　　那既然要超越它总要有一个理由:</p><p>　　1.评估所用的训练-验证-测试划分集来估计模型的准确性时保留的数据集往往不全面。</p><p>　　2.测试集中往往包含与训练数据相同的偏差，这种方式可能高估了模型在真实世界的性能</p><p>　　3.通过Accuracy一刀切的方式很难找出模型失败在哪里，以及如何修复它。</p><p>　　对此本文提出的Beyond 方式又是如何呢？</p><p>　　Behavioral Testing of NLP Models with CheckList！也即用CheckList对NLP模型做行为测试。</p><h4 id="1、We-should-test-NLP-models"><a href="#1、We-should-test-NLP-models" class="headerlink" title="1、We should test NLP models"></a><strong>1、We should test NLP models</strong></h4><p>　　训练NLP模型的主要目标之一是泛化，虽然Accuracy是评价泛化的主要方法，但它往往高估了NLP模型的性能，用于评估模型的替代方法要么侧重于单个任务，要么侧重于特定的行为，benchmark的准确性不足以评估NLP模型。</p><p>　　除此之外许多额外的评估方法已经被提出来了，例如评估对噪声或对抗性变化的鲁棒性、公平性、逻辑一致性、可解释、诊断数据集和交互式错误分析。然而，这些方法要么侧重于单个任务，如问答或自然语言推理，要么侧重于一些能力（如鲁棒性），因此没有提供关于如何评估模型的全面指导。</p><p>　　因此在这这篇论文中，作者提出了CheckList(检查表)，一种新的评估方法和配套工具，用于NLP模型的综合行为测试。</p><h4 id="2、Software-engineering-gt-NLP"><a href="#2、Software-engineering-gt-NLP" class="headerlink" title="2、Software engineering-&gt;NLP"></a><strong>2、Software engineering-&gt;NLP</strong></h4><p>　　软件工程研究提出了测试复杂软件系统的各种范式和工具。特别是“行为测试”（黑盒测试）是指在不了解内部结构的情况下，通过验证输入输出行为来测试系统的不同能力。虽然有明显的相似之处，但软件工程的许多见解还没有应用到NLP模型中。</p><p>　　作者借鉴软件工程中行为测试的原理提出了CheckList：一种和模型、任务都无关的测试方法，它使用三种不同的测试类型来测试模型的各个功能。</p><p>　　作者用三个任务的测试来说明检查表的效用，识别商业和SOTA模型中的关键错误。在一项用户研究中，一个负责商业情绪分析模型的团队在一个经过广泛测试的模型中发现了新的、可操作的bug。在另一个用户研究中，使用CheckList的NLP实践者创建了两倍多的测试，发现的bug几乎是没有检查表的用户的三倍。</p><p>　　<img src="https://i.loli.net/2020/10/28/qk64rwGIt1lBJWE.png" alt="img"></p><p>　　图1</p><h4 id="3、What-is-CheckList"><a href="#3、What-is-CheckList" class="headerlink" title="3、What is CheckList"></a><strong>3、What is CheckList</strong></h4><p>　　CheckList包括一个通用语言能力和测试类型的矩阵，有助于全面的测试构思，以及一个快速生成大量不同测试用例的软件工具。从概念上讲，用户通过填写矩阵中的单元格来“检查”模型（图1），每个单元格可能包含多个测试。CheckList应用了“测试与实现脱钩”的行为测试原则，即将模型视为一个黑盒，允许对不同数据上训练的不同模型进行比较，或者对不允许访问训练数据或模型结构的第三方模型进行比较。</p><h4 id="4、What-to-test：capabilities"><a href="#4、What-to-test：capabilities" class="headerlink" title="4、What to test：capabilities"></a><strong>4、What to test：capabilities</strong></h4><p>　　CheckList通过提供适用于大多数任务的语言能力列表，指导用户测试什么。CheckList引入了不同的测试类型，比如在某些干扰下的预测不变性，或者一组“健全性检查”的性能。</p><p>　　虽然测试单个组件是软件工程中的常见实践，但现代NLP模型很少一次只构建一个组件。相反，CheckList鼓励用户考虑如何在手头的任务上表现出不同的自然语言能力，并创建测试来评估这些能力的模型。例如，词汇+POS能力取决于一个模型是否具有必要的词汇，以及它是否能够恰当地处理具有不同词性的单词对任务的影响。对于情绪，我们可能需要检查模型是否能够识别带有积极、消极或中性情绪的单词，方法是验证它在“这是一次很好的飞行”等示例上的行为。</p><p>　　基于此，作者建议用户至少考虑以下性能（capabilities）：</p><p>　　词汇+POS（任务的重要单词或单词类型）</p><p>　　Taxonomy（同义词、反义词等）</p><p>　　健壮性（对拼写错误、无关更改等）</p><p>　　NER（正确理解命名实体）</p><p>　　公平性</p><p>　　时态（理解事件顺序）</p><p>　　否定</p><p>　　共指（Coreference），</p><p>　　语义角色标记（理解诸如agent、object等角色）</p><p>　　逻辑（处理对称性、一致性和连词的能力）。</p><p>　　通过以上，CheckList实现包括多个抽象，帮助用户轻松生成大量测试用例，例如模板、词典、通用扰动、可视化和上下文感知建议。然而此功能列表并非详尽无遗，而是用户的一个起点，用户还应提供特定于其任务或域的附加功能。</p><h4 id="5、How-to-test"><a href="#5、How-to-test" class="headerlink" title="5、How to test"></a><strong>5、How to test</strong></h4><p>　　作者提示用户使用三种不同的测试类型来评估每个功能：最小功能测试、不变性和定向期望测试（矩阵中的列）。</p><p>　　1）最小功能测试（MFT）:它是受软件工程中单元测试的启发的一组简单的示例（和标签）的集合，用于检查功能中的行为。MFT类似于创建小而集中的测试数据集，尤其适用于在模型使用快捷方式处理复杂输入而不实际掌握功能的情况下进行检测。</p><p>　　<img src="https://i.loli.net/2020/10/28/2bFIGQ17wgVB3su.png" alt="img"></p><p>　　2）不变性测试（INV）：当对输入应用保留标签的扰动并期望模型预测保持不变时。不同的功能需要不同的扰动函数，例如，更改NER情感功能的位置名称，或者引入输入错误来测试健壮性能力。</p><p>　　<img src="https://i.loli.net/2020/10/28/c4Wa9IZzusY1Kj7.png" alt="img"></p><p>　　3）定向期望测试（DIR）：与不变性测试类似，只是标签会以某种方式发生变化。例如，我们预计，如果我们在针对某家航空公司的推文末尾添加“You are lame.”（图1C），情绪不会变得更积极。</p><p>　　<img src="https://i.loli.net/2020/10/28/Y78FjaetnpdgOJb.png" alt="img"></p><h4 id="6、可视化效果"><a href="#6、可视化效果" class="headerlink" title="6、可视化效果"></a><strong>6、可视化效果</strong></h4><p>　　调用test.visual_summary()</p><p>　　<img src="https://dingyue.ws.126.net/2020/0711/f079207eg00qdafs60095d200hr005pg00it0061.gif" alt="img"></p><p>　　在代码中调用suite.summary()（与test.summary相同）或suite.visual_summary_table() 显示测试结果如下：</p><p>　　<img src="https://dingyue.ws.126.net/2020/0711/c3640090g00qdafs701u7d200hr00bcg00it00c0.gif" alt="img"></p><p>　　模型保存和加载：精简至极！</p><p>　　<img src="https://i.loli.net/2020/10/28/KQDGslUnygeLESV.png" alt="img"></p><h4 id="7、更方便的大规模生成测试用例"><a href="#7、更方便的大规模生成测试用例" class="headerlink" title="7、更方便的大规模生成测试用例"></a><strong>7、更方便的大规模生成测试用例</strong></h4><p>　　用户可以从头开始创建测试用例，也可以通过扰动现有的数据集来创建测试用例。从头开始可以更容易地为原始数据集中可能未充分表示或混淆的特定现象创建少量高质量测试用例。然而，从头开始编写需要大量的创造力和努力，这通常会导致测试覆盖率低或者生成成本高、耗时长。扰动函数很难编写，但同时生成许多测试用例。为了支持这两种情况，作者提供了各种抽象，从零开始扩展测试创建，并使扰动更容易处理。</p><h4 id="8、使用CheckList测试SOTA模型"><a href="#8、使用CheckList测试SOTA模型" class="headerlink" title="8、使用CheckList测试SOTA模型"></a><strong>8、使用CheckList测试SOTA模型</strong></h4><p>　　作者通过付费API 检查了以下商业情绪分析模型：微软的文本分析、谷歌云的自然语言和亚马逊的Constract。我们还检查了在SST-23（acc:92.7%和94.8%）和QQP数据集（acc:91.1%和91.3%）上微调的BERT base和RoBERTa base。对于MC，作者使用了一个经过预训练的大BERT 微调阵容，达到93.2 F1。</p><p>　　<img src="https://i.loli.net/2020/10/28/pn7OqdKUx4shRLD.png" alt="img"></p><h4 id="9、测试商业系统"><a href="#9、测试商业系统" class="headerlink" title="9、测试商业系统"></a><strong>9、测试商业系统</strong></h4><p>　　作者联系了负责微软服务销售的通用情绪分析模型的团队（表1中的q）。由于它是一个面向公众的系统，模型的评估过程比研究系统更全面，包括公开可用的基准数据集以及内部构建的重点基准（例如否定、emojis）。此外，由于该服务已经成熟，拥有广泛的客户群，因此它经历了许多错误发现（内部或通过客户）和后续修复的周期，之后在基准测试中添加了新的示例。</p><p>　　作者的目标是验证检查表是否会增加价值，即使在这样的情况下，模型已经用当前的实践进行了广泛的测试。</p><p>　　作者邀请小组参加了一个持续约5小时的检查表会议。该团队集思广益地进行了大约30个测试，涵盖了所有功能。</p><p>　　从质量上讲，该小组称检查表非常有用：</p><p>　　（1）他们测试了他们没有考虑过的能力；</p><p>　　（2）他们测试了他们考虑过但不在benchmark中的能力；</p><p>　　（3）甚至他们有基准的能力（例如否定）也用检查表进行了更彻底和系统的测试。</p><p>　　他们发现了许多以前未知的错误，他们计划在下一个模型迭代中修复这些错误。最后，他们表示，他们肯定会将检查表纳入他们的开发周期，并要求访问我们的实现。</p><h4 id="10、用户研究"><a href="#10、用户研究" class="headerlink" title="10、用户研究"></a><strong>10、用户研究</strong></h4><p>　　作者进行了一项用户研究，以在一个更可控的环境中进一步评估检查表的不同子集，并验证即使是没有任务经验的用户也能获得洞察并发现模型中的错误。</p><p>　　尽管用户在使用CheckList时不得不解析更多的指令和学习新的工具，但他们同时为模型创建了更多的测试。</p><p>　　在实验结束时，作者要求用户评估他们在每个特定测试中观察到的失败的严重程度，研究结果令人鼓舞：有了检查表的子集，没有经验的用户能够在2小时内发现SOTA模型中的重大缺陷。此外，当被要求对检查表的不同方面进行评分时（1-5分），用户表示，测试环节有助于他们进一步了解模型，功能帮助他们更彻底地测试模型，模板也是如此。</p><p>　　评估特定语言能力的一种方法是创建挑战性数据集。我们的目标不是让检查表取代挑战或基准数据集，而是对它们进行补充。CheckList保留了挑战集的许多优点，同时也减轻了它们的缺点：用模板从头开始编写示例提供了系统控制，而基于扰动的INV和DIR测试允许在未标记的自然发生的数据中测试行为。</p><p>　　最后用户研究表明，CheckList可以轻松有效地用于各种任务：用户在一天内创建了一个完整的测试套件进行情绪分析，两个小时内创建了的MFTs，这两个都揭示了之前未知的严重错误。</p><h3 id="专访吴彤霜：最佳论文何以花落此家"><a href="#专访吴彤霜：最佳论文何以花落此家" class="headerlink" title="专访吴彤霜：最佳论文何以花落此家"></a>专访吴彤霜：最佳论文何以花落此家</h3><p>　　到这里我们大概明白了这篇论文到底在讲什么，但是我们还是心存疑惑，何以它能获得最佳论文殊荣？</p><p>　　<strong>以下为专访实录：</strong></p><p>　　<strong>AI 科技评论：</strong>首先恭喜您和您的团队斩获ACL2020最佳论文！我们想先了解一下这项工作的完成大概花了多长时间，把软件测试带入NLP模型检测的想法是最近才有的吗还是说之前就有了最近才实现？</p><p>　　<strong>吴彤霜：</strong>这个项目最早是一作快博士毕业时我们开始合作的，中间因为各种原因搁置了一段时间，实际做大概一年吧。引用软件测试应该可以算是一个新的想法。以前有很多nlp模型分析的论文本质上也可以说是我们论文里提到的那种“行为测试” (behavioral testing)，比如各种NLI challenge set。只不过这些工作大部分是针对某一个任务在做某个特定种类的分析，每次都要从头开始。我们工作其中的一个主要的一个贡献就是把这些分析做归一化，提供一个测试框架+开源系统。</p><p>　　<strong>AI 科技评论：</strong>这项测试系统是不是可以理解为对抗扰动系统啊？或者相比有什么优势呢？</p><p>　　<strong>吴彤霜：</strong>不变性测试 (INVariant test) 可以相当于扰动，就是模型在预测一个句子s和经修改后的版本s’时结果类似。CheckList还支持别的测试类别 (test type)：定向测试 (DIRectional test) 是用来测试预测结果的特定变化，最小功能测试 (Min Func Test) 不需要有配对的例子，只要一个能生成单个测试例句的模板就可以了。</p><p>　　只和INV（不变性测试 ）相比而言，现在NLP的大部分对抗句经常是在改拼写或者会产生乱码，比较难保证句子的连贯性，而能保证句子连贯性的居多是改近义词 (it’s a good movie -&gt; it’s a great movie)。CheckList因为允许自定义改写函数 (perturbation function)，所以可以更多样地测试模型的性能，比如看情感分析模型是否能辨认虚拟语气 (it’s a bad movie -&gt; it would have been a good movie)。这种测试也更系统化，因为可以生成很多对改写方法类似的句子/测试用例 (test case)。</p><p>　　当然相应的checklist的自动化就比较差，需要人来定义这些测试 :)</p><p>　　<strong>AI 科技评论：</strong>请问你们团队成员之前有过软件测试的经验吗，在CheckList的设计环节有什么亮点以及代码实现过程中有什么难点？</p><p>　　<strong>吴彤霜：</strong>应该不算有经验，没有在工业界实战过，顶多就是在软工课写单元测试，所以最开始其实也认真学习了一下软工 :)</p><p>　　设计上我觉得最大的亮点是对于性能 (capability) 的定义 。我们遇到一个瓶颈就是试图给每个NLP task设计需要测试的不同方面，但这样就非常繁琐，也失去了可拓展性。直到后来我们和其他researcher聊的时候意识到其实大部分的模型需要测试的“capability”基本比较稳定，只是不同任务里对标签的影响会有区别，比如[改主语]对NLI影响会比对情感分析要大。这样一个稳定的capability集合就让整个框架干净了很多。</p><p>　　开源上，其实NLP部分还是比较整洁的，但是为了让大家能比较流畅地在notebook里浏览和回顾test集，我们下了很大功夫研究怎们做交互插件，是一个大坑，但是最终效果还挺好看的，可以到readme里看看preview感受一下，哈哈。</p><p>　　写作上，因为marco在微软，我们很幸运能近水楼台找微软情感分析的工程组来做用户研究，让我们真的看到了CheckList在已经被认为是大规模测试过的模型仍然很有用。</p><p>　　<strong>AI 科技评论：</strong>很开心你们把这项工作开源，我想这项工作只是一个开始对吗？（大家都可以在你们开源的代码上进行哪些尝试和改进呢，比如自定义测试模板之类）</p><p>　　<strong>吴彤霜：</strong>最重要的是希望能看到大家提出的测试实例！其实比起很多NLP模型，CheckList是一个比较依靠人的力量的项目，测试者仔细设计实例才能用它最大程度暴露模型可能存在的缺陷。我们考虑的一个想法是希望可以做一个类似模型排行榜的测试榜，大家可以上传分享自己的测试集，甚至是顶或者踩别人的测试，最终让每个任务都能有一个比较稳定的测试集，也方便模型间的比较。</p><p>　　其次，我们也很期待看到大家会不会有关于如何让CheckList更自动化的想法，实现一键测试这个终极目标 :)</p><p>　　以及更研究向的：</p><p>　　我个人对于设计更稳定的测试也很感兴趣。CheckList对具体实例比较敏感，不同人在测试同一个模型性能时，如果实例设计不同，最终测试结果可能会有一些偏差。有没有什么交互手段能帮助测试者尽量穷尽一个性能所有的相关改写？甚至还有没有什么办法能慢慢形成一些自动的测试拓展？这个可能也和上面提到的自动化有一些关系。</p><p>　　最后测试带来的一个恒久不变的问题：so what？一个模型有问题之后，应该用什么样的标准来决定一个模型是不是可以被公开部署 (比如可能公平性测试的容错率可能远低于拼写错误)？应该如何改进它？</p><p>　　<strong>AI 科技评论：</strong>请问软件测试的思想只适用于NLP领域吗 ，在CV领域可行吗，应该怎么去设计测试系统？</p><p>　　<strong>吴彤霜：</strong>我相信是可行的！抽象来讲，本文图1的这种框架似乎能直接套用在CV上。</p><p>　　比如说一个最简单的狗和狼的分类，这个模型首先得能辨认有动物出现 (MFT)，然后改变图片的背景应该不影响预测 (INV)，但改变动物的头的形状应该是要影响的 (DIR)。vision里的“改写”效果其实比NLP好很多，也许更好用也说不定 :)</p><p>　　对设计系统而言，我觉得比较重要的是抽取基本组件。在NLP版本的CheckList里有一个重要组件就是写生成template/模板；也许在vision里则是需要提供一些基础像素之类的。</p><p>　　当然也可以考虑除了行为和单元测试之外的测试思想，比如如果是pipeline模型，考虑如何设计集成测试也许也会很有用 :)</p><p>　　<strong>AI 科技评论：</strong>可以简单介绍一下你们的团队成员吗，以及你们的近期工作、未来研究方向？</p><p>　　<strong>吴彤霜：</strong>隆重介绍一下没有出镜的一作吧，marco也是华大的博士，2018年毕业以后就加入了微软研究院，主要在做模型可解释性和分析，之前很有名的LIME（一种解释机器学习模型的方法——Local Interpretable Model-Agnostic Explanations）就是出自他手。除了CheckList，他今年在CVPR上也有一篇合作论文，是分析vqa model的稳定性的。现在主要在做vision模型的错误分析以及模型比较。</p><p>　　我们现在也在合作一个新工作，这项工作更多是关于如何人去探索模型的可解释性。虽然现在主要做的都是人如何检查模型，但是我们对于模型如何能反过来规范人或者帮助人也很感兴趣 :) 三四作Carlos和Sameer都是marco的导师，分别是ML和NLP的大佬。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>　　虽然CheckList目前也有一些不足比如CheckList不能直接用于非行为问题，例如数据版本控制问题、标记错误、注释器偏差、最坏情况下的安全问题或缺乏可解释性。</p><p>　　但是不可否认的是，使用CheckList创建的测试可以应用于任何模型，这使得它很容易被纳入当前的基准测试或评估pipeline中。用户研究表明，CheckList很容易学习和使用，对已经对模型进行了长时间测试的专家用户以及在任务中缺乏经验的实践者都很有帮助。</p><p>　　另外对吴同学的专访，我们相信， 本篇论文工作确实开创地把软件测试系统引入NLP模型的测试之中并且提供了完善的测试工具。 这将会给社区和企业带来很大的商业价值，比如CheckList测试工具将会节省很大的人力成本。</p><p>　　最后，我们相信，这种系统引进软件测试的思想也将会在CV乃至整个AI领域大有作为。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>N-Gram模型</title>
      <link href="2020/10/27/N-Gram%E6%A8%A1%E5%9E%8B/"/>
      <url>2020/10/27/N-Gram%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>转载：<a href="https://zhuanlan.zhihu.com/p/32829048" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32829048</a></p><h2 id="一、什么是n-gram模型"><a href="#一、什么是n-gram模型" class="headerlink" title="一、什么是n-gram模型"></a><strong>一、什么是n-gram模型</strong></h2><p>N-Gram是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。</p><p>每一个字节片段称为gram，对所有gram的出现频度进行统计，并且按照事先设定好的阈值进行过滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量维度。</p><p>该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。</p><p>说完了n-gram模型的概念之后，下面讲解n-gram的一般应用。</p><h2 id="二、n-gram模型用于评估语句是否合理"><a href="#二、n-gram模型用于评估语句是否合理" class="headerlink" title="二、n-gram模型用于评估语句是否合理"></a><strong>二、n-gram模型用于评估语句是否合理</strong></h2><p>如果我们有一个由 m 个词组成的序列（或者说一个句子），我们希望算得概率 $P(w_1, w_2,…,w_m)$，根据链式规则，可得</p><p><img src="https://i.loli.net/2020/10/27/ye3X8vh6LcKTzjr.png" alt="image-20201027172437915"></p><p>这个概率显然并不好算，不妨利用马尔科夫链的假设，即当前这个词仅仅跟前面几个有限的词相关，<strong><em>因此也就不必追溯到最开始的那个词，这样便可以大幅缩减上述算式的长度\</em></strong>。即</p><p><img src="https://i.loli.net/2020/10/27/XjDPInlQY9GN7ZU.png" alt="image-20201027172458291" style="zoom: 67%;"></p><p><strong><em>这个马尔科夫链的假设为什么好用？我想可能是在现实情况中，大家通过真实情况将n=1，2，3，….这些值都试过之后，得到的真实\</em></strong>的效果和时间空间的开销权衡之后，发现能够使<strong><em>用。\</em></strong></p><p>下面给出一元模型，二元模型，三元模型的定义：</p><p>当 n=1, 一个一元模型（unigram model)即为 ：</p><p><img src="https://i.loli.net/2020/10/27/UMNPucLtdw3zxgo.png" alt="img"></p><p>当 n=2, 一个二元模型（bigram model)即为 ：</p><p><img src="https://i.loli.net/2020/10/27/UmZkyzIK4fws7Nj.png" alt="img"></p><p>当 n=3, 一个三元模型（trigram model)即为</p><p><img src="https://i.loli.net/2020/10/27/Vr1LZIG7snCvokK.png" alt="img"></p><p>然后下面的思路就很简单了，在给定的训练语料中，利用贝叶斯定理，将上述的条件概率值（<strong>因为一个句子出现的概率都转变为右边条件概率值相乘了</strong>）都统计计算出来即可。下面会给出具体例子讲解。这里先给出公式：</p><p><img src="https://i.loli.net/2020/10/27/locAQnW9bGpXa3K.png" alt="img"></p><p>对第一个进行解释，后面同理,如下：</p><p><img src="https://i.loli.net/2020/10/27/iGbljDacPTSWu92.png" alt="image-20201027172711422"></p><p>下面给出具体的例子。</p><h2 id="三、二元语言模型判断句子是否合理"><a href="#三、二元语言模型判断句子是否合理" class="headerlink" title="三、二元语言模型判断句子是否合理\"></a><strong><em>三、二元语言模型判断句子是否合理\</em></strong></h2><p><strong><em>下面例子来自于：\</em></strong><a href="https://link.zhihu.com/?target=http%3A//blog.csdn.net/baimafujinji/article/details/51281816" target="_blank" rel="noopener">自然语言处理中的N-Gram模型详解 - 白马负金羁 - CSDN博客</a>和《北京大学 常宝宝 以及 The University of Melbourne “Web Search and Text Analysis” 课程的幻灯片素材》</p><p>假设现在有一个语料库，我们统计了下面的一些词出现的数量</p><p><img src="https://i.loli.net/2020/10/27/ufWOseUyrAm9qV3.png" alt="img"></p><p>下面的这些概率值作为已知条件：</p><p><img src="https://i.loli.net/2020/10/27/G2Y6zKZNxBCitVo.png" alt="img"></p><p><img src="https://pic1.zhimg.com/80/v2-a7c0d77143e0c997abd45e1535eaeb8c_1440w.jpg" alt="img"></p><p>$p(want|<s>) = 0.25$</s></p><p>下面这个表给出的是基于Bigram模型进行计数之结果</p><p><img src="https://i.loli.net/2020/10/27/G2Y6zKZNxBCitVo.png" alt="img"></p><p><img src="https://i.loli.net/2020/10/27/az7ewSWFYZGUbIu.png" alt="img"></p><p>例如，其中第一行，第二列 表示给定前一个词是 “i” 时，当前词为“want”的情况一共出现了827次。据此，我们便可以算得相应的频率分布表如下。</p><p><img src="https://i.loli.net/2020/10/27/9WhUqaZtcC3xDn7.jpg" alt="img"></p><p>比如说，我们就以表中的$p(eat|i)=0.0036$这个概率值讲解，从表一得出“i”一共出现了2533次，而其后出现eat的次数一共有9次，$p(eat|i)=p(eat,i)/p(i)=count(i,eat)/count(i)=9/2533 = 0.0036$</p><p>下面我们通过基于这个语料库来判断$s1=“<s> i want english food</s>” $ 与 $s2 = “<s> want i english food</s>“$哪个句子更合理：通过例子来讲解是最人性化的，我在网上找了这么久发现这个例子最好：</p><p><strong>首先来判断$p(s1)$</strong></p><p>$P(s1)=P(i|<s>)P(want|i)P(english|want)P(food|english)P(</s>|food)$</p><p>=$0.25×0.33×0.0011×0.5×0.68=0.000031$</p><p><strong>再来求$p(s2)$</strong></p><p>$P(s2)=P(want|<s>)P(i|want)P(english|want)P(food|english)P(</s>|food)$</p><p>=$0.25×0.0022×0.0011×0.5×0.68 = 0.00000002057$</p><p><strong>通过比较我们可以明显发现0.00000002057&lt;0.000031,也就是说s1= “i want english food&lt;/s&gt;”更像人话。</strong></p><p><strong>再深层次的分析，我们可以看到这两个句子的概率的不同，主要是由于顺序i want还是want i的问题，根据我们的直觉和常用搭配语法，i want要比want i出现的几率要大很多。所以两者的差异，第一个概率大，第二个概率小，也就能说的通了。</strong></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PPL-语句通顺</title>
      <link href="2020/10/27/PPL-%E8%AF%AD%E5%8F%A5%E9%80%9A%E9%A1%BA/"/>
      <url>2020/10/27/PPL-%E8%AF%AD%E5%8F%A5%E9%80%9A%E9%A1%BA/</url>
      
        <content type="html"><![CDATA[<h3 id="语句通顺-一些调研"><a href="#语句通顺-一些调研" class="headerlink" title="语句通顺 - 一些调研"></a>语句通顺 - 一些调研</h3><ul><li><p>BERT模型通过在大量语料的训练可以判断一句话是否通顺</p></li><li><p>理解 NNLM <a href="https://zhuanlan.zhihu.com/p/65446874" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/65446874</a></p><p>以上推理就是</p><ol><li>用词汇的联合概率表达一个语句是否通顺；</li><li>将计算联合概率转换为计算条件概率；</li><li>将条件概率由不定长度的且一般较大的t维降到一般较小的n-1维;</li></ol></li><li><p><a href="https://blog.csdn.net/blmoistawinde/article/details/104966127" target="_blank" rel="noopener">https://blog.csdn.net/blmoistawinde/article/details/104966127</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/76912493" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76912493</a></p></li></ul><h3 id="PPL-评价指标"><a href="#PPL-评价指标" class="headerlink" title="PPL-评价指标"></a>PPL-评价指标</h3><p>在得到不同的语言模型（一元语言模型、二元语言模型….）的时候，我们如何判断一个语言模型是否好还是坏，一般有两种方法：</p><p>1、一种方法将其应用到具体的问题当中，比如机器翻译、speech recognition、spelling corrector等。然后看这个语言模型在这些任务中的表现（extrinsic evaluation，or in-vivo evaluation）。但是，这种方法一方面难以操作，另一方面可能非常耗时，可能跑一个evaluation需要大量时间，费时难操作。</p><p>2、针对第一种方法的缺点，大家想是否可以根据与语言模型自身的一些特性，来设计一种简单易行，而又行之有效的评测指标。于是，人们就发明了perplexity这个指标。</p><p><img src="https://i.loli.net/2020/10/27/UcxVWKjtlCi5Tw7.png" alt="image-20201027113452950" style="zoom: 25%;"></p><p>困惑度（perplexity）的基本思想是：<strong>给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，</strong></p><p>由公式可知，<strong>句子概率越大，语言模型越好，迷惑度越小。</strong></p><p>$P(W_1, W_2, … , W_t)$</p><p>= $P(W_t | W_{t-1}, … , W_2, W_1) * P(W_{t-1}, … , W_2, W_1))$</p><p>= $P(W_t | W_{t-1}, … , W_2, W_1)$ <em> $P(W_{t-1} | W{t-2}, …, W_2, W_1) $ </em> $ P(W{t-2}, …, W_2, W_1)$</p><p>= $P(W_t | W_{t-1}, … , W_2, W_1)$ <em> $ P(W_{t-1} | W_{t-2}, …, W_2, W_1) $ </em> $ … $ <em> $ P(W_2 | W_1) </em> P(W_1)$</p><p>一些 ngram 模型经 训练文本后在测试集上的困惑度值：</p><p><img src="https://i.loli.net/2020/10/31/bYtXO3hgJs8TE6f.jpg" alt="img" style="zoom: 50%;"></p><ul><li>求通俗解释NLP里的perplexity是什么？ - 习翔宇的回答 - 知乎 <a href="https://www.zhihu.com/question/58482430/answer/412012509" target="_blank" rel="noopener">https://www.zhihu.com/question/58482430/answer/412012509</a></li><li>也可以用交叉熵损失函数来表示</li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Visual Grounding in Video for Unsupervised Word Translation</title>
      <link href="2020/10/18/Visual-Grounding-in-Video-for-Unsupervised-Word-Translation/"/>
      <url>2020/10/18/Visual-Grounding-in-Video-for-Unsupervised-Word-Translation/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>我们的目标是使用视觉基础来改进语言之间的非监督词映射。其核心思想是通过学习母语教学视频中未配对的嵌入语，在两种语言之间建立一种共同的视觉表达。</p><p>本文的工作就是<strong>向机器提供不同的教学视频</strong>，这些视频的内容是人们用本国语言的教学视频。比如说，说中文和英文教别人榨橙汁的教学视频。这类视频有两个特点：视频网站上<strong>大量存在</strong>和<strong>内容相似度高</strong>，非常适合用于训练。但是这些视频也有一些弊端，会有很多无关废话（如“观众老爷们记得素质三连哦~”）。</p><p>即使如此，这种基于视觉的翻译提高了翻译的精度。</p><h3 id="Unsupervised-Multilingual-Learning"><a href="#Unsupervised-Multilingual-Learning" class="headerlink" title="Unsupervised Multilingual Learning"></a>Unsupervised Multilingual Learning</h3><p><img src="https://i.loli.net/2020/10/18/qzv2QN3bd8oInSx.png" alt="image-20201018172741937"></p><p>一个无监督的系统，该系统通过将语言嵌入视频中翻译单词。其中，不需要任何配对数据来学习翻译。</p><p><strong>Our method</strong> is unsupervised in that it learns the correspondences between two languages $X$ and $Y$ (e.g. English and French) without any parallel (paired) corpora.</p><p>given two distinct collections of instructional videos, i.e. n videos narrated with language $X$and another m different videos with language $Y$.</p><p><strong>Our goal</strong> is to learn to map languages $X$ and $Y$ by leveraging the shared visual modality $Z$ – the videos.</p><p><strong>Loss function</strong></p><p><img src="https://i.loli.net/2020/10/18/uYrymIKnJwvL2G6.png" alt="image-20201018173014409" style="zoom: 25%;"></p><h4 id="Multilingual-Visual-Embedding-Architecture"><a href="#Multilingual-Visual-Embedding-Architecture" class="headerlink" title="Multilingual Visual Embedding: Architecture"></a>Multilingual Visual Embedding: Architecture</h4><p><img src="https://i.loli.net/2020/10/18/XU2slICkuhyLBRT.png" alt="image-20201018172904212" style="zoom:33%;"></p><p><strong>yaya:</strong>  通过 视觉将两种语言做一种映射是存在困难的。文中列出了三点：<br>（1）learning video-text embeddings from instructional videos is difficult as the speech in these videos is only loosely related to the scene.</p><p>（2）in multilingual setting, such errors compound since both languages have this low video-text relevance;<br>（3）visually similar videos may not be semantically similar.</p><p>因此本文不同video 作为桥梁直接学习两种语言的映射，而是采取了间接的方式：we learn a joint (monolingual) video-text embedding space from instructional videos.</p><p>对于一种语言X, 学习视频及其字幕的映射，对于另一种语言，也学习一种映射，同时，在这种语言上加一个Adaptlayer, 使得 X和Y 能够映射到一个共同的空间。</p><p><strong>模型细节：</strong></p><p>其中X编码器 = WordEmbed + （Liner + ReLU MaxPool) + Linear</p><p>（WordEmbed层，度向量的转换；Linear层，建立与 Joint Embedding Space的映射）</p><p>而Y编码器则多了一个调整层（AdaptLayer），进行的是跨语言共享模型的权重分配，尽量让Y语言的词和X语言的词有相似的嵌入。</p><h4 id="MUVE-Improving-Unsupervised-Translation"><a href="#MUVE-Improving-Unsupervised-Translation" class="headerlink" title="MUVE: Improving Unsupervised Translation"></a>MUVE: Improving Unsupervised Translation</h4><p>略</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>[VIVO] Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training</title>
      <link href="2020/10/18/VIVO-Surpassing-Human-Performance-in-Novel-Object-Captioning-with-Visual-Vocabulary-Pre-Training/"/>
      <url>2020/10/18/VIVO-Surpassing-Human-Performance-in-Novel-Object-Captioning-with-Visual-Vocabulary-Pre-Training/</url>
      
        <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>VIVO可以在没有文本标签的数据上进行文本和图像的多模态预训练，摆脱了对配对图文数据的依赖，可以直接利用ImageNet等数据集的类别标签。借助VIVO，模型可以学习到物体的视觉外表和语义之间的关系，建立视觉词表。</p><p>这个视觉词表是啥呢？其实就是一个图像和文本的联合特征空间，在这个特征空间中，语义相近的词会聚类到一起，如金毛和牧羊犬，手风琴和乐器等。</p><p>预训练建好词表后，模型只需在有少量共同物体的配对图文的数据上进行微调，模型就能自动生成通用的模板语句，使用时，即使出现没见过的词，也能从容应对，相当于把图片和描述的各部分解耦了。</p><p>所以VIVO既能利用预训练强大的物体识别能力，也能够利用模板的通用性，从而应对新出现的物体。</p><p><img src="https://i.loli.net/2020/10/17/gakiQWDp3YAITCd.png" alt="image-20201017185401941" style="zoom:50%;"></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文要针对describe novel objects which are unseen in caption-labeled training data。This paper presents VIsual VOcabulary pretraining (VIVO) that performs pre-training in the absence of caption annotations。</p><p>By breaking the dependency of paired image-caption training data in VLP, VIVO can leverage large amounts of <strong>paired image-tag data</strong> to learn <strong>a visual vocabulary</strong>.<br>This is done by pre-training a <strong>multi-layer Transformer model</strong> that learns to align image-level tags with their corresponding image region features. Given that tags are not ordered, we employ <strong>the Hungarian matching loss</strong> for tag prediction optimization. </p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="https://i.loli.net/2020/10/17/JuOMYRtzW7LEgfo.png" alt="image-20201017193144259"></p><h4 id="VIVO-Pre-training"><a href="#VIVO-Pre-training" class="headerlink" title="VIVO Pre-training"></a>VIVO Pre-training</h4><p>We pre-train the Transformer model on a large-scale dataset with abundant tags, e.g., the Open Images training set with <strong>6.4K classes of image-level tags.</strong></p><p><strong>The training objective</strong> is to predict the missing (masked) tags given a bag of image-level tags and image regions. </p><p>We denote the training set: N images $I_i$ and their corresponding tags $G_i$. 一个image有多个tags.</p><p>use <strong>bi-directional attention mask</strong> in VIVO pre-training.</p><h4 id="Fine-tuning-and-Inference"><a href="#Fine-tuning-and-Inference" class="headerlink" title="Fine-tuning and Inference"></a>Fine-tuning and Inference</h4><p>After pre-training, the Transformer model is fine-tuned on a dataset where both captions and tags are available, e.g., the COCO set annotated with tags from 80 object classes and captions.</p><p>the input to the model during <strong>fine-tuning is a triplet of image region features $V$, a set of tags $T$ and a  caption $C$</strong>, where $V$ and $T$ are constructedin the same way as described in pre-training, and $C$ is a sequence of tokens. During fine-tuning, we <strong>randomly mask outsome of the tokens in a caption sentence</strong> for  prediction, and optimize the model parameters using the cross-entropy loss.</p><p>during fine-tuning we apply <strong>the uni-directional attention mask</strong> on a caption sequence to prevent the positions from attending to subsequent positions.</p><p>During inference, we first extract image region features and detect tags from a given image. Then the model is applied to <strong>generate a sequence, one token at a time,</strong> until it outputs the end of sentence token or reaches the maximum length.</p><p><strong>detect tags</strong> ：We use an object detector trained on the Open Images dataset （500 classes bboxes）to detect object tags for all datasets.</p><p><strong style="color:red;"><strong>yaya：</strong> tags detector的限制，仅能输出 500个类别tags, 因此，novel objects 的生成也是受到限制的</strong></p><p>以下这个表就可以说明问题，当不预训练时，是第一行的数据；当仅使用tags detector 的500个类时，是第二行的数据；当使用open-image 所有的 6.4k 个类时，是第三行的数据。因此，在inference阶段，使用 tag detector 来提供tags 是存在问题的。至少限制了模型的性能。</p><p><strong>改进</strong>：本文的model，pre-training, fine-tune，都是在一个fix model上。但是pre-training 的目的，仅仅是为了构建 image-tag vocabulary, 可以先构建，然后再离线使用！！！</p><p><img src="https://i.loli.net/2020/10/18/eyKJkcQPibhX3nS.png" alt="image-20201018115538016" style="zoom:33%;"></p>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
          <category> region-word-embedding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> region-word-embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vokenization Improving Language Understanding with Contextualized, Visual-Grounded Supervision</title>
      <link href="2020/10/18/Vokenization-Improving-Language-Understanding-with-Contextualized-Visual-Grounded-Supervision/"/>
      <url>2020/10/18/Vokenization-Improving-Language-Understanding-with-Contextualized-Visual-Grounded-Supervision/</url>
      
        <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><h4 id="现存问题"><a href="#现存问题" class="headerlink" title="现存问题"></a>现存问题</h4><p>人类学习语言都是结合多模态信息，但是当前的 language pre-training frameworks 仅通过自监督的方式，学习语言这一种模态。</p><p>虽然这种自监督的方式取得了很大的成功，但是它们没有利用grounding information from external visual word.</p><blockquote><p>Emily M Bender and Alexander Koller. 2020. <strong>Climbing towards nlu: On meaning, form, and understanding in the age of data.</strong> In ACL.</p><p>Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata,<br>Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. 2020. <strong>Experience grounds language.</strong> In EMNLP</p></blockquote><h4 id="本文的解决"><a href="#本文的解决" class="headerlink" title="本文的解决"></a>本文的解决</h4><p><img src="https://i.loli.net/2020/10/17/1cz3MHUdNXA9o5J.png" alt="image-20201017095623630" style="zoom:33%;"></p><p>本文：介绍了一个 <strong>视觉</strong>监督语言模型，如图1，该模型使用 language tokens 作为输入，使用token-related images 作为视觉监督。本文将这些images称作 vokens，which act as visualizations of the corresponding tokens.</p><p>假若a large aligned token-voken dataset 存在，那么模型可以通过voken-prediction task 从这些vokens中进行学习。但是不幸的是，不存在这种大型数据集，主要是有两个挑战：(1) 视觉性单词与 其他非视觉性单词之间，数量上存在很大的差异。如，在visually-grounded language datasets中仅有120M tokens, 但是在BERT的训练数据中有3300M tokens。grounded language 一般会更短，偏向于instructive descriptions, 因此在句子长度和有效词的数量上与其他语言类型的分布不同。(2) 自然语言中的大部分单词是 not visually grounded，因此对是否建立一个 visual supervision的数据集提出了质疑。粗略估计，英语维基百科中 grounded tokens 的比例仅为大约28％。 这种 low grounded ratio 导致以前方法中的视觉监控覆盖率低。</p><p><img src="https://i.loli.net/2020/10/17/27sWCBNOiqp13Vz.png" alt="image-20201017111702738"></p><p>为解决以上的两个挑战，本文提出了一个 <strong>vokenization method, that contextually maps the tokens to the visualized tokens (i.e., vokens) by retrieval.</strong>  而不是直接使用具有visually grounded的语言数据集来监督语言模型。</p><p>解决第一个挑战：(1) relative small datasets to train the <strong>vokenization processor</strong> (2) generate vokens for large language corpora.<br>our visually-supervised language model will take the input supervision from these large datasets, thus <strong>bridging the gap between different data sources,</strong> which solves the first challenge.</p><p>解决第一个挑战：low grounded ratio 的第二个挑战似乎是语言的固有特征。 但是，我们发现，考虑到它的上下文，可以将一些非可视化的tokens 有效地映射到相关图像。by our contextual token-image matching model (defined in Sec. 3.2) inside our vokenization processor, where we map tokens to images by viewing the sentence as the context.</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>Using our proposed vokenizer with a <strong>contextualized</strong> token-image matching model, we generate vokens for English Wikipedia. </p><p>Supervised by these generated vokens, we show consistent improvements upon a BERT model on several diverse NLP tasks such as GLUE (Wang et al., 2019), SQuAD (Rajpurkar et al., 2016), and SWAG (Zellers et al., 2018).  We also show the transferability of our vokens to other frameworks (i.e., RoBERTa).</p><h3 id="Vokenization"><a href="#Vokenization" class="headerlink" title="Vokenization"></a>Vokenization</h3><p><img src="https://i.loli.net/2020/10/17/fUWtQpIO8kZjAcY.png" alt="image-20201017120304025" style="zoom:50%;"></p><p>we <strong>retrieve an image for a token</strong> from a set of images $X$ = {$x_1; x_2; … ; x_n$} regarding a token-image-relevance scoring function $r_\theta(w_i; x; s)$. This scoring function $r_\theta(w_i; x; s)$, parameterized by $\theta$</p><h4 id="Contextual-Token-Image-Matching-Model"><a href="#Contextual-Token-Image-Matching-Model" class="headerlink" title="Contextual Token-Image Matching Model"></a>Contextual Token-Image Matching Model</h4><p>输入：The model takes a sentence $s$ and an image $x$ as input.</p><p>输出：The output $r_\theta(w_i; x; s)$ is the relevance score between the token $w_i \in s$ and the image $x$ while considering the whole sentence $s$ as a context.</p><p>Model: an inner product of the language feature representation $f_\theta(w_i; s)$ and the visual feature representation $g_\theta(x)$: $r_\theta(w_i; x; s)$ = $f_\theta(w_i; s)^T$ $g_\theta(x)$</p><p>token-image paris: 使用MS-COCO image caption pairs， 将caption中的所有tokens的vokens 都指定为该 image.</p><p>Training: 训练模型，maximizing the relevance score of these aligned token-image pairs over unaligned pairs. 使用 hinge loss.</p><h3 id="Visually-Supervised-Language-Models"><a href="#Visually-Supervised-Language-Models" class="headerlink" title="Visually-Supervised Language Models"></a>Visually-Supervised Language Models</h3><p>Based on these vokens, we propose a new pre-training task for language: voken classification.</p><h4 id="The-Voken-Classification-Task"><a href="#The-Voken-Classification-Task" class="headerlink" title="The Voken-Classification Task"></a>The Voken-Classification Task</h4><p><img src="https://i.loli.net/2020/11/04/j32ZMfNUpHWwez4.png" alt="image-20201104160728232"></p><p>BERT 的结果，会在每个token $w_i$的位置输出一个localized feature representation ${h_i}$，因此这将会很容易增加一个 token-level classification task, 而不需要修改模型的结构。Suppose the vokens come<br>from a finite set $X$, we convert the hidden output to ${h_i}$ a probability distribution ${p_i}$ with a linear layer and a softmax layer. </p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>受到这篇文章对的影响，是否可以结合视频，设计一个这种模型，比如有一些动词，仅能在视频中体现出来。</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> region-word-embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Re-evaluating Evaluation in Text Summarization</title>
      <link href="2020/10/16/Re-evaluating-Evaluation-in-Text-Summarization/"/>
      <url>2020/10/16/Re-evaluating-Evaluation-in-Text-Summarization/</url>
      
        <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>ROUGE 在 text summarization 任务中被广泛使用，但是，关于ROUGE是否可能偏离human judgement 以及这种偏离可能改变有关baseline 和 proposed methods的相对优势得出的结论的讨论很少。</p></li><li><p>为了表征<strong>评估指标</strong>的相对优势，有必要执行meta-evaluation。</p><p>where a dataset annotated with human judgments is used to test the degree to which automatic metrics correlate there with.</p></li><li><p>现在存在关键问题：现有的人类判断数据集很少，尚不清楚<strong>现有指标</strong>在<em>当前得分最高的摘要系统</em>上的表现。</p></li><li><p>在本文中提出一个问题：摘要模型中模型开发的快速发展是否需要我们<strong>重新评估</strong>用于文本摘要的<strong>评估过程</strong>。因此，在本文中，收集了一个large benchmark 来用于 meta-evaluating summarization metrics。</p><ul><li><p>数据来源于：25 top-scoring extractive and abstractive summarization systems on the CNN/DailyMail dataset.</p></li><li><p>Automatic: traditional metrics (e.g. ROUGE) and modern semantic matching metrics (e.g.  BERTScore, MoverScore).</p></li><li><p>Manual evaluations: 使用轻量级金字塔方法（Shapira等，2019），我们将其用作summarization systems 和 automated metrics的黄金标准。（yaya: 收集的human judgements 既可以作为评判systems好坏的标准，也可以作为评判metrics好坏的标准）</p><blockquote><p>Ori Shapira, <strong>Crowdsourcing lightweight pyramids for manual summary evaluation.</strong>  NAACL 2019</p></blockquote></li></ul></li></ul><h3 id="标注过程"><a href="#标注过程" class="headerlink" title="标注过程"></a>标注过程</h3><ul><li>对于一个document，仅存在一个reference，对该reference, 提取SCUs, 如下表展示出来的所示。该步骤由作者本人完成。</li><li>对于该document 的 candidate summary, 查看 SCUs 是否出现在 candidate summary 中，并标注为 “present” 或者是 “not present”。该步骤的操作由4个workers共同完成。</li><li>对于 each documents，查验是否存在 noisy worker, 即对于一个SCU，大多数认为其”present”，但是他却认为”not present”，在该document的大多数SCUs中他的annotations都与众数不同，则定义为 noisy workers，并将其标注的结果去除掉。</li></ul><p><img src="https://i.loli.net/2020/10/16/m7Cxl6hnvOwPeSc.png" alt="image-20201016121420397"></p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>本文想要研究的核心问题：“does the rapid progress of model development in summarization models require us to re-evaluate the evaluation process used for text summarization?”</p><p>因此本文从四个方面 to meta-evaluate current metrics。(1) evaluate all systems; (2) evaluate top-k strongest systems; (3) compare two systems; (4) evaluate individual summaries.</p><h4 id="evaluate-all-systems"><a href="#evaluate-all-systems" class="headerlink" title="evaluate all systems"></a>evaluate all systems</h4><p><img src="https://i.loli.net/2020/10/16/Cy7RSaKrP6XOUnY.png" alt="image-20201016160058531" style="zoom:33%;"></p><p>通过对比不同的metrics 在 system level 的相关性发现，在不同的数据集上，metrics的相关性，性能并不一致。</p><blockquote><p>that metrics run the risk of overfitting to some datasets</p></blockquote><p>本文建议，在不同的数据集上，使用不同的metric来评估该数据集上不同的systems 的性能好坏</p><h4 id="evaluate-top-k-strongest-systems"><a href="#evaluate-top-k-strongest-systems" class="headerlink" title="evaluate top-k strongest systems"></a>evaluate top-k strongest systems</h4><p><img src="https://i.loli.net/2020/10/16/Zfn4Hh6g8uG5eXM.png" alt="image-20201016160240565"></p><p>结论：当 top-systems 数量较少时，或者说数量不稳定时，不同的metrics在同一个数据集上的效果也不稳定。</p><h4 id="compare-two-systems"><a href="#compare-two-systems" class="headerlink" title="compare two systems"></a>compare two systems</h4><p>we only have 100 annotated summaries to compare any two systems, sys1 and sys2, we use paired bootstrap resampling,</p><p>对于人类在sys1/2 上对所有的summaries 都有一个得分。</p><p>现，要比较两个system, 若有95%以上的confidence认为sys1 better than sys2, 则 ytrue=1, 否则 ytrue=2, 如果confidence&lt;95%, ytrue=0.</p><p>同理，对于所有的metrics, 通过同样的方式，也可以有此比较得分 ypred。</p><p>现有 J 个systems, 则可以得到 J<em>(J-1)/2 个compaired paris. 即，得到 长度为 J</em>(J-1)/2的mask.</p><p>计算 ytrue_mask 与 ypred_mask 的F1score.即可评估metrics在 compare two systems上的性能与human 的一致性。</p><p><img src="https://i.loli.net/2020/10/16/ys241wqHUpGmltO.png" alt="image-20201016161340716" style="zoom:33%;"></p><p>结论：Different metrics are better suited for different datasets. For example, on the CNNDM datasets, we recommend using R-2 while, on the TAC datasets, we recommend using JS-2.</p><h4 id="evaluate-individual-summaries"><a href="#evaluate-individual-summaries" class="headerlink" title="evaluate individual summaries"></a>evaluate individual summaries</h4><p><img src="https://i.loli.net/2020/10/16/iWLrRBSITJGpMoN.png" alt="image-20201016162009349" style="zoom: 50%;"></p><ul><li>以上三个实验都是system-level , 此实验是 summary-level</li></ul><p>分析：在不同的数据集上，同一metrics的性能不一致。如，R_1在TAC上与human相关性较低，但是在CNNDM上相关性较高。</p><p>另，前有文章表明，automatic metrics趋向于在system level 与 human的相关性较好，但是在 instance-level (summary-level) 相关性较差。我们进行实验发现，这种现象仅在TAC-2009 上表现明显。</p><p>结论：autometrics 在 summary-level 与 system-level 上的性能是一样的。</p><blockquote><p>Even though some metrics might be good at comparing summaries, they may point in the wrong direction when comparing systems</p></blockquote><p>另外，<strong>some metric在不同的数据集上的性能是不同的，因此是有必要在不同的数据集上测试各个评价指标的有效性</strong></p><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>（1）metric的选择不仅取决于不同的任务（例如，摘要，翻译），而且还取决于不同的数据集（例如，TAC，CNNDM）和应用程序场景（例如，系统级别，摘要级别）。未来的meta-evaluating 工作应调查效果这些设置对指标性能的影响。</li><li>（2）指标很容易在有限的数据集上过拟合。多数据集meta-evaluate 可以帮助我们更好地了解每个指标的特殊性，从而在各种情况下获得更好的指标选择。</li><li>（3）我们收集的人工判断可以用作监督，以实例化最近提出的预训练-然后-微调框架（最初用于机器翻译）（Sellam等，2020），学习一个强大的文本摘要指标。</li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 评价指标 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shiyaya-Instruction-Final-bilibili</title>
      <link href="2020/09/17/shiyaya-Instruction-Final-bilibili/"/>
      <url>2020/09/17/shiyaya-Instruction-Final-bilibili/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning</title>
      <link href="2020/09/15/Learning-to-Compose-Topic-Aware-Mixture-of-Experts-for-Zero-Shot-Video-Captioning/"/>
      <url>2020/09/15/Learning-to-Compose-Topic-Aware-Mixture-of-Experts-for-Zero-Shot-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Survey of Evaluation Metrics Used for NLG Systems</title>
      <link href="2020/08/28/A-Survey-of-Evaluation-Metrics-Used-for-NLG-Systems/"/>
      <url>2020/08/28/A-Survey-of-Evaluation-Metrics-Used-for-NLG-Systems/</url>
      
        <content type="html"><![CDATA[<h3 id><a href="#" class="headerlink" title=" "></a> </h3><h3 id="Recommendations-Possible-future-research-directions"><a href="#Recommendations-Possible-future-research-directions" class="headerlink" title="Recommendations (Possible future research directions)"></a>Recommendations (Possible future research directions)</h3><ul><li><p><strong style="color:blue;">为所有的评估指标构建一个通用的工具包</strong></p></li><li><p><strong style="color:blue;">构建一个包含 human judgments 的数据集。</strong></p><p>（1）human 会从不同的角度进行评估</p><p>（2）根据收集的数据，可以训练出来一个评价指标</p></li><li><p><strong style="color:blue;">提出 task-specific and context-dependent metrics</strong></p><p>类似于 dialog 这个任务，reference response 与 right prediction response 之间的word overlap 很小，因此，only reference dependent 的评价指标是有缺陷的，还需要结合 context 来设计评价指标</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh954nipuej319z0k4q9z.jpg"></p></li><li><p><strong style="color:blue;">提出具有可解释性的评价指标</strong></p><p>当前大部分的评价指标， 对预测仅仅给出 a single score, 没有任何具体的指向。但是对于 human evaluation, 会从具体的层面，eg: fluency, adequency. coherence 来进行评价。因此 a single score 不够具有可解释性。</p><p>应该设计不同的评价指标，每一个评价指标，从特定的层面进行评价。</p></li><li><p><strong style="color:red;">Creating robust benchmarks for evaluating evaluation metrics </strong></p><p>early metrics, 例如，BLEU，METEOR 等，已经在各种各样的任务上进行了验证。</p><p>但是，最近新提出的评价指标还没有被 examined critically，为了实施这一研究，需要收集一个 <strong>对抗 evaluation benchmarks</strong>， <strong>这个 benchmarks可以测试这些metrics 的鲁棒性。</strong></p><p>举个例子：对于dialog，给定一个context， 可以收集一些 adversarially crafted responses（与 passage 有较高的 word overlap, 但实际上是不相关的，或者是不正确的）  。查看evaluation metrics 是否会对这种手工创造的对抗例子给<strong>低分</strong>，已验证其鲁棒性。</p><p><strong>除了这种 adversarial evaluations， 还需要研究 提出的evaluation metrics 是否有specific biases.</strong> 比如，GAN based evaluators 会在一些systems上进行训练，则其更容易对这些 systems 给高分。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 评价指标 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>correlation coefficient</title>
      <link href="2020/08/26/correlation-coefficient/"/>
      <url>2020/08/26/correlation-coefficient/</url>
      
        <content type="html"><![CDATA[<h1 id="correlation-coefficient"><a href="#correlation-coefficient" class="headerlink" title="correlation coefficient"></a>correlation coefficient</h1><ul><li>spearman 和 kendall 计算的都是对排序 之间的计算</li><li>pearson 计算的是直接的数值，协方差，标准差之间的计算</li></ul><h3 id="pearson"><a href="#pearson" class="headerlink" title="pearson"></a>pearson</h3><ul><li><p>代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下两种方式都可以</span></span><br><span class="line">scipy.stats.pearsonr(array_1, array_2)</span><br><span class="line">np.corrcoef(array_1, array_2)</span><br></pre></td></tr></table></figure></li><li><p>计算公式</p><p><img src="https://i.loli.net/2020/08/26/lXA4uMz9UkG2vFx.png" alt="image-20200826144001717" style="zoom: 33%;"></p></li><li><p>适用范围</p><p>当两个变量的标准差都不为零时，相关系数才有定义，皮尔逊相关系数适用于：</p><p>(1)、两个变量之间是线性关系，都是连续数据。</p><p>(2)、两个变量的总体是正态分布，或接近正态的单峰分布。</p><p>(3)、两个变量的观测值是成对的，每对观测值之间相互独立。</p></li><li><p>注意</p><p>公式的分母是变量的标准差，这就意味着计算pearson时，变量的标准差不能为0（分母不能为0），也就是说你的两个变量中任何一个的值不能都是相同的。如果没有变化，用pearson是没办法算出这个变量与另一个变量之间是不是有相关性的。</p><p>就好比我们想研究人跑步的速度与心脏跳动的相关性，如果你无论跑多快，心跳都不变（即心跳这个变量的标准差为0），或者你心跳忽快忽慢的，却一直保持一个速度在跑（即跑步速度这个变量的标准差为0），那我们都无法通过pearson的计算来判断心跳与跑步速度到底相不相关。</p></li><li><p>使用Pearson线性相关系数有2个局限：</p><ol><li>必须假设数据是成对地从正态分布中取得的。</li><li>数据至少在逻辑范围内是等距的。</li></ol></li></ul><h3 id="spearman"><a href="#spearman" class="headerlink" title="spearman"></a>spearman</h3><p><a href="https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php" target="_blank" rel="noopener">https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php</a></p><ul><li><p>代码实现</p><ul><li><p>对于一般情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result, _ = scipy.stats.spearmanr(array_1, array_2)</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>对于离散整数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spearmanr</span><span class="params">(set_1, set_2)</span>:</span></span><br><span class="line"></span><br><span class="line">    ar = np.apply_along_axis(scipy.stats.rankdata, <span class="number">0</span>, set_1)</span><br><span class="line">    br = np.apply_along_axis(scipy.stats.rankdata, <span class="number">0</span>, set_2)</span><br><span class="line">    d = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(ar)):</span><br><span class="line">        d.append(ar[i] - br[i])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    d_sq = [i ** <span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> d]</span><br><span class="line">    sum_d_sq = sum(d_sq)</span><br><span class="line">    n_cu_min_n = len(set_1) ** <span class="number">3</span> - len(set_1)</span><br><span class="line">    r = <span class="number">1</span> - ((<span class="number">6.0</span> * sum_d_sq) / n_cu_min_n)</span><br><span class="line">    <span class="keyword">return</span> r</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>计算公式</p><ul><li><p>存在 并列排序时：</p><p>先排序，对排序值 pair 计算 pearson 系数</p></li><li><p>不存在并列排序时，</p><p>先排序，计算d<sub>i</sub> 再按照下面第一个公式进行计算</p></li></ul></li></ul><p><img src="https://i.loli.net/2020/08/26/ON5iLc2kl6EAM1p.png" alt="微信截图_20200826151113"></p><ul><li><p>另外一种说法</p><ul><li><p>一般情况：</p><p>先排序，对排序值 pair 计算 pearson 系数</p></li><li><p>对于数值为离散的整数时，</p><p>先排序，计算d<sub>i</sub> 再按照吐下的公式进行计算</p></li></ul><p><img src="https://i.loli.net/2020/08/26/z3iJWqUxRcBSegV.png" alt="微信截图_20200826151324"></p></li><li><p>适用范围</p><p>spearman 对数据条件的要求没有皮尔逊相关系数严格，只要两个变量的观测值是成对的等级评定资料，或者是由连续变量观测资料转化得到的等级资料，不论两个变量的总体分布形态、样本容量的大小如何，都可以用spearman 来进行研究</p></li></ul><h3 id="kendall"><a href="#kendall" class="headerlink" title="kendall"></a>kendall</h3><ul><li><p>代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scipy.stats.kendalltau(array_1, array_2)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Kendallta</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    Lens = len(a)</span><br><span class="line"></span><br><span class="line">    ties_onlyin_x = <span class="number">0</span></span><br><span class="line">    ties_onlyin_y = <span class="number">0</span></span><br><span class="line">    con_pair = <span class="number">0</span></span><br><span class="line">    dis_pair = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Lens - <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, Lens):</span><br><span class="line">            test_tying_x = np.sign(a[i] - a[j])</span><br><span class="line">            test_tying_y = np.sign(b[i] - b[j])</span><br><span class="line">            panduan = test_tying_x * test_tying_y</span><br><span class="line">            <span class="keyword">if</span> panduan == <span class="number">1</span>:</span><br><span class="line">                con_pair += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> panduan == <span class="number">-1</span>:</span><br><span class="line">                dis_pair += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> test_tying_y == <span class="number">0</span> <span class="keyword">and</span> test_tying_x != <span class="number">0</span>:</span><br><span class="line">                ties_onlyin_y += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> test_tying_x == <span class="number">0</span> <span class="keyword">and</span> test_tying_y != <span class="number">0</span>:</span><br><span class="line">                ties_onlyin_x += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    result = (con_pair - dis_pair) / np.sqrt(</span><br><span class="line">        (con_pair + dis_pair + ties_onlyin_x) * (dis_pair + con_pair + ties_onlyin_y))</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure></li><li><p>计算公式</p><p><a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient" target="_blank" rel="noopener">Kendall_rank_correlation_coefficient</a></p><p>有好几个计算公式</p></li><li><p>适用范围</p><p>kendall与spearman 对数据条件的要求相同，可参见<a href="http://blog.csdn.net/wsywl/archive/2010/09/02/5859751.aspx" target="_blank" rel="noopener">统计相关系数(2)—Spearman Rank(斯皮尔曼等级)相关系数及MATLAB实现</a>中介绍的spearman 对数据条件的要求。</p></li></ul><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><ul><li>这三种 相关系数，计算 array_1 与 array_2 之间的相关性，若array_1 或者 array_2 中的元素都相同（eg: array_1 = np.array([5,5,5,5,5])） 则会使得输出为NaN.</li></ul><p>三种方法的适用场合</p><h4 id="主要参数methods介绍"><a href="#主要参数methods介绍" class="headerlink" title="主要参数methods介绍:"></a>主要参数methods介绍:</h4><ol><li>pearson correlation coefficient（皮尔逊相关性系数）。<br> 常用的相关系数求法，采用协方差cov(X,Y)/标准差的乘积(σX, σY)。<br> 数据要求： 线性数据、连续且符合正态分布；数据间差异不能太大；变量准差不能为0，即两变量中任何一个值不能都是相同。</li><li>spearman correlation coefficient（斯皮尔曼秩相关性系数）。<br> 根据原始数据的排序位置进行计算。<br> 数据要求：用于解决称名数据和顺序数据相关的问题，适用于两列变量，而且具有等级变量性质具有线性关系的数据，能够很好处理序列中相同值和异常值。</li><li>kendall correlation coefficient（肯德尔相关性系数）。<br> 等级相关系数，适用于两个变量均为有序分类的情况<br> 数据要求：肯德尔相关性系数，它也是一种秩相关系数，不过它所计算的对象是分类变量。</li></ol><p>所以针对【连续、正态分布、线性】数据，采用pearson相关系数；针对【非线性的、非正态】数据，采用spearman相关系数；针对【分类变量、无序】数据，采用Kendall相关系数。一般来讲，线性数据采用pearson，否则选择spearman，如果是分类的则用kendall。</p><p>作者：王叽叽的小心情<br>链接：<a href="https://www.jianshu.com/p/f9304da68d98" target="_blank" rel="noopener">https://www.jianshu.com/p/f9304da68d98</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><h3 id="相关系数和P-value-值"><a href="#相关系数和P-value-值" class="headerlink" title="相关系数和P-value 值"></a>相关系数和P-value 值</h3><p>看两者是否算相关要看两方面</p><p>显著水平以及相关系数</p><p>（1）显著水平,就是P值,这是首要的,因为如果不显著,相关系数再高也没用,可能只是因为偶然因素引起的,那么多少才算显著,一般p值小于0.05就是显著了；如果小于0.01就更显著；例如p值=0.001,就是很高的显著水平了,只要显著,就可以下结论说：拒绝原假设无关,两组数据显著相关也说两者间确实有明显关系.通常需要p值小于0.1,最好小于0.05甚至0.01,才可得出结论：两组数据有明显关系,如果p=0.5,远大于0.1,只能说明相关程度不明显甚至不相关.起码不是线性相关.</p><p>（2）相关系数,也就是pearson spearman等,通常也称为R值,在确认上面指标显著情况下,再来看这个指标,一般相关系数越高表明两者间关系越密切.R&gt;0 代表连个变量正相关,即一个变大另一个随之变大</p><h3 id="需要的数据量"><a href="#需要的数据量" class="headerlink" title="需要的数据量"></a>需要的数据量</h3><p><a href="https://bbs.pinggu.org/thread-3240378-1-1.html" target="_blank" rel="noopener">https://bbs.pinggu.org/thread-3240378-1-1.html</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 评价指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>评价者之间的一致性-Kappas</title>
      <link href="2020/08/06/%E8%AF%84%E4%BB%B7%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7-Kappas/"/>
      <url>2020/08/06/%E8%AF%84%E4%BB%B7%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7-Kappas/</url>
      
        <content type="html"><![CDATA[<h3 id="评价者之间的一致性—Kappas-Inter-rater-agreement-Kappas"><a href="#评价者之间的一致性—Kappas-Inter-rater-agreement-Kappas" class="headerlink" title="评价者之间的一致性—Kappas Inter-rater agreement Kappas"></a>评价者之间的一致性—Kappas Inter-rater agreement Kappas</h3><p>inter-rater reliability == inter-rater agreement == concordance</p><p>评价者之间的一致性的Kappa分数代表着在打分判断中，他们有多少共识，有多一致。</p><p>Kappa分数处于0-1之间，具体地：</p><div class="table-container"><table><thead><tr><th style="text-align:center">K</th><th style="text-align:center">Interpretation</th></tr></thead><tbody><tr><td style="text-align:center">&lt;0</td><td style="text-align:center">Poor agreement 不一致</td></tr><tr><td style="text-align:center">0.0-0.20</td><td style="text-align:center">Slight agreement</td></tr><tr><td style="text-align:center">0.21-0.40</td><td style="text-align:center">Fair agreement</td></tr><tr><td style="text-align:center">0.41-0.60</td><td style="text-align:center">Moderate agreement</td></tr><tr><td style="text-align:center">0.61-0.80</td><td style="text-align:center">Substantial agreement</td></tr><tr><td style="text-align:center">0.81-1.0</td><td style="text-align:center">Almost perfect agreement</td></tr></tbody></table></div><h3 id="Cohen’s-Kappa"><a href="#Cohen’s-Kappa" class="headerlink" title="Cohen’s Kappa"></a>Cohen’s Kappa</h3><p>Cohen’s Kappa 计算了评分者之间的一致性。当评分者对同一项任务给出了相同的判断或分数，那么他们的一致性得到了体现。</p><p>Cohen’s Kappa 只能在以下的条件下使用：</p><ul><li>两个评价者分别对每个样本进行评分</li><li>一个评价者对每个样本进行两次评分</li></ul><p><strong>Cohen’s Kappa 计算</strong></p><p>要注意的是，一般情况下，Cohen’s Kappa 的计算背景是：有<strong>两个</strong>评分者对每个样本进行<strong>二分类</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">postive (rater A)</th><th style="text-align:center">negative (rater A)</th><th style="text-align:center">Total</th></tr></thead><tbody><tr><td style="text-align:center"><strong>postive (rater B)</strong></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B11%7D" alt="n_{11}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B12%7D" alt="n_{12}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B1.%7D" alt="n_{1.}"></td></tr><tr><td style="text-align:center"><strong>negative (rater B)</strong></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B21%7D" alt="n_{21}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B22%7D" alt="n_{22}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B2.%7D" alt="n_{2.}"></td></tr><tr><td style="text-align:center"><strong>Total</strong></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B.1%7D" alt="n_{.1}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B.2%7D" alt="n_{.2}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D" alt="n_{11}+n_{12}+n_{21}+n_{22}"></td></tr></tbody></table></div><p>计算公式为：<br> <img src="https://math.jianshu.com/math?formula=k%20%3D%20%5Cfrac%7Bp_o-p_e%7D%7B1-p_e%7D%20%3D%201-%5Cfrac%7B1-p_o%7D%7B1-p_e%7D" alt="k = \frac{p_o-p_e}{1-p_e} = 1-\frac{1-p_o}{1-p_e}"><br> 其中，<img src="https://math.jianshu.com/math?formula=p_o" alt="p_o"> 代表评价者之间的相对观察一致性（the relative <strong>observed agreement</strong> among raters）<br> <img src="https://math.jianshu.com/math?formula=p_o%3D%5Cfrac%7Bn_%7B11%7D%2Bn_%7B22%7D%7D%7Bn_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D%7D" alt="p_o=\frac{n_{11}+n_{22}}{n_{11}+n_{12}+n_{21}+n_{22}}"><br> <img src="https://math.jianshu.com/math?formula=p_e" alt="p_e"> 代表偶然一致性的假设概率（the hypothetical probability of <strong>chance agreemnet</strong>）<br> <img src="https://math.jianshu.com/math?formula=p_e%3D%5Cfrac%7Bn_%7B.1%7D*n_%7B1.%7D%7D%7B(n_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D" alt="p_e=\frac{n_{.1}*n_{1.}}{(n_{11}+n_{12}+n_{21}+n_{22})^2}+\frac{n_{.2}*n_{2.}}{(n_{11}+n_{12}+n_{21}+n_{22})^2}=\frac{n_{.1}*n_{1.}+n_{.2}*n_{2.}}{(n_{11}+n_{12}+n_{21}+n_{22})^2}">%5E2%7D%2B%5Cfrac%7Bn_%7B.2%7D<em>n_%7B2.%7D%7D%7B(n_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D)%5E2%7D%3D%5Cfrac%7Bn_%7B.1%7D</em>n_%7B1.%7D%2Bn_%7B.2%7D<em>n_%7B2.%7D%7D%7B(n_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D)%5E2%7D)<br> <em>*例子</em></em></p><p>rater A和rater B对50张图片进行分类，正类和负类。结果为：</p><ul><li>20张图片两个评价者都认为是正类</li><li>15张图片两个评价者都认为是负类</li><li>rater A认为25张图片是正类，25张图片是负类</li><li>rater B 认为30张图片是正类，20张图片是负类</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">postive (rater A)</th><th style="text-align:center">negative (rater A)</th><th style="text-align:center">Total</th></tr></thead><tbody><tr><td style="text-align:center"><strong>postive (rater B)</strong></td><td style="text-align:center">20</td><td style="text-align:center">10</td><td style="text-align:center">30</td></tr><tr><td style="text-align:center"><strong>negative (rater B)</strong></td><td style="text-align:center">5</td><td style="text-align:center">15</td><td style="text-align:center">20</td></tr><tr><td style="text-align:center"><strong>Total</strong></td><td style="text-align:center">25</td><td style="text-align:center">25</td><td style="text-align:center">50</td></tr></tbody></table></div><p><strong>Step1</strong> ：计算<img src="https://math.jianshu.com/math?formula=p_o" alt="p_o"><br> <img src="https://math.jianshu.com/math?formula=p_o%3Dnumber%5C%20in%5C%20agreement%2F%5C%20total%3D(20%2B15" alt="p_o=number\ in\ agreement/\ total=(20+15)/50=0.70">%2F50%3D0.70)</p><p><strong>Step2</strong> ：计算<img src="https://math.jianshu.com/math?formula=p_e" alt="p_e"><br> <img src="https://math.jianshu.com/math?formula=p_e%3DThe%5C%20total%5C%20probability%5C%20the%5C%20raters%5C%20both%5C%20saying%5C%20postive%20%5C%5Cand%5C%20negative%20%5C%20randomly%20%3D(25%2F50" alt="p_e=The\ total\ probability\ the\ raters\ both\ saying\ postive \\and\ negative \ randomly =(25/50)*(30/50)+(25/50)*(20/50)=0.50"><em>(30%2F50)%2B(25%2F50)</em>(20%2F50)%3D0.50)<br> <strong>Step3</strong> ：计算<img src="https://math.jianshu.com/math?formula=k" alt="k"><br> <img src="https://math.jianshu.com/math?formula=k%3D%5Cfrac%7Bp_o-p_e%7D%7B1-p_e%7D%3D%5Cfrac%7B0.70-0.50%7D%7B1-0.50%7D%3D0.40" alt="k=\frac{p_o-p_e}{1-p_e}=\frac{0.70-0.50}{1-0.50}=0.40"><br> <img src="https://math.jianshu.com/math?formula=k%3D0.40" alt="k=0.40"> 代表<strong>fair agreement</strong></p><h3 id="Fleiss’s-Kappa"><a href="#Fleiss’s-Kappa" class="headerlink" title="Fleiss’s Kappa"></a>Fleiss’s Kappa</h3><p>Fleiss’s Kappa 是对 Cohen‘s Kappa 的扩展：</p><ul><li>衡量<strong>三个或更多</strong>评分者的一致性</li><li>不同的评价者可以对不同的项目进行评分，而不用像Cohen’s 两个评价者需要对相同的项目进行评分</li><li>Cohen’s Kappa 的评价者是精心选择和固定的，而Fleiss’s Kappa 的评价者是从较大的人群中随机选择的</li></ul><p>举一个例子对 Fleiss’s Kappa 的计算进行说明：14个评价者对10个项目进行1-5的评分，<img src="https://math.jianshu.com/math?formula=N%3D10%2Cn%3D14%2Ck%3D5" alt="N=10,n=14,k=5"></p><blockquote><p>关于这  “10个项目” 的理解：比如在 NLI 数据标注中，需要为很多 promise-hypotheses pair 进行打分，这每一个pair就是一个pair.</p></blockquote><div class="table-container"><table><thead><tr><th style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7Bij%7D" alt="n_{ij}"></th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th><th style="text-align:center">5</th><th style="text-align:center"><img src="https://math.jianshu.com/math?formula=P_i" alt="P_i"></th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">14</td><td style="text-align:center">1.000</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">6</td><td style="text-align:center">4</td><td style="text-align:center">2</td><td style="text-align:center">0.253</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">3</td><td style="text-align:center">5</td><td style="text-align:center">6</td><td style="text-align:center">0.308</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">0</td><td style="text-align:center">3</td><td style="text-align:center">9</td><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">0.440</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">8</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0.330</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">7</td><td style="text-align:center">7</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0.462</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">3</td><td style="text-align:center">2</td><td style="text-align:center">6</td><td style="text-align:center">3</td><td style="text-align:center">0</td><td style="text-align:center">0.242</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">2</td><td style="text-align:center">5</td><td style="text-align:center">3</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">0.176</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">6</td><td style="text-align:center">5</td><td style="text-align:center">2</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0.286</td></tr><tr><td style="text-align:center">10</td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">3</td><td style="text-align:center">7</td><td style="text-align:center">0.286</td></tr><tr><td style="text-align:center">Total</td><td style="text-align:center">20</td><td style="text-align:center">28</td><td style="text-align:center">39</td><td style="text-align:center">21</td><td style="text-align:center">32</td><td style="text-align:center">140</td></tr><tr><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=p_j" alt="p_j"></td><td style="text-align:center">0.143</td><td style="text-align:center">0.200</td><td style="text-align:center">0.279</td><td style="text-align:center">0.150</td><td style="text-align:center">0.229</td></tr></tbody></table></div><p><strong>Step1</strong> ：计算<img src="https://math.jianshu.com/math?formula=p_j" alt="p_j"> ，以<img src="https://math.jianshu.com/math?formula=p_1" alt="p_1">为例，评价者随机打1分的概率<br> <img src="https://math.jianshu.com/math?formula=p_1%3Dthe%5C%20total%5C%20number%5C%20of%5C%20the%5C%20column%2F%5C%20the%5C%20total%5C%20number%5C%20of%20%5C%20tasks%20%3D%2020%2F14*10%3D0.143" alt="p_1=the\ total\ number\ of\ the\ column/\ the\ total\ number\ of \ tasks = 20/14*10=0.143"><br> <strong>Step2</strong> ：计算<img src="https://math.jianshu.com/math?formula=P_i" alt="P_i"> ，以<img src="https://math.jianshu.com/math?formula=P_2" alt="P_2">为例,14个评价者对第2个任务达成共识的程度<br> <img src="https://math.jianshu.com/math?formula=P_2%3D%5Cfrac%7Bthe%5C%20sum%5C%20of%5C%20suqare%20%5C%20of%5C%20the%5C%20row%7D%7Bn*(n-1" alt="P_2=\frac{the\ sum\ of\ suqare \ of\ the\ row}{n*(n-1)}=\frac{0^2+2^2+6^2+4^2-14}{14*(14-1)}=0.253">%7D%3D%5Cfrac%7B0%5E2%2B2%5E2%2B6%5E2%2B4%5E2-14%7D%7B14<em>(14-1)%7D%3D0.253)<br> <strong>Step3</strong> ：计算<img src="https://math.jianshu.com/math?formula=P_e%2CP_o" alt="P_e,P_o"><br> ![P_o=\frac{1}{N}\sum_{i=1}^{N}P_i=\frac{1}{10}</em>3.78=0.378](<a href="https://math.jianshu.com/math?formula=P_o%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DP_i%3D%5Cfrac%7B1%7D%7B10%7D*3.78%3D0.378" target="_blank" rel="noopener">https://math.jianshu.com/math?formula=P_o%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DP_i%3D%5Cfrac%7B1%7D%7B10%7D*3.78%3D0.378</a>)</p><p><img src="https://math.jianshu.com/math?formula=P_e%3D%5Csum_%7Bj%3D1%7D%5E%7Bk%7Dp_j%5E2%3D0.143%5E2%2B0.200%5E2%2B0.279%5E2%2B0.150%5E2%2B0.229%5E2%3D0.213" alt="P_e=\sum_{j=1}^{k}p_j^2=0.143^2+0.200^2+0.279^2+0.150^2+0.229^2=0.213"></p><p><img src="https://math.jianshu.com/math?formula=k%3D%5Cfrac%7BP_o-P_e%7D%7B1-P_e%7D%3D%5Cfrac%7B0.378-0.213%7D%7B1-0.213%7D%3D0.210" alt="k=\frac{P_o-P_e}{1-P_e}=\frac{0.378-0.213}{1-0.213}=0.210"></p><p><img src="https://math.jianshu.com/math?formula=k%3D0.210" alt="k=0.210"> 代表<strong>fair agreement</strong></p><blockquote><p>[1] Landis JR, Koch GG. The measurement of observer agreement for categorical data. Biometrics.     1977;33(1):159–74</p><p>[2] <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.pmean.com%2Fdefinitions%2Fkappa.htm" target="_blank" rel="noopener">http://www.pmean.com/definitions/kappa.htm</a></p><p>[3] <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.statisticshowto.datasciencecentral.com%2Fcohens-kappa-statistic%2F" target="_blank" rel="noopener">https://www.statisticshowto.datasciencecentral.com/cohens-kappa-statistic/</a></p><p>[4] <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.statisticshowto.datasciencecentral.com%2Ffleiss-kappa%2F" target="_blank" rel="noopener">https://www.statisticshowto.datasciencecentral.com/fleiss-kappa/</a></p><p>[5]  <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Famirziai%2Flearning%2Fblob%2Fmaster%2Fstatistics%2FInter-rater%20agreement%20kappas.ipynb%5D(https%3A%2F%2Fgithub.com%2Famirziai%2Flearning%2Fblob%2Fmaster%2Fstatistics%2FInter-rater" target="_blank" rel="noopener">[https://github.com/amirziai/learning/blob/master/statistics/Inter-rater%20agreement%20kappas.ipynb](https://github.com/amirziai/learning/blob/master/statistics/Inter-rater</a> agreement kappas.ipynb)</p><p>[6] <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fqq_31113079%2Farticle%2Fdetails%2F76216611" target="_blank" rel="noopener">https://blog.csdn.net/qq_31113079/article/details/76216611</a></p></blockquote><p>作者：Luuuuuua<br>链接：<a href="https://www.jianshu.com/p/f9c383b39859" target="_blank" rel="noopener">https://www.jianshu.com/p/f9c383b39859</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Bridging by Word Image Grounded Vocabulary Construction for Visual Captioning</title>
      <link href="2020/08/01/Bridging-by-Word-Image-Grounded-Vocabulary-Construction-for-Visual-Captioning/"/>
      <url>2020/08/01/Bridging-by-Word-Image-Grounded-Vocabulary-Construction-for-Visual-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>当前 普遍使用的 CNN-RNN 策略，基于整个 training dataset构建 vocabulary，但是，这会<strong>导致生成的句子中的 N-grams 也是在训练集中常见的</strong>，但是语义上却与given image 无关。</p><p>为了解决这个问题，本文提出了构建一个 image-grounded vocabulary。具体地，提出了一个 two-step approach，通过结合 visual information 和 relationships among words来构建 新的vocabulary。</p><p>并提出了两个策略在 text generation过程中<strong>利用</strong>构建的vocabulary。（1）generator 从image-grounded vocabulary中挑选words （2）soft-attention聚合 vocabulary information 到RNN cell 中来生成下一个单词。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>当前生成单词的方式，是从整个vocab 中select，但是当描述一个 particular image时，the possible words 应该是从一小部分单词集中挑选出来。因此，可以想一个方案，在image caption generation 过程中，有效的约束 word selection space。这将会解决 生成的句子中常常是 irrelavant n-gram problem. </p><p>本文，提出构建一个 image-grounded vocabulary，</p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>整个结构包括两个阶段：(1) image-grounded vocabulary construction, (2) text generation with vocabulary constraints.</p><p><strong>The image-grounded vocabulary</strong> constructor builds a  vocabulary related to a given image.</p><p><strong>The text generator</strong> with vocabulary constraints generates captions using the constructed vocabulary in two different ways. (1) words generated are strictly limited to those in the image-grounded vocabulary. (2) words in the image-grounded vocabulary are re-weighted within the RNN cell such that they are more likely to be generated.</p><h4 id="Image-Grounded-Vocabulary-Construction"><a href="#Image-Grounded-Vocabulary-Construction" class="headerlink" title="Image-Grounded Vocabulary Construction"></a>Image-Grounded Vocabulary Construction</h4><p>caption 中的单词，一般可以分类两类，一类是直接与image content 相关的单词（entities or objects depicted in the image），另一类是function words or words which 没有和image content 有直接的对应关系。</p><p>本文假设， directly-related words 可以由视觉信息来决定，而第二类单词，可以由第一类单词之间的relationship 来决定。因此提出了两步策略来构建 image-grounded vocabulary。</p><ul><li><p>第一步</p><p>使用 <code>From captions to visual concepts and back</code> 中提到的方法，获取 textual concept as H.</p><p>H中words 与 image 的相关性分布：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbctjwumaj30cz02ga9z.jpg" style="zoom:33%;"></p></li><li><p>第二步</p><p>计算  full vocabulary <code>V</code> 中单词的 相关性分数，</p><p>The probability distribution of words in <code>V</code>:</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbd1t40doj30if03b3yt.jpg" style="zoom:33%;"></p></li></ul><p>本文挑选 top k words 来构成 <strong style="color:red;">the image-grounded vocabulary ($W_i$) </strong>for given image.</p><h4 id="Text-Generation-with-Vocabulary-Constraints"><a href="#Text-Generation-with-Vocabulary-Constraints" class="headerlink" title="Text Generation with Vocabulary Constraints"></a>Text Generation with Vocabulary Constraints</h4><p>提出了两个不同的策略来利用 the image-grounded vocab-ulary $W_i$ 和 word relevance distribution  $S_i^{(V)}$ : 一种，使用$W_i$ 作为 hard constraint; 另一种，聚合每个单词的相关性到 RNN cell 来生成 caption.</p><p><strong>Generator with Hard Constraint</strong></p><p>正常的方法，是生成 full vocab 的 概率分布，然后取 argmax。</p><p>但是在 hard constraint 下，生成 $W_i$ 的概率分布，再取 argmax。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbdj6x0ruj30m402haa0.jpg" style="zoom: 50%;"></p><p>对于在 $W_i$ 中没有出现的单词，打掩码：a mask operation $m_i$ is introduced  to replace the $j_{th}$ value in the vector with 1 if $w_j$ is not found in $W_i$ 。</p><p><strong>Generator with Soft Constraint </strong></p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbdt8khggj31cg0mw4i2.jpg" style="zoom:50%;"></p><p>图中展示的 image-grounded vocabulary 其实是 $S_i^{V}$</p><p>结合到RNN cell: </p><p>这个新的 RNN cell 结合了image-grounded vocabulary，因此，会更加容易生成该vocab中的单词。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbe0h10qcj30pt0avmy9.jpg" style="zoom:33%;"></p>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations</title>
      <link href="2020/07/31/Aligning-Visual-Regions-and-Textual-Concepts-for-Semantic-Grounded-Image-Representations/"/>
      <url>2020/07/31/Aligning-Visual-Regions-and-Textual-Concepts-for-Semantic-Grounded-Image-Representations/</url>
      
        <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Understanding the image, which necessitates the acquisition of grounded image representations. </p><p>以下，提供了几种方式来 表达image content.</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha8v7kk80j316b0ciqmw.jpg"></p><p><strong style="color:blue;">[1]</strong> 基于 R-CNN 的方法可以获得 regions，但是却没有与 actual words关联起来，这将会造成两个域之间的语义不一致，并且需要由 downstream systems 自己学习 alignments。</p><p><strong style="color:blue;">[2]</strong> 此外，这些representations 仅包含局部特征，缺少全局结构信息。 这些问题 使system 难以有效地理解图像。</p><p>因此，本文提出一个 Mutual Iterative Attention (MIA) 模块，在编码阶段，从<strong style="color:red;">视觉域和语言域</strong>（解决[1]） 构建<strong style="color:red;">聚合的 image representations</strong>(解决[2])。</p><p>we perform mutual attention <strong style="color:blue;">iteratively </strong> between the two domains to realize the procedure <strong>without annotated alignment data.</strong> </p><p>The visual receptive fields gradually concentrate on salient visual regions, and the original word-level concepts are gradually merged to recapitulate corresponding visual regions. </p><p>In addition, the aligned visual features and textual concepts provide a more clear definition of the image aspects they represent. </p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha9crz1pbj30de0s4jxg.jpg" style="zoom:50%;"></p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha9gjk5vbj31h50j3jvs.jpg"></p><h3 id="textual-concepts"><a href="#textual-concepts" class="headerlink" title="textual concepts"></a>textual concepts</h3><p>从下面这篇论文中提取 text concepts</p><blockquote><p> <strong>From captions to visual concepts and back.</strong> In CVPR, 2015</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Improving Image Captioning with Conditional Generative Adversarial Nets</title>
      <link href="2020/07/31/Improving-Image-Captioning-with-Conditional-Generative-Adversarial-Nets/"/>
      <url>2020/07/31/Improving-Image-Captioning-with-Conditional-Generative-Adversarial-Nets/</url>
      
        <content type="html"><![CDATA[<p>From:  <a href="https://zhuanlan.zhihu.com/p/39890390" target="_blank" rel="noopener">GAN in Image Captioning</a></p><h3 id="Improving-Image-Captioning-with-Conditional-Generative-Adversarial-Nets"><a href="#Improving-Image-Captioning-with-Conditional-Generative-Adversarial-Nets" class="headerlink" title="Improving Image Captioning with Conditional Generative Adversarial Nets"></a><strong>Improving Image Captioning with Conditional Generative Adversarial Nets</strong></h3><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>本文提出了一种新的基于条件生成对抗网络的图像字幕框架，作为传统的基于增强学习(RL)的编解码结构的扩展。为了应对不同的目标语言的指标之间不一致的评价问题，，论文设计了两种鉴别器网络来自动地、逐步地确定生成的描述是人工描述的还是机器生成的。</p><p>生成器是采用传统图像描述的模型，<strong>在强化学习自我批判算法（SCST）下进行优化</strong>。</p><p>由于基于CNN和RNN的结构各有其优点，因此引入了两种鉴别器结构。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha02tza7dj31im0hvtk0.jpg"></p><h4 id="CNN-discriminator："><a href="#CNN-discriminator：" class="headerlink" title="CNN discriminator："></a><strong>CNN discriminator：</strong></h4><p>（1）首先创建了一个feature map,编码了图像与句子特征。</p><p>（2）接着采用了m组有不同窗大小，核数目的卷积核来获取不同的特征，</p><p>（3）然后把所有特征作max pooling操作再联结在一起，并用一个highway架构提升性能。</p><p>（4）最后激活特征通过全连接层与sigmoid 转换来获得决策器的输出。输出在[0,1]之间。</p><h4 id="RNN-discriminator："><a href="#RNN-discriminator：" class="headerlink" title="RNN discriminator："></a><strong>RNN discriminator：</strong></h4><p>基于RNN的决策器采用了一种标准的LSTM架构，把图像特征输入到第一个LSTM，接下来输入的LSTM是输入是单词编码信息。最后通过全连接层与softmax层获得RNN决策器的输出。</p><h4 id="固定G，更新D："><a href="#固定G，更新D：" class="headerlink" title="固定G，更新D："></a>固定G，更新D：</h4><p>Discriminator的目标函数为：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha04mt3u7j31m4070tal.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha02tyrdqj31j50kxdv1.jpg"></p><h4 id="固定D，更新G"><a href="#固定D，更新G" class="headerlink" title="固定D，更新G"></a>固定D，更新G</h4><p>设计reward，以强化学习来更新generator。</p><p>在强化学习的设定下，本文采用GAN与RL结合的reward来权衡<strong style="color:red;">图像描述的保真度</strong>（在评价标准下获得高得分）与<strong style="color:red;">自然性</strong>（生成描述符合人类的风格）。</p><blockquote><p><strong style="color:blue;">这也是本文主要的创新点，结合 保真度 的评价</strong></p></blockquote><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha06jgm6tj31e409cmz3.jpg" style="zoom: 50%;"></p><h4 id="整个算法的伪代码如下："><a href="#整个算法的伪代码如下：" class="headerlink" title="整个算法的伪代码如下："></a>整个算法的伪代码如下：</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha09ceqgkj30ns0wg13f.jpg" style="zoom: 50%;"></p><h3 id="GAN-image-captioning-task"><a href="#GAN-image-captioning-task" class="headerlink" title="GAN + image captioning task"></a>GAN + image captioning task</h3><p><strong>[1703.06029] Towards Diverse and Natural Image Descriptions via a Conditional GAN</strong></p><p><strong>[1703.10476] Speaking the Same Language Matching Machine to Human Captions by Adversarial Training</strong></p><p><strong>[1705.00930] Show, Adapt and Tell Adversarial Training of Cross-domain Image Captioner</strong></p><p><strong>[1805.00063] Improved Image Captioning with Adversarial Semantic Alignment</strong></p><p><strong>[1804.00861] Generating Diverse and Accurate Visual Captions by Comparative Adversarial Learning</strong></p>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
          <category> GAN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAN简介</title>
      <link href="2020/07/30/GAN%E7%AE%80%E4%BB%8B/"/>
      <url>2020/07/30/GAN%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>From: <a href="https://blog.csdn.net/shanlepu6038/article/details/84335117?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param" target="_blank" rel="noopener">GAN（一）:基本框架</a></p></blockquote><p>一个GAN包含两部分，一个generator,一个discriminator（互相对抗）<br>generator和discriminator就像是猎食者和猎物之间的关系，一个产生图片，一个辨别图片的真假，互相促进，使得最终产生的图片接近realistic</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ol><li><p>随机初始化 generator 和 discriminator</p></li><li><p>In each training iteration：</p><ul><li><p>固定generator，更新discriminator</p><p>Discriminator learns to assign high scores to real objects and low scores to generated objects.</p></li><li><p>固定discriminator， 更新generator</p></li></ul></li></ol><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh9bgltng3j30hu0djage.jpg"></p><h3 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h3><p><strong>G和D是互相促进的：</strong>G的目的是产生的图像让D感到模糊不知道该分成realistic（看起来像是现实的）还是fake（看起来是假的），D的目的是将realistic和fake的图像准确分辨。所以G产生的图像会越来越真，D的辨别能力会越来越强，最终达到一个平衡。</p><p>P<sub>data</sub> 表示真实数据的分布，P<sub>g</sub> 表示generator产生的分布，最终的目的就是让P<sub>g</sub> 的分布尽可能的和P<sub>data</sub> 相同。<br>我们用D(x)表示真实图像经过discriminator后的分数，G(z)表示随机变量z经过generator后产生的图像，那么有：D(G(z)) 表示generator产生的图像经过discriminator后的分数</p><p>第一阶段，固定 generator，更新discriminator，最大化下面对的这个式子：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh9bxggo1rj30gv01lwf1.jpg"></p><p>第二阶段，固定discriminator，更新generator，最大化下面的这个式子：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh9bxgfqjnj30ad01s3yt.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Motivation of Learning based Evaluation Metrics for Text Generation</title>
      <link href="2020/07/30/Motivation-of-Learning-based-Evaluation-Metrics-for-Text-Generation/"/>
      <url>2020/07/30/Motivation-of-Learning-based-Evaluation-Metrics-for-Text-Generation/</url>
      
        <content type="html"><![CDATA[<p>From : <strong>Evaluation of Text Generation: A Survey</strong></p><p>现在评估 text generation 的方法有三种：</p><p>(1) human-centric evaluation metrics, </p><p>(2) automatic metrics that require no training, </p><p>(3) machine-learned metrics. </p><h3 id="Human-centric-evaluation-metrics"><a href="#Human-centric-evaluation-metrics" class="headerlink" title="Human-centric evaluation metrics,"></a>Human-centric evaluation metrics,</h3><p>就是由人<strong>手工</strong>为 生成的文本进行打分，但是这种方式比较耗费人力</p><h3 id="Automatic-metrics-that-require-no-training"><a href="#Automatic-metrics-that-require-no-training" class="headerlink" title="Automatic metrics that require no training,"></a>Automatic metrics that require no training,</h3><ul><li><p>n-gram Overlap Metrics for Content Selection</p><ul><li><p>F-SCORE(F1)</p></li><li><p>BLEU</p></li><li><p>NIST</p></li><li><p>ROUGE</p></li><li><p>METEOR</p></li><li><p>HLEPOR</p></li><li><p>RIBES</p></li><li><p>CIDER</p></li></ul></li><li>Distance-Based Evaluation Metrics for Content Selection</li><li>n-gram-Based Diversity Metrics</li><li>Explicit Semantic Content Match Metrics<ul><li>SPICE </li></ul></li><li>Syntactic Similarity-Based Metrics</li></ul><h3 id="Machine-Learned-Evaluation-Metrics"><a href="#Machine-Learned-Evaluation-Metrics" class="headerlink" title="Machine-Learned Evaluation Metrics"></a>Machine-Learned Evaluation Metrics</h3><ul><li><p>Sentence Semantic Similarity Based Evaluation</p></li><li><p>Evaluating Factual Correctness</p></li><li>Regression-based Evaluation</li><li>Evaluation Models with Human Judgments</li><li>BERT-Based Evaluation</li><li>Composite Metric Scores</li></ul><p>由于语言的灵活性，GT texts 不可能覆盖所有的情况，下图展示了两个例子。在这两个任务中，给定输入，由模型生成的输出都是合理的，但它们与真实的输出不共享任何单词。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh954nipuej319z0k4q9z.jpg"></p><p>一种解决办法是使用 embedding based method，来衡量语义相似性，而非 word overlap。但是 embedding-based methods 不能解决 generated output 与 reference 语义不同的情况，如上图中 dialog 的例子。</p><p>在这种情况下，可以构建 machine-learned models (在 human judgment data上 进行训练) 来模拟 human judgments，进而评估output 的质量，例如事实正确性，自然性，流利性，连贯性等。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment</title>
      <link href="2020/07/30/Align2Ground-Weakly-Supervised-Phrase-Grounding-Guided-by-Image-Caption-Alignment/"/>
      <url>2020/07/30/Align2Ground-Weakly-Supervised-Phrase-Grounding-Guided-by-Image-Caption-Alignment/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文提出使用caption-to-image retrieval 作为下游任务，来引导 phrase localization。</p><p>第一步，学习 RoIs 与 phrases 之间的隐式对应，并利用这些匹配的RoIs来生成具有判别性的image representation。</p><p>第二步，learnedd representaion 与caption 对齐。</p><p>本文的贡献是，构建了“caption-conditioned” image encodinng，这件所有的任务都耦合在一起，并使得弱监督可以有效的引导 visual grounding。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li>监督学习的方法：依赖 region-phrase correspondence 数据。</li><li>弱监督：grounding free-form textual phrase，从image-caption pairs 这种weak correspondence中进行学习。</li></ul><p><strong>weakly supervised paradigm 的一个关键是，紧密耦合监督学习任务（image-caption matching）和无法获得显示标签的任务（region-phrase matching）。联合推理确保前者的监督损失可以有效的引导后者的学习。</strong></p><blockquote><p>[1] Andrej Karpathy and Li Fei-Fei. <strong>Deep visual-semantic alignments for generating image descriptions.</strong> In CVPR, 2015</p><p>[2] AndrejKarpathy,ArmandJoulin,andLiFFei-Fei. <strong>Deep fragment embeddings for bidirectional image sentence mapping.</strong> In NIPS,  2014</p></blockquote><p>[ 1 ] [ 2 ] 采用了 such paradigm，一般地，这种模型存在两个阶段：（1）local matching mudule: 得到 region-phrase 的隐式对应，进而生成local matching information.（2）gobal matching module: 使用（1）中得到的information来得到 image-caption matching.</p><p>需要注意的是，这种方案的设计，primary objective 是 image-caption matching 而不是 phrase matching。这种训练方式，将会放大selective regions 和 phrases 之间的相关性。举例说明：如果 第一阶段中，a small subset of phrases 存在很强的 match, 那么将会传递到第二阶段，<strong>via average pooling of the RoI–phrase matching scores</strong>, 使得image 和 caption 之间存在 high matching score。</p><p>这将会<strong>使得模型不去学习 准确的ground <strong style="color:red;">所有的</strong> phrases</strong>。分析可得，将visual grounding 作为 primary aim 不是一个有效的解决办法。<strong>这种“作弊”倾向，使模型学会了在下游任务上做得很好而不必在中间任务上做得更好。</strong></p><p>本文将这种现象称之为：“selective amplification” behavior</p><p>本文解决这个问题：我们通过提出一种novel mechanism 来解决这一问题，该机制以使两个阶段之间更紧密耦合的方式 to relay this information about the latent, inferred correspondences</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh8vyf8slej30wt0q914q.jpg" style="zoom: 50%;"></p><p>Our novelty lies in designing this effective transfer of  information between the supervised and unsupervised parts  of the model such that the quality of image representations for the supervised matching task is a direct consequence of  <strong>the correct localization of all phrases.</strong> </p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>our proposed model uses a novel technique that builds a discriminative image representation from the matched RoIs and uses this representation for the image-caption matching. </p><p>Specifically, the image representation that is used to match an image with a caption is conditioned only on <strong style="color:red;">the subset of image regions</strong> that align semantically with <strong style="color:red;">all the phrases</strong> in that caption. </p><p>本文认为，与标准的 pooling-based method 相比，这种结构的设计使图像字幕对的监督，成为 visual grounding 的更强学习信号。</p><h4 id="The-Local-Matching-module"><a href="#The-Local-Matching-module" class="headerlink" title="The Local Matching module"></a>The Local Matching module</h4><p>将 region 和 phrase 映射到相同的空间，然后计算 cosine similarity。</p><p>infer the matched RoI for a phrase最直接的方法是 选择top scoring box，但是，这种方案容易<strong>过拟合</strong>，因为模型经常持续选择 相同的错误region。</p><p>改进：使用attened region vector 作为matched RoI，虽然这种方法在其他的多模态任务中是有效的，但在这个任务中不是一个有效的方法。这是因为在训练过程中，多个匹配的RoI的加权平均似乎会损害匹配的RoI的辨别力（discriminativeness）。</p><p>再次改进：选择 top-k (k=3) scoring RoI candidates，然后随机的选择其中的一个作为 query phrase 的 匹配 RoI。这种策略通过在巡林过程中探索多样性的选择，进而可以增加鲁棒性。</p><h4 id="The-Local-Aggregator-module"><a href="#The-Local-Aggregator-module" class="headerlink" title="The Local Aggregator module"></a>The Local Aggregator module</h4><p>这个模块的设计比较玄学。说是为了generate a caption-conditioned representation of the image.</p><p>设计的模块：a two-layer Multilayer Perceptron (MLP) with a mean operation. </p><p>这个模块的输入，是从上一步中得到的 matched RoIs for correspondance phrases.</p><h4 id="The-Global-Matching-module"><a href="#The-Global-Matching-module" class="headerlink" title="The Global Matching module"></a>The Global Matching module</h4><p>将 caption 映射到与 上一步中得到  representation of the image 相同的空间，然后利用 cosine similarity</p><h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh939pfhcgj30tq040wen.jpg" style="zoom:33%;"></p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><p>motivation 挺好的 ，但是local aggregator 的设计比较朴素。</p>]]></content>
      
      
      <categories>
          
          <category> Visual Grounding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Visual Grounding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Improving Image Captioning Evaluation by Considering Inter References Variance</title>
      <link href="2020/07/26/Improving-Image-Captioning-Evaluation-by-Considering-Inter-References-Variance/"/>
      <url>2020/07/26/Improving-Image-Captioning-Evaluation-by-Considering-Inter-References-Variance/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li><p>one-to-one metrics的方法存在缺陷：without considering the intrinsic variance between ground truth captions.  </p><p>bertscore 是最新的one-to-one metric， 可以实现与human很好的相关性，但是如果一些问题可以解决的话，可以能够进一步的提升性能。</p><p>本文则基于 bertscore，提出了一个新方法。</p></li></ul><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p><strong>当前的评价指标</strong>—over penalize：对于M个reference，metric 通过one-to-one 的方式得到M个得分，通过 pooling 操作，得到最终得分。<br>但是不同的reference caption关注到image的不同方面，因此会存在 variance。所以基于 pooling 的操作，太过简单。</p><p>如果度量标准仅查看一个参考字幕，那么为这种过度惩罚而寻找补救措施是一项挑战。</p></li><li><p><strong>bertscore</strong>—under penalize</p><p>由于在计算 bertscore 时，采用了贪婪搜索的方式，而且对于每个reference word, pick 一个最大值，这里有可能 没有candidate word 与 reference word相匹配，却给了一个高分。因此，导致 under-penalize</p></li><li><p><strong>分析</strong>： In one-to-one evaluation，尽管很难直接考虑所有reference，但可以使用来自预训练语言模型的上下文嵌入将references 合并为单个reference。</p></li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><ul><li><p>3.1  Preliminary concept of references<br>combination  </p></li><li><p>3.2 Mismatch detection with overlap and<br>cosine similarity  </p></li><li>3.3 The combination of references  </li><li>3.4 Importance of different words</li><li>3.5 Summary and metric formula   </li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
          <category> 评价指标 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 评价指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>automatic metric 总结</title>
      <link href="2020/07/23/automatic-metric-%E6%80%BB%E7%BB%93/"/>
      <url>2020/07/23/automatic-metric-%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="automatic-metric-总结"><a href="#automatic-metric-总结" class="headerlink" title="automatic metric 总结"></a>automatic metric 总结</h1><p>转载：<code>肝了1W字！文本生成评价指标的进化与推翻</code></p><h2 id="基于词重叠率的方法"><a href="#基于词重叠率的方法" class="headerlink" title="基于词重叠率的方法"></a>基于词重叠率的方法</h2><h2 id="机器翻译-amp-摘要-常用指标"><a href="#机器翻译-amp-摘要-常用指标" class="headerlink" title="机器翻译 &amp; 摘要 常用指标"></a><strong>机器翻译 &amp; 摘要 常用指标</strong></h2><p>基于词重叠率的方法是指基于词汇的级别计算模型的生成文本和人工的参考文本之间的相似性，比较经典的代表有BLEU、METEOR和ROUGE，其中BLEU和METEOR常用于机器翻译任务，ROUGE常用于自动文本摘要。</p><h3 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a><strong>BLEU</strong></h3><p>BLEU （Bilingual Evaluation Understudy，双语评估辅助工具）可以说是所有评价指标的鼻祖，它的核心思想是比较候选译文和参考译文里的 n-gram 的重合程度，重合程度越高就认为译文质量越高。unigram用于衡量单词翻译的准确性，高阶n-gram用于衡量句子翻译的流畅性。实践中，通常是取N=1~4，然后对进行加权平均。<img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAg4GOn0EZAicLNZY3lw97TdibGCPyA09s0Ms9KZT1CpypiaZ0EwLuG5ZGrA/640?wx_fmt=jpeg" alt="img" style="zoom: 50%;"></p><ul><li>BLEU 需要计算译文 1-gram，2-gram，…，N-gram 的精确率，一般 N 设置为 4 即可，公式中的 <em>Pn 指 n-gram 的精确率</em>。</li><li>Wn 指 n-gram 的权重，一般设为均匀权重，即对于任意 n 都有 Wn = 1/N。</li><li>BP 是惩罚因子，如果译文的长度小于最短的参考译文，则 BP 小于 1。</li><li>BLEU 的 1-gram 精确率表示译文忠于原文的程度，而其他 n-gram 表示翻译的流畅程度。</li></ul><p>不过BLEU对词重复和短句有着非常不好的表现，所以改进的BLEU分别使用 <strong>改进的多元精度（n-gram precision）</strong> 和<strong>短句惩罚因子</strong>进行了优化。</p><h4 id="1-改进的多元精度（n-gram-precision）"><a href="#1-改进的多元精度（n-gram-precision）" class="headerlink" title="1. 改进的多元精度（n-gram precision）"></a>1. 改进的多元精度（n-gram precision）</h4><p>假设机器翻译的译文C和一个参考翻译S1如下：</p><blockquote><p>C: a cat is on the table<br>S1: there is a cat on the table</p></blockquote><p>则可以计算出 1-gram，2-gram，… 的精确率（参考文献里写的是准确率(accuracy),我理解是写错了，此处应该是精确率(precision)）</p><p>p1 计算 a cat is on the table 分别都在参考翻译S1中 所以 p1 = 1</p><p>p2  (a, cat)在, (cat is) 没在, (is on) 没在, (on the) 在, (the table)在 所以p2 = 3/5</p><p>p3  (a cat is)不在, (cat is on)不在, (is on the)不在, (on the table)在 所以 p3 = 1/4</p><p>依次类推(上面的在或者不在, 说的都是当前词组有没有在参考翻译中)。直接这样算, 会存在很大的问题. 例如:</p><blockquote><p>C: there there there there there S1: there is a cat on the table</p></blockquote><p>这时候机器翻译的结果明显是不正确的，但是其 1-gram 的 Precision 为1，因此 BLEU 一般会使用修正的方法。给定参考译文S1,S2, …,S<em>m</em>，可以计算C里面 n 元组的 Precision，计算公式如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgXmshfK57WrgarjibbBcy4ZdtxHS9Y3EtyDfTzjTNNl2GMxwJAIPmfbA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>针对上面的例子  p1 = 1/5  (因为there在C和S1中都出现了 我们按最少的次数来)</p><p><strong style="color:red;">yaya: 从公式中，可以看到，对于m个reference，取max 的方式，进行聚合</strong></p><h4 id="2-惩罚因子"><a href="#2-惩罚因子" class="headerlink" title="2. 惩罚因子"></a>2. 惩罚因子</h4><p>上面介绍了 BLEU 计算 n-gram 精确率的方法， 但是仍然存在一些问题，当机器翻译的长度比较短时，BLEU 得分也会比较高，但是这个翻译是会损失很多信息的，例如：</p><blockquote><p>C: a cat<br>S1: there is a cat on the table</p></blockquote><p>因此需要在 BLEU 分数乘上惩罚因子</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgaghJVhQZHwspVre1F1yyaAkZj4UnUwUDIurNHI8aPb8vlNP3GWg0Lw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><h4 id="3-优点"><a href="#3-优点" class="headerlink" title="3. 优点"></a>3. 优点</h4><ul><li>它的易于计算且速度快，特别是与人工翻译模型的输出对比；</li><li>它应用范围广泛，这可以让你很轻松将模型与相同任务的基准作对比。</li></ul><h4 id="4-缺点"><a href="#4-缺点" class="headerlink" title="4. 缺点"></a>4. 缺点</h4><ul><li>它不考虑语义，句子结构</li><li>不能很好地处理形态丰富的语句（BLEU原文建议大家配备4条翻译参考译文）</li><li>BLEU 指标偏向于较短的翻译结果（brevity penalty 没有想象中那么强）</li></ul><h3 id="ROUGE"><a href="#ROUGE" class="headerlink" title="ROUGE"></a><strong>ROUGE</strong></h3><p>英文全称Recall-Oriented Understudy for Gisting Evaluation，可以看做是BLEU 的改进版，专注于<strong>召回率而非精度</strong>。换句话说，它会查看有多少个参考译句中的 n 元词组出现在了输出之中。</p><p>ROUGE大致分为四种（常用的是前两种）：</p><ul><li>ROUGE-N （将BLEU的精确率优化为召回率）</li><li>ROUGE-L （将BLEU的n-gram优化为公共子序列）</li><li>ROUGE-W （将ROUGE-L的连续匹配给予更高的奖励）</li><li>ROUGE-S  （允许n-gram出现跳词(skip)）</li></ul><p>ROUGE 用作机器翻译评价指标的初衷是这样的：在 SMT（统计机器翻译）时代，机器翻译效果稀烂，需要同时评价翻译的准确度和流畅度；等到 NMT （神经网络机器翻译）出来以后，神经网络脑补能力极强，翻译出的结果都是通顺的，但是有时候容易瞎翻译。</p><p>ROUGE的出现很大程度上是为了解决NMT的漏翻问题（低召回率）。所以 ROUGE 只适合评价 NMT，而不适用于 SMT，因为它不管候选译文流不流畅</p><p>这里只介绍 ROUGE_L</p><h4 id="ROUGE-L"><a href="#ROUGE-L" class="headerlink" title="ROUGE-L"></a>ROUGE-L</h4><p>ROUGE-L 中的 L 指最长公共子序列 (longest common subsequence, LCS)，ROUGE-L 计算的时候使用了机器译文C和参考译文S的最长公共子序列，计算公式如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgfXBibHaKdNJDXNPgWhZ0L9FKG1b8LuabowzXZiaMhGXB3WQSepe0gYiaw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img" style="zoom:33%;"></p><p>公式中的 RLCS 表示召回率，而 PLCS 表示精确率，FLCS 就是 ROUGE-L。一般 beta 会设置为很大的数，因此 FLCS 几乎只考虑了 RLCS (即召回率)。注意这里 beta 大，则 F 会更加关注 R，而不是 P，可以看下面的公式。如果 beta 很大，则 PLCS 那一项可以忽略不计。</p><p><strong style="color:red;">yaya: 对于含有多个reference的情况，先分别计算 R<sub>LCS</sub> 和 P<sub>LCS</sub>， 再分别取max，得到 max_R, max_P之后，再带入 F<sub>LCS</sub> 中。</strong></p><h3 id><a href="#" class="headerlink" title=" "></a> </h3><h3 id="METEOR"><a href="#METEOR" class="headerlink" title="METEOR"></a><strong>METEOR</strong></h3><p>和BLEU不同，METEOR同时考虑了基于整个语料库上的准确率和召回率，而最终得出测度。</p><p>METEOR也包括其他指标没有发现一些其他功能，如<strong>同义词匹配</strong>等。METEOR用 WordNet 等知识源扩充了一下同义词集，同时考虑了单词的词形（词干相同的词也认为是部分匹配的，也应该给予一定的奖励，比如说把 likes 翻译成了 like 总比翻译成别的乱七八糟的词要好吧？）</p><p><strong>在评价句子流畅性的时候，用了 chunk 的概念</strong>（候选译文和参考译文能够对齐的、空间排列上连续的单词形成一个 chunk，这个对齐算法是一个有点复杂的启发式 beam serach），chunk 的数目越少意味着每个 chunk 的平均长度越长，也就是说候选译文和参考译文的语序越一致。</p><p>最后，METEOR计算为对应最佳候选译文和参考译文之间的准确率和召回率的调和平均：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggzxernqjrj30pe0hut9u.jpg" style="zoom:33%;"></p><h4 id="1-理解"><a href="#1-理解" class="headerlink" title="1. 理解"></a>1. 理解</h4><p>看公式总是挺抽象的，下面我们还是看看来自维基百科的例子吧。计算的最基本单元是句子。算法首先从待评价字符串和参考字符串之间创建一个平面图如下：<img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgruCbgRHd3tQITE2N2mQsfJUficbIQav7TBGZA7wvUpnyiaoMlwfYYxrg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>所谓<strong>平面图</strong>，就是1元组之间的映射集。平面图有如下的一些限制：在待评价翻译中的每个1元组必须映射到参考翻译中的1个或0个一元组，然后根据这个定义创建平面图。<strong>如果有两个平面图的映射数量相同，那么选择映射交叉数目较少的那个。</strong> 也就是说，上面左侧平面图会被选择。状态会持续运行，在每个状态下只会向平面图加入那些在前一个状态中尚未匹配的1元组。<em>一旦最终的平面图计算完毕，就开始计算METEOR得分</em>：</p><p>1元组精度：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggzxernte4j307d04fmx0.jpg" style="zoom:33%;"></p><p>其中m是<em>在参考句子中同样存在的，**待评价句子中的一元组的数量</em>。wt是<em>待评价翻译中一元组的数量</em>。</p><p>1元组召回率：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggzxersvbuj307s04h745.jpg" style="zoom:33%;"></p><p>m同上，是参考翻译中一元组的数量。</p><p>然后使用调和平均来计算F-mean，且召回的权重是精度的9（上面说的超参数α）倍。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggzxerunotj30eo054jre.jpg" style="zoom:33%;"></p><p>到目前为止，这个方法只对单个单词的一致性进行了衡量，还没有用到为了评价流畅性的 <strong>chunk</strong> 。chunk 块的定义是在待评价语句和参考语句中毗邻的一元组集合。</p><p>在参考和待评价句子中的没有毗连的映射越多，惩罚就越高。为了计算惩罚，1元组被分组成最少可能的块（chunks）。<em>在待评价语句和参考语句之间的毗邻映射越长，块的数量就越少</em>。一个待评价翻译如果和参考翻译相同，那么就只有一个块。惩罚p的计算如下：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggzxeruhhij30dj05pwei.jpg" style="zoom:33%;"></p><p>（假设参数都已经设置好了）其中c就是块的数量，Um是被映射的一元组的数量。p可以减少F-mean的值。最后：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggzxertdasj30fn02xjrb.jpg" style="zoom:33%;"></p><h4 id="2-优点"><a href="#2-优点" class="headerlink" title="2. 优点"></a>2. 优点</h4><ul><li>该方法基于一元组的精度和召回的调和平均，召回的权重比精度要高一点 ， 与人类判断相关性高</li><li><em>引入了外部知识，评价更加友好了。</em></li></ul><h4 id="3-缺点"><a href="#3-缺点" class="headerlink" title="3. 缺点"></a>3. 缺点</h4><ul><li>实现非常复杂，目前只有java版本</li><li>α、γ和θ 均为用于评价的默认参数。这些都是对着某个数据集调出来的（让算法的结果和人的主观评价尽可能一致，方法我记得是 grid search）。参数一多听起来就不靠谱（给个眼神体会一下）</li><li>需要有外部知识。如果很多词不在wordnet，那其实就没什么意义了</li></ul><h2 id="image-caption-常用指标"><a href="#image-caption-常用指标" class="headerlink" title="image caption 常用指标"></a><strong>image caption 常用指标</strong></h2><h3 id="CIDEr"><a href="#CIDEr" class="headerlink" title="CIDEr"></a><strong>CIDEr</strong></h3><p>CIDEr 是专门设计出来用于图像标注问题的。这个指标将每个句子都看作“文档”，将其表示成 Term Frequency Inverse Document Frequency（tf-idf）向量的形式，通过对每个n元组进行(TF-IDF) 权重计算，计算参考 caption 与模型生成的 caption 的余弦相似度，来衡量图像标注的一致性的。</p><ul><li>公式<br><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgicMuoIiabcIRSiaXj1tLEmgWU5ysVK6ZO4FlTJmfc5S3j3vS7tyzuibkEg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></li><li>举例<img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAg6qxl9VK30SIG0T4LBfVOkbrqRlx4DyBcDK9tRk9vMrwCKfvZYC1ZBw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></li></ul><h4 id="1-好处"><a href="#1-好处" class="headerlink" title="1. 好处"></a>1. 好处</h4><p>是一种加权的评价指标，他更关注你是否说到了重点，而常见的词权重则没有那么高。在 Kaustav_slides image caption的综述里，也提到这个评价指标和人类的评价相关性更高一些</p><h3 id="SPICE"><a href="#SPICE" class="headerlink" title="SPICE"></a><strong>SPICE</strong></h3><p>SPICE 也是专门设计出来用于 image caption 问题的。全称是 Semantic Propositional Image Caption Evaluation。</p><p>我们考虑如下图片：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgl3KCxN5scYCpo7RnLCVOWpTmaG8scssC1iaibPwzaNCQNATuOsU7Dq3g/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>你很快会发现尽管生成的句子与参考句极为相似（只有basketball一词不一样），但我们仍认为这是一个糟糕的生成。原因在于考虑了语义的情况下，模型把网球场错误的识别成了篮球场。这个时候BLEU或者其他指标就不能很好的评价生成效果了。</p><p>SPICE 使用基于图的语义表示来编码 caption 中的 objects, attributes 和 relationships。它先将待评价 caption 和参考 captions 用 Probabilistic Context-Free Grammar (PCFG) dependency parser parse 成 syntactic dependencies trees，然后用基于规则的方法把 dependency tree 映射成 scene graphs。最后计算待评价的 caption 中 objects, attributes 和 relationships 的 F-score 值。</p><p>还是已上图为例，a young girl standing on top of a tennis court (参考句) 可以被SPICE做如下处理：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgRYbeW3NVtIAQDicktJATMl9o5KkyImngjsiaIdnX8SdvwFEibsxA59UyQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">得到了若干个三元组之后，我们通过下面的公式来计算候选句c和参考句（或集合）S的得分：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh01gh6umpj313d0fx408.jpg" style="zoom:33%;"></p><p>这里有一个例子：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgwIdbmLATA8Zk5uKHZDAAGkyJlf0J0ZDHPicGUibNgAlUCdrPlT82javA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><h4 id="1-好处-1"><a href="#1-好处-1" class="headerlink" title="1. 好处"></a>1. 好处</h4><ul><li>对目标，属性，关系有更多的考虑；</li><li>和基于n-gram的评价模式相比，有更高的和人类评价的相关性</li></ul><h4 id="2-缺点"><a href="#2-缺点" class="headerlink" title="2. 缺点"></a>2. 缺点</h4><ul><li>不考虑语法问题</li><li>依赖于semantic parsers ， 但是他不总是对的</li><li>每个目标，属性，关系的权重都是一样的（一幅画的物体显然有主次之分）</li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
          <category> 评价指标 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 评价指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Describing like Humans on Diversity in Image Captioning</title>
      <link href="2020/07/17/Describing-like-Humans-on-Diversity-in-Image-Captioning/"/>
      <url>2020/07/17/Describing-like-Humans-on-Diversity-in-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>当前的automatic metric 仅仅评估 generated caption 与 human annotations 之间的相似性。</p><p>但是，一张图片中包含很多内容和细节，不同的人对 image content 会有不同的兴趣点，则 human captions 也会不同。</p><p>基于此，仅仅去评估accuracy 是不足以评估captioning models的性能的。生成的captions 的多样性也应该考虑进来。</p><p>本文，针对 多样性-diversity 提出了一个评价指标。</p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>即，该文要评价的系统 是可以生成多个caption的系统。是来评估，生成的这几个captions 之间的多样性是怎样的。</li><li>本文提到一句话：The drawback of using retrieval model is that the fluency of the captions could be poor [20], and using a very large weight for the retrieval reward will cause the model to repeat the distinctive words.</li></ul><h3 id="以下转自知乎"><a href="#以下转自知乎" class="headerlink" title="以下转自知乎"></a>以下转自知乎</h3><p><a href="https://zhuanlan.zhihu.com/p/67904095" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/67904095</a></p><p>这周读了CVPR 2019一篇有关测试Image Captioning多样性的文章，<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1903.12020" target="_blank" rel="noopener">Describing like humans: on diversity in image captioning</a>，其主要的亮点是用Latent semantic analysis (LSA)方法来定量地衡量多样性。</p><h3 id="Latent-semantic-analysis-LSA"><a href="#Latent-semantic-analysis-LSA" class="headerlink" title="Latent semantic analysis (LSA)"></a>Latent semantic analysis (LSA)</h3><p>1990年的论文<a href="https://link.zhihu.com/?target=http%3A//lsa.colorado.edu/papers/JASIS.lsi.90.pdf" target="_blank" rel="noopener">Indexing by latent semantic analysis</a>最早在信息检索领域提出了LSA，其原理比较简单，核心思想是利用SVD来提取文本的潜在语义信息。</p><p>在Image Captioning这个特定的应用场景中，对于一张图片，假设有 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]"> 条与之相关的caption，且词汇表的总数为 <img src="https://www.zhihu.com/equation?tex=d" alt="[公式]"> ，则这组caption可以用一个 <img src="https://www.zhihu.com/equation?tex=d+%5Ctimes+m" alt="[公式]"> 的矩阵 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 表示，其中的每个列向量是一条caption的bag-of-words表示。 对 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 应用SVD进行分解，得到： <img src="https://www.zhihu.com/equation?tex=M+%3D+USV%5E%5Ctop" alt="[公式]"> 。 参考Quora上<a href="https://link.zhihu.com/?target=https%3A//www.quora.com/What-is-an-intuitive-explanation-of-singular-value-decomposition-SVD%23" target="_blank" rel="noopener">对SVD的讨论</a>，我们可以认为，SVD的最大作用是找到数据的“pattern”：</p><ul><li><img src="https://www.zhihu.com/equation?tex=U" alt="[公式]"> 的每一个列向量代表一种pattern，在这里可以引申为一个topic；</li><li>singular values <img src="https://www.zhihu.com/equation?tex=S%3Ddiag%28%5Csigma_1%2C...%2C%5Csigma_m%29" alt="[公式]"> （ <img src="https://www.zhihu.com/equation?tex=%5Csigma_1%3E%5Csigma_2%3E...%3E0" alt="[公式]"> ）则代表这些pattern对原数据的影响程度。当 <img src="https://www.zhihu.com/equation?tex=%5Csigma_1" alt="[公式]"> 明显大于其他所有 <img src="https://www.zhihu.com/equation?tex=%5Csigma_i" alt="[公式]"> 时，表示原数据仅仅受一种pattern“支配”，体现了较差的多样性；而当所有 <img src="https://www.zhihu.com/equation?tex=%5Csigma_i" alt="[公式]"> 都趋向于一致时，表示原数据由 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]"> 种pattern共同组成，说明原来的 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]"> 个caption彼此之间差异较大，体现了丰富的多样性。</li></ul><p>因此，文章认为，一组caption的多样性可以用 <img src="https://www.zhihu.com/equation?tex=r+%3D+%5Cfrac+%7B%5Csigma_1%7D+%7B%5Csum%5Em_%7Bi%3D1%7D%5Csigma_i%7D" alt="[公式]"> 度量。结合上面的分析， <img src="https://www.zhihu.com/equation?tex=r" alt="[公式]"> 的值域是 <img src="https://www.zhihu.com/equation?tex=%5B1%2Fm%2C+1%5D" alt="[公式]"> ，为了把多样性映射到 <img src="https://www.zhihu.com/equation?tex=%5B0%2C+1%5D" alt="[公式]"> ，文章对 <img src="https://www.zhihu.com/equation?tex=r" alt="[公式]"> 进行了对数变换，得到最终的多样性分数 <img src="https://www.zhihu.com/equation?tex=div+%3D+-%5Clog_m%28r%29" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=div" alt="[公式]"> 越大表示多样性越强。</p><h3 id="结合CIDEr的核方法"><a href="#结合CIDEr的核方法" class="headerlink" title="结合CIDEr的核方法"></a>结合CIDEr的核方法</h3><p>根据SVD的定义，得到的singular value等于矩阵 <img src="https://www.zhihu.com/equation?tex=K+%3D+M%5E%5Ctop+M" alt="[公式]"> 的eigenvalue的平方根。可以把 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 看作一个核矩阵，其中的元素 <img src="https://www.zhihu.com/equation?tex=K_%7Bij%7D" alt="[公式]"> 表示caption <img src="https://www.zhihu.com/equation?tex=i" alt="[公式]"> 和caption <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 的相似度，因此，完全可以应用核方法（Kernelized Method），直接构造出一个 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> ，然后通过 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 的eigenvalue计算出 <img src="https://www.zhihu.com/equation?tex=div" alt="[公式]"> 。 CIDEr是一个度量两个句子相似度的指标，主要从n-gram和TF-IDF的角度来进行度量，比起上述基于bag-of-words的方法，它显得更加合理。所以文章把CIDEr作为核函数，即 <img src="https://www.zhihu.com/equation?tex=K_%7Bij%7D+%3D+CIDEr%28c_i%2C+c_j%29" alt="[公式]"> ，从而得到另一种多样性分数，称为Self-CIDEr。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>文章对多个Image Captioning模型进行了Accuracy和Diversity的测试，分别用常规的CIDEr和上述的Self-CIDEr度量。实验结果如下，其中的红色五角星代表人类标记员的表现，可以看到，人类的标记结果很好地兼顾了Accuracy和Diversity，与实际的情况相符。</p><p><img src="https://pic2.zhimg.com/80/v2-27a4cd22fa5acf55682d4b77d742311e_1440w.jpg" alt="img"></p><p>但这还不足以充分证明文章这种度量方法的合理性，于是文章计算了自动化多样性指标与人工标注的多样性分数之间的相关度，结果如下：</p><p><img src="https://pic4.zhimg.com/80/v2-8546e7cad02e0de2e3a5448e109049f0_1440w.jpg" alt="img"></p><p>其中的mBLEU-mix计算了一组caption内部的平均相似度，被一些研究者用来简单地度量多样性，但从这个实验结果可以看到，mBLEU-mix与人工标注的相关度远不如该文章提出的基于LSA的方法，证明了后者的合理性。同时，应用了核方法的Self-CIDEr又优于朴素的LSA。</p>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
          <category> 评价指标 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 评价指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>完整性-pair</title>
      <link href="2020/07/11/%E5%AE%8C%E6%95%B4%E6%80%A7-pair/"/>
      <url>2020/07/11/%E5%AE%8C%E6%95%B4%E6%80%A7-pair/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>完整性分析</title>
      <link href="2020/07/03/%E5%AE%8C%E6%95%B4%E6%80%A7%E5%88%86%E6%9E%90/"/>
      <url>2020/07/03/%E5%AE%8C%E6%95%B4%E6%80%A7%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>Class: flying kite Video: 8iPflOxQaao_000018_000028</p><iframe src="//player.bilibili.com/player.html?bvid=BV1Xf4y1y7AP&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] An announcer is talking while a kid is showing how to wind a power flyer and let it go into the air.<br>[2] A boy winds a powered kite engine then launches the kite, which flies then is displayed on a table.<br>[3] A boy is outside winding up a toy plane,then throws it in the air to fly.<br>[4] An announcer is talking while a kid is showing how to wind a power flyer and let it go into the air.<br>[5] A boy winds a powered kite engine then launches the kite, which flies then is displayed on a table.<br><strong>Prediction:</strong><br>[baseline]: a young boy in a red shirt is holding a baseball bat .<br>[graph_kl]: a little girl is standing in a field and is holding a kite .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A boy is jumping on a trampoline and doing some flips.<br>A boy is jumping on a trampoline, doing various jumps and flips.<br>A boy is jumping on a trampoline and does a flip.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A little boy is flying a kite and some person is helping him to fly<br>A woman is trying to help a young boy fly a kite.<br>A little boy is flying a kite and his mother helps him.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>Someone is flying a kite up in the sky and trying to reel it in.<br>A sky with clouds and a break of sun fades to black, before a woman riding a kite sail is on the water.</p><hr><p>Class: carving ice Video: UArdunmwEdA_000049_000059</p><iframe src="//player.bilibili.com/player.html?bvid=BV1mt4y1X7fE&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A man chiseling and picking at a large block of ice.<br>[2] A man shaves ice off a large block and talks to a worker.<br>[3] A man shaves ice off a large block and talks to a worker.<br>[4] one person carving an ice block with a tool and another person holding a ice carving machine<br>[5] An ice sculptor shows how he begins a typical project.<br><strong>Prediction:</strong><br>[baseline]: a man is using a chainsaw to carve a block of ice .<br>[graph_kl]: a man is using a tool to carve a block of ice .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A man makes an ice sculpture using a carving saw.<br>A man uses a tool to carve an ice sculpture while a class of people watch him.<br>A man showing his skills in an outdoor ice carving competition.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A person is carving an ice sculpture from a block of ice with a chainsaw.<br>A man makes an ice sculpture using a carving saw.<br>A man uses a tool to carve an ice sculpture while a class of people watch him.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A person is creating an ice sculpture as viewers take it all in.<br>A person is making an ice sculpture and it appears to be a seahorse.</p><hr><p>Class: tobogganing Video: vTSO26j_g3E_000006_000016</p><iframe src="//player.bilibili.com/player.html?aid=286089586&bvid=BV1Rf4y1y7nN&cid=203899418&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A man and a baby sled down a snow covered hill in the winter.<br>[2] A man and a small child ride a sled down a snowy hill.<br>[3] A man and a small child ride a sled down a snowy hill.<br>[4] An adult is sliding down a snowy hill on a sunny day.<br>[5] A man and a child are sledding down a hill.<br><strong>Prediction:</strong><br>[baseline]: a person is sledding down a snowy hill on a sled .<br>[graph_kl]: a person is riding a sled down a snowy hill .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A little boy gets a on a sled and goes down a snowy hill.<br>An adult and a child share a sled ride down a snowy hill.<br>A man and a child are sliding down a mountain on a sled with other people around them.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A woman and child are sledding down a snowy hill together.<br>An adult and child are together sledding down a snowy slope.<br>An adult and a child share a sled ride down a snowy hill.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A woman and a young child are sliding on a sled in the snow.<br>A little boy gets a on a sled and goes down a snowy hill.</p><hr><p>Class: jogging Video: gf37sAjEfRc_000013_000023</p><iframe src="//player.bilibili.com/player.html?bvid=BV1Bz4y1Q7Ai&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] Two boys are running in slow motion towards playground equipment.<br>[2] a group of people running together in a park as they pass by each other having fun<br>[3] Two young men are racing a course outside and going in slow motion as they pass a woman<br>[4] Two young men are racing a course outside and going in slow motion as they pass a woman<br>[5] A woman high fiving people who are  running past her in the park.<br><strong>Prediction:</strong><br>[baseline]: a group of people are standing in a field and one of them is holding a ball .<br>[graph_kl]: a man is teaching a young boy how to do a karate kick .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>Two young boys are chasing each other playfully around a park<br>A young boy runs down the street in order to catch a ball.<br>Two boys playing around in a park and running around.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A group of people are playing basketball on an outdoor court.<br>A group of boys are playing a game of soccer outside.<br>Two young girls are dancing to music at a restaurant.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A group of adults and children a grouped together outside as a small group is lowering a object into the ground.<br>A bunch of kids outside playing a game of socceer together on the field</p><hr><p>Class: throwing ball (not baseball or American football) Video: GXO1eYu4kr8_000000_000010</p><iframe src="//player.bilibili.com/player.html?bvid=BV1JT4y1J7R6&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] a man sitting on a gym floor throwing a black ball<br>[2] A man seated is throwing a ball to a man standing indoors.<br>[3] A man sitting on a gym floor with legs spread passes a ball to another man who is standing.<br>[4] A man sitting on a gymnasium floor throws a large ball to a standing man.<br>[5] A man, seated on the floor with his back against the wall and legs spread apart, throws a ball to a standing man in a gymnasium.<br><strong>Prediction:</strong><br>[baseline]: a young boy is practicing his basketball skills in a gym .<br>[graph_kl]: a man is standing in the middle of a gym and throws a ball .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>Grown men in the gymnasium play dodgeball, one blocks a ball with another ball that his friend catches and then two men get hit with balls.<br>Men are in a gymnasium playing dodge ball with their hands<br>A group of men throwing several balls to children indoors.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A man throws a football to someone in an indoor gym<br>A young man is kicking a soccer ball to somebody else on the gymnasium floor.<br>A man in a gym steps back and throws a football.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>a person in a gym by themselves throwing a football to the other side of the gym<br>A man in a gym kicks a white ball back and forth.</p><hr><p>Class: sipping cup Video: saXahlRV7s4_000022_000032</p><iframe src="//player.bilibili.com/player.html?aid=286068284&bvid=BV1df4y1y7oD&cid=203899246&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A little boy holding a sippy cup of juice smiling and giggling.<br>[2] A little boy is sucking juice out of a cup while a man is talking to him.<br>[3] A baby drinks juice from a sippy cup and smiles while a person hold the bottom of the cup them pokes his belly.<br>[4] A baby is sipping a juice in a bottle and a man speaks to him<br>[5] A little boy holding a sippy cup of juice smiling and giggling.<br><strong>Prediction:</strong><br>[baseline]: a baby is drinking from a sippy cup while a woman talks to him .<br>[graph_kl]: a baby is sitting on a couch and drinking from a sippy cup .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A baby sips liquid out of a sippy cup with great gusto.<br>A little boy holding a sippy-cup begins to mumble and then takes a sip from the cup.<br>A young child drinks out of straw in a sippy type cup.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A toddler drinks from a sippy cup while talking to a woman.<br>A man holds a baby while she drinks from a sippy cup.<br>A man holds a young child while they drink from a sippy cup.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A baby is holding onto a pink sippy cup with two hands and drinking from it.<br>A toddler drinks from a sippy cup while talking to a woman.</p><hr><p>Class: welding Video: Hgo7xkutPno_000059_000069</p><iframe src="//player.bilibili.com/player.html?bvid=BV1Ez411e7QD&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] The man used a welding tool while covering his face with a protective mask.<br>[2] A man is welding metal together while wearing protective gear.<br>[3] Someone is welding while there is rock music playing in the background.<br>[4] A person with a mask works welding under an overhead light.<br>[5] A person wearing a welders helmet, using a welding torch on an object.<br><strong>Prediction:</strong><br>[baseline]: a group of people are welding a piece of metal .<br>[graph_kl]: a person is using a torch to blow a piece of wood .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>a person using a tool to do some welding wearing protective gear<br>A man in proper protective gear is welding metal together.<br>A man in a mask welding pieces of metal together.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A man with a welding mask on is speaking while welding an object.<br>The man is wearing a face mask while welding pipes together.<br>A man in proper protective gear is welding metal together.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A person welds metal creating sparks and bright flashes of light.<br>Someone uses a welding tool to weld something together, it creates a very bright light.</p><hr><p>Class: cutting pineapple Video: NRC5oMoNHn0_000003_000013</p><iframe src="//player.bilibili.com/player.html?bvid=BV12t4y1X7tM&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A man is teaching how to cut a pineapple by cutting off top and bottom.<br>[2] In a kitchen, a chef instructs and demonstrates how to cut a pineapple.<br>[3] A chef chops a pineapple on the counter; removing the top first and then the rest.<br>[4] A man is teaching how to cut a pineapple by cutting off top and bottom.<br>[5] In a kitchen, a chef instructs and demonstrates how to cut a pineapple.<br><strong>Prediction:</strong><br>[baseline]: a man is demonstrating how to cut a pineapple with a knife .<br>[graph_kl]: a man is demonstrating how to cut a pineapple with a knife .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A person is demonstrating how to cut up a pineapple in a kitchen.<br>A person shows how to cut a pineapple in order to get to the core of the fruit.<br>A man is cutting a pineapple with a small knife.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A person is demonstrating how to cut up a pineapple in a kitchen.<br>A woman is demonstrating how to cut an orange with a knife.<br>A man demonstrates how to cut into an orange using a knife.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A man is cutting a pineapple with a small knife.<br>A person uses a knife to demonstrate how to cut the ends and skin off of a pineapple.</p><hr><p>Class: making paper aeroplanes Video: IbDcOoO9m6g_000139_000149</p><iframe src="//player.bilibili.com/player.html?bvid=BV1ni4y1x7xP&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A kid is demonstrating some origami and is folding the paper into a triangle.<br>[2] A person folds a piece of paper over a decorative dark board.<br>[3] A person shows how to fold the napkin to decorate the hotel tables<br>[4] A person shows how to fold the napkin to decorate the hotel tables<br>[5] A person shows how to fold the napkin to decorate the hotel tables<br><strong>Prediction:</strong><br>[baseline]: a person is folding a piece of paper into a triangle .<br>[graph_kl]: a person is demonstrating how to fold a piece of paper .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A person shows how to fold a piece of paper folding in the different corners and then the top.<br>A young boy is showing a technique to fold a piece of paper into a paper airplane.<br>A little boy demonstrates how to fold a paper plane using paper.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A man demonstrates folding a piece of green paper into origami.<br>A young boy is showing a technique to fold a piece of paper into a paper airplane.<br>Someone is demonstrating how to fold a piece of paper to make an airplane.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A woman folds a napkin in half to form a triangle, then folds the points towards the center.<br>a woman is showing how to fold a napkin in order to create something</p><hr><p>Class: trimming shrubs Video: tGiBJQ5RhZQ_000077_000087</p><iframe src="//player.bilibili.com/player.html?bvid=BV1cV411k7yR&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A man is standing and using clippers to trim a bush.<br>[2] A man is standing and using clippers to trim a bush.<br>[3] A man uses a gardening tool to cut shrubbery as he details the process.<br>[4] A man cutting plants it lengthy branches and give instruction to other.<br>[5] a gardener is showing how to trim parts of a hedge<br><strong>Prediction:</strong><br>[baseline]: a man is using a chainsaw to trim a tree .<br>[graph_kl]: a man is using a chainsaw to cut a piece of wood .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A man with garden shears describing how to trim bushes<br>A man using pruning shears is explaining and demonstrating how to trim an evergreen tree.<br>A guy stands outside as he snips away at the branches in a bush.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A man using pruning shears is explaining and demonstrating how to trim an evergreen tree.<br>A man with garden shears describing how to trim bushes<br>A guy stands outside as he snips away at the branches in a bush.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A guy stands outside as he snips away at the branches in a bush.<br>A man in an orange shirt trims trees while standing high up in the branches.</p><hr><p>Class: using circular saw Video: _YWpv2_K8Pk_000054_000064</p><iframe src="//player.bilibili.com/player.html?aid=456121030&bvid=BV1D5411W7zB&cid=203800090&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] Someone is cutting down wood on a saw carefully with another piece pushing it.<br>[2] A man is cutting a block of wood with a table saw and pushing it along with another piece of wood.<br>[3] A person is feeding a piece of wood into a table or bench saw to cut it.<br>[4] A person cutting a wooden board on a flat table saw<br>[5] A guy is running a wood board through a saw a few times.<br><strong>Prediction:</strong><br>[baseline]: a man is using a tool to cut a piece of wood .<br>[graph_kl]: a man is using a tool to cut a piece of wood .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>a man is cutting construction wood using a table saw<br>A man had a wood on his lap while carving it with a knife<br>A person is using a skill saw to cut through a piece of wood.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>The man uses a circular saw to cut a piece of lumber.<br>A person with a circular saw is cutting a piece of wood.<br>A person uses an electric saw to cut through a piece of wood.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A person is rolling dough on a table with a roller.<br>a man measures a long piece of wood next to a machine</p><hr><p>Class: walking the dog Video: KvVRY61JS7A_000023_000033</p><iframe src="//player.bilibili.com/player.html?aid=841065180&bvid=BV1U54y1B7ga&cid=204027776&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] Two people are walking two dogs down their street together.<br>[2] A man and a boy walking their 2 dogs on the street but the boy fell down and cried.<br>[3] A man and a toddler are walking two dogs down the street.<br>[4] Two people are walking two dogs down their street together.<br>[5] A baby bumps into a dog and cries while walking down road.<br><strong>Prediction:</strong><br>[baseline]: a woman is walking on a sidewalk with a dog on her back .<br>[graph_kl]: a person is walking down the street and a man is walking .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A boy is walking a dog on a leash on a street.<br>A little boy is walking his tiny dog on the streets.<br>A boy is skateboarding and walking his dog at the same time.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A woman and a child are walking down the sidewalk.<br>A woman and her young child are walking down the sidewalk.<br>A little boy is walking his tiny dog on the streets.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A person uses a power washer to remove paint from an asphalt surface.<br>A kid in a white shirt hits his head against a wall and falls to the ground.</p><hr><p>Class: blowing leaves Video: VNXpHy5Tb_U_000064_000074</p><iframe src="//player.bilibili.com/player.html?bvid=BV1if4y1y71S&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A little boy runs a leaf blower for a short minute then cuts it off.<br>[2] A small boy is blowing the leaves with a leaf blower and the blower stops.<br>[3] A boy using an electric leaf blower on debris outdoors suddenly stops working and walks away.<br>[4] a person in front of different leaves trying to blow them away with a  machine<br>[5] A young boy using a leaf blower moves leaves in a yard then shuts off the leaf blower and talks to someone else.<br><strong>Prediction:</strong><br>[baseline]: a man is using a chainsaw to cut a pile of wood .<br>[graph_kl]: a man is using a leaf blower to blow leaves in a yard .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A man is blowing off leaves using a leaf blower off of the pavement<br>Man rides lawn mower with leaf-blower attachment over cleared grass and blows fallen leaves under trees.<br>An individual uses a leaf blower to get rid of several leaves.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A man is blowing off leaves using a leaf blower off of the pavement<br>An individual uses a leaf blower to get rid of several leaves.<br>A person uses a leaf blower to blow leaves from a house gutter.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A man picked up a stick with a mound of mud at the base<br>A boy is stick in a muddy ditch trying to get out of it.</p><hr><p>Class: ice swimming Video: dC2Ih_JFoOM_000000_000010</p><iframe src="//player.bilibili.com/player.html?bvid=BV1cD4y1Q7BX&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] Two men in underwear step into a pond and one of the men dips under the water very quickly and immediately heads to get out of the pond.<br>[2] Two men wade into the water and one dunks under the water.<br>[3] Two men wade into the water and one dunks under the water.<br>[4] Two men wade into the water and one dunks under the water.<br>[5] two men are walking into the water in a cold day<br><strong>Prediction:</strong><br>[baseline]: a group of people are in a pool and one of them jumps into the water .<br>[graph_kl]: a group of people are in a pool and one of them jumps into the water .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A man wearing swimming trunks goes for a swim in freezing cold water.<br>A man in orange swim trunks and a white beanie walked into a pool of cold winter water.<br>A man was swimming in icy water and then came out<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A man walked into a cold pool of water and walked back out while talking to another person.<br>A man is walking into a wide body of water.<br>A man is wading into a body of lake water.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>Two men are wading in water and one tries to catch something with a strange tool.<br>A man is wading into a body of lake water.</p><hr><p>Class: opening door Video: lcUJjVgqXp4_000000_000010</p><iframe src="//player.bilibili.com/player.html?bvid=BV19V411r7FF&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A person is showing how to unlock a locked door using a folded playing card.<br>[2] A person dhowing to take a playing card and unlock a door to get it.<br>[3] A man used a folded card to open the lock of a door<br>[4] A person dhowing to take a playing card and unlock a door to get it.<br>[5] A man is showing how to open a door using a folded playing card.<br><strong>Prediction:</strong><br>[baseline]: a person is using a machine to open a door .<br>[graph_kl]: a person opens a door and opens the door and opens the door .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A man is demonstrating how to pick a lock on a door.<br>A very close up angle shows a guy picking the lock to a door.<br>A person demonstrates using lockpicks to open a standard lock.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A man is demonstrating how to pick a lock on a door.<br>A boy is opening a door lock using a pin.<br>A man attempts to unlock a door using some simple tools.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A man in a mask power cleans a portable machine.<br>A man is withdrawing some money and a receipt out of an ATM machine</p><hr><p>Class: scrapbooking Video: uh50grtCQSA_000026_000036</p><iframe src="//player.bilibili.com/player.html?bvid=BV1ma4y1Y7HQ&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] a person writes something on a colorful canvass that they have in front of them<br>[2] A little girl is explaining her notebook and how she likes it as she flips the pages.<br>[3] A young girl writes a note on her scrapbook, then flips through it.<br>[4] A girl is showing the only thing she has done in her scrapbook.<br>[5] A child is showing and describing what she did with her scrapbook.<br><strong>Prediction:</strong><br>[baseline]: a person is folding a piece of paper on a table .<br>[graph_kl]: a person is showing how to fold a piece of paper .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A girl colors in a hand-drawn map of the countries of the world.<br>A woman quietly looks through the pages of a homemade scrapbook.<br>A person is showing designs on different pages in a scrapbook.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A woman quietly looks through the pages of a homemade scrapbook.<br>A person is showing designs on different pages in a scrapbook.<br>A person is showing how they draw in a sketch book.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A person folds a cloth into a triangle then folds the corner in again.<br>A person is using cardboard to properly fold a shirt.</p><hr><p>Class: waxing chest Video: cb3RvnukQVs_000159_000169</p><iframe src="//player.bilibili.com/player.html?bvid=BV1vk4y1z7GR&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A guy laying flat on his back as a woman attempts to wax his chest and stomach.<br>[2] A woman looks at a man’s bare chest and removes excess wax with a paper towel.<br>[3] A man is laying down on his back shirtless while a woman uses wax strips to help remove the hair.<br>[4] A woman cleans up hair of a man’s belly after he was waxed.<br>[5] A woman cleans up hair of a man’s belly after he was waxed.<br><strong>Prediction:</strong><br>[baseline]: a man is getting his chest waxed by a woman .<br>[graph_kl]: a man is laying on his stomach while another person is waxing his chest .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A man lays face down as a woman waxes his back.<br>A woman is waxing a man’s chest hair.<br>A woman applying hair removal wax strips to a man’s chest<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A woman is waxing a man’s chest hair.<br>A man is laying on the table getting his chest hair waxed.<br>A man is having his back waxed by a woman<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A person is removing strips of wax from another persons back to remove the hair.<br>A man covered in wax and a person pulling the wax off of his back.</p><hr><p>Class: diving cliff Video: 5D4HjS92zSQ_000126_000136</p><iframe src="//player.bilibili.com/player.html?bvid=BV17i4y1G7rb&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A boy does a backflip off a precipice into a body of water<br>[2] A person does a backflip off a cliff into water while others are already in the water.<br>[3] On a high cliff boys jump into a water quarry.<br>[4] A man does a back-flip off of a cliff, while his friend watches from the water below.<br>[5] A person is doing a flip off of a cliff into a lake.<br><strong>Prediction:</strong><br>[baseline]: a man jumps off of a cliff into a body of water .<br>[graph_kl]: a man jumps off a cliff into a body of water .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A man does various flips while jumping off of a cliff into the water.<br>A man does a front flip off of a cliff into a body of water, then a backflip.<br>One of the men from the top of the cliff jumped off and landed in the water<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A man does various flips while jumping off of a cliff into the water.<br>A man does a front flip off of a cliff into a body of water, then a backflip.<br>A boy jumps off a cliff into a pool of water.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A person jumps off a cliff into a large body of water.<br>A boy jumps from a cliff into some water and another boy is climbing out of the water</p><hr><p>Class: giving or receiving award Video: kRdxCmOsY2Y_000003_000013</p><iframe src="//player.bilibili.com/player.html?bvid=BV15A411i7Cg&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A man stands on a stage in front of an audience as he hands an award to another man walking on the stage.<br>[2] Four people are called up onto a stage, and one accepts an award.<br>[3] A man is presenting a prize to a group of four.<br>[4] A man stands on a stage in front of an audience as he hands an award to another man walking on the stage.<br>[5] A man presents a group of young adults with an award.<br><strong>Prediction:</strong><br>[baseline]: a man is standing in front of a crowd and singing a song .<br>[graph_kl]: a group of people are on a stage in front of an audience .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>An older gentleman is standing at a podium accepting an award when another gentleman walks across the stage and shakes his hand.<br>People are cheering as a man is welcomed on stage.<br>A man is on a stage introducing himself to an audience.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>An older gentleman is standing at a podium accepting an award when another gentleman walks across the stage and shakes his hand.<br>A man is standing on a stage and speaking to a crowd.<br>A man is on a stage introducing himself to an audience.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>Three people are on stage performing a play and one of them is singing.<br>Two men and a woman are performing a play on a stage.</p><hr><p>Class: fly tying Video: _PdB7OGolCo_000191_000201</p><iframe src="//player.bilibili.com/player.html?aid=838500851&bvid=BV1Eg4y1q7fp&cid=203798508&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A spool of thread is wrapped around a small hook several times.<br>[2] A man uses an object to wrap string around another object.<br>[3] A person ties very fine string around a small hook.<br>[4] A spool of thread is wrapped around a small hook several times.<br>[5] Someone shows how to wrap a fish hook with thread using a tool to hold it.<br><strong>Prediction:</strong><br>[baseline]: a man is demonstrating how to make a fly fishing lure .<br>[graph_kl]: a man is demonstrating how to make a fishing lure .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>a person using their fingers to wrap string around a hook<br>A person holds a piece of string with a loop at the end, they place the loop over a hook and tighten the loop.<br>A man is carefully wrapping the hook with some thread.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A man is demonstrating how to wrap a hook with string.<br>A person is demonstrating how to tie a fishing string to a hook.<br>A man is demonstrating how to secure wire to a hook.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A man describes how to create snags using a line and a hook.<br>A person holds a hook and connects material to create a fly hook.</p><hr><p>Class: bottling Video: 38Jn4r_pcpg_000167_000177</p><iframe src="//player.bilibili.com/player.html?bvid=BV1Xt4y1X7L1&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A man is making beer and pours some of his beer into another container to even them out.<br>[2] A man is in a kitchen and pulls out bottles from a pot that is sitting on the counter.<br>[3] A man is in a kitchen and pulls out bottles from a pot that is sitting on the counter.<br>[4] A guy has two bottles in his hands and is pouring liquid from one bottle to the other.<br>[5] A man picks up to amber beer type bottles and pours fluid from one into the other.<br><strong>Prediction:</strong><br>[baseline]: a man is demonstrating how to make a recipe in a kitchen .<br>[graph_kl]: a man is demonstrating how to use a machine to make a meal .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>a man is pouring a bottle of beer into a large glass.<br>A man pours a bottle of beer into a large beer glass.<br>A man standing in a kitchen holds a bottle and a cup then pours whats in the bottle into the cup.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>a man is pouring a bottle of beer into a large glass.<br>A man pours a bottle of beer into a large beer glass.<br>A man picks up a brewing pot and pours water into a cup.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A man picks up a brewing pot and pours water into a cup.<br>A male is in the kitchen showing how he pours water out quickly out the pot.</p><hr><p>Class: dancing gangnam style Video: 53Ia-BtpM5A_000002_000012</p><iframe src="//player.bilibili.com/player.html?aid=841116130&bvid=BV1X54y1B7pu&cid=203898322&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A toddler girl dances across the floor while listening to music.<br>[2] A little girls is dancing to music next to her toy scooter.<br>[3] A toddler dressed in pink is dancing while music is playing.<br>[4] A toddler girl dances across the floor while listening to music.<br>[5] A little girl is dancing to the popular Korean song ‘Gangnam Style.’<br><strong>Prediction:</strong><br>[baseline]: a little girl is sitting on the floor and is dancing to music .<br>[graph_kl]: a little girl is sitting on the floor and dancing to music .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A little girl on a small table dances along to a video playing on tv.<br>A young girl is giving a tap dancing performance while music plays in the background.<br>A group of young people are shown while a girl is shown dancing to music.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>Two young girls are dancing to music at a restaurant.<br>a young boy is in his room listening to music and dancing<br>A young man dances on a kitchen floor as music plays.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>a small girl riding her small pink bike while a bell clinks<br>This little girl is dancing on her toes like a ballerina.</p><hr><p>Class: falling off bike Video: 7QyQVBclhT4_000000_000010</p><iframe src="//player.bilibili.com/player.html?bvid=BV18a4y1Y7Bn&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A kid rides a bike down a hill going too fast and has a wipe out.<br>[2] A young kid riding a bike across sand and crashing it<br>[3] Uncontrolled bicycle, careening down a desert hill, leaves boy with multiple sand abrasions.<br>[4] A kid rides a bike down a hill going too fast and has a wipe out.<br>[5] A boy is speeding down a sand hill on a bicycle which wobbles and throws him off.<br><strong>Prediction:</strong><br>[baseline]: a man is riding a bike on a beach and then falls over .<br>[graph_kl]: a man is riding a bike on a dirt road .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A guy on a bike goes down a steep hill and wrecks, tumbling down the hill.<br>A boy tips forward and falls on his face while trying to do a bicycle trick.<br>Slow motion action of young man jumping with bicycle doen a hillside and tumbling to the bottom.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A man rides his bike in the snow and wipes out.<br>A man on a bike goes down a hill then falls off from his bike.<br>A person rides a dirtbike up a steep hill then falls.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A person rides a bicycle down a hill and over a dirt berm<br>A man rode a dirt bike over a very steep dirt hill</p><hr><p>Class: long jump Video: QyMTEHd-VCc_000004_000014</p><iframe src="//player.bilibili.com/player.html?bvid=BV1v54y1B7X4&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A person is running and then performs a long jump into a sand box.<br>[2] A woman is practicing her long jumps while her friend records.<br>[3] A woman runs then does a high jump to create distance.<br>[4] A woman runs then does a high jump to create distance.<br>[5] A person is running and then performs a long jump into a sand box.<br><strong>Prediction:</strong><br>[baseline]: a man is running down a track and jumps into a pit of sand .<br>[graph_kl]: a man runs down a track and jumps into a sand pit .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A person running and then leaping to perform a long jump<br>An athlete runs down the track and performs a long jump.<br>People watch as a woman runs down a track, then jumps into the air for a long jump.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>An athlete runs down the track and performs a long jump.<br>A man is running and doing a long jump, while people are watching.<br>A young woman runs down an asphalt track then performs a long jump.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A competitor is taking a running start and then successfully performs a high jump.<br>A person is outside and they are jumping over a vault for some type of track event.</p><hr><p>Class: knitting Video: t-COcCPV-T4_000020_000030</p><iframe src="//player.bilibili.com/player.html?bvid=BV1ff4y1y7xc&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] Someone is showing how to crochet through some sort of material.<br>[2] A gir lis trying to stitching up the black and white knitting kit with a knitting needle<br>[3] A young girl shows how to crochet and encourages her own type of technique.<br>[4] A gir lis trying to stitching up the black and white knitting kit with a knitting needle<br>[5] Someone is showing how to crochet through some sort of material.<br><strong>Prediction:</strong><br>[baseline]: a person is demonstrating how to fix a bicycle .<br>[graph_kl]: a person is using a needle to sew a piece of fabric .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A woman holds needle and yarn and she demonstrates how to knit.<br>A woman demonstrates how to knit using needles and yarn.<br>A tutorial with a women demonstrating knitting techniques using yarn.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A woman is demonstrating how to work with a piece of fabric.<br>A woman demonstrates how to knit using needles and yarn.<br>A person is demonstrating how to weave a small basket.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A person is using cloth to design an image of a frog.<br>A person with glasses works on the tubing of a bicycle tire.</p><hr><p>Class: historical reenactment Video: oGpVHf4xooM_000160_000170</p><iframe src="//player.bilibili.com/player.html?bvid=BV1Bp4y1D7NJ&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] People are watching as men in uniforms are shooting each other.<br>[2] People are watching as men in uniforms are shooting each other.<br>[3] A group of people in uniforms fire guns while a crowd watches.<br>[4] A group of people watch a reenact-ion of a fight from the American revolution from behind a fence.<br>[5] People are moving across a green field and firing rifles, while spectators look on.<br><strong>Prediction:</strong><br>[baseline]: a large group of people are gathered around a large group of people .<br>[graph_kl]: a group of people are standing in a field and a man is talking .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A group of people are watching a war reenactment and walking around.<br>Civil war reenactors are shooting at each other as someone is recording them.<br>A group of people are in a field re-enacting a battle scene with guns.<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A group of people are watching a war reenactment and walking around.<br>A group of men in a field are doing an reenactment of a battle and firing guns.<br>Several people are in a field doing a war re-enactment.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A group of people are outside, they are dressed up and recreating the civil war.<br>People are sitting outside talking as some other people are standing up.</p><hr><p>Class: home roasting coffee Video: WU5KZsG_mQk_000483_000493</p><iframe src="//player.bilibili.com/player.html?bvid=BV1XC4y1a7Cf&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p><strong>Two reference sentences corresponding to this video:</strong><br>[1] A man is pouring hot coffee beans from a roaster onto a flat surface.<br>[2] A person illustrates and demonstrates how to brew several types of cocoa beans for coffee making.<br>[3] A man is pouring hot coffee beans from a roaster onto a flat surface.<br>[4] A person is removing a set of beans on a table that  are being prepped for dinner.<br>[5] A person illustrates and demonstrates how to brew several types of cocoa beans for coffee making.<br><strong>Prediction:</strong><br>[baseline]: a man is using a power tool to cut a piece of metal .<br>[graph_kl]: a person is using a machine to heat a piece of metal .<br><strong>[STS] Top 3 most similar sentences in all val corpus:</strong><br>A man using a roasting machine to roast coffee beans.<br>A person lifts up a switch which drops coffee beans into a machine that stirs them.<br>A man is stiring some coffee beans in a strainer with a wooden spoon<br><strong>[bertscore_idf] Top 3 most similar sentences in all val corpus:</strong><br>A man using a roasting machine to roast coffee beans.<br>The lady is doing a demonstration on the equipment to roast coffee beans.<br>A person is roasting coffee beans in a wok and then puts them in a cup.<br><strong>[video_text_retrieval] Top 3 most similar sentences in all val corpus:</strong><br>A man wearing protective headwear uses a soldering  iron to solder two piece of metal together.<br>A man has put coffee beans into a metal strainer and using a wooden spoon is grinding the beans so they will be ready to be used to make a special coffee.</p><hr>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>图像描述评价指标的论文总结</title>
      <link href="2020/07/02/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E7%9A%84%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/"/>
      <url>2020/07/02/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E7%9A%84%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h3 id="简单总结"><a href="#简单总结" class="headerlink" title="简单总结"></a>简单总结</h3><p><strong style="color:blue;">(AAAI 2020) Going Beneath the Surface Evaluating Image Captioning for Grammaticality</strong></p><ul><li><p>语法正确性</p></li><li><p>准确性</p><blockquote><p>we investigate is truthfulness, that is, whether a candidate caption is compatible with the content of the image it is supposed to describe.  </p></blockquote></li><li><p>多样性</p></li></ul><p><strong style="color:blue;">(ACL 2019) VIFIDEL Evaluating the visual fidelity of image descriptions</strong></p><ul><li><p>准确性</p><blockquote><p> We focus on one such criterion, visual fidelity. </p><p>This criterion aims to measure how faithful a description is with respect to what is depicted in the image (i.e. systems should be rewarded for describing elements depicted in the image and penalised for describing things that are not depicted). </p></blockquote></li></ul><p><strong style="color:blue;">(EMNLP2019) TIGEr Text-to-Image Grounding for Image Caption Evaluation</strong></p><ul><li>没有针对，就是使用 image conten 作为 GT，设计了一个评价指标</li></ul><p><strong style="color:blue;">(EMNLP-IJCNLP 2019) REO-Relevance, Extraness, Omission A Fine-grained Evaluation for Image Captioning</strong></p><ul><li><p>相关度  Relevance</p><blockquote><p>relevant information of a candidate caption with respect to the ground truth  </p></blockquote></li><li><p>多余  Extraness</p><blockquote><p>extra information of a  candidate caption beyond ground truth data  </p></blockquote></li><li><p>缺少  Omission</p><blockquote><p>missing information that a candidate fails to describe from an image and humangenerated reference captions.  </p></blockquote></li></ul><p><strong style="color:blue;">(ICLR 2020 workshop)Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models</strong></p><ul><li>多样性？？</li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
          <category> 评价指标 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Going Beneath the Surface: Evaluating Image Captioning for Grammaticality, Truthfulness and Diversity</title>
      <link href="2020/07/02/Going-Beneath-the-Surface-Evaluating-Image-Captioning-for-Grammaticality-Truthfulness-and-Diversity/"/>
      <url>2020/07/02/Going-Beneath-the-Surface-Evaluating-Image-Captioning-for-Grammaticality-Truthfulness-and-Diversity/</url>
      
        <content type="html"><![CDATA[<h3 id="yaya-总结"><a href="#yaya-总结" class="headerlink" title="yaya 总结"></a>yaya 总结</h3><ul><li>本文提出了一个 GTD ， Grammaticality, Truthfulness and Diversity 三个方面对captioning model 进行评价，为未来 captioning model metric 提供了一个探索的方向。</li><li>本文需要特别说明的是，（1）本文是在自己设计的一个diagnostic dataset 上进行的实验，并不是在真实的captioning dataset 上进行的实验。（2）本文并没有提出一个具体可用的评价指标，而是提出了一个类似于协议的东西，即，本文主要是探究，在未来的评价指标里，需要去评估captioning model 的哪些层面，来弥补当前评价指标的不足，与当前的评价指标做一个互为补充！（英文： GTD, consider it as an evaluation protocol covering necessary aspects of the multifaceted captioning task, rather than a specific metric.）</li><li>该文选用 diagnostic dataset的原因：  <ul><li>The primary motivation is to reduce complexity which is considered irrelevant to the evaluation focus, to enable better control over the data, and to provide more detailed insights into strengths and limitations of existing models.</li></ul></li><li>该文不提出一个具体的评价指标：<ul><li>这得抨击一下啊。。。这给未来的研究提供了一个方向。但是，，，，feel is bad</li></ul></li></ul><h3 id="yaya学到了"><a href="#yaya学到了" class="headerlink" title="yaya学到了"></a>yaya学到了</h3><h2 id="使用ERG进行语法分析，若能够获得语法分析，则该句子在语法上被认为是格式正确的。反之。。"><a href="#使用ERG进行语法分析，若能够获得语法分析，则该句子在语法上被认为是格式正确的。反之。。" class="headerlink" title="- 使用ERG进行语法分析，若能够获得语法分析，则该句子在语法上被认为是格式正确的。反之。。"></a>- 使用ERG进行语法分析，若能够获得语法分析，则该句子在语法上被认为是格式正确的。反之。。</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><ul><li><strong style="color:red;">【当前评价指标存在的问题】</strong>: 当前存在的评价指标，只去关注于a candidate caption and<br>a set of reference captions之间的相似性，但是不去检查 generated caption 与 当前视觉内容之间的相关性。</li><li>本文提出了一个为 image captioning task 设计的 evaluation framework. 目的是直接评估生成caption的语法性、真实性和多样性(GTD).</li><li>使用本文新提出的evaluation framework 为 image caption models 进行评估, 并在多个数据集上进行测试，来证明本文提出的评估框架的潜力。</li><li>本文用实验来证明，与 <strong>diagnostic dataset</strong> 结合使用，本文的GTD 评估框架可以提供对 caption model 的 能力以及限制(capabilities and limit)的进一步思考。并补充 standard evaluations 的不足。</li><li></li></ul><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li>BLEU，METEOR，ROUGLE，CIDEr基于 n_gram 的评价指标仅能够评价cadidate caption 与 reference caption之间的<strong>表面相似性</strong>，</li><li>SPICE，将 captions转化为 graph-based 语义表达，进而来评估候选与参考之间的<strong>语义相似性</strong>。</li></ul><p><strong style="color:red;">automatic metric 使用的依据 (reference captions 来作为参照的使用依据)</strong></p><ul><li>【1】The rationale behind these evaluation metrics is that human reference captions serve as an approximate target and comparing model outputs to this target is a proxy for how well<br>a system performs. Thus, a candidate caption is not directly evaluated with respect to image content, but compared to a set of human statements about that image.</li><li>这里存在一个假设条件，即假设认为 human reference captions 是一个近似的target. 将caption model 输出与该target 进行比较是评估该model 性能好坏的一个proxy。基于这种假设，一个candidate caption 可以不直接与image content进行比较评估，而是与人类对image的statements 进行比较。</li></ul><p><strong style="color:red;">reference captions based automatic metric 存在的缺陷</strong></p><ul><li>reference-based metrics</li><li>由于视觉场景存在多个objects 和 relations，进而存在多个具有多样性的合理描述。但是由于reference caption 数量的限制，未必能够对图片的完整内容进行充分的描述。因此用reference captions 来近似替代 image 的内容是令人怀疑的。</li><li>当前存在的评价指标对于真实世界的 captioning评估是非常有用的，这是不可置疑的。这些方法仅关注于近似的表面比较，限制了对captioning models 学习过程和最终行为的进一步分析 。</li></ul><p><strong style="color:red;">本文提出的解决办法</strong></p><ul><li>为了解决这个问题，本文提出了几个原则性的评估准则，来对image captioning model 的语法性，真实性，多样性进行评估。这几个准则对应着image captioning systems的必要需求：（1）输出是符合语法规则的（2）对于image, 输出的陈述是对的（3）输出是多样化的, 反映training captions 的可变性。</li><li>GTD 当前只能在人造的数据上进行实际的评估。本文设计了一系列的数据集来进行图像描述的评估。我们将这个诊断评估数据集称为ShapeWorldCE. </li><li>BLEU 和 SPICE 不能捕捉 true caption-image agreement in all scenarios, 但是 GTD 对于现有模型如何很好地应对各种视觉环境和语言构造，却能有一个细粒度的观测。</li></ul><h3 id="GTD-evaluation-framework"><a href="#GTD-evaluation-framework" class="headerlink" title="GTD evaluation framework"></a>GTD evaluation framework</h3><ul><li>将GVD视为涵盖多方面字幕任务的必要方面的评估协议, 而不是一个具体的评估指标。</li><li><p><strong style="color:red;">【Grammaticality】</strong></p><ul><li>在一般情况下，对语法的完全准确的评估本身是一项艰巨的任务，但在诸如我们的诊断语言数据这样的非常有限的情况下，变得更加可行。【即在真实场景下的数据集是无法实现的？！！！】</li><li>如果我们使用ERG获得语法分析，则该句子在语法上被认为是格式正确的。</li></ul></li><li><p><strong style="color:red;">【Truthfulness】</strong></p><ul><li>candidate caption 与 image content 之间是否相容。</li></ul></li><li><p>truthfulness, that is, whether a candidate caption is compatible with the content of the image it is supposed to describe.  </p></li><li><p><strong style="color:red;">【Diversity】  </strong></p><h3 id="一些实验结果的分析"><a href="#一些实验结果的分析" class="headerlink" title="一些实验结果的分析"></a>一些实验结果的分析</h3></li><li>越高的BLEU得分，并不意味着准确性的提高。 换句话说，与reference captions的重叠度高（high n_gram overlap），并不代表语义内容是正确的. 仅仅是由于reference caption的词表有限所致。<strong>具体的分析可以查看论文实验部分：Correlation between the BLEU/SPICE scores and the<br>ground truth.</strong></li><li>虽然CNN可以为视觉任务提供丰富的视觉表达，但是对于多模态任务，其是否可以满足<br>但是这种浓缩的视觉表达对于需要higher-level 场景理解，视觉推理的多模态任务是否足够充分，仍然是一个开放性问题。</li><li>生成句子的多样性受到训练数据是否多样性的影响</li><li>未来的工作：本文发现：the caption agreement 并不总是随着训练损失的下降而提高。<br>理想情况下，训练目标应该与模型最终被评估的方式保持一致。在未来，计划实现一个GTD-aware loss,进而研究如何把GTD信号融合到训练过程中。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
          <category> 评价指标 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning</title>
      <link href="2020/07/02/REO-Relevance-Extraness-Omission-A-Fine-grained-Evaluation-for-Image-Captioning/"/>
      <url>2020/07/02/REO-Relevance-Extraness-Omission-A-Fine-grained-Evaluation-for-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul><li>BLEU 和 CIDEr 是当前广泛使用的评价指标，来评估模型的整体性能。</li><li>但是这个<strong style="color:red;">得分往往不具备充分的信息量来说明被评估系统的specific errors.</strong></li><li>该文提出了一个细粒度的评估方法REO来自动的评估图像描述系统的性能。REO从三个层面来分析captions的质量：（1）与Ground Truth 的相关性（2）与ground truth 无关的额外性（3）在images 和 human references 中被忽略的元素。</li><li>在三个benchmark datasets上进行实验，本文提出的方法实现了与human judgments的高度一致性；与当前可用的评价指标相对比，该文提出的方法可以提供更加直观的评估结果。</li></ul><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><ul><li><p>当前研究方法存在的问题</p><ul><li>当前自动的评估图像描述系统仍然是一个挑战，尤其是在量化图像描述系统生成的error仍有难度。</li><li>当前存在的评价指标可以分为两类，（1）rule-based metrics, 基于确切的字符串匹配。（2）learning-based metrics, 通过学习来预测test-caption 是人类生成的可能性。</li><li>大体上，先前的工作评估一个文本生成系统时，主要关注的是，相对于 ground truth data, 生成的description是否充分。虽然这个方面在之前的评价指标中已经被强调，但是当前存在的评价指标存在的一个共同的限制是，由于只能为caption quality 提供一个合成的分数，因此缺乏对 description errors的一个可解释性。缺乏这种细粒度的分析，开发人员便不能得到他们开发系统的确切的description errors.</li></ul></li><li><p>本文的解决办法：</p><ul><li>因此本文提出了一个称为REO的评估方法，从三个层面来评估caption, (1) Relevance: 相对于ground truth, candidate caption 与其相关性。(2) Extraness: 与ground truth 无关的额外性（3）Omission: 相对于在image content和 human references, condidate caption 中被忽略的元素。</li><li>如果将 caption generation看做image imformation embedding 的编码过程，我们可以通过根据有关图像内容的解码信息的<strong>相关性</strong>以及<strong>丢失或多余</strong>信息的数量来衡量解码过程的有效性，从而评估图像字幕系统。</li><li><strong>同时使用image 和 reference captions 作为评估的ground truth information</strong>, 本文的方法建立在一个由 grounding model 定义的共享的image-text embedding space， 该grounding model在一个大的benchmark数据集上进行预训练。</li><li>基于向量相似性来计算candidate caption 与 ground truth 之间的相关性。通过应用向量正交映射，来等同在candidate caption中携带的多余的和丢失的信息。</li></ul></li><li><p>实验部分</p><ul><li>在三个数据集上测试我们的方法，实验结果证明我们提出的方法与现存的评价指标相比，our method 与人类的评估更加一致。</li><li><strong style="color:red;"><strong>在本文的研究中发现，相对于caption与given image的相关性，人类标注者对caption中多余或者丢失的信息更加关注。</strong></strong></li><li>相比于单独仅仅使用image 或者 references作为ground truth, 同时使用效果更佳。</li></ul></li></ul><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><ul><li><p>Feature Extraction</p><ul><li>C: candidate caption; R: reference caption; V:region feature</li><li>Based on the SCAN model, extract the context information from the caption words for each detected region.</li></ul></li><li><p>Metric Scores</p><ul><li>具体的查看论文</li><li><strong style="color:red;">Relevance</strong> : The relevance between a candidate caption and a ground-truth reference based on the i-th region is computed by the cosine similarity</li><li><strong style="color:red;">Extraness</strong> : The extraness of C is captured by performing an orthogonal projection of a<sub>i</sub><sup>C</sup> to gi, which returns the vertical context vector a<sub>i</sub><sup>C</sup>? to represent the irrelevant content of C to the ground truth at the ith region.</li><li><strong style="color:red;">Omission</strong>: 类似于Extraness</li></ul></li></ul><h4 id="Experiments-Results"><a href="#Experiments-Results" class="headerlink" title="Experiments Results"></a>Experiments Results</h4><ul><li>==【yaya疑问】==<ul><li>关于pascal-50S 这个数据集，没太弄明白是怎么回事，如何计算accuracy</li><li>Kendall’s tau (τ) rank correlation 如何计算也不知道呢。。</li></ul></li><li><p><strong style="color:red;">【Can extra &amp; missing information be captured?】</strong></p><ul><li>在Extraness 和 Omission 这两个方面，本文提出的方法是用candidate caption 与 reference caption vector 上应用 orthogonal projection来计算 vector difference， 进而来得到Extraness/Omission representation。</li><li>但是这是<strong>一种</strong>本文设计的表征方式，为了评估该表征方式的合理性，本文作者又随机的采样了一些数据，手工标注出这些数据中真实的extraness和 omission。这样相当于得到了对于 extraness和omission的 ground truth</li><li>使用本文设计的方法而计算得到的 Extraness/Omission representation与 该文作者手工标注的  Extraness/Omission ground truth计算cosine similarity. 结果发现，相似度很高，说明本文提出的方法可以有效的捕捉 extraness和omission的表达。</li><li><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gc9jnm2041j30i90clq92.jpg" alt="搜狗截图20200226092626.png"></li><li>==【yaya疑问】== 该如何获得actual extraness E<sup>C</sup> 以及true omission O<sup>C</sup> 的特征表示？？？</li><li></li></ul></li><li><p><strong style="color:red;">【Do error-aware evaluation metrics help?】</strong></p><ul><li>相比于之前的评价指标，REO，尤其是extrance和omission, 在tau (τ) 上带来了显著的提高，</li><li>实验结果表明，human evaluation更加关注于候选caption 与 GT之间的无关性（相比于有关性而言）。</li><li>同时将image 和 reference captions作为 GT reference, 来评估candidate caption的效果好于单独使用这两个元素的效果。</li><li>human written descriptions 在单词的选择和句子的结构上更加灵活，当candidate caption与多样性的 reference  captions进行比较的时候，就会存在一个挑战。</li><li>extraness metric更加适合评估machine-generated captions</li><li>omission metric 更加适合评估testing data 是human-written descriptions</li><li></li></ul></li><li><p><strong style="color:red;">【What can we learn from the metric outputs?】</strong></p><ul><li>该段主要是想说明，本文提出的三个层面的评估，可以为开发者提供一个关于被开发模型的errors的具体分析，使开发者可以从evaluation score中分析model到底存在哪些specific error.</li><li>eg: 若generated caption是对图片内容的细节性描述，则Relevance 指标会提高，但是omission评价指标会降低.</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
          <category> 评价指标 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</title>
      <link href="2020/07/01/An-Empirical-Study-of-Unsupervised-Evaluation-Metrics-for-Dialogue-Response-Generation/"/>
      <url>2020/07/01/An-Empirical-Study-of-Unsupervised-Evaluation-Metrics-for-Dialogue-Response-Generation/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文对 <code>对话响应生成系统的评估指标</code> 进行研究，这些对话响应生成系统没有监督标签。当前常采用的metric 采用了机器翻译任务中的metric，以将模型的生成响应与单个目标响应进行比较。</p><p>但是，本文发现，这些指标与非技术性Twitter领域中的人为判断之间的关联非常弱，而在技术性Ubuntu领域中则根本没有。（yaya: 即与human judgement的一致性很差）</p><p>本文提供定量和定性结果，<strong style="color:red;">突出显示现有指标中的特定弱点</strong> ，并为将来开发更好的对话系统自动评估指标提供建议。</p><h3 id="现有指标的缺陷"><a href="#现有指标的缺陷" class="headerlink" title="现有指标的缺陷"></a>现有指标的缺陷</h3><ul><li>从例子出发来进行阐述</li></ul><p>BLEU、METEOR、ROUGE，这些 metric 假设有效的答案与地面真实答案的单词重叠很大。对于对话系统，这是一个强有力的假设，在对话系统中，对给定上下文的有效响应空间存在很大差异。如下边的例子。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggbjo3dqgzj30rc0f3tam.jpg" style="zoom: 33%;"></p><p>在本文中，我们针对多种响应生成模型，研究了几种<strong>自动评估指标的得分</strong>与对话响应质量的<strong>人工判断</strong>之间的相关性。（即，也是进行了human judgement 数据的收集）</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggbjvhzcnmj31gb0k24d8.jpg" style="zoom:50%;"></p><p>我们表明，这些指标在面向聊天的Twitter数据集上只有很小的正相关，而在技术性Ubuntu Dialogue语料库上则根本没有相关。</p><p>我们的结果表明，研究界必须改变这些指标，并强调需要一种与人类判断更紧密相关的新指标。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>USR An Unsupervised and Reference Free Evaluation Metric for Dialog Generation</title>
      <link href="2020/07/01/USR-An-Unsupervised-and-Reference-Free-Evaluation-Metric-for-Dialog-Generation/"/>
      <url>2020/07/01/USR-An-Unsupervised-and-Reference-Free-Evaluation-Metric-for-Dialog-Generation/</url>
      
        <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>缺乏有意义的自动评估指标是开放域对话框生成研究的重要障碍。标准语言生成指标已被证明对对话评估无效。</p><blockquote><p><strong>Survey on Evaluation Methods for Dialogue Systems</strong></p></blockquote><p>在没有公认的有意义的自动度量标准的情况下，开放域对话框研究人员开始依赖人工评估。由于其时间和成本密集的性质，人工评估通常仅用于最终的对话模型。因此，在开发过程中，对话系统通常针对不相关的自动指标（例如F-1，BLEU，PPL）进行了优化，这可能会导致不佳的人类评估分数。</p><p>为了促进开放域下对话模型在有意义的自动评价指标下发展，本文提出了无监督、无reference的评价指标。</p><h3 id="当前-automatic-metric-的缺陷"><a href="#当前-automatic-metric-的缺陷" class="headerlink" title="当前 automatic metric 的缺陷"></a>当前 automatic metric 的缺陷</h3><p>从dialog 的特性出发来分析 当前 automatic metric 的缺陷</p><ul><li><p>dialog 的一对多性质会使那些基于计算words overlap 的 metric无效，无法对偏离groun-truth response 的有效系统输出进行评分。</p><p>一对多：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggbir2x8l4j30ir0okmy9.jpg" style="zoom:33%;"></p></li><li><p>人类往往会对dialog 评估多个属性（例如，适当，有趣，一致）。而 automatic metric 将对话质量的多面性浓缩为一个无法解释的得分。</p></li><li><p>关于什么是<em>好的对话</em>，有很多定义，因此，构建<em>“一刀切”</em>是不可行的。根据任务和数据，对话系统的期望质量可能会有所不同</p></li></ul><h3 id="本文提出的指标"><a href="#本文提出的指标" class="headerlink" title="本文提出的指标"></a>本文提出的指标</h3><p>USR是一种reference-free metric ，由几个可解释的子指标组成，这些子指标以可配置的方式组合在一起。无需依赖于ground-truth reference response，而是可以训练无监督模型来测量所需的对话质量（例如，有趣，自然）。<br>因此，USR：<br>（1）减轻了标准度量标准的一对多问题，<br>（2）为dialog的所需属性提供了可解释的度量标准，<br>（3）提供了一种可配置的机制，用于将多个子度量标准合并为总体质量得分。</p><h3 id="本文的贡献"><a href="#本文的贡献" class="headerlink" title="本文的贡献"></a>本文的贡献</h3><ul><li><strong style="color:blue;">【提出了一个评价指标】</strong>提出了a strongly-correlated, unsupervised and reference free metric，用于评估开放域对话系统。</li><li><strong style="color:blue;">【提出了一个带有人类质量注释的数据集】</strong>进行了全面的human quality annotation 并发布了该数据集，以方便将来使用对话评估指标的 <strong>benchmarking</strong>。</li></ul><h3 id="Human-Quality-Annotation"><a href="#Human-Quality-Annotation" class="headerlink" title="Human Quality Annotation"></a>Human Quality Annotation</h3><ul><li><p>数据集构成<br>为了评估自动度量与人类判断的相关性，在两个开放域对话语料库之间进行了人类质量注释。</p><p>每个上下文包含的相应有：（1）模型生成的response，（2）人工再编写一个response，（3）原始的真实响应。<strong>Topic-Chat</strong> 的 每个上下文进行了六个响应（四个系统输出，一个新注释的人类输出，一个原始的真实响应）。<strong>PersonaChat</strong>  的 每个上下文进行了五个响应（少一个系统的输出）。</p><p>其中，每个数据集包含60个dialog context.</p></li><li><p>人类打分</p><p>human worker 在 这两个数据集上进行 human quality annotation。</p><p>每个相应均得到6个不同的分数：可理解（0-1），自然（1-3），保持情境（1-3），有趣（1-3），使用知识（0-1），总体质量（1- 5）。三个human为每个响应进行打分。</p></li><li><p>人类打分的Instructions</p><p><strong style="color:red;">任务说明非常详细，以最大程度地减少human quality annotation 中的主观性。</strong> 例如，个人对<code>有趣</code>的定义可能有所不同（例如，有些人发现足球很有趣，而另一些人则没有）。因此，这些说明中包含了一个对 <code>有趣</code>清晰的定义，尽管有些僵化。但是，“ <em>总体质量”</em>注释的说明不太严格，因此这些注释包含一些特定于注释器的主观性。</p></li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggbi8h871nj31ed0r5ae2.jpg" style="zoom:33%;"></p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li><p>看一下，他引用的参考文献，说 automatic metric 不好，文章中是怎么说的</p><blockquote><p>arxiv: 1603.08023</p></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Full Instructions</title>
      <link href="2020/07/01/Full-Instructions/"/>
      <url>2020/07/01/Full-Instructions/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">U2FsdGVkX19eUZUU0tvtEKzxL5yKHBWouxr91LoF0lQrxu9r7vsJko/jWs77UKFQ9PcOENYkOi55nD3vU3a+AExkWL+Cl7jWGUoIE7Q30JzA/OgrqPeDORRm5va8FzGypKXseSZs11yMaxNkLyzMCvEIpJAy1sNnM643FlZBm4JPI4ylR304DcKgykjogvVdRDwTMmvHXHhpFeNy8GRM9I3+9ZfTMK3n9+OJDRGYQj0pXjstHdGntBCQYt52rKv4jaxKioT9J3A2r+8MKiAUzyWWXxz0lw1TcIwy5hLCqauqM6qGMdIo7+zMhFGd4guD4BtrFbwPlT4LOQVzlLvvph42PZEoYmapiUUeMyGNf1i7+TXtcE1p1a45EExIMYtwkejkfpJGiye963+jzqQWIFJeLLyKaJDWt7T1TrX7WLCJG/YGAizb22mYNy2COXdCym6HT5fIJf4UNKwuqs7TloQkYI+MOBq7rxYm96QD1oP5d1NWscG1RFUYw0HnSqZ5JB8vuPpdjmy3ukPtSXWpOJiYvB9UwEhJvv3662Kbb8e4KZYv+zTmjLPBT8hue/o3ZgfowvAhh8fxvvU+/Rg4j6cLkCv9m9rjrnzoss/XKI8cRMMMCD8aE4WaBsLb6WVLH4/Md2DFaayyFRZ77UPk1+XGhLp+L+VbcAGLnIm62y3T2onD0I1YExaXa3lmn8PYYX+qdGAyKjkTSDM5vIIHQSgl0ah/Ly18RfUJjtrYH0AK0XuDAtfk3CaLIQSdw9Kh8+lcQRItQ05PXzN3E8GyriRiPu21F34LADeN9AKaEV3a0QuUPDkwcotFM4LRTRWqUf7xXJQTnQ2jhSFAwSkj0c0LXz8kA80gXcFi0Vss4z+SBKaywJy8hgpPxsxMPZJJF/9kLA7tcFJoRONOAyv4kjThidoJ1AVp96NQALTcjgnNFZxrPyyzFyc+X1zwhbR9nPk101Zlnrz6JQsB6K69Hu374RbMaL2RQMATlOscm/yyPMbYbsMb524zto01Po0hFlo54FXa1976JSkxaJJxYGbbF3fgxHWa3jQUHc5EbFFPkejDdo+BhbbUtnUQWuUq9poTi2FffXjaSiPTgazontpmuKBVDzuxgfbNoTjNL5v/4dkb2SgBqJTTniLzLPTNitWzSCdyWcC3sId2MVrqdDRWuOfzNG2OlpeMk3tgH7Nt2AlOxgafxj1BulIo04EuiP4gwCpK/U6/Ju2QmsILHQjGWaSM90N8jr3KETpy9A0MKlCpbMZzJTPZ2OK+KpA8DmEyOqeOeZ/9xtgZ9+2Q6Yv9zdI9y7q/kvNiITC1DRK4CbP7IzMDYb5tFoBK0fLPx0uw9q9ZV2yklOYMEp5fO4Ay0ExBr3es8D5nGXEXcvBvCMkPeQeB9OXKjGAk7tYTQr93rbdCCq1V5RoESWgfUADmzJqs5U7yNf54CDVmCbXw90YcDyncNjEzlLOIFpZSJikpfMvQRZagSvF5eZfoMIZOQ8jozu9TshXEeq2t+P6rLUAwbATuRMpmFVAXI7OZSMqrkX7O/JZbyEuiKWVzSdyN/byPci/R/uPBld5yhg4Nar06RB7WUB74tujN/5s5sWZv+4sth1hfMz0kKj8ubIwDZilbRtT94o0w7BxBPmiKU2TcnO+wUi9CWzgotlQPwBnm8GVYESxYLjxDIGNHG1FMY8rYSz4DeyAWRqD/Pdme4MBa+ZHwtdYQVfMNHZ3+UbajtNjMP+t3xt5tclVNy9zuXgW9j8yXiJAF+AlOhnC4LGHn4k0VhziUQzTRT57lZ/yEENaskpZTaCTdhluYHrCPGeVEN0jEVNeJbOHmAQvETBIYHGwBq9tbVpEb2KV1r+V6wNcOoCoYdDFW2JVlXrMkWo3EExS/iMpgldSL1KLROqYYMdgLTxlVlaRlkuE5p84k+JMAUEjBD2sLc0dEsDY+TBhs9FA+Dn2OOFcE6qqt9HaIMr9AY/OzxdWMPKpqyLgccJN2Niw33zTWf0E5D/QgWQRRSFF2TYOzKZX7d+iuiUtiGEo/sqa8ETN/dRMvxz8B5l2HaLwXLjlWZKZJVQHTm8xlXz/GtHsTZRmGCS0qRQJslLQV7ZdU+77D3LmawtNKjCi+wwx6dLjAv1eBzCUAdqr6qFOdbnaKCRBoQt9X6umvuqR69bB97sfSpgDtGUYn5WkbdgyHi4OSRbm1VrGg2h4nqBLNf6qKLWfF7VBUU4x679say7EfZsu5csTR3FfCeIb1wIskUU8BmP1FSvuxKZsyGGY3KC0KWzV9ybajE+ceynd0JAPy8VImFDuyFTNbuTUvVZRx3ImcWXpt/Iti7+nts1y6j/Xub5m5HtPu+w2iq8DRiSsAISsS96X9Cxkq1oEEAWPdk/reJNusyYpKdAU6xCZD/347U7wtUhrBekblYTvSexOOWe3KPyJKKwXciBPsHwjmdlDvx7L8I+QMi045T2mNXH9ixx6IcROxkvaXhME18HHDgQzK1M7eCbxw2uaGWR2KrsKL1BdBeGxdVDEo89BF/p5ORE15tykFgL8orPCrZBYDZCv4UJ3oPO22E4HKJBMoZE82+u8PCoDSdxQopR/rWiUCWegjROGl1THtyJrRitVQehhYC0H4O8Hx5mhLNuPYlnVUh3lcwJdjkot9lm9CI8GrqIPIhZMrDm7YCnvWsrc9zf2CLsJxP3fIdicx21wtR1YpZ0RDvz16NSoUqXknjTqgBy6ugFiQFngV1YDSHraGtfV7487AAAFVcV3L5ujn9ZQhHvHmu2p3RIk6VTEQL5bMSqu7/KsvGEeWpT7UYVvw4OPOOR9ZQvHDejeoBb9o3wGYkk38Pc1qYLwZrhnDXm29Rx4CYSbtlCuX4qU7N+6kK97gJZTeS15NOiqMS0bUrsXz4koGhBH0IfSnLa0kPxilchEjAFuxH7CEJo7pioVXUslBbaWSU7ghHi3o7RuWCycjAfROSY2RBcAhtnHVkAGXXk+Nf6KCZBOBZCRlwRsbrTqx9fwt46vqBRpmGI4FWhPold0IjAivxw1p2HhF5o4CslJkevqeKnIdq07TDLkTpEumbngCnugFh7sS+d4c+kn6I8w5BmN4ylFdKSXeeyGnoWgNlW8Vci7Th56a4kX4sfmCkCNDmZRNlVKQllluoWSEwirl5OrGLZbO5YECPjGF+mptjIUYTa2JB/6FTQ+dq843qDFcu0yXNRZ+7u9VwOa86RmVkEu4d2E8vK/FSJU0yzo1v+fauhZU3NxXN6UNiImkQqxe9qDfYtsUfImHqkndfsQcL3P5D3uVJSxz7jM9jzwElF7Fh3ZpGm8p12mzLtwtKMtOVhrq1UhB5HfGq5YoQBh1NSBPAlxkwP7r5izmhsVbU3t4R34yvRtbC/Pi9jA4MOhvkhIVOhOWnray6CdIzXJVuljwx676IDpU6j1iubuc+Xy8P5gJNaTQrBmWQglD8xej5LwEg3J6PHay0QRnP9y/sX65XrTBrB9mbjBij5XM8FJ1FtwlL8rS6lFPcNPLNam+HbsoQF3ZmjGEkNzmT7aWCekFgbSrdQymNgKbvkuatY1gyt689wr6hlDMF5Lia4rbtrbyuVkRJtQ6Gf/9BKS3FKPiF0sNHeHGbqwzJqoTHt9p3kBsBbg2Ej8ctR+3uzkvJSha72WT8tSbqRZBRlXRLcjaqqYXI34Wyp6IyQKYGkA50Q82Btzjcaq65zLTmdQeS2nYK9kAeBvupt2g4QfinPKr7PfbQwLHwT3IduYUePY3jGsYNHSEyDp8jCblKD6c1z4m+3c6xTrG5NnFjx+CJiHR5VdCshcGf78FVojapkdfUxzhCPleVUjOAUo6yItK1GuvbpiB28ijvMFbYaDz/IULE8w0hBF9embbvy5jgBImYA87aeTVCMDweZHML2qZr3lNNC7cSniIivkkahHuIX38QL3q14rHXNIOm904sH6QOITaLhf21A56Gt1Dy4kwfSV4vUiaqm2IzF9DauFPS7DLSYM0HYf1N0H72o/EP4BntXUhqrKUfU5WLUxYbgpSx7fmQUqLbVE28p6rPb+b+4CiaN5UVTLZxJT+zKFiWLvDRhqDHt60TgrKGz42ZbnZZzaaJTdou3z001QiV5wo2/OENh6l/rXCoM15p7ZzHoy9JcsVkdlQsid7ypsXMVCaC3GA/lG1yLR97lzU3pZCxqtn3fIFVY62e5L/ZvibLL9ttuwChwMzJbEIQMvt7dh2OPDGDxjHDkDPhjFIJ4jEOzS9VTJxdTKU/bdosPzDK9df8w+HAox+8O+zD5h25Xd4Sr6Aln8lWPjyQih1yzR7P1agK6ZlzPzu8+WzujJ0IT3THqAgvha5+a2HuMnblo1Y9reGDAyWOuPIS2iPRi/xOaTaef0th7MnJ6kjN6XBikKZd7bDv+Wi/+eZOuC38BwoazNdxnkSx037up1HaISZO5fDtOawy3HbFTI8eMmaXozoP5Ni33PjjW8NcZlU16cpVnJsybnS37AMgLML7BGX4h46rSm4nVhZGuIeR1RtWNJtx+OVwyU2CySZ+G75NZEprku1MovktJUNvE72yDqW+M+Q0xeVmH/UxcGO08f879REY1f79a+JQcqdBnl1Pl798t+am1cUrGrun2FdNvzNDWsoZoyq20VX/GW1foW6n+L92ys8BG0v+8mQnbvH01pcN3uVPA/WrpNdnk/+fMm/nmPOJ0foW/rrbMefFHIdDfRl6TQbJEbLPi7yI4eser9wsbG4tndSbs4ylz+yFRRf6zBUtWsG/hg+d/7Uc6iEibOixWztkMjWsS6Ym/OJurWRQuDQgMy0Oo2W2KLXg0qgwFzs2BAIq7sxtINAEy0515bIrTW4VvN5VjrUrFS/u0UQtYKbJrPkjvqJY+i4m4SlcSV0w6f/T6pgrc62ws3Zn+aKDRhE8X+MOZANT682YyKGbloPttlYDe9Y7rH8Br4lEDMhkmZcdYXyci32BVSze92NrqTTf9uK9TIAde6ysiTHbjPOOc6gkGBcXKiaK7tSDkY5MCTFEvUQaaeU7BhEsgNCFS/VgDNbLteDoFefZeXCvYfG6y0xCh4s/lQQMNNLqbKyoIgQ9X32UKpSs40NdWEbRTszCk7zAlwa7tmgEVqYwR/8ojvEOwY3sPGT/XIYQB9RCqdkpmHdSRVZ2LeRkqTe5/5Y7iwtkgeu/aTP4ohgIX0KZGzruSbFkg/26TkqiSpP/STaaGVgkt6Pjm5Erq26XDsVNhnmJL7apKb1a1GC/G7B3cahJ7BZqc0kO5m58Glis1RZaHZth8QwYsBBNrKN2mQ9QpSxf94NwpecIkTKikztS3rUl1UvjXkciIWeAdZIS+inHdrfNy0ccZUHhjT0y1QFbz3MT6xA6v5hmwdH6FVjMRWBSuoCrPefpU1lj4DwVdC+ODI3ODnKE1KOoBf675yKLzroT2nnBHKPL+Kn7Z20WJxeCIanpuQ8lsGHc0abYlFMtfgJ3y+l9+M+CJcgpgNEFC26ynMMQFK8VbbkBLNLtBVNKo+120F3vm6aJDE60VAPfAsFHeu8s+uUl3dQpzcyXeSjT1GZ1Wlo7ObzrZTlH+aJA85pdcypSPFEaGb8Om2qFbhBX5lxHkzuDwNXWPFornPNIPljPYLxEkjymUwCkbjLpp9enyUQKwzdDRQus+S8aYHKkj6Y8WPXVKggHtlJkNp9qiZmfkP1cjBPPna0hGp7uLu0ik/wPi2Cxaf+IRNarfrko49S6Cv0CRAJ49QtDwDwOY62jvftRCa7AA56mCeGOZCDL9o+fl24F90ewBgULDuFwWQX8dPWmEggGlVaNV8bkzxKpaAbnxFhqtsuZNW+NkHiofpJn13iaa4Kbdj78iwTTMCD7ELpLF1JMyAOwbu4MypzwpQ4kq1QJML0xQofKMmM9E+314szeiZe2eD8KVQ5yygm+rgaDRjg2xC6XsaMI+LuDg43RW95EQahswD19LsMDvAW7OuuW8vVsGi5Od3VmEsr5R7cOdqdOYC1sfT3aubh+pnBq+n8VXFNK0iAL3LhX2BjOxPI9A6WR/4r1U7AwNnclsr27wfjpjC11NxnzyiAQlboDNlxcx7RsIy6oRAA3LDEELFQ/i1NKSSWhvm/51Cjh6cBVL+u45BvDhOnjmvJYczejO8E1K86IOPrUWU13Wcn0GL6ZIuqAl7JBTSqrTj2FgSnCQGej/hCGCx+V2VrV5cnsbqoh4CFVoSk0N8rlXIRAIow6ssVsEajxs/TmtNGq7sKPtGhwHEWcvs81KrkFCuVYoO0za1vhxrngxZ4VPKWFnbMzd6Add/ae5WpQKKk+xlwOGRHyxpYszbHYGgeGzjuVNvwNdDDR6wsYZ9fcT0VZyyk5aox8qiIZXVaZ4Ued91iW0H7YxSx2CHoFjMdpitfpeBpmHDh8x0gVfSZdf7Vl6Q98SE2vQ2+5m5RBouc1D3c3jrykr5x8Fl+cguCVWxCxs5vg9PxDYENeBzXsGTqMAuoxwKay45+w7fMprl3Fl6L/ODDmxr24ogn6ZkA75yxx4msrFpzGrUkzzo3XHpmgH9VHJzLceaV3XeSQbtQhHzrbTaTbrtTCEbItmnIsSfYJir4pQp/MeCx/EqzMVCLcHmfLz/r+fw2ULMmXagUFnUDu/tUr8RhCSCZDArlmpC293g+R5RBalOpFT534QdSEg9wPV/o1kfoyPtH76kWNPechAA8XcSSH7NJzd0eLEMHOKehtX+6f3iEocsunaKW6wMqRotTT21i/Mdlg6JAEwRv+N2GnVoYAB1w8L9AkyteOpCgIegfNUjf++w8xgbzHuDR9o7/3CYCUvswopfvYBRFC0+euVp9U7WjCpsjNg86qOuOB2hp7Bty3qf2wwoxGlRFBYwbvSEHCau4pr6AO2JGQZUmQd3e5wp6c6mRNY0hi7FduE1rhUqTD1GaqQGr6awxmHorSzyF40sjvwWLhDrn7zPl2Ij7YcVMAJeh5qSiXB1gLn2PlIvs1wbLeq/ThVYbMoPduYfobdRA8/npaqG4r1MRJwRkNkJ6LuqsTvNfeKStDKDeWimBf4H+NPa8XVtaZIhlMojO8M+fzrDAFkIJczt7pwJJr2aAYEbBIjkRmW4WWlS6uz22IF+vur93eAVgJoUvSqD+KRKCNWpxEJfjTF+tYJeZrkClXig9p06rhSikR4amm9WDfYvCpsl/DNmOzLU+DokirLDfdNX/8/bBPwA6GQgMx/rwkQDqDWEO+96XX7jgoSSl+eerWCF672pHglo1pAcucqOZV7Rnuj8isEOrIZCsuUo+bu6coS0GLS5JekiRdZ9CRUeH5bPUk1sNLBoLhK4SJRzk1awtQI+/2+0nwSVwAf3eOtbcCsIfBh33JIqKjnsUby0AZl9HNL9Wc0TNHcE6SbZ7TajlUtlpcUnIcOhSLr8ZsiQP9S0YRHX5JSdMA/4r6y7gtrrM7xTEqYosdClnfIvd+6MiNlVdPA3lHkW7sA2ja/S8q6b+MRL0zSmpm+8aa1iCxY3FW71AVfCT5R8dI7mObjqjc/e4j3DNOHMaYGvlLxR8S+bBApZiFh37T17DKiKGzwfPjRggSf/v9/tLlSdsQJjqBtJICK78VhshwfFOpREam1JUhTSWqqIy065nQG2wCx5rU0qibDdvXcjTa7+cn/Yla4IW+fglX8LK25PX39rC/U/e+1VHZIjSMfNpxClF9/s7FWYx9XSyXP0Bv7vlwhEqk0AnMRUt886iF8Lt6WmRDZ+Gz0qjV5aRTc2W+YiQhV+WehCbV3AVirP24TzatSEXwJvPl8lUpnQjdv1DYwFV8KqWrL/VdMrwCHU2mtoU+njOb1lMhHIaoKIW/UKWfDnguH86U5Pf9iPyl+eRWbjtrPzBbazGWsEGDHmRsWb8Lj8R68eLhZOvKVSTxr8ED0bF0PXRrhr5ZjIOl77OnyAD857nI3xhDH3Ba89mu0SVxgJJgXu/cz497FlFL4ViyhM7Qh+WPdIilXFgrZ3sqTEFoNRI8tveuTnuGZMoQBMsA2ICuLwmM/k6zGWiKNSD/Cx8dTMc32QL3akCRcOx+VyFVkPJMdBRFKjr4+hS8gcjRHchW6qhk5q/85BU2sJ7GiXdd1r4Za7YFQIs6Lj1m00uaQZc0954RVrnC8Dk4UDRjTkQe+qBRk1BCyijTrjf9vwad/G90VbThYDO6ElfN90ozQDuXcLUIsOA9g5p0vCb+rFYcbPwMXNAZutgj97DSbfq9uX7XF5wrdRZraYo8fpMfecsSBgMhbg+hoGtn1HyJdi6hUlJvohO9Zji1Q8RpgXZFmRcexPnXU5nOjUAuvYyT+zlXNdiJpZVidv7peNBvyOu95RZIbMhTo7WdQrTZH8TvhplR+POhfZw00IdJHcesN0IqG2T9X9Te7aau/GZmGdFVqFtVd8CRzU0l3+kuBA13a77aoHBKdvJD0tWq7G2g30wyfj6NEV2DxUXL9tvcX+FWMAHgg3UnHHCDtC6DBxaqzmYe1C7DDiVreMJv7/yCB20Qa1R0AmDgvWAk2uO/Vds5f9RsxHRwbJErezRAwTDqrcE98bY3/mMqJJ6A70KbgdC9ad+1gLjdoTRcPRCoW2aHHH4J5u5aA9I5sRcyKuREyS3y5LxNKz+oeXZilKmyrDMB1QY4/Z7JH+XWQ/66rj+pbpH9dYgUmeJvIEj7N55jqLuExixpebq8UXLiqWjaNLxUnxYQ5jQvHSP1ql/vMywa+LSCCdMuWhVPN5sSnLl9FZHTyOyzulgYi48F7nl3aBpvh75HAOTumk0OzX6Km0WxenX7PW/ZxEMQ86D0I9YExPcPjKeMZfUmHo/9S4wFFT6Lki8BQC7wQIlf5hSuJm9U2/XXEGU42dAzUrq5nLjO3lRz6avo9djuIjDK1YcedgOQEHMMVGdgJDf6aSMfOySdqa+Nauk+HnDFPMYkX89teSoTgxAqhbqsizp75gURzV0QE/gk2EOAv8q8ootv9s5NsC5P5q4wH64yMjPAm09ObgyURZcsFKPG73j6/+ar6YJFWSXVQ4Uij+m+P8vxHuDFOkem1PYJUvFy9SPE7W7E3CogZAucTve2Qp6ygiaGmX3pyzgHYgFxN0aGojfOzOD3A0IfqzUhIBtsyly0JBWcQe8msYpAvZJaOw6qhlw+Wzs1aJbiYKXV/AcKBPp7nk7Bmmhq/fsB1ISqA+7SLmmDgaBKPtbDeX4PzoVKlArpS9AUIfUJd0/4FmtcWsDommB0cKPMf5yehTGOoFosXYK7Q2SGGrefbrJjJuxFtcpbY2Xl/nlFbo8S/07f8oVSU0n7uB2ITe5cLEEOgor/zj+yQLBA2Xm3sWJ1XIIt6XIGCyqlx+gZ5Mg9I0+aRgO3r2nwSuqkMGrq5syiyWNdKKxy8LqAtLPvsrBFSi6QorY64hxreyinEMrqkSZ/q4I1nmWLhV6TKugRFiCnbun9Vczvh0yI9pI1DJOq2xyMLSydlD++LKO76zzo9tv98T2CbwP3dg3XsLmu2Y/VIElRCdP1sRXi7nLQuS86UjpLcDSROqRv28wUNYBmkp8ULRyKsxEc9iRNsAYE+Pb9wKzNqPF2F7BbAa/ZhRAp2Vw+rzWxc+Mj+dWOloJjVndBCOPUFAGHKZhUkOAUFG73bMons+LG6FW+OyQb5Li9gnhHRyl+1WVDXkNgVUZTtEbAy8do2TzQ/b3DJjvszAYUQpljIWphr4Os3z1GWntq/ClVFxOuKGH/AHdqSPvnYpm27AknyugAf30gyHyQbX998n6YrQFU8BVRjMIwItdmVqA0yMOlnlqo1L19OKwiqQX5liVBr2rSQ5vd0JdfC/xpkrADuCg6vNZC7wquoXiZhzK+slFzjBdiObJXf+dDQn67CEqEQLtrLb8J64xL9GLkXaWVRWLfQmplqjjZIar9KUauNEuZniCbJeEegcrkfUWoZN4M/Hj0K0LRMXzmBK8OP03Afb+AaL+ftEAewYZ3s3laxRYVQ9Rlx5B3ji/DXaS6l9VsGYfMiSthNV8zoM5hGpMiMGwikKB173IFlc9VZlJ3aDlwtKxGwae4obXvDSV3e/ST22vuUGx+J6YDkomG3b5IaulfoSmFMXNxjwVeHC2nbLVsW7EbsckQk16WnT/xJU+eAvEFRky/cZw7H/EtWhWmCnjoiG6NoaJfkU+xlNpkDrTrL8SrSfVpZHmKfL28vUH39XiKO++WF53cVF5kSpxKaT89DNUb8gS8vtJkHCwVM2ryrPFEcWyVx0oBh0olPu8HAN+wmE1Sw1+M1c4uKHQOkepcmFFBHfr2YNqzrgZRyIAcQiiLzEnJqLurndGBLmNJ/JUQKN6FMzu7aXtoA4SGtCf9YYVNG+iOTQwgskh22Sx7BMf+b7ahGPqxyf0STsdDlm4Br/ynuOPP6OIKSpn6KUuUlxnALsqAe9R+ZKWzHio3PIPhPTTkRgsx19X+w6guzZNC99dPliSkN2ThmjA+wRHxZ2+Xwc6fFRjnLF+yYDhI3XVcbKIs/lacKqTvWz6vqJBA/EN+WQsFpoMyfBq+39ANZ7Hvz3z2ZwlEt8IAXwSpsyA+M5p4QI6zMGtxUSqIWooUurAi/oqrF8xbPORUBXBnbbVKVcvoPwCWKKxafFJxq1BRjPN6nt00YQOSTWJysmipoU7WH749Q7t7nNDz8pl9d1+7uos2kdg4SSg2F6Alwtg2rE47DUTVWNdHm52xwlUEK8ByfugvoMDehw2iw57JG5W9FTLXv29QRBB9QcPOBMTL2GkSA2FUU2BTL8rhybcC7Qua3/5n18bg6VcpD+j8KY+skF2Zlj9Fy2LRI+uDfWj0YU3GLtRtw8PUWAc3fhJcM/5kt9qnxrsS2cOdqDtwK48yiFxyOFjWVJUE5JJQcxb2ONy+Y+fiWenNd3VDWWXX3LBNzPNXUjAn8G0SWAYiX/OnUwN2yE8rTVVVDNKxfSKHsbsl77W2oz0xB9TJlm3py5vxknrpU4mkL9hxaeHnqxraJgLGTeYJlmaxhDVkn5fuUiD2CPt+zWsr6FU/7z5edgF9fg/TfPtLd8E8OwLyYO/IDPICbWOiu0o54bR45fEBOEuh0lG0FCDzRYEYZ4ry5fwTpZ9YGLrURucSB7IHpDS7MYnDP3cq03PHWEi9J0ZzhZaAo1mEwytUGjISXT1HcOej0Mt8jqL23wfxzwIsPjqyPXeE5qWV8XupOuPvmBycdeZsajx6/vaH4MEg1D6VYHN3TgHgR+yp4ENKuGbz6+92ZVCWjgKToFtAromookbqr2XWiY8Kv0MzVGdJcQXsIDCeP4OfkfjmvZ0CNQN+XEcZ53dcobMxp86yMVjic1vzSgHPp5+ielXDJNpNnikYxNt/MLxn6Vi4VQrrt0B5XdABTkwEZ7iDevgG4NWOBlMfqXVYs+QaeeDj0l8VIepsyLfJLyo+18EqzOLIkzSEGpZfCE90UoqGujII9H1nEfrDjepgMxeW5vlapR3MkgJK4L9std03zK5HfLXeqXjGB5SFGk0eA23jgBUymiBz/wDxPlo7ARuDZCJ8IUGkulm6K5vrs2fT00itRR1LsZBXnr/WhV13QQxaM46aARKpNxoHZfKzUYQvuZe/OTL5dDivN41MusXuI3rL2pKPAusjkCc4mRjOkD/2rFsFBN4e6egy2AAGzuNC1MVZgFGNMPutFC53b/FVFxWfln4TKmnrK+O69YwQP23czJAhf4EjWddA4f3FCXR3MaLVG24Af0/TASjZBtqK8YqJ49vSCOK9BXTG+onqcNYKkw0VChsWsuI/AGtkOWSsGS2an930DiigYfI9yyrERwedRnrHE8JFgNBPuHPIwSwXApTa30T3JRCoIptI36gJrCD4HgoGZhoBhPgghRmSDyCQ/7u6Ev0YmPFJz/Yha4bS/HZZYZv2OfAJaKMEOiD7ls4GtOj74XCWdv7/G17ihHy0qc77hbIW+KYa/VB1+Hk7cKoLdbTJhubmPVqo0xMepdeWk8rXEIIUXmGZ680tbCy2asjmtJo+0zPbPT74tNeBepzwURAn2aQAlm3ny0nHETs9qVWXOrbMt7i43jVqH+xpiEFU1QWbNyy1OIvf2HpjGPmP8bdIwpwvzUTv/73Gfd+/MVnJJZOtK1DK2PcPCtOhHIOsPlCpcT+jBVAm6MQzZjIAM10mXyAq50y07rmf7UTsSV2ffA3qLLVUY9IyppBGlL3L2IvgvTabIEEzTCsMVXDdOwYWxCE3Mjkp1pZaJRqoQ11qIc2lSO6lD3NF89zVkJYc0NVEmniRuK2v+J09HdYBox1hPA7wOkWRo6qtmXwakD3f7YhOtqZDSMfI8Wld9d9RPS/TRGY2jH0hbmCo6Oz1jv2Nzz0mXkARG4jnqgbJPu3omitf8zBsgSqiRkKOyEpp4fm/mH4CgE1yM7D0ULPALwpxWzt8lrpNPG+13n2gWH7qEM2KVpptZ1QBOJvAhHKopg9ZrZhs9vOQKc9e1U6opGwONpNe7v93pUI7DtiyjVjWQz2nBLp8ynD3DMrKdfbL8uV0jOs5zvtdqyU7K8yNKmMty8IuLBGkl5POIyvvgNE6wVXEfz0yK4Nniem/BGjuPUOMuT3AP+KGDxVebexZ7jNVnp9+EqO2CMYuRvvwzLE4NZsbS10hUtFKUw1uBxDw+ZdflxXiCXGGL2QFf2A4jGL1tU1mmR2cWwgx/kVvmSZzVJ/2tDn/dzQ5RKSoBLyqkyQjjQZfBPGc0fGpQ2vldlo/DH8DAG27NnUFtRamuucUdp94wbcW29jnNqarVG5fUNbqX/CfFq0BSqhrPt/NCsz8626DpEI6XsXVIYhbRyqA7al/L5pf+NFUcDHN3F5q9vQbiXs9ldv9TsaSFYRGyiOIzRFVn9XS0d6jKwgJjg5rwg/W4Spzs1C9+tf3nYRRrU/jq9tfwdDpQORsJXrExBkUrg31fK+UM32Ojcn2KRVM1rdK4MOB/eZBh2YyNs59SiFsQ/xyHizX/Ft19ieQVNgH2OYy4lRfJaMt8yhedwO5f3qTScO2q+VX97fFcZRANA2M6PjYLt+Jpp4KYaK9i5ACKR4DvpP4lNl3mt7XurjkQGvq6yLJIn3GzYKqh0VLTgpr0MAk5KL+AHgTPIyD1u2h7xMZ2aFpZs8VFax2LnK9/4Jamx9yTiLEhSs5LYLqbTtJDpkaPfOGl08VxaOZjqiIYf6QixElLCuHj84+1GjCSRsPdmyAjdBjuBZhJGCrmkZsLEdDKCkeqKoOcgP7QHKDM35zRvnca7nBbFhkntv8HS9L4YJssC7ydaUtNC3EO6Zy/joA/XBN+d5IXmQpNpu8Jxf+rWySX+FosEUhR1zlpyRzRvU2AN1p3wtDqcf7kx7mWfjoB4OkPKs5HB/VjvD3Cakt/6w0+M25UVYF0wBXAVsQxuWLi56WAH1RPwrf+HTTrFIFQimGg00gacYHe91uUjjTIo4sRGDODlS2jXGhf17JKCdBVjQbSRndus4j3bxw/IyaUbsSCOR5k2UCPsFoEbueSZVm/vURF/2rH5EQnSyCZUa+NBpJlGBbRCkI3ttNSwXtRynS6WukbRSPlxLBEuifzCbjK914/P5bSblyFJyuseUd18Vw71ry7oDc/dcIchtTAA0mEOKavQrNhlLYaP4bSydIYPzjH8EtJtfek7+FG1VcJzTe9IACWBdfzzEJ8Y+/lKqWYGKpUY0dUl8ZqGnDVL1nqB4EdAkLWT826Unxg8dv2SdFE5BDfVNYoDOKsIbjZAC9pmPbR2u3ACE6LBaZSRaWmMCpUCIPU/i8z0p4SmFCX/xdKi5GwNH1oi6L1ymUyWgJZrmieJTpFb29S6+OA/s+vWnRsQWjesryZoF3mVmFQZhq+S9J0C82mIZzMbJNgP+xzZuvFS6woYAohh9arDCgbiJ5PekhrzwkoGs+KX1T0z34k0BP+6VsMsscwuGr3tkzdhurXTMAyfvSqZn9yB3x6gIaX+gnPPFe2tLxIUGngN2e9ou8nLTVe/ewqzqNEMzGjXI8QBQbdOp2sRUDqQVf8pBdP/ab5TaR1rM8noMMTtDvQiskuFqBHZcwVDBN9YRDK/x3L7mzX5QOO+hS3+a99+hLTxGgq01kSFgRqC2I9Zx+wkiWOQfKKpU/qpL7gTPjJ6JaynDmsD6yHW3kc75mR/KcdCEuCK5fScEMxPKI6KxVvj7i2eKBBDjwwumny0kla6OjYx2G0W7ZECRo2QDVIEdxTz+MPMZulS+HL02ranQoxLbxZgQ7w/lAnyDPHsKvHB+unYm0KLVqEj5B3hkb2RMYVJCRIFvdyIOp7/kbodCNKG1NKkQAw/9O9q9RpGz376FvDDesXA8U+3LQNCyUhjdc5/RjjpnmFqVxQc14EiTtXi9LOAXwuvNKu6s1iGR8LSzWAc0SIx5H07CPYnlh3GcsE7v1yehfikFGQ6gsTN6MYxDqqK4F+HCtTGlLuLHlSPOFtqHr8Uuz15S7ZcpwI87MAHP0RvFM67k7WaF95481CjNkMUC32ChOLeo74GCBtbhwJbNTtndtClcR+L7Q0r6n0T/ItOzry42xqamQcC/PuXK7rJL6std03RwsuvIb0TFXF9feotsFWXTTYUSN77zNf0JWD36NNq75HT0btuIO7OlJuhN0LdErSfTaIKkD9oABTlx7FlqAYKTg2V3h+ooGRWmZVGwbQmLBv16m9MBbspd8ZMQlh1kJWt3LDCSG3DTRO3puSfyW6zqIGfLn0X2l87i4nIKtS+jE1wRXv8LPedd33smV+yLyU/q1Tg4ohjdXqS+o9XM0FGQ0ev3E1hsnv9mS2DKYbd8N5oe6eYDwsnFie10a3oH8Fxb/08aLXIv7Vugdb1vtmsOi3FHUhXBgbodSDxqxYtQwdroprcL60P1QFai3yWa80qiqTkJHvssL6WVZWFSdnQPvrm5u1vOHEwRkUKj0WzQqFnCKqS1TIrEx9lj3ETDYn4++kWoI3PopuzKNIbpvetGcc76W+wP4yzF6Aiy7e9GzRhxe2kjBC62WJkhbEMJ/6Dpa/qwkVN9/CWWgPcCQxctdBK4Z52findn3oJhc0OQrff44I50zJonIh0ejFgOLqPCPhIukKXZAOTzn9on8+fZw50BNN1EoQWup2r/PPXS1ms5ZKrw4IynN2OEk2GxUuwKF90QiPPKRoGT+6P3GMX2MsLx0W2PJc54LDwOsDmjOrbUbVn37TwGBH9bbLdq6s/9UPufi4RKQp0LAhk4yeRjAkWS8Qd7wVhHJ+AL9MEoVVP86rZvD7eNlyxSVRieEiQRryvXvoTycia5CRcd5/Jg19CKPSLJ9cGsk+axwq6Q2Fc6acOdAC6K9lRb9s1B2ftkjbKiwN4IqiAUzek+t5fJom/iyP0TyOHjSu+EtPT8/9z5IPkk2AbBDQtV1ETjiwSnwyPByA3xHvhmNgZ2eUNY4R95KcSLt23knGsyZEdtY3u2TjbubU6+AumXAdhf99w/BYflrqPIijE7mc2wy+p6l5HEXrJKV8b9XedG4b3x9GyZWwWXELkS55ZzZYD3SuHzlvDUutMSwmCkhio5jOJhfpbnvULDY5TueTreIKLpWnOvjqcURRoqFb9ten7pnDfdmA7Yf7x96qGuHz2CuwwOvsG56JHIiQkVK3+9AW3j/dFfSifcn2ZBLt2mzHcBuXrHKD06oV7namj6fU5xJB13AfEqxARYj4tnOjkvh3h9AlwDAezHBBG8DjaJ+mjxispjoZmp2lVNrt/2zOr+GbIs8+ZMGu4cjaxZoPY3HaW18HUpFPfY6pA8H1uMINuHybAgcl5Vc2u/MHGWax5vXlz+hI5D8VG/rUABYWfUx0FWDCDBgAeNnrSyuARu6huUWN94pjjPBqfXHXptCmwrzaOv3Q+BKQzghmZYFOtD8dv75kOjZJt2NpxPj5yWwcJYfc31QT036Z0sHrl0xzcNRHN0tcUGSM5mC18NdwrroyJ6n6ixVI/nzNM+SBoc/Kg9JArX2Czc46q7rVN6x7muOBUzo8qBeneD+gX2hn/ZrO8v+Ujnp5odp1L/3t1KUCn6r5Y8jKrIotXS1kHZ1gtIZ291Fjgm8ITY6nvtReznt8pGqxIL9gk2+JMZOwUQgph/rsY2RTgxxsccPH37QNf0SyFlILhyc1oLn1pa1PvRuYRhQ/ecHaz2bsSogA0eTdWu3st1Oo0JG/aeUcMVL0f9hAVRFjQYC4MW7L7RSLz5ia8bQyXU1K1OISTRh4g6egPyIGRltLnjy283w/tFm9EHUt/02ppCiJui9YBut5Gu+3HgbUVEwTtSziI/XcN5mxGc/r5ZHudrqammdNgYQQnLzg/fyLcNaFkyXQmzJLOeb458PqxVFTbQPUPuC2f7Dzf0dyvR+qeBvLhZLEFRHyLcyQDsKmvEL8dx8txXSzJ3LXjpWpJpia7rcN9P5j2T+Ownn2/q+/VuaYSbHyWpIJB3QXczRcCuABkJUQgzbYT5/6O1xKUXKEPX20p2VI5i0lQqxqXZi3M5PWTAyjniGTCLbIj9/5OYEcf3nfiWbvakF/mnbV/gUxZU24OkoYNfYGN1+XdgZj+XLCR8+TJAa2JoVqfFhLtHEFP8f4JWwJi7pv6e+OWeYaGEv6Y/s4y2Uaz1vYTHGjJPVsWAYJIAA6D45JAgbvVfTe81+bjZp2bwVdvxR/TeXr2RZsVYIex+/5Jsf5ZNuaCpVZ2K3KAFTfL0w3/bh/7mCLCe/TMVwHevzZyIXuq967EtsqqvwRd0vbgLbcXfeCR+2kSkgDY5dl25L99m6Ctj88reP6iuMrjMlJq2Y2MyMGap5zYhzV2Mtlo6MsyZ78clog6tDieYxhucii3AlIJf/FkiJ+4InzvkfVrxpqAaarMJrcgmvbZ+9YeW87Pu+EqUMXRJbZJOEiP4IiOxTs18FydAwfVMubCJHSQccZC+BrPFph5rwO1IYFjbGmKBQ01kyGDDCUHbSTAKuMxHPgBRZFmns9Kr9f08axQ5YTtTeb8DPtY1Q+qBpvzRsukEao5OnYP7pomJ5dy/6UELVxQIQRhXRAcgyBgj5Niml11LlJcMNC4xA7QmzoXHHsd8viABVIjUb8dvJx3d0vY7q3n/V2DGiaWleKZb18yLPnGi21CKZbeUv+Lua0ZD58UY45+GQfqxHj7k0Aml6cqz0y/QuXkfdlu5h1z+xdDAGwH0v19SlL0XkBMwYflnBfrdjy4RoXzWHUGfgbR+/6o1rBHH6oiRHKBJy5BbLGORpVGFrI0zitkNN/l7hWsoNc9+3Kc7/sKfluHq0xsicvl9k/+Rc4RiEj/41dcBaBr2XWQQwk6Mibt1qyeome+3IrdTfapWqjWA0WTvO0HwkgmGZi8uZorjInT1fsRosyrXPr5LFFnjBv+eZ1cvCwN2GRFK45GJZm376XTKakGjB2odIuzLIU5YM3e9ys3jDsQhK6bX5wJ/dPhVloQY6Wus38Ku4f9oLOGz7uHfJ6tCaody7t4otVps4h0Zo/GesuF0U21RaBDE1VSZ0WoNXwg/s3zE9oQZRW0jhtbXVv6Ue99oOLQC9WcKdfmdraAVcPxIX4uqli/31vaCOKwsnn04BzmJXSMiqXr3Oh6N5sqB6P8TazHIHmJT6S5RvrjiWw1+YDycKcIEk4mUZAAtFAU9MckYsKyP54bjVlEbPvE2yZgIwSnT460A29WEnILEA8sOlFmviV0kFcPgurweLUfyIFhgPdJZkWwTpw7JEsfVD8evn0veUmSm3FYyPS3PpkIsI4pSoTiJjZvGg49vVpqM5UHnnxcFAIByoyE3S2nh4xylkdI4Yelv8GkZsviJax0+30lFFQdnB/CRXxcv7sX1ZLhwNlkluLOKj7dU+VVwtRi1jlV6lDxsjGWmbcx+fWtmaS0bgevNmEJwwsdW8rnlenNvcmEh4FyNYYQv8SALomGeONPBDlecbeSGuMTaibVz7iPWqlehAgY8y0vqAd90NTnnAbqftXJzv1af3WQqPwcr4kmCBgiUh7ace+wLd3j/rB0nd3lKpi/z3VuijfUoogAU7De6u78jKeO+PCbRC3yypwOEf2N/P7lYVdCTKJJvFgwOVZ8dXJ7fdPbv9j157hZVgJ3APicjOiDUoksFj/Tgmex4+OE5xfo/qGOkhQvKQwcAaVUMVm1WHX8TGM0lwrshj8K5DA/F8NIA5r7uj32d9tNONwt1jwfVPK+hudtH8M/cGzb6m4RUfGdVCfuXjXM/D9xDBoVqb+X+6ZfJix6ncYxCNitsbtnU1MJGmnC0VClGRsnrQF0pVrPpFuiehcF4Br0FcL5BqWSx7EmxfcqT9YFcZ/ruVbLQdLtjj/7CK+RQXOiftzF2+WD94Kw3y5fQT5YOVlrbq6DSGjiE+XlJVIRnlLlr24dnlSnE8ltY5Fh10Bjup8nArnIxD+yWWQv/WrjltEKyWSfIFliR7+ZTUhR7gDoxuZi7xpOjD9A/7U3QaFoD89KuOHKwdWhFW1yXTd3IAmttx3Rr+cP9ULwEFh/sbCJCJYJc2qjFSuQEcScXYBMBkoFXpVxb84ZCAiPgXKD0lkQRBSV5sZ1qRVFMiC60JzM3pZaDoSBSKlLFgWZ3SMN/aLgjV/V0QxtJeBEWbx94xp2TLQ4CXERsxsOhl+g3zzolULZUAuL8pFVXcnoKQb3rjUkM9YtQ5KnE5W8OvwkglL8JC/NqmvsFRh0r1maNbFGQ0bL22bCwX7F10G42eUkevH6Uc0a8bOUb7ZRbOoChgSfvJSxq3hyCMgFa53bJSMXoGZYyVm/dNeW2Myeyyzz03gqz3gjnq81EdaRHfqb32w25ar+85D8uKmBrGqFLMLJ9HWuFeR4YrVjpUlxH2+HHWS84tDyXEgRg7L5iQjljyXwmBGgZltk0PSeXy31Px9AJIxfeSOm9qt0O4qrzC8ptHxeoGgujQ4x6uWq9sj47joCQdBfOkL5viNgghlfA2fXMMDChaR6EbXkTVu6MmZOzbS1U/VhAC50z698Bk5flAg7P41XkyMQLD/IBAQtyOTwRH/2Z9UB/aRuPC4dhBOYdj6iUgNI3TkoYXmYoxLRDos2Yiv7DezYdEDQAUtb2OMahYHKVcTCUuIMUnvl0l1XYHOfaPNh+HeeO7ZtB0oxk70qoGCszkWkjJqc9FzPBcEALLQ6VCZakZ0E3NYxDMcxsAuBIu2sAOh1HOOSYWMlYaIga4rUoO5+U6W5Y7FMsCMQ5bCzNHmWfD0NUlAwIbl0tKyxJkxRXc6ty004GLFizAjQr6wIQFFfOVXgD0gsziTsf4Z8SNDMnHA2VH5ByeNFPa7iIAbxLF5PPuJFzAgBZi1DfaCzSmL8xtbqQHzfQXq1vLmrxWB3H3/fj9zmhcki/ujnvD217o88lV4/PcQDo5joCEyKMZsthPIv/ov8B+1sCEZ511ivLt35d+qqgBD/so0NNwq29kdjuWobsaASTAPot2Zi5liCv6w7xdXLK7+hrcElTUtalAS04sQg6+hKh5xzXgELiP7gY0eI7RAtMCRvv1HJvG5adqkqLJh+mSsSW21hrTP7uVyHaGHy/CsDi6t0vcZhsY2oCI+3vGfj5NDo/Lxj+QANU37KfWaSaC9Zm4batQH/5q1PmV+Ne/QvB/Ur7JYN2pa+khJyV/Nz+7Ksxrg0SojnATHyX5KowzqI4UlP/bErLpd6E1ujgZQ/35DJ5ZECM54d2YLHSiRvBfuz42m4LwmWgPh8nHty5SdoYHsGqN7S9vB8lbg0FThA/W2ms5HfnSFhBbN0C+TCokMlYmHcIXlUFswNnV1Gdi2oTFfaZSIP/uYCnOmAEnypVjX6IJkGmSO1F2bLeGPS6lKZheXaF2CHQ1OVjQhnzzLHPHW58Mq4C2U29KWG2LSO4bsHrEUvpy3fwAjF41zMLKMQSiaxgj+IkzjBIOWx1pUIC6PoJsjQuVdU/CjNLl5zNbWAui2FrrAnNY4X6MhUW4Lke1aGkfo9rsdqXjzL5dYH/w3/qwZ6K0J9iKCgl1fUI/pNfUfcJRiYFiC/+/cPUHJAVuhwBADyfGYPNNdJa3al+qHQsR3o/34WxcSjhNE9u0gMkHC0MdWtV+sxcMtIihEwta4nrElZY6QPKc7UQPW+wXcS2sy2o1v1qwOMB4UrDZASPKxtSsJiI2lynHWtDtwAdf3NVatQI3L8FLguNtbZw/NUZKcACcncg3YlI6Xk7A5jeoh/b9meg74zLqWaBpXpx4zYJPBCyXABEmNnu9a3VxIutBd0Q4K/cXszLpeWmmAIarLXu8EgawyW/aZtslRRigSSw8WZDKhVUV4IAW2NFpfKmJ1L/VOGl20Qh4VkfaP9/5we6VkSI4acr5lXKQzUHyxktKzCwcN1w44cvgY0rFsv75RDmzVShxxFMID+r9yQv8PBCmhJbYHkdCyVSfY/SWKHS0F5axXBnTJwGtGJi9RESjlEryUnm+9ESg/026BvX0yzDBPTdCmof7MFK3irqhWfEP2bcIhOm7q1SrsjAfGFytIyJK6W0czG7JBYNruCr73PpxMHnb6u90o8YBFtHxOA9yrAG1Um68Mx64rCLlwxQcge4xivClp3D9KhlbGkM0urQ/rTtandDlWekFSHc/zG1mxAK68Be/o7YjEhApkJO6U9m3ogpZnc1jrhB3zbFNBzESCbgohhLjnvM0dV/04B70Ys9S0fvanlQZWCyM5SoGIEpo9GZjjdCVPoqZaSwokiSvmVh0FYJtUWJ6M3dIhBDvkvPtm33jhPnizBK0C72OpyTDVuTsitz8V4leXvyB4S86zU4Z744y9/46fWAzRnZa8UIEaGuvUL7aJu5SrLR85lHbIVTQz29av8m5tGwmLwRRi/al0QjP6TYO9/ubVxIBp0ih84tYMZhNRShEjFieMURvzISjHy/kP8AB4UrPmFVZKCgRpYh8lR92cvFkKgRwcp32JRIsemdH0UNHGUnUeBmNRgtdCI3HqprAEcde2SvsRrkJt5O6rmWsmpvMOdOaxRj8XNK7Ura2ohnqhjon06keLzdvW7VrSkB94+7xhQBXA21dOEXDWL1eKFehBDQSr1ixm1w78IQRy3neIdGtAoTp+2DUKmwCUQGEJ1SLtRTPz3jGloS3c+Rd50HHybEds9p6o94LsM14IGKP5nAK80dhquZ+JmwLrrotHCLDCsCjogdsoK5ulPpHiCBo2pTFJvXJPzUtsvgCBxkOqDImSwgJeCQTurrxhQBd/U1gZPpR0uW1KKmawUmB4rsxvJ6ZWCsFm7PfUfrqCgdQLPc9Up/VvMhIqJRdecr3hKVcXFNGyvfupdvNm4Y3Nu7B2bszj++Jlh56MB2vVx/6FbXa3XsviaJkEtiOiPWWl7G49dOeB7tpzUAn+k0G2llAIz2xXli0wP0QKoZb1PNRSEp0XqDHHlez2JH5d89LmBS8AjGxogJOgKGpIvEFH6xekAtW1Xb/VtA3PHyZOCTjJGRCkNeNv2ZiXNy6DU5qUk9lCRPK9/4WNkjMOmK5VrhBmMITOudZasfU1DAHSaaS85y/7kUGrZ1Z/j/xgFR7Ble9WcG5bT65aDlEIJeIORClHO56MqYK66AAJ1lptgvOK05AF6lb1bSNP3jWHQCtEZEuMOAJaj00vqGAL21RSpb5Ap7hp6gyNly4zOSSKL3DLUxm1mNVmru895a4kQ3M4LBZ8B5FPpHynLN7wqx41AJYWGQ1BHX7IOFFHm4PdYlUjAgEDivSqopf6CLE63UP+xFFK0pikrWsspAFiFHr6J2sigKSr1AuV0s22at8IXlzEohV5vfeCd+Mmu2N0fTw+Ml3AatV+6/POCwMDKU8c2tNAOznRaTzMgNY0o38s/B7jml/N+txzxv6WpUA9R/5VkAJ2XMG8WLw9OftrDDFFFHW/z4QXisbwWAoseESi/lqDLElW8sErb71vusUnDjIIVGd2lbnOVPOliaB1LYbZWtt9tguAGeKcvN7gRtwJiyB2o4TT9TmLx8M+/Fzk4gDx57pA3NR1vp2jukjInXe0YAtrebLD7FMOlTpXxhybVaH4RZD35EeaxuSd+wFPjIj9q26lVtfMOwBcTLlm32yUufIddBs0Nmw3PGzJopn1Hme4g0wbdiuReO3c9kaJD4TefNCB+Qj5HxmJaXYpPFdHTxTnC4unlR5eTuxaNgF1o7tm/MNlYldmefpp6g4FLaDnSYILqtNr85AqlK3f9INUZ5c5MMcUKMSK9kVRTEvF6NxYs81kkEc22VKMj/rGm1tBD7MQ7A7QuTgWP4rRT3ZfkXaxEl9PN5EQ6ynmEUo7YOhfnJS2RcomLp/egxF94bT3K7fpyZkxJ4C2QPZU3c56XJXkKu0LBpncA6EFYaPUULZpHnmwWZJqsozZ46AR78zuNvRlnoqeMwdpronONSBqPJOAsAeCpM/X6IwVPNxUDCuGEiXasCDSWN4H5kfK/2ROAQ7ibnl/MsXvRNoIHQ/xN8Dskw29K2Xttk5dFRibcPgsAzw7VUdnbM0A+a4chx8LaLEHLqg/l3f094EQiynjwsrnnQbxsJoM7gdXUaoywhQU41Te4l4y1QfEe0WZtNXrxu8Nd7K+0gdiWXrN2x0ReY2x2o2/1KEP3RvToOgl7liWo7/2gVeeHoR1PF0aPWC2QH/n9vHaLymR2w6wyhlbtwNCUK020RJdu9fLVqOZ0QDQ+u2XTe6Cbk+ijhVIxMxnhui65DpRUCqAdq12Z9s/XTxG0WpWTzqKxxpVc4B7E2nHmqknsRHP8TGBnA74h2LiASrouVLNfxLS1A0ne4UQtMyu/mZS/pwzuGFxMW7s7g+BCrW83uwJDtIczxfFvrm6X0SzCApLlX7G4bU4zLcVKCCNDDNicpdJ+PA7iDbEeUdyiMJMOpgbdfVIL3+RYkse5MmFEyJExoqnZpS+9k8mhI7HoRbNKLNkMw76v26krUjQzJWqihfVzLPttsJ7OIaGqOUTfaRy2oJXMKQjJ+lZk2KBYnDH6pcFyTjm1ZGbF6xhFduH9biI2+3I7+pJ+SOSouHyLsqX97dLfPX8V/q/0S5UxYPqINg8TTL4V1MOOOrgNXM4BzI2AE7lB1fYJqb3mcKopGgJNsQ0rvxKekIYHTabgMPzDRGYdZaYQICxx2ddwqkhLI2iyRfkL3cK/qHKGkWaMPbmCOjcwpqlnS+HF/l9PDU3/I1NEr1J+vxJlDKGPix1HesJjF+eTyOv3Ufmoh26vnkcL09sY4p9wq/iNmfMaip6ODPlYHfJhiKmDjwBRcK1W5SxKbHM+eH49tGtqz2xgIJrX0SOdKb+LPeytZ0NInei5vBpvm8Rkb9mWe1yWG1o7GkEufENqaTuPB7cHQe3PoakAmNM1NLG7kn1GOvvKqBfkCxrMx//Gl9p8yc8Huk1liWsHEX39kpqtWbMYAUl7TXXxZfUelx41alazd/st7UBOWE/7I+i8GCSd7zE4Q7qMnmisArJ65UBVQngwlN7pbl7bWPGuZw/DXw08fP8mZA4WaeftaUpFMdYhYua7CdUng4wYReB1zwFqvYhEDJp+yBz3ab77cURd/OCXu3djqj3BMIjXBSnuLKDoPe7QNly0EzDJHi2oHTpjY3evxHP8ggkO4/NcptoXj9zwUeuNZ58pbknwSFrSL4ibCQXAyedfDW7RxdWfLS7xBCepZbhFnU6AH38HythVut6sI+sxXv66UYxgb44ogcoKkBHoW+XhQtx+wDu+hStT/CZIufl3mK7TBtbCo50daFA37bstQoBULpXHuAAaZMWbq8HswEgvP5bHgb3CpCsL99xvJZ58tEEae2zENQsmULw1fEQuD6Q4Xc8tWAbytGT1qmzoxBHLppz/w+5eaLQiLdtFSyyEbSP5D5EXHxrYenjsy1zkIwy4LAEmKzDz/YBeGVUQ85KR3mgK+lPRtP1TU/iuLs33z7/RB4XOJqs7QbUsdVIvoprKUBbrkOCRq53YqnQYpVZeMg4yea3abg20fvg0w8ezPBe4JaQ0lHuqnRs8w8Owr+VdxiaNgX1hZGCYVjg9RT5kWMR03Ac1tNlxAynk1VyEJ2TXTvA7MWsnOoyg+9MB09BpzZXR1ZZUO5QUSe/XKIe85l3o8IlJoFzPBR2tayI6bs0JfQYeWO5sSMgigOn63zaAZvMAGK3Y4i9m4PFWV9vYUV0sFNvb40u8e6W+OFj//fQZkxuMmqsuhfpVX1VKblnF8vLhvEn1INn37EwkzFxB3UG1DxHG/Gzq5XrEHjyYKaESz4PMLIGFQwFlHU9M9pOxiqf+Ez5wPSBPBJ7DZtH6HQULfRFiKQKoRuSZ11GUKEYVaGMQs5FXIrGmtExf1aa2disfQZ48SsXGaMGeJerPKTbKiDNKoEiVus3fS8I9/cTE/Siux6v/HCaSB6PJ9u9j6N3au2CMTrjtuJhEMl5q2DQLnV/b3kJiyB1X2R+dM05eOnRWEq8B7XWlhcNuiopoUN1kdK1rovn2bR1vFcEc/ARm8rRn7TQULUwnwAPFKDKTQ64E7Od2l9vQKSyeY2zsftFsPJgNjaUCe+WL08fjwR/OeyhpHiDXY5ymKp5fhjqGVHeYw5gRpLKsILlQCRK27D1INe/YTHlOf0WCFUCLtiqz81o/AcyWRiSoyuOLiKvFMRRoDQTgui1G8rOIr1b/C1JjxWT/mkupXdQLFKsf7I9NtFknvR/XjyQqTpoyL7JEQrhCQkMtCP0MEC6RseE5gvJnSTh1OaPdPTiAcz1eUfoz5kyXWETyG7mtDGwDw4xZkdir4elh/weV4ZyfeugtruC37qSROSTo4c5eC350bOEOCDnAqvxssqXYx8iQ7jIzPXO0sbocfG+hvKABLVIwUtC6L//fp3Py17AvKjnVFep+xDeq/QAGo1a71rN9H+5lKj4lEzNafHwHgb0BmXxFPHN/Uuaj00HgBdiZbcfsdSUGk+SKT5HVYoNxLYrF0lhPHH2F0zqg7Gi/m6hhh1VzjZoRqJrx0fcMDW/EhSKzqPxI1WHZ0kJ3Z4jMMR/sbxqMBNXIo2PnQlD4BaZGx5Ea48gicODdUxsiN3tmqgkLNNE2woZZfsaK45nvn/P62MO83aLjgQEUN7xd4es9UNT2aV1Lxz8xO3kHvekduO+q126iPZu8buKl3vDW+e6zN7GXb1G7TeMV9vf1u93WjOjz9enw81i0nW5ktm+IUOuo8HW79Sm/wB8Wxz3Mow+a5H4SPy0aZdlXevEzq8sScLSKvZpUX8OTq1EJ3epMCx3qMrP307gbc4GOVUGB/QRuxzvQhD6W62b/Y6wZpeUz/32Bsg9ZcGHS8Wpgqo45s/epBvLUxxIHLSlOeQVyx0CnLdyHSC4ovr0kyOrsoe5swdbNbeua7ZHx7veLbx/IIGil3aqc2AJsdcX2oEsSR0wJn8KTgsG366JzAT+0SE+xS1bN0rgYP+DWBtjqGrFiOjfFm0M6qgvlzvlNGZibtV4AsW6duELOWjY6g9zoYkUKkXS0EEgNWn3djyvm1c3vXkyl00P09e/FJvcQfmlOx/TTICKHj4Tl5KIS66uzj9NK6URYMpNksYlCW2rdPgXz+hufBxGIkzV4c826Dt8eSnaOmQb/0jg7qmcUpE6Bl+azfPQ8wSuM1SDGxKefxlUb4VwKXGy+cCRzd+PmhVGvsw52f0WdwEIKoP79xLa+DlyCtIGO+j4NbavK2lTBsX8wwlHx+K4cY39CvU+F978UvNBq8GUP2GopeZ1X9U16gZ7Yu3tS7zLR2NiMgfHtcHH/XWKj5Mh1bVEjEIkcrOMKUv+1qxGBoeEDFwspBWDUmzWPkCNyoPkHK9K1lFOhDeIBeJO5B8Pa72M579HqssnOj8QcykUIF03Kw0fVZYV/lhEAKtEgA+ioVUevjWKxwdaFXd7bi+SOr2Gylt0+YDZ6PuqucJ2jkxJWUT3sNi9z9uQwGB/B0abu038rwVxFVWdMpwbasBwnRumf4hLSPjwL0nE9XQRlHvwhc3A1h257sepScB402nJwQQqbf1fFXxdQunkgMN9qgj8fFluxXsWmp5pEIHC00Iaw4ECUIZvl6DdCRlHOils6ExMB8awOrMX3kRhxtNCIvWoD+Prs7Vly0Iw+sG+hAOIiqBEuJLdiz/DkfWUOl/zpCSfXecJDzPxE9gPWOFjfE0Dp3htBb7cmT8DbJ2oA2+Qkzx72VDQK8wbY608RfhO9peqb6DohX3dkN0KVa70tiw6L6Urv2oqZGUkHL/d3kAWP0DxTvArMiTFTUGrXQHZECzvSjnVYZEmqVtLXrJKAJjLrozvY58Ic969DAL0kbdqVaOr2StKxb25Ts1Zs2E3WlVE5Owp1/3RBBcIH2DnPUsv/8ysxyg2VwBs+sUAhrCCLa0mcSEf9B8xObzqLPvz4NQ7lGY9bLWrTT/Fmd5gOtM3TiX7aMkIu4dfQfjQrRrrIIQkQVgZwGyZmVEQK5qz843B9TpH17+hNfzZUrvbRhEyc84E+2ULpYCtCm6b3Nu0aPB8LN+IdbXUuQkrzHf7az07DnwY6aLAaP/tA6hwK38a/eX8PoDJ3FU5Pkamv1mOokKg7bPcPp0PFinSu5w17zCBOq9vQCj8TPh5pIwJrH8JA/yFwyi/wefHtDIIYcT7mB5rkYhgWkINxrJy3KiYh81SJl3vOg3pp/WxS8pB8ngfe5USJ+HRm6sl7Wd/WFTE80/iw0Ltp14/tNiER8M9EgtArizTuXgBjlpmp6W2R56ayjYxWO7dmJQb2owShM8RCvNMbPtFZRmKO9X9ie+aH3XVa0oxO8ShFHEK+4MaoLTjil/NN5a6lz7Z5CNWNSal/XcnurgGpFiM6Gwect/84gLLowXyJKKNiipq/xZ3L6fEUy/PQXbMhzP8ojFei1oLpNjaSPykEMV0eG5Wcqew/1Vvo/4vHeIUF4Yobe8AoB07HxsITNPPfwCkX2kZVlipgn9dSuXwACo5cGxMCZ7Y42dxYWD/tq+j8FJQiKCCwGie5/zzQtZBWZl40mp0+AWNG2LKSGY7QUu06Shx/M66c4pnuMvcx4JdXs06QRsNgznN3G7NwgJmekOt+hNPOCvN3R6G7ucRCJff++rOWZMBuY09GTsVh677h1vnw8Cboa0dxHUl083Skxmw4kVNQe+iEa0KiMb73dnlOu+oWbil4tdALY2eyTqkoKuE+Gi9eBVWBt4h30+rfJhMP2Qi+Ecvr/ms1AhPKzQttgSqWnMyFHvtPK+6NjOa/ilURSkkkRCR9t1xcywa0SK53g1GmtxQ5J5JRoO0NlQR+S2rQ2g+vRwosupiVypjGdVccYEu2WZMFB5yVMfbY+hYKUTrRk1GUWMHSm1tVRi+rX1B98KO+IsSMTc4eHb94zGRH64r6X9DwrH9GuU3B9ZRSZvX2qy6fbdyLfnIHsmaF8fwaHYa04s6HUub3Wr03YFDmune+8M9Q7O/seND1fDVxAcyctewTU+8y7YcRPX0GBJerTSQ8oG0KNpsE3dNUO0ODeU6U5lXhGOx/vmavUKWQTQ/K4/KHdgkyRa0zGdcGcXVs/YqT0Fl0zxjbm27zxTvFUHbvr5WjDcHcLhJiRStR8VHY8EF2xWQHBnJNQttZx4p6TEXR5R2wnE1UfG8lgVF7NXaucUbD1tmiH6DCB3MmoPt39E3/tyAuU+VVKd5wY+KmyyuOCe5i5hJLpH4F0AUx9en9nxOsaO4Fa7nd0WprEXiV3LjzWjkuY526AW3iTUKvJXX6bMiupBh40LS5cvpTjn/bHvLIJFRXwBSJKpAlBY6Lg63u3inlovSfndD8fxQyzjie6tk0iU9QGLoVvb8CvJ+xCU/Lt74QKqgqKBuEcZPF6pLNGW6VnfJAdWc5BYqXY9XLqe1unG4rKQEl1EV36HZulWCckyS43ZWUc9yDRID8MAioyRX5nyVSYPSDzDqn1plQLE7jeYfDSiiRbn5UbQ7uRDOQjoBho7HwNyQJHdinlb45DvhMok10Um3A+6kI03PM3UMqIGg1+DOQzCo+jCRuhuyBeaOaaPJzkN9AgJEzYbJ/Yz4gCm6981t931nmfc39mPebA5CGEtC7VG7O7Unbv1eS4zgE25/S2Sm7nwPXfi4zQAEvyKz8XCNU3CFXrlaGGbfJ8EQKAoio3AaLhx4Y0QYj7bJ9f8jalD/FetKXIeISAd+w02fvrjnQBZfTQmMovId5j5DYMh076RkZrrspemCRzdg6rAx1YEFNYC8GshDPTaph5jSiOl1JuG2hDHZAOJ9SRr05GVw73xLwMBqu+ktSlB4upBflF+fgpaINoWq3V/gUavYM08CtWvtG1LDQMH6T5mBH4mm37pS9PYiEX1QtbiOnyHpu+V2Wx8VjEkzbt9a9tK/Tw58LkionTV+9gLih1cX7D6UqadeErS0y4D6CN/Y91YhLq+aIjmnsuivRGZ/d1fWnith2Bmjex135xrwEyEm8m5bOFDsGqd6AeZiEBTGJnqcUBMRFIIK60RWvY2W+fVCTlyQJDFcX5SBCSJkORLIdectWad10dZax6Q5WAjKjGhEYRf07lmOoy37sQQIIZOUDRC0YY2gSjukqfGKm1U21ZTpkmWwk2qB6Smm6QmMUAU6ClhQqoATBmotfLAMoRIyAvOeDFzULQphXkvaZ3g076p4yJuyaiVl0hfPAu8qmK6I4RzJKlh9jr8dcBYE+6ZFt8J3Yv8fVGKXgZ7ASTrESiPqAnY3e6XBGrMpZjcjJpqpDOthayyxbSlsFW86B4pFuKamgW2pGHoEwYntx21plIskts0CvRKCeYo1Eo637gKeftxHYD+FtIdsy9Q2BixfdVe3hXpjPfesKY/k+BKtaklNqEaItxCThV+uCJd7iNt3XhzPvCT5cZX1OH/MKOn9O+meryAvwZXIoECZzHwxNKH/1lnhIyYUjpzoYlTg4d2F8f79R2xKy0FRmxOIO/a8fXIRE2fQ2eGlAznbMUlHapNNY0Rq36anwb/kEKPG/ZIlntUmp6UapSOd8u6XMQ/dWTIhZTvSAwq8EabVGDFEs+Z6IMgYj+BXOiLV9bbwkaCEy1Wb60/W5WUnfrdqgH9iAB88b/WSOFeY3K2BieByp5ejCsQIVo0Kp5OJ91YIYlEfd49MY50z17v+u5j7ff9sE4fP1YMSFhGzSL57+VoVaxrNNw8FXKFwzBFejf3MLsKd40X91D+srfXtZFOWutoi7B7ZTXBYkt/smcG9ynKecp5NcF5cOc2XHUZTj5spIfnnQSyo6c568uD6wDI4izE7tPukZDlLhtiesLnK7m7DvwtOZFdafC3IHBhe5CrqVtdFa4t9s9xZL+LwfPT3v653cWHFOPARnETd4tEUjwZnnK5E7mwTpzzqLaBbqz6PYr5SPZujiB5XUKnCEAJZ4/Yncx50f/OhIIIFMvEH9WBTExThveJVCs23sa4GnG1Tltq4F4HUhNOnrfVJzH6mzN2BH25Y5uJb8hiLhzcTKI0rZY3gmbeZTZq643E0YhRywIoDwESGjuIeuGV5wHOOAN41CcrWZWOYcqMN5roNTg7cyXumLr5AyFH4uLaxfEZ0ThdOD/EtP2KuwCSy+AVO2c5w/SbxCPWQxypzX4KuNN4vi//J/3X86eQkECCgNKoou8IBtc8ZYN7TQeqs37AfLyDSzEBIy8xLwqmOb7QxO8buhSB/pmJGJpeUKCFW7JFHXOFNoqWRgEVrAn0/FNID3zZCZ8EfwTxOA4OwVI436r6T0qzKNU9F+JVyjSP60l8Z7lsOtyhM6AwisZNKxbBPjq6TJdKO3lOdbQldFYB9HB/wdGF1HaseRywX2+mqsuMxvR7hBfjoJT0169VVBYosCHWt7VRljaZ9MEPAbvPxuWEl4BcdTcS8m72lNaVK+15PJc/vwxwbsqxUTwHGR/Mk8FP3Wgy4xsR6QmfaFf6Elc/HH+xQsJDnUyXOGvMyot3ELDPGMi/GplGLcYlWfsy2wK/BK1zDZhii4kas2PBS3RxitazPRVgLKRKI1kTHep0kdgKAJOi8RzhdUE3jmllNHcgzr64KKvoHPeu/oRmqIMMTJmC5Lo2JXbd8TGYHyEzNhKnscEAoU0y7pZev1QlvxRX7uQ81W9C381MVlsIEfpjuzbGeRCtKLvQEZILViWQZBi531Nppr86QLsE2hEazdfRFVN43Dd3BUn43sViHtAN/v9bikOKjUuNbVWxOcPqzNWOf2zgDwH22DUTNzvmUhFfGkxUrICH7CdTI85OUw6bOGbzpO2zX+fkc7Outzw9qkDDCnCjgcm9Xg3+nMQpbq3+lLmknvjbierdD5iD+MBlEfJVKlhzpi6sEdt22cvndOvyB/DCoDYQUAITzziGVAbkqV/8LmPoq4zxw1eWgQ/v1fOVUCyikMO+vGL7um7mQ9FmvRGIpvPE2AxD5msrbMhjjYPieXNhhkALcKGTUOAYhADzhEfzTXNBS6mu60gbbu79sOCsIQpyawFAaidhZwMHIuwIUlKUEGZhhhp5ddgpJ7s5KMRPw8/jC+dutNpCAwQW3TlHqPvduJKFel01d+OQpTG8jyci3Bc8ymWJyNa5Nd9KjkKwNsA9ex+B5yr9r2jFqHa4t1UCUXi/fClBZVbfNdIo1U2IW4bD/2UlRASflLyMwbgeBPM5bmMeHWC2wzar5Sr+JPcVORjfiuiuqGkxO4iAkLZpC4rAqHN2zvhqBWtRYIcHgiXFFv9af5TMk87rc+EILythNmtq8KC3SmQd0EZhe1/VdBLMNoEFzmiWrLiuO/5WlvtzpYiGfb2PKt3rmKO1Zg8LENGM90I+OsAZV7a+1ASrd25JGzZn4yB9NtRnf4I2Otytfg0tFvgSCDOr8ScpP9ecncSdwPRPdAQvcT0UPbtwY8LpYBaZq+2GV8hiobUHGvtyPsMVGej3a4VcgpOPyCeEpYPj8Vacv6DJNvTepWM6rZGWhMwGXNJTGQUcaQorgfqEFGSKxfo//rShR38IWk8S1yvBGjNvlOJBcA9GkJuVG25m/JLkUL7mmSG0nNktXgIrEQarQ6A3GB/4s1ieztJ0OGCjQ5zAEJTdkP07XKpSClQLP2qVGkST94qBGaAfeS91nMvdveHVsvPMneE8wWbk9KAIRmEEZyqoXiA8Ocmx4OIrHfoqWmrefASqLPG5EjygEdexkF0/oKyfPm3fQg78TAdhBH7H9M8WJOb2Ewp9ZLljZmI9gARBAg7uapEYuDCo7trzT0apzyWSHcT0tZmosUQXVIeX8aW9SqPDNASbCwvFc4II7HPQjAKtVQNK58hy+oIXnHt4/EiNPMS2MUVoBAD6wg2KyAZFscWgBatkmJLd+CeAUVMTUHpclO3XamZ6ZPgOgGvOwUkIWMLIZKbRRsiLcfWFUTwMQYht62nPGSl2ZQe83ETmY+PdodCeyQs6jzrIPrP504iL4V7INMg9OxtnT8OfVcrM0G8A0Ju7WqwkS36SiPKr3jCLrFArwKM832tqe9M4GQc9LXvabRYaREOeP2WCnL6iDEe0kqABSC1ZXvfkk7wGYz0aZbYUr7vgmmqA8LqFUfGX0Yc4qzPggmuED9MMmRfgyyAd+wP7IhDX3spQkWaqH+ImHogXIj9CtSnHxh00J0fHhuozAS5kQ2STcYRph66conrlsYIjN6TUqrPgebyuSLniLM23rUYrm8iYTvyEFfJ4PjHgU3oZSsXalsKkTOIE62UVfye8mdURHPJjqwvPMvYhE6FSbCBFe3zDZV/enopfGLvYkx55tKyuPgUDryna91nxYUkG52UyBMcgxMTAMnRNidn0gfzj+KylzSZri7oYJ+xX0d3aT/NmWS6Gg8S9ScTF7piHz3vbkq/jM7L9fAEhuTHX45YIdioiMVsYC5po45j6LJLWVCVPj0LmmcQ9FQVzx0LJTNQElQUCGo12U9pS+MKlquZUwwxrRYt6gK9XzAmw0xRRrjQ4Cns6/G/V88pm8xdetmqEZJxtoG4TlYXCZmn3gM1IjjYL2zjfsXayEJ88JolBqwmL8WrKmE+9I4O4irLTlsIgFybjQhVuvg6LADE3DatZWTVeiDmBHy/XVP4QdLpc20E87776buIQmz4f1fkdmrNpUoaCg/xQVAnBcxXQfYD6y7rN9dUi8KglyrU9xsWGzAGEA0OmmxZnoKwUBjOzCdMig5z8S9bivXRVWXxfE6mGV5tHDCy2BAa18j+DYcjwHnHb1i+hYTTCUkrUWrJ+RJnOLKnVNKwDT9xvPwcMxeVp86zcsqHn9jLRs2xctYW0X6spANYSqgL+8ZZ+MXyHgKKrozUTwRZ2By+AEPlrgJ/HYD4t0QHWnabHfMm88XLfuIeSDJufRlsRN0727axnn/BlSvXO/pFUPwf5S9L7PnOuPW3jFa5ZrRWbsIVsljYx22SgBSn0yHIT4uXdEDdyj1sQEDsn0H7DW7PBgqYF0ezz5NdHZKxbpsxZLqzq5LpfAsfiA/cR8K5FX/S7egt/nGeLWlrSU/60Mf+SUWWc6YC4dhJxkNxeEBlM5ebA8vY6fNyl2iA6WaIdFNY0j8+OgihTCge/T/SMECtQ0DGHV4FPTOvbgveda1enwze3KQQ+uTV0m7y162KP2IQamT2UGDaj//7Oik9mLq1mwcRAHgKBfcahWoN94zTvgBPANvkV7eYLKuf8r9BIwn/2hnYWHL/AkRlbP3talaTQeKTchHFdBZKl/Q1PW4oAgsyioqo8ypBLdRBJrfd2TAbjhKFHamplrKL4DDHBEQSh9aVN1Om8bwCYBerifb4RwDe+QrcZ+UhjdwK/62p7tJYQGe4u9otlkTmc+G1PkGnleS/u5LymRs+7k0HKQP98vEkeBQnF3BEYfmxH9su1MQN0thD0O6NTdWpBl1+f8iVtlWsdty0/IDndxOKQGgvKl3chbtkaC+3R2OowYXhQlyls1bdEjPaWdbZzGkCtVHPIU6nrZ9lmAFjV9uq3O9Ogy5cA1OFG3fW6fR+UAg91ui1zp7a03CtGmNlaGYrkQpAyFCB17hl1JQYZLnVUfYcXAyV/g0xLeGX0iwjHpBqHL/Tqa2Dci3n7dUN2O97x+ghoB9OOD14d+CUUf8Vq0cHqhmEwqKPt6n5ykTT0ps6GkX8hZfi7isx4m0L6osaowHvXhNTxdUhKvzM6lwqvr3DfewyWxSa4w1OHb3QTmf++d1WrywGJitXSDyDhHd1rrWPmCdy9DIumFN4GInd8dgv2IAtpVenYeImRhMfTFayI3+vKCpoQUDPfS5BK2Bl1qvmh0lASp/OXjCzEeuFDZY/eM7VxX2LoRSC3S3EzfF5VwcyutxI421At79L0jmqBhqtIiyx9LCIfxxc3GwRaTUq7RNkqhWQMqIbpfSLW7RCQfvCPkvkfYQt0Raaq38s9fkzUVcLMrfCVVVxC81y5347MF15q03rxDWoMqdl+JqGKk55QXnOL0vciiNExFpdHyV8r5oWpugC2naSn4D58TcsgolxMHxHbEULKrN9Fk6UkRfvLzCRz2FwB3/iDbbNE00qFl6ppCwzS1cJ/UKeMNyOx9jDDTC2nNzNvP5ezNXb4W3bV3PX1shm67QZ40F+MHAK3FZ469XtGgTTyTaoOmYyFShRpbhtNy99tYhO3K8pFWqy3Lun9SNZaH3EraRtwljPWyWwjDbxPVsRJFvbEylk2pqI81XGpREbi0fpJfvgXTyhsmvbKRwNy9uVMNalvH13kMxa8t9gzpXe7TDtryg4e/td/HtvUTm67ZQuepAsE6U2iyYhhh7q2HWuojJ10QePKP15G//IzkbaLZ9arlBuWJEuOLfqLKpEbdLNEMs6yr3a9ON9SFBMooxIXjXJOblIU0L+C6nDFNxv3KNbVfOaZxyK+vFNuasqi7fY+xhe8JzN8onZ1CxyPwMSVITr1Dyd5+W9mdsW79u2eBxU0uFtzS3IsP+d3fgWv3evCBGWO3CwiIFqPnXQ9vAJEzxz9JS/jqqvPB3+nG57vpDF3ISmsF1Wg3LiEJBal1Fa2FCRfTwp6aAdY191iSqrgvEDZFG+zTsDZp/3n+sjf8CGu5EoqHvbw6+tZ3XWvC3M5Un5OWQAkme9x6KNCPHWSwOhq/OHX5ZKTCO52ZlEyKet8+2C7Hy4oq34JTS2Qs3GnCLbL6MDg5Q3Uw6RppNL8ftBElaLP/vy+1skOPsJoSjPNQbxgeaJQ/Bwg9f/HcfoS6C3r1CLrGKghE1bsiogBkPB9qsKE3zrEDCJxfed0WPxc8B3VaPY38yEWzZLFZA4I0mkt9AinuKuPAKKKmqZfx6Ig9C7cYas+wpkAOxN50F80Qv51PkdRzUct+cctXpSEeukSZwN6CQw8vdXkZJJDev7YsvgFMT+A7AYQHNXhAp7Lcip+0pHiduhv3r7AjTNrlYhOatPPlf8oIXSzRZMt+OuTBZwnE2RKm8y89R7PbO6o62u0zflm9pA3oOubAn/JlMCxwZhQ30+St4F0AbDwOMF/6OXqJeHNbs2nTbBF+xOvjgxoBBnaoW4OfShAnVi4HgB4C7wqxiQ4htsspShho0x+1YWLuZu6cZSg9QbzR/I5+J26el07LhXtHBn8aABn/9rcRwDubctqnUjNsDkocEipq4IBzvOioiHEnCEqBTe4hzNa0yEFSDtXp+HKd/5pXWH4wPMgA9EbK2RuKPnX4pl5a1tApmps8y82uSKdOtsbDEi05Iy5Q+V9wltSExyhXbxN2NltvUH2Oysp8zlaEfE5JnF2ft6RnwBn1H3gtTu07wBhSSPn9YiXzuR0TXpWH1kcHPM0spSG6T4+Lh/jMCVocztpYnFCkpp12fnUAfQSUBtKUArpncCJA2G5fERCWvJyVZ4BLSgdAFPMDEbuI/PI2H7tJqUobDrasoWbcQSHG4VH877PxNGhObRtg2X3w3T1yIxSuXHmZRPp0PexS68Odj7BxYIpYRO0AujbyY+TJNT4THhdqPBD+V+Uo1z0Kh+L2SzOsagB0c/dh9KbPbcqVXfewC/zUCfjqbP+h5RxLoOcYWAYqGS1baHgYQHGgLJevKmJzq4fyjbMVWS1G/f/7j2Uri+myZLk9T75SlDqy4lVzdhAYeAr/odRlEklcjlJFE56ux1Q/oj4nMfUcyvXTT4LDUDPe44doj2dRvF+tcPtoxWrMVSg94AVdkQdpWUwBK4UQffnIFRe1Do5REdq/y2nX2bGduwJXqFoXrB+OUBRa+MxcgivrSC5B5I7VEmdTr1+BHlNFcylPp8KYMCES4jSS4Q90xJTFJnnvt++o4PMtXMem0S35299KH3ULeZRfThRdVH+I6SRVSjALTOs9HDLwAlWUVQ+diPAANHsIAiMMBdQKsXUg1C68S5FgwhGyGjL1dNTvMR6sREL2aDKmMErcqNfQ+Pn/11sjMJZoircHW2jz/1b3vu6j70x6FCXxcHbgNJXWeNUQze7FZmN6Yw9Cg1Ueyi7i0g3Ck4xG/SuaY4thVyQ7WqJ8rTE0B+hWqwseNqr+b2z6TYgguw/ZEgWDsoyLPuHl+3Bj19kQRyJziuOSkUDh+0Q+3oi9K2FZqFLqtx35pMu9jV/NeTE5SXlWlTrrYnTICGu029YfQ2OU1qq7Mb3eICKsRznnKjDAvSPRXKUaFE137R/jNjIK/Qgx60/NeC/bn1iqT/Y4S0D5x2KWfQvQ1v732Te5/gdp4mXxRfZTLSZe8CRt4+Ed9zUCjn3k7bNwHYEmMmVm9RxvlEDMQa2nu+WnRPPALjd9ICxShvE3uJULIv1iZK8JlpDWkXdlONgDQTyswICxFeKsnKIU83TC9uiWsxB9woPpRoJ/C80VUrJlr29+9PLw53KOXtK8zWiG1PRg6EWg4KpJShomp8N1WTggoXFJcmkI2HNgCndc6M/CSZgoGHeyWlEuwFzVoPSzF9CqfcYul7wyiVPY91zo85xe5CdVhbG/mQG2L0NboH9xelFrXfGDCGBBXPpqhFJw0xqHsAtmk3FLLIA5ce3LH2uSH0oeSOyOmXWxcWjqwoNnTIVwyXX6YA6nvUWw7mLN44a/3E+/7Hk3tw3l0vD9xn9tVxzm4Div3ARsjLfibBw/Vo/laXpNcfaKzXg17ljzhY2o/5wavPLf8IDZgXo4Ixc8g8oWrDaNiGr+G4J03qIfwPAAoTNlmmdnzG/NGKroX/g/pDoIDYKnZ9n+IkafRiqZSgwjrDvY3A3emn5/ctNAtft0Hewkjh8PpigFRBkqm1dQtFUPb5QtHqHgX6DaO7FjVQKJX3db+oMsu/5vYp/aKYVW0/DlFhl7pHtOJ/sbuq05TiLqdtDMhz3eHHoDosurYOdZwFe9DXfwzFoKQLFbTcu+8ycg9FW7LOyx4gBrjXQqTHoqTeYBXHrDfBrBdofC55E8mq42zYXerrwRJvxSnmLPuYQ6gu0PIvFfn4QgvctbXcxYh7hIDdHXy95pevLa//yesNwZvjAQE3y8OgTl695gQX39lMDqzda4d6eH7NpW/dXAn1w4doWpdetG9fO8rfIeYu5rsteL5pqCWk++HpY1kmhKUEt35mKPtARdZeKYMDfzrusXE8pYco7otg8/U9C6LW1gxpGof35d5UV20C7+uyRljBDe7XtGk+RwN3FoN3LEjRhBpr/XOXCdo1bPC6G+v3q/RD4xmq6Afg7YX9+8ik40uDd6D8GVUpl7GW2QCTQRC3ANHziT7BfgEATX+N1NIhrMwg4IRiZkcdr4Ye4dZEca6Gj4dNdyMmUHYTDFvB+zUwZbPCykiR7PVmuU7k/5MbfBH8ra6ewKSobAN91xDH4EiCLLO5myMUR8FGNeH/vtMMJ+vNKw4S1msUfc1o6GnyRqvAfLpLyn1on4gtPjNOWVp3TwNcy2GInbIF7cZNY0byS8PA3BJ2B/AK1OVoUeo2qHDtIBSnWeMa5o3vhJyKBJTU4ZduXVOebDVDqpcpIGWdWTPYuc6OvX7RpM8YbZg2drab0O+USibtS22F71bRwjke28fNtD0+jsItaKFc79PQ3U4XJfw68yOzULV7mfRFNIUP8pxafi8Pbic2e5pOQLLUhoCSm8EdPpMmUL53NX3ZiIAizAkIcfn/xesG/HgJgp/VB75fOYtj6RU784nuIZZSCjJv6ezNYgfbH8+XTF/RZB+7oEZ9Nr/FgA8GgkDL0jggpxv/N0JdaGuyoxZS2l3vvfDkh/sELgaHFvaGvQE+KVSnum6qMs2lDu2zmn/kMv7RI+m4yZXIz4x10Zfu70KHQ+im6Jwb+HAq8Bfzvj8DPJ6/TzFn5wLa1ISnhjepuxEfEQayvvWnk7ogjaicAF7ylV7s+BhQ+sRyz1PmesEdR7qyX7XA3NqF77S01SG1Yx52OKNnxkbZovdFh7Wd+EKbasyzYhPqF8RAxaEDuDvCrHOQkOpXeAcyuhIWwdu5HMQic4PHAbBuL891Pdm3qSN6ZH8eoMfu/g4UR628a328G3MUaL4gHMKI/VE2IcDy9aki8AT4b2yDoBUs54GLxA3yMfNpLi/+3RlTKKDXj5qEDTR1g/zGhQY07DVjF6AE8KZkA0k8h6wuR3jmcXN3ASWStxZjiRohVsz2hp3pOJI42m45E6hTrhJdTSgFCIb6ZZhUK0WrMplrHj6oXzFR6VhmDdgWsnHluGEcIGu4JmUy5fQuL88Fn+ot9k/7laVREaxwRN4BXNFXiVLq9grLMLZqMzuVjshzEBoN4sYZh3pnu/Lu5EIAsGYMFQBwq0ZiwuRttRhdgHwkcrgZIDBYVuhSEyNE0E8XhnnHB0spoP0zSxStpfZnY/xmftvufhKpYsmKaeZ4+tZiipHCuS+UQRe26/ifNgR5Muf49T9drWPRdwf47pKouyiBYvslZ3e57DPJyUMNfVZBZkj8ASXzKmUMJ5WcMl8aQu/Bbu1Rn2QSjEY56e4o73SYKJfNO26VMREUbtNhOArDHbDUNxOPJACiqnfpT/ZKmas/EJQhMWhw0Hr80t+OJHZJLiq+QlmAcLjXQu8UaZh4+VW6rI39Z5fvM1gst68fDEdIr+Xz6rDtpzO25RUJ+OP1F4W9ttUK1IpYU/+G3XjxmOAUFhwmUZKF0U+h/F5NDbp+e9rqfCUCUpkvp6TfFejunEan8F0cLbQYYKx6QcpbDnUPOTuKwn49XTm+/MXfneYJBs9I7IPatwP1bv8JnpfwS/HNEdWUl2ENEdmJam6chBLCPmRIUof89w6ZYgWWHIwyWsnCdjNmiSTmhGXs2B/lRnlo+/jmOn7zzmxBvt8ZipuwOjrvvD+CRVf+OocJRPqCskEq6+jO1QUQ75bwH5wRlueLjXKAGSijDhMvLZ2jUjTuFkEm7Vw41B5sRYFC/xIHE7OP/fhREBSbCCkTNNrIPLXMLHeOGS9zdMSP2Emo7jogtK05xVa7Cd15PF6pwqWdOwVOXiuByoj3hD+hh6l/2tvRvZpQodrhPbuDcOkZtwb12b2bLWeNLv0UpStc9dLfL0ljvc09SCckGxfC3imOE8lCVl9TYlm7UOlyCGGFxzKmt579aDNzEWiOzeSK8846thanfgr1EJkmFS4zStYwZR/26iZlSbeFF2Hgm2LrGYm/f18bZTK6BcJsgF3ThPIcjHReMDQb3iub0qmxZc7WGBxN+Q05AM8KfLS5Od6jfB1cLyD4q1XLsFQTGxUfIIg9sPRFq4jzZ68jQGLRWnUqTh0w50wqG8sxJAdX2vHFAuP4cJYtbWhb82OXCoQCrTZHQzBwnHoASutTC2v0m47hozi8FuW/oNpcu9yyD39PLXag6RValKG76bX10AmCbYLSAUq9cJ+SkpeazmWetSn26ezDX40NQJT1nRzrIX0dljCEtaV4SM8t5i3qNBVGdoj8q6VuHjBcGjhX2xF2mtPGQ1QCRKWDMRj7WGqEbi4wbbvPnaFx7M9e/PKMCZ7cNgXzl1vNjIcQ40IhnvHD1aDlOVSaSmpr0oG18/ALFOXcY/pCgviZ1VIOv7CzIlr3PepDkMhOaKxBIUPizWAlpHDTvQEZ0+dMDseHM5XAMTodN4k8pS14HRlLTnuPATPoBp5vKsSsY1ix3cGOZaB/w4urSM1PCAERGMkR6KW9BiIOFJLyX+XRireQsA+WvV+Af/BFP3M3J6E8pTsTbK7NWphF/A9oYVk7cp1VB+vSS2bUCvSAJm2vrAhI1b4020jv5ngV6fZxiARCZIv8yZfa26rJEpvaL8y5K8hycrNj6T6cBL7pOYpBuC46Bb5ku69Izygb/+cyR3pezz3svKK2zLeJ2eJl3f/4ewPHx/mzfiiDg0/VnARhM6LAtNY2GRSKB2bEIh//MV6aOg466J6csTYv/zF7H1svXhztvH1Pwz+UForV4NFU2Tn5/VnVrJ2G0nQoXWIQx+LzvMpA4W/zM1/6lc0ZI2zOLMCBGopUqNAD5LNJpSSctg78WIVZxbMRi7KmQRJxXKlgwyF4p9PM03TZ+3dQIRXFBRpAF3V+hedLykoqcLJ+Znau9kwoM9UvUU9yRXyUwGFGqbGaMd9Vui3f3XqjYn3vH+yFLA6WTcn588KjUhjKSdwKbPkkcyG3Y2hngqMmAOCL/LXqe4Z7xg/5gFZUvh5vuL6khhck0sENNirfeDU8ZIiBylFOt9VQZt5/LHqS3aubgZmAFh1ful9IIokm0S7IE2mR3aCpm2SpO8GFu5vkSvlMc1lgTXGLv0lO4bkkj38T7Q46EVAadiFDnEVUEpQ=</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Task-Oriented Intrinsic Evaluation of Semantic Textual Similarity</title>
      <link href="2020/06/29/Task-Oriented-Intrinsic-Evaluation-of-Semantic-Textual-Similarity/"/>
      <url>2020/06/29/Task-Oriented-Intrinsic-Evaluation-of-Semantic-Textual-Similarity/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>语义文本相似性（STS），为了评价设计的 STS system 的性能，通常使用 Pearson 相关性进行比较。</p><p>在本文中，证明了依靠具有Pearson相关性的内在评估可能会产生误导。在三个基于STS的常见任务中，我们可以观察到Pearson相关性特别不适用于为该任务检测最佳STS系统，而其他评估措施则更适合。</p><p>In this work we define how <strong>the validity of an intrinsic evaluation can be assessed</strong> and compare different intrinsic evaluation methods.</p><font color="red"> 即本文来分析metric 的有效性</font><h3 id="key-words"><a href="#key-words" class="headerlink" title="key words"></a>key words</h3><p>intrinsic evaluations == metric</p><p>an STS based task  == a task that heavily depends on the output of an STS system </p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>下面的图中，四种不同的图，计算得到的pearson 系数都是0.86. </p><p>即通过仅比较皮尔逊相关性，所有系统都将被视为同样良好。</p><p>同时也可以看到，<strong>Pearson</strong>相关对局外点敏感，只能测量线性关系以及两个变量需要近似正态分布的局限性。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gg89uax2zvj31fs0sl41x.jpg"></p><p><strong>Spearman</strong>的排名相关性不使用实际值来计算相关性，而是使用值的排名。因此，它对异常值，非线性关系或非正态分布的数据不敏感。 但是，大多数STS系统的内在评估仅报告了Pearson相关性。</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>本文还提出了一个 <strong>predictiveness</strong> 的概念。由于，STS system 不会单独使用，常常会将其应用于下游任务。对于那些STS system 上得分高的，且在下游任务上得分也高的，称其为 high predictiveness。</p><p>为了探索各种metric 哪种更加适合 STS system 的评价（适合==high predictiveness）。本文用各种  metric 来评估14个 STS system。并使用spearman’rank 来计算，这些metric  得到的STS system <strong>得分</strong> 与 STS based task本身任务的<strong>得分</strong>，这两个得分之间的相关性。相关性越高，则有 high predictiveness, 即，该metric 更加适合用来做STS system 的评价。</p><p>通过下图，可以看到，Pearson 作为metric 并不是很好。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gg8a5wby91j30wb0qmte1.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Score_Videos</title>
      <link href="2020/06/21/Score-Videos/"/>
      <url>2020/06/21/Score-Videos/</url>
      
        <content type="html"><![CDATA[<p>Video: 8iPflOxQaao_000018_000028</p><iframe src="//player.bilibili.com/player.html?bvid=BV1Xf4y1y7AP&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: UArdunmwEdA_000049_000059</p><iframe src="//player.bilibili.com/player.html?bvid=BV1mt4y1X7fE&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: vTSO26j_g3E_000006_000016</p><iframe src="//player.bilibili.com/player.html?aid=286089586&bvid=BV1Rf4y1y7nN&cid=203899418&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: gf37sAjEfRc_000013_000023</p><iframe src="//player.bilibili.com/player.html?bvid=BV1Bz4y1Q7Ai&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: GXO1eYu4kr8_000000_000010</p><iframe src="//player.bilibili.com/player.html?bvid=BV1JT4y1J7R6&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: saXahlRV7s4_000022_000032</p><iframe src="//player.bilibili.com/player.html?aid=286068284&bvid=BV1df4y1y7oD&cid=203899246&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: Hgo7xkutPno_000059_000069</p><iframe src="//player.bilibili.com/player.html?bvid=BV1Ez411e7QD&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: NRC5oMoNHn0_000003_000013</p><iframe src="//player.bilibili.com/player.html?bvid=BV12t4y1X7tM&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: IbDcOoO9m6g_000139_000149</p><iframe src="//player.bilibili.com/player.html?bvid=BV1ni4y1x7xP&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: tGiBJQ5RhZQ_000077_000087</p><iframe src="//player.bilibili.com/player.html?bvid=BV1cV411k7yR&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: _YWpv2_K8Pk_000054_000064</p><iframe src="//player.bilibili.com/player.html?aid=456121030&bvid=BV1D5411W7zB&cid=203800090&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: KvVRY61JS7A_000023_000033</p><iframe src="//player.bilibili.com/player.html?aid=841065180&bvid=BV1U54y1B7ga&cid=204027776&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: VNXpHy5Tb_U_000064_000074</p><iframe src="//player.bilibili.com/player.html?bvid=BV1if4y1y71S&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: dC2Ih_JFoOM_000000_000010</p><iframe src="//player.bilibili.com/player.html?bvid=BV1cD4y1Q7BX&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: lcUJjVgqXp4_000000_000010</p><iframe src="//player.bilibili.com/player.html?bvid=BV19V411r7FF&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: uh50grtCQSA_000026_000036</p><iframe src="//player.bilibili.com/player.html?bvid=BV1ma4y1Y7HQ&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: cb3RvnukQVs_000159_000169</p><iframe src="//player.bilibili.com/player.html?bvid=BV1vk4y1z7GR&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: 5D4HjS92zSQ_000126_000136</p><iframe src="//player.bilibili.com/player.html?bvid=BV17i4y1G7rb&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: kRdxCmOsY2Y_000003_000013</p><iframe src="//player.bilibili.com/player.html?bvid=BV15A411i7Cg&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: _PdB7OGolCo_000191_000201</p><iframe src="//player.bilibili.com/player.html?aid=838500851&bvid=BV1Eg4y1q7fp&cid=203798508&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: 38Jn4r_pcpg_000167_000177</p><iframe src="//player.bilibili.com/player.html?bvid=BV1Xt4y1X7L1&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: 53Ia-BtpM5A_000002_000012</p><iframe src="//player.bilibili.com/player.html?aid=841116130&bvid=BV1X54y1B7pu&cid=203898322&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: 7QyQVBclhT4_000000_000010</p><iframe src="//player.bilibili.com/player.html?bvid=BV18a4y1Y7Bn&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: QyMTEHd-VCc_000004_000014</p><iframe src="//player.bilibili.com/player.html?bvid=BV1v54y1B7X4&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: t-COcCPV-T4_000020_000030</p><iframe src="//player.bilibili.com/player.html?bvid=BV1ff4y1y7xc&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: oGpVHf4xooM_000160_000170</p><iframe src="//player.bilibili.com/player.html?bvid=BV1Bp4y1D7NJ&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe><p>Video: WU5KZsG_mQk_000483_000493</p><iframe src="//player.bilibili.com/player.html?bvid=BV1XC4y1a7Cf&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Score_sentences</title>
      <link href="2020/06/21/Score-sentences/"/>
      <url>2020/06/21/Score-sentences/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Instructions</title>
      <link href="2020/06/20/Instructions/"/>
      <url>2020/06/20/Instructions/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>数据集收集过程--各论文汇总</title>
      <link href="2020/06/06/%E6%95%B0%E6%8D%AE%E9%9B%86%E6%94%B6%E9%9B%86%E8%BF%87%E7%A8%8B-%E5%90%84%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/"/>
      <url>2020/06/06/%E6%95%B0%E6%8D%AE%E9%9B%86%E6%94%B6%E9%9B%86%E8%BF%87%E7%A8%8B-%E5%90%84%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<h3 id="Stanford-Natural-Language-Inference"><a href="#Stanford-Natural-Language-Inference" class="headerlink" title=" Stanford Natural Language Inference "></a><font color="red"> Stanford Natural Language Inference </font></h3><blockquote><p>A large annotated corpus for learning natural language inference</p></blockquote><h4 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a><strong>数据收集</strong></h4><p>从Flickr 上的human written captions 作为 <strong>premises</strong> 。</p><p>（1）AMT workers 手工写 与 premises 相对应label下的句子：（不提供图像的前提下）Asked AMT workers to supply hypotheses for each of our three labels entailment, neutral, and contradiction. 即得到<strong>hypotheses</strong> 。 (这种label, 称为 author label)</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfhjx642jrj30k90pk441.jpg" style="zoom:50%"></p><p>（2）基于得到的premises-hypotheses pair，再由4个 AMT workers进行评估，从三种类别中【entailment, neutral, and contradiction  】中，选择一个label，基于此，则对于每个premises-hypotheses pair，有5个label，基于共识，为该pair选择一个最终的label（称为 gold label.）</p><blockquote><p>5个label: </p><p>(1) original author 算一个，因为他是首先在给定promise 和 label 的情况下，写出的hypotheses</p><p>(2) 根据 给出的 promise 和 hypotheses， 来给标签，【entailment, neutral, and contradiction】</p><p>共识：如果有&gt;=3个人给出同一个标签，则赋予为该标签</p><p>但是，若没有这种共识（2% of cases），则分配一个占位符。（但是，这种数据没有什么用，无法应用于NLI任务）</p></blockquote><h4 id="Data-validation"><a href="#Data-validation" class="headerlink" title="Data validation  "></a><strong>Data validation  </strong></h4><p><strong>为了评估，基于上述的数据标注，得到的数据是否可靠</strong>。又从整个标注的数据中取了5%，再次由AMT workers做如（2）中的标注工作。看看前后两次标注的结果是否相关。这次AMT workers 选择的label 称为 Individual label 。得到如下的统计结果。可以看出Individual label 与之前的 gold label/ author’s label 有较高的一致性，即，之前的标注工作是可靠的。</p><ul><li><font color="red"><strong>yaya</strong> </font>个人觉得，这是一种马后炮的行为，没什么用，因为之前的标注，已经完成了，钱也花了。这种验证，即便是验证结果不好，那也没有什么修正措施。</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfhl3kdkdtj30gj0na42d.jpg" style="zoom:50%"></p><p>表格中还使用 Fleiss 来计算human annotator 之间的一致性</p><blockquote><p>Fleiss : <a href="https://www.jianshu.com/p/f9c383b39859" target="_blank" rel="noopener">简书</a><br><a href="shiyaya.github.io/2020/08/06/评价者之间的一致性-Kappas/">yaya blog</a></p></blockquote><h3 id="Visual-Entailment-Dataset"><a href="#Visual-Entailment-Dataset" class="headerlink" title=" Visual Entailment Dataset  "></a><font color="red"> Visual Entailment Dataset  </font></h3><h4 id="数据收集-1"><a href="#数据收集-1" class="headerlink" title="数据收集"></a>数据收集</h4><p>该数据集是在 Flickr 和 SNLI dataset 的一个简单集成。</p><p>任务是，给出一个image-text pair，希望model 预测该pair 的匹配程度[Entailment, Contradiction, Neutral]。</p><p>数据的收集：在 SNLI dataset 就是基于 Flickr30k image captions 构建的，</p><ul><li><strong>Entailment</strong> holds if there is enough evidence in image to conclude that text is true.</li><li><strong>Contradiction</strong> holds if there is enough evidence in image to conclude that text is false.</li><li><strong>Neutral</strong>, implying the evidence in image is insufficient to draw a conclusion about text.  </li></ul><h4 id="该文提出了几个构建数据集的准则"><a href="#该文提出了几个构建数据集的准则" class="headerlink" title="该文提出了几个构建数据集的准则"></a><strong>该文提出了几个构建数据集的准则</strong></h4><p>基于在SNLI, VQA-v1.0, VQA-v2.0, and CLEVR, 这个几个数据集上的经验， 这里提出了四个准则来开发一个新的数据集:</p><ul><li><strong>Structured set of real-world images.</strong> The dataset should be based on real-world images and the same image can be paired with different hypotheses to form different labels. </li><li><strong>Fine-grained.</strong> The dataset should enforce fine-grained reasoning about subtle changes in hypotheses that could lead to distinct labels. </li><li><strong>Sanitization.</strong> No instance overlapping across different dataset partitions. One image can only exist in a single partition.  </li><li><strong>Account for any bias.</strong> Measure the dataset bias and  provide baselines to serve as the performance lower bound for potential future evaluations.  该文中提出了一些单纯仅仅使用</li></ul><h3 id="WMT-Shared-Task"><a href="#WMT-Shared-Task" class="headerlink" title="WMT Shared Task  "></a><font color="red">WMT Shared Task  </font></h3><h4 id="用户界面"><a href="#用户界面" class="headerlink" title="用户界面"></a>用户界面</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfijyrn8qsj30zw0gvwl7.jpg" style="zoom:50%"></p><h4 id="Human-judgement-quality-control"><a href="#Human-judgement-quality-control" class="headerlink" title="Human judgement quality control"></a>Human judgement quality control</h4><ul><li><p>每个标注者，每次HIT任务：给定100个 （reference+ candidate）pair, 针对给定的reference, 评估生成的candidate的好坏。</p></li><li><p>100个pair中有60个用于quality control，40个由participating systems 生成的翻译组成。</p><p>（1）这60个pair，是官方设计出来的，包括三类，repeat pairs (expecting a similar judgment), damage MT outputs/ bad reference (expecting significantly worse scores) and use references instead of MT outputs (expecting high scores). 因此仅仅会有20%的资源消耗：bad reference; good reference</p><p>Specifically，先从正常的MT system 中 得到30个 （reference, MT output）pair，如 table 5 中的 original system output， 然后1)对1-10对，进行重复，得到10对。2）对11-20对，将MT output搞破坏。或者是对reference caption搞破坏，得到10对。3）对21-30对，取corresponding reference—&gt; (reference_1, reference_2)，得到10对。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gepxhtqcpgj311h052abz.jpg"></p><p>（2）within each 100-translation HIT， 每个articipating system<strong>等比例的贡献</strong>a（within each 100-translation HIT, the same proportion of translations are included from each participating system for that language pair.  ）这是为了确保每个参与的 系统含有近似的相同数量的评估。同时，这也从三个方面得到了公平性的评估：1）每有一个workers做一个HIT, 则就会为所有参与的系统增加human judgement。2）不会轻易受到worker个性差异的影响，因为每个worker都会给所有参与的系统进行评估。3）尽管DA判断是绝对的，但众所周知，判断者会根据观察到的总体翻译质量来“校准”他们使用量表的方式。 对于每个HIT（包括所有参与的系统），这种影响都是平均的。</p></li></ul><h4 id="Annotator-Agreement"><a href="#Annotator-Agreement" class="headerlink" title="Annotator Agreement"></a>Annotator Agreement</h4><p>（1）由于 bad reference pairs 的质量应该是显著偏低的，通过查看人类在这类pairs 上的评分是否也是显著偏低。来过滤掉可信赖度低的human assessors。</p><p>set（A, bad reference） 与  set（A, translatin_B）这两个集合上的人类评估，计算一个p-value， 若p-value&gt;0.05 则说明该human assessor的可信度低。</p><p>（2）对于 repeat pairs, 查看得到 repeat assessments的程度。</p><h4 id="Producing-the-Human-Ranking"><a href="#Producing-the-Human-Ranking" class="headerlink" title="Producing the Human Ranking"></a>Producing the Human Ranking</h4><ul><li><p>Standardized </p><p>为了消除不同的人类评估者的评分策略的差异，首先根据每个人类评估者的总体平均得分和标准差得分对翻译的人类评估得分进行<strong>标准化</strong>。</p></li></ul><h3 id="VIOLIN-Video-and-Language-Inference"><a href="#VIOLIN-Video-and-Language-Inference" class="headerlink" title="[VIOLIN] Video-and-Language Inference "></a><font color="red">[VIOLIN] Video-and-Language Inference </font></h3><p>yaya blog: <a href="https://shiyaya.github.io/2020/03/28/VIOLIN-A-Large-Scale-Dataset-for-Video-and-Language-Inference/" target="_blank" rel="noopener">https://shiyaya.github.io/2020/03/28/VIOLIN-A-Large-Scale-Dataset-for-Video-and-Language-Inference/</a></p><h4 id="数据集收集简介"><a href="#数据集收集简介" class="headerlink" title="数据集收集简介"></a>数据集收集简介</h4><p> 该任务是在给定 Subtitles  和 video 的情况下，推断一个statement 是否与 video 相符合[entailed (label 1) ，contradicts (label 0) ]。</p><p><strong>positive statements 的收集</strong>：给出 subtitles + video，然后annotators 写出与其想对应的 statements。</p><p><strong>negative statements 的收集</strong>：（1）要求annotators通过只更改positive statements 的几个单词或短语来编写negative statements。（2）进行<strong>对抗匹配</strong>：对于每个视频，从其他视频的陈述库中选择具有挑战性和令人困惑的陈述作为否定陈述。具体地，对于video_i/j 已经分别有其相对应的 positive statement H_i/j , 则通过查找与H_i 最相近的H_j 作为 negative statements。 对抗匹配的方式可以消除 human bias的影响。</p><h4 id="数据收集-Instruction"><a href="#数据收集-Instruction" class="headerlink" title="数据收集 Instruction"></a>数据收集 Instruction</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfil6timz7j31c60van7q.jpg"><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfil6tix9ij313e150na4.jpg"><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfil6tj9shj30ts14mtl2.jpg"></p><h4 id="用户界面-1"><a href="#用户界面-1" class="headerlink" title="用户界面"></a>用户界面</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfilt0g35qj316o128n4z.jpg" style="zoom:50%"></p><h3 id="VCR"><a href="#VCR" class="headerlink" title="VCR"></a><font color="red">VCR</font></h3><p>yaya blog: <a href="https://shiyaya.github.io/2020/05/16/From-Recognition-to-Cognition-Visual-Commonsense-Reasoning/" target="_blank" rel="noopener">https://shiyaya.github.io/2020/05/16/From-Recognition-to-Cognition-Visual-Commonsense-Reasoning/</a></p><h4 id="用户界面-2"><a href="#用户界面-2" class="headerlink" title="用户界面"></a>用户界面</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfioadgc3qj30r20pbn3e.jpg"></p><h4 id="Crowdsourcing-quality-data"><a href="#Crowdsourcing-quality-data" class="headerlink" title="Crowdsourcing quality data"></a>Crowdsourcing quality data</h4><p><strong>Automated quality checks</strong></p><p>在众包UI中加入了一些<strong>自动</strong>的检测，比如，workers 在写 question、answer、rationale 时，有单词数量的限制且必须要指定一个detection</p><p><strong>Instructions  </strong></p><p>鼓励workers，编写的question，是比较high-level的（需要一定的推理步骤），同时不要编写一些general questions, 即不针对image 本身的那些问题。</p><p>同时为workers 提供了一些例子展示。</p><p><strong>Qualification exam  </strong></p><p>再进行正式的标注之前，先做一个测试，验证该worker 具有标注VCR数据的能努力。</p><p>The qualification test included a mix of multiple-choice graded answers as well as a short written section, which was to provide a single question, answer, and rationale for an image.  </p><p>在完成这个质量测试之后，由发布该任务的requester(即VCR论文作者)来手工看，这个worker是否具有资格。</p><p><strong>Work verification</strong>  </p><p>查看 workers 编写的 question、answer、rationales是否符合要求。由于这项检查工作，工作量也很大，因此，将这种检查工作也当做另外一种 HIT 当做任务进行发布，由那些outstanding workers in previous annotation work 来完成这项检查工作, 每为 一位another worker完成了检查工作，将得到 $0.4</p><h3 id="Composite-dataset"><a href="#Composite-dataset" class="headerlink" title="Composite dataset"></a><font color="red">Composite dataset</font></h3><p>From: <code>From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge</code></p><h4 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h4><p>该篇文章是做 image captioning 任务，为了更好的评估该captioning model 的性能，将模型生成的预测上传到AMT，让人类去评估。（为了对比，除了评估our_captioning_model 的预测，本文还评估了gold-standard description 和 the output from a state-of-the-art image captioning system.  ）</p><p>该文中从两个方面进行评估：相关性和全面性:  <strong>how much the description conveys the image content</strong> (relevance) and <strong>how much of the image content is conveyed by the description</strong> (thoroughness)。</p><h4 id="Instruction"><a href="#Instruction" class="headerlink" title="Instruction"></a>Instruction</h4><ul><li>很简单的 Instruction，也没有给范例。</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfiysqscgfj316r0e943t.jpg"><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gfiysr0uf8j317i0g2an3.jpg"></p><ul><li>没有说在数据标注过程中，加入一些监督workers的操作。（这主要是本篇的目的不是为了进行数据收集，而只是简单的去评估 captioning model）</li></ul><h3 id="Reinforcement-Learning-in-Image-QE"><a href="#Reinforcement-Learning-in-Image-QE" class="headerlink" title="Reinforcement Learning in Image QE"></a>Reinforcement Learning in Image QE</h3><p>From: Reinforcing an Image Caption Generator Using Off-Line Human Feedback  </p><p>该文是引入强化学习，以及human judgement score, 来做 image captioning 任务</p><p>为了评估提出的captioning model 的性能，将其在数据集上的预测结果，提交到众包平台，让人类进行打分。提出了两种评估方式：Single-caption evaluation  和 Side-by-side caption evaluation  </p><p><strong>Single-caption evaluation</strong></p><p>给出image-captioning pair,然后，让6个不同的raters进行打分[good, bad], 然后对这6个评分取平均，或者是取majority voting，作为该pair 的最终得分</p><p><strong>Side-by-side caption evaluation</strong>  </p><p>给出 (image, our_model_prediction, baseline_model_prediction) 这个三元组，然后让raters, 选一个更贴合image的sentence。<strong>并且，从三个方面进行评估，信息性、正确性、流畅性</strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>图像描述-评价指标-中用到的数据集汇总</title>
      <link href="2020/06/04/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-%E4%B8%AD%E7%94%A8%E5%88%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B1%87%E6%80%BB/"/>
      <url>2020/06/04/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-%E4%B8%AD%E7%94%A8%E5%88%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<p>为了评估 提出的metric 与 human judgement的相关性，提出了一些数据集。这些数据集，包含image-text-human_score, 通过利用统计学分析 metric_evaluation 与 human_score 的相关性，来验证提出评价指标的合理性。</p><h1 id="Caption-level-Correlation"><a href="#Caption-level-Correlation" class="headerlink" title="Caption-level Correlation"></a>Caption-level Correlation</h1><h2 id="Flickr-8k-Dataset"><a href="#Flickr-8k-Dataset" class="headerlink" title="Flickr 8k Dataset"></a>Flickr 8k Dataset</h2><ul><li>website: <a href="http://academictorrents.com/details/9dea07ba660a722ae1008c4c8afdd303b6f6e53b" target="_blank" rel="noopener">http://academictorrents.com/details/9dea07ba660a722ae1008c4c8afdd303b6f6e53b</a></li><li>论文: Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics   </li></ul><font size="2"> **数据集介绍** </font><ul><li><p>Cite: Framing image description as a ranking task: Data, models and evaluation metrics.  </p></li><li><p>对于图像描述任务：该数据集包含8092张image。训练集6000张，验证集1000张，测试集1000张，，很奇怪。。数据加起来对不上。。每张image 都对应有人类标注的5个句子。</p></li><li><p>对于图像描述评价指标任务： </p><p><strong>测试集1000张</strong>，<font color="blue"><strong>通过image-text retrieval 算法</strong></font> 检索candidate caption，为每张图片从整个测试集的语料库上进行检索（检索的数量没有固定，像是根据检索结果阈值截取的）。由于是在整个测试集的预料库上进行检索，则，也有可能检索到自身image对应的groundtruth。This dataset also includes 5822 testing captions for 1000 images。  </p><p>得到这些新的image-text pair，对于每个pair，再由<strong>三个人工</strong>去标注image与text的匹配程度( give a score from 1 (not related to the image content) to 4 (very related))。</p><p>则，构建了一个可以衡量metric 与 human judgement 相关性的一个数据集。</p></li></ul><font size="2"> **使用 Note** </font><ul><li>在TIGER [1] : <strong>Because 158 candidates are actual references of target images, we excluded these for further analysis。</strong> <font color="red">在TIGER 的实验设置中：若Flickr 8k数据集中检索到了本image对应的reference，则去掉该条检索。</font>去掉了 158条，则剩余 5822-158=5664条</li></ul><font size="2"> **评估方式**</font><p><code>Kendall</code> and <code>Spearman</code> rank correlations reflect the similarity of the pairwise rankings whereas <code>Pearson’s</code> p captures the linear association between data points.</p><h2 id="Composite-Dataset"><a href="#Composite-Dataset" class="headerlink" title="Composite Dataset"></a>Composite Dataset</h2><p>论文：From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge  </p><font size="2"> **数据集介绍** </font><p>这个数据集是由三个数据集组成的。包括：testing captions for 2007 MS-COCO images, 997 Flickr 8k pictures, and 991 Flickr 30k images.  </p><p>每张图片对应3个candidate captions，包括1个human written reference和 2个machine generated。</p><p>这里总计有11,985 candidates, 标注与image 之间的相关性，from 1 (not relevant) to 5 (very relevant)。</p><font size="2"> **评估方式**</font><p><code>Kendall</code> and <code>Spearman</code> rank correlations reflect the similarity of the pairwise rankings whereas <code>Pearson’s</code> p captures the linear association between data points.</p><h2 id="Pascal-50s-Dataset"><a href="#Pascal-50s-Dataset" class="headerlink" title="Pascal 50s Dataset"></a>Pascal 50s Dataset</h2><ul><li>website: <a href="http://vrama91.github.io/cider/" target="_blank" rel="noopener">http://vrama91.github.io/cider/</a></li></ul><font size="2"> **数据集介绍** </font><p>Cite: <code>CIDEr: Consensus-based Image Description Evaluation</code></p><p>从 UIUC PASCAL Sentence Dataset中提取1000张image，原数据集中，每个image配有5个human written sentence。</p><p>对于图像描述评价指标任务： </p><p>在以上基础上每个image 又由 AMT workers标注了50个captions。以此构成了pascal 50s 数据集。</p><p>不同于以上的两个数据集评估image-text 之间的匹配，该数据集考量candidate 与 reference之间的匹配。具体地：对于一个image，（1）使用48 of 50 human written caption as <strong>reference</strong>。（2）剩下的两个human written caption as <strong>candidate</strong>，同时也使用 machine generated caption as <strong>candidate</strong>，另外other image 的 human written caption通过检索的方式也可以当 这样candidate可以当做<strong>candidate</strong>。</p><p>基于此，构建三元组：（A, (B, C)）—(reference, (candidate_1, candidate_2)) 根据(B, C) 组合方式的不同，分为四类：HC，HI，HM，MM。（1）human–human correct pairs (HC), where we pick two human sentences describing the same image. （2） human–human incorrect pairs (HI), where one of the sentences is a human description for the image and the other is also a human sentence but describing some other image from the dataset picked at random. （3）human–machine (HM) pairs formed by pairing a human sentence describing an image with a machine generated sentence describing the same image. （4）machine–machine (MM) pairs, where we compare two machine generated sentences describing the same image</p><p>则，可以得到 1000image × 48reference(A) × 4(B,C) = 192000个三元组</p><p><strong>human judgement 的标注</strong> 对于任意给出的一个三元组(A, B, C)。A 是一个reference sentence, (B, C) 是两个candidate captions pair. 标注者被要求从B和C中选择一个与A最相似的句子。这样就可以收集到一个human judgements for each triplet. 如果B的投票对于C则认为B is winner.</p><font size="2"> **使用 Note** </font><ul><li>解读如下的表格的acuracy是如何计算的吧！首先在PASCAL-50s数据集里含有4种模式：HC、HI、HM、MM， 即对于（A,(B, C)）中的pair（B, C）含有四种模式。当前AMT workers对pair(B, C)已经有了排序，当proposed metric也对这些B，C sentences进行评分的时候，自然也会有一个对B，C的排序，即score高的sentence, 排序就在前。基于人类已经给了人工的标注排序，即获得了GT，那么就可以去评判 proposed metric 对该pair的评分是否正确。进而可以得到对该类HC/HI/HM/MM的准确率。其实也可以在整个数据集上进行测试得到一个准确率。</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gcdc0x4s91j30ex04uaas.jpg" alt="搜狗截图20200229160306.png"></p><font size="2"> **评估方式**</font><p>Pairwise Classification Accuracy</p><h1 id="System-Level-Correlation"><a href="#System-Level-Correlation" class="headerlink" title="System-Level Correlation"></a>System-Level Correlation</h1><h2 id="the-2015-COCO-Captioning-Challenge-for-12-teams"><a href="#the-2015-COCO-Captioning-Challenge-for-12-teams" class="headerlink" title="the 2015 COCO Captioning Challenge for 12 teams"></a>the 2015 COCO Captioning Challenge for 12 teams</h2><font size="2"> **数据集介绍** </font><ul><li><p>Cite: The coco 2015 captioning challenge. <a href="http://mscoco.org/dataset/#captions-challenge2015" target="_blank" rel="noopener">http://mscoco.org/dataset/#captions-challenge2015</a>.  </p></li><li><p><a href="https://panderson.me/spice/" target="_blank" rel="noopener">spice website</a> 提供的链接：<a href="https://cocodataset.org/#captions-leaderboard" target="_blank" rel="noopener">https://cocodataset.org/#captions-leaderboard</a></p></li><li><p>use human judgements collected in the 2015 COCO Captioning Challenge for 12 teams who participated in this captioning challenge.</p></li><li><p>We report</p><ul><li><p>M1: Percentage of captions that are evaluated as better or equal to human caption,</p></li><li><p>M2: Percentage of captions that pass the Turing Test,</p></li><li><p>M3: Average correctness of the captions on a scale of 1-5 (incorrect - correct),</p></li><li><p>M4: Average amount of detail of the captions on a scale of 1-5 (lack of details - very detailed) and</p></li><li><p>M5: Percentage of captions that are similar to human description.</p></li><li><p>While M1 and M2 were used to rank the captioning models in the COCO challenge.   </p><p>M3, M4 and M5  are not used to rank image captioning models , but are intended for an ablation study to understand the various aspects of caption quality.  </p></li></ul></li></ul><font size="2">**yaya** </font><p>由(CVPR 2018) Learning to Evaluate Image Captioning中的描述：“we don’t have access to the COCO test set annotations, where the human judgments are collected on, we perform our experiments on the COCO validation set. There are 15 teams participated in the 2015 COCO captioning challenge and we use 12 of them that submitted results on the validation set. We assume the human judgment on the validation set is sufficiently similar to the judgment on the test set. “</p><p>大致是说，<strong>我们无法访问到coco test set annotations，因此，假设system 在测试集和验证集上的human judgements是一致的，所以使用val set prediction caption 和 test test human judgements.</strong></p><p>IJCV LCEval 采用的是和他们一致的思路</p><p>SPICE 是将他们的code 发送给COCO官方（因为spice authors也无法访问 test captions）</p><font size="2">**使用 Note** </font><p>计算system-level correlation, （1）需要为每个 caption model 来计算一个metric score, 这个分数聚合了由该model 生成的所有的caption 的 metric socre。（2） 然后，该captio model 的aggregate metric score 与 system-level human assessments之间的相关性被计算。</p><font size="2"> **评估方式**</font><ul><li>Compare proposed metric with others on the <strong>Pearson’s ρ correlation</strong> between all common metrics and human judgments collected in the 2015 COCO Captioning Challenge. </li></ul><h1 id="论文引用情况"><a href="#论文引用情况" class="headerlink" title="论文引用情况"></a>论文引用情况</h1><div class="table-container"><table><thead><tr><th></th><th>Caption-level Correlation</th><th></th><th></th><th>System-Level Correlation</th></tr></thead><tbody><tr><td></td><td>Flickr 8k</td><td>Composite</td><td>pascal-50s</td><td>2015 COCO Captioning Challenge</td></tr><tr><td>CIDEr</td><td></td><td></td><td>√</td><td></td></tr><tr><td>SPICE</td><td>√</td><td>√</td><td>√</td><td>√</td></tr><tr><td>(CVPR 2018) Learning to Evaluate Image Captioning</td><td>√</td><td></td><td></td><td>√</td></tr><tr><td>(EMNLP-IJCNLP 2019) REO-Relevance, Extraness, Omission A Fine-grained Evaluation for Image Captioning</td><td></td><td>√</td><td>√</td><td></td></tr><tr><td>(ACL 2019) VIFIDEL Evaluating the visual fidelity of image descriptions</td><td></td><td>√</td><td>√</td><td></td></tr><tr><td>(EMNLP2019) TIGEr Text-to-Image Grounding for Image Caption Evaluation</td><td>√</td><td>√</td><td>√</td><td></td></tr><tr><td>(IJCV)Learning-based Composite Metrics for Improved Caption Evaluation</td><td></td><td>√</td><td>√</td><td>√</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
          <category> 评价指标 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 评价指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
      <link href="2020/05/31/Sentence-BERT-Sentence-Embeddings-using-Siamese-BERT-Networks/"/>
      <url>2020/05/31/Sentence-BERT-Sentence-Embeddings-using-Siamese-BERT-Networks/</url>
      
        <content type="html"><![CDATA[<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>BERT 和 RoBERTa 在 sentence-pair regression tasks (eg: semantic textual similarity ) 上取得了非常不错的成绩，但是由于需要将两个句子都送入到网络中，这将造成计算量过载。举个例子，若在10000个句子中要找到最相似的对，则需要50 million的推理计算（需要计算一个上三角阵：（10000+1）*10000/2）, 使用BERT，则需要 65 hours。</p><p><strong>这可以看到the construction of BERT，不适于semantic similarity search、 像聚类这种无监督任务、information retrieval via semantic search等等。</strong></p><p><strong>因此本文：</strong> 本文提出了 sentence-BERT, 是建立在 预训练的BERT上的一种修改，使用siamese 和 triplet 网络结构来得到语义上有意义的sentence embeddings, 从而方便的计算cosine similarity.</p><h4 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h4><p>三种训练策略，现在只说到了一种，分类损失，，另外两种，是在哪些数据集上使用的？</p><h4 id="yaya-启发"><a href="#yaya-启发" class="headerlink" title="yaya 启发"></a>yaya 启发</h4><p>一、本文中提出的当前 BERT存在的缺陷，也正是 video-text retrieval ，这种多模态任务存在的缺陷。</p><p>二、We showed in (Reimers et al., 2016)[1] that Pearson correlation is badly suited for  STS. Instead, we compute the Spearman’s rank correlation between the cosine-similarity of the sentence embeddings and the gold labels  </p><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p>[1] Nils Reimers, Philip Beyer, and Iryna Gurevych. 2016.<br><strong>Task-Oriented Intrinsic Evaluation of Semantic Textual Similarity.</strong><br>In Proceedings of the 26th International Conference on Computational Linguistics (COLING), pages 87–96.      </p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读论文 tips</title>
      <link href="2020/05/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87-tips/"/>
      <url>2020/05/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87-tips/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h4 id="快速阅读：划分结构层次"><a href="#快速阅读：划分结构层次" class="headerlink" title="快速阅读：划分结构层次"></a>快速阅读：划分结构层次</h4><p>对于快速阅读，一个小的技巧是图文浏览。因为一些好的论文必然是图文并茂，所以只要弄清楚论文中表格和图片的标题和注释，就能够获得这篇论文八、九成的信息。</p><p>读者在读论文的时候也应该要有逻辑，首先要清楚论文中的表达是否是我想要学习到的；其次，我能从论文中学到多少呢；最后，这篇论文的背景是什么——是什么样的背景让这篇论文变得重要和有趣。</p><h4 id="仔细阅读：批判思维"><a href="#仔细阅读：批判思维" class="headerlink" title="仔细阅读：批判思维"></a>仔细阅读：批判思维</h4><p>以评判性阅读开始，带着质疑的心态问问题。如果作者论文中声称解决了一个问题，那么你就要在心里问自己：<strong>论文是否正确、真正地解决了问题？</strong> <strong>作者论文中所用方法是否有局限性</strong>？如果<strong>所读的论文没有解决问题，那么我能解决么</strong>？我能采用<strong>比论文中更简单的方法解决么</strong>？所以，一旦进入仔细阅读的状态，要在读论文之前对自己说：这篇论文可能有问题，我要找出来。</p><h4 id="创造性阅读：积极思考"><a href="#创造性阅读：积极思考" class="headerlink" title="创造性阅读：积极思考"></a>创造性阅读：积极思考</h4><p>问自己：在我所读的论文中，作者有<strong>哪些点还没有想到</strong>？如果我现在做这项研究，我<strong>能做的新事情是什么</strong>？创造性的阅读需要<strong>把你所读的论文和其他相关的论文建立联系，从而产生一些新的想法</strong>，这些想法可以支撑你进行三个月到五个月的研究。</p><p>如果你真正想理解你所读的论文，那么就写一个摘要吧，最好做一个口头展示，这样你会发现，只有把东西写下来或者说出来才能真正深刻理解。如果你能做一个报告，那就更好了，因为做报告的时候，别人可以问你问题，这会强迫你理解所读的论文。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gf9in4p85tj30f8088t92.jpg"></p><p>在做这个演讲之前，我曾经向我的同事、学生询问了关于论文阅读有哪些问题可以“问自己”，上面这张图片是一个总结，图片的上半部分是比较客观的问题，包括论文的核心观点是什么？主要的局限性是什么？代码和数据是不是可得的？论文的贡献是否有意义？论文中的实验是否足够好？</p><p>图片的下半部分是比较主观的问题，包括我错过了什么相关论文么？这对我的工作有何帮助么？这是一篇值得关注的论文么？这个研究领域的领头人是谁呢？其他的人对这篇论文有何看法呢？如果有机会见到作者，我应该问作者什么问题？</p><p>当你在阅读论文的时候如果能回答出上面列出的问题，我相信你会对你所读论文有非常深刻的理解。</p>]]></content>
      
      
      <categories>
          
          <category> 杂类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>From Recognition to Cognition: Visual Commonsense Reasoning</title>
      <link href="2020/05/16/From-Recognition-to-Cognition-Visual-Commonsense-Reasoning/"/>
      <url>2020/05/16/From-Recognition-to-Cognition-Visual-Commonsense-Reasoning/</url>
      
        <content type="html"><![CDATA[<h4 id="Visual-Commonsense-Reasoning-VCR"><a href="#Visual-Commonsense-Reasoning-VCR" class="headerlink" title="Visual Commonsense Reasoning (VCR)"></a>Visual Commonsense Reasoning (VCR)</h4><p>VCR: Given an image, a list of regions, and a question, a model must answer the question and provide a rationale explaining why its answer is right. </p><p>标注数据：为了避免生成式问题中评价指标的缺陷，本任务设计成 <strong>选择题</strong> 任务，即，提供一个image，一个question，多个answer，该任务要求从多个答案中选择一个正确的答案。对于correct answer：给定一张图片，要求AMT workers 写一个question，一个answer。对于wrong answer：使用adversarial matching 来获得其余的negative answer。</p><h4 id="The-Motivation-of-Adversarial-Matching"><a href="#The-Motivation-of-Adversarial-Matching" class="headerlink" title="The Motivation of Adversarial Matching"></a>The Motivation of Adversarial Matching</h4><p>在构建数据集时，常常存在两种挑战：</p><ul><li><p><strong>A crucial challenge</strong> in constructing a dataset of this complexity at this scale is how to avoid <strong>annotation artifacts</strong>. </p></li><li><p><strong>A recurring challenge</strong> in most recent QA datasets has been that human-written answers contain unexpected but distinct <strong>biases</strong> that models can easily exploit. 现实世界中的偏置</p></li></ul><p>通常，这些<strong>偏见</strong>非常明显，以至于模型无需看问题就可以选择正确的答案。</p><h4 id="Adversarial-Matching"><a href="#Adversarial-Matching" class="headerlink" title="Adversarial Matching"></a>Adversarial Matching</h4><p>negative answer的生成可以在correct answer上进行改造，但是这个过程非常耗钱，更甚，可能会引入annotation artifacts，subtle patterns that are by themselves highly predictive of the ‘correct’ or ‘incorrect’ label. 【1，2，3】</p><p>The key idea of Adversarial Matching is to <strong>recycle</strong> each correct answer for a question exactly three times — as a <strong>negative answer</strong> for three other questions.  这样每个answer 将会有1/4的机会是正例。这可以<strong>解决掉 answer-only bais 的问题</strong>，从而避免了模型总是选择 most generic answer. </p><p>在为每个image 选择negative answer时，希望<strong>negative answer: relevant as possible to the context/question (so that they appeal to machines), while they cannot be overly similar to the correct response (so that they don’t become correct answers incidentally).</strong> </p><p>因此计算一个weight，能够同时考虑到与query中的questeion相关度大，但是与query 的correct answer的相似性小。在本文中使用bert来计算相关度，用ESIM+ELM来计算相似性。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1geuprb6zmvj310305qwfg.jpg"></p><p>为了获得多个negative answer，需要执行多次的双向匹配。为了确保nagetive pairs是多样性的，在依次获得negative answer的过程中，在下一次从其他的image中查找negative answer时，需要遍历当前所有的negative answer，然后取最大值。（replace the similarity term with the maximum similarity between a candidate response rj and all responses currently assigned to qi.）</p><h4 id="Language-Priors-and-Annotation-Artifacts-Discussion"><a href="#Language-Priors-and-Annotation-Artifacts-Discussion" class="headerlink" title="Language Priors and Annotation Artifacts Discussion"></a>Language Priors and Annotation Artifacts Discussion</h4><p><strong>Answer Priors</strong>: A model can select a correct answer without even looking at the question. </p><p><strong>Non-Visual Priors ：</strong>A model can select a correct answer using only non-visual elements of the question. </p><p>这些priors可能是来自于现实世界中的偏置，比如，当问消火栓是什么颜色的，模型常常预测出，红色。这是由于现实世界中消火栓是红色的。 </p><p>又可能来自于annotation artifacts ， 人们在编写class-conditioned answers 时出现的模式。比如：标注者经常使用否定之类的方式写与句子相矛盾的句子。</p><ul><li><strong>实验证明，对抗匹配的方式，可以帮助消除 artificial bias。</strong> </li></ul><h4 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h4><ol><li>The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task.  </li><li>Annotation artifacts in natural language inference data. </li><li>Hypothesis Only Baselines in Natural Language Inference. </li></ol>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WMT Shared Tasks -- Human Evaluation </title>
      <link href="2020/05/14/WMT-Shared-Tasks-Human-Evaluation/"/>
      <url>2020/05/14/WMT-Shared-Tasks-Human-Evaluation/</url>
      
        <content type="html"><![CDATA[<h3 id="WMT-Shared-Tasks-—-Human-Evaluation"><a href="#WMT-Shared-Tasks-—-Human-Evaluation" class="headerlink" title="WMT Shared Tasks — Human Evaluation"></a>WMT Shared Tasks — Human Evaluation</h3><h4 id="Human-Evaluation"><a href="#Human-Evaluation" class="headerlink" title="Human Evaluation"></a>Human Evaluation</h4><p>两种评估的方式：direct assessments (DA); language pairs evaluated with relative ranking (RR)</p><p>但是DA相比于RR更具有优势，namely，对翻译质量的评估采取 absolute score 的方式。可以<strong>实施quality control</strong> 。</p><h4 id="Human-judgement-quality-control"><a href="#Human-judgement-quality-control" class="headerlink" title="Human judgement quality control"></a>Human judgement quality control</h4><ul><li><p>每个标注者，每次任务：给定100个 （reference+ candidate）pair, 针对给定的reference, 评估生成的candidate的好坏。</p></li><li><p>100个pair中有60个用于quality control，40个由participating systems 生成的翻译组成。</p><p>（1）这60个pair，是官方设计出来的，包括三类，repeat pairs (expecting a similar judgment), damage MT outputs/ bad reference (expecting significantly worse scores) and use references instead of MT outputs (expecting high scores). 因此仅仅会有20%的资源消耗：bad reference; good reference</p><p>Specifically，先从正常的MT system 中 得到30个 （reference, MT output）pair，如 table 5 中的 original system output， 然后1)对1-10对，进行重复，得到10对。2）对11-20对，将MT output搞破坏。得到10对。3）对21-30对，取corresponding reference—&gt; (reference_1, reference_2)，得到10对。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gepxhtqcpgj311h052abz.jpg"></p><p>（2）within each 100-translation HIT， 每个articipating system<strong>等比例的贡献</strong>a（within each 100-translation HIT, the same proportion of translations are included from each participating system for that language pair.  ）这是为了确保每个参与的 系统含有近似的相同数量的评估。同时，这也从三个方面得到了公平性的评估：1）每有一个workers做一个HIT, 则就会为所有参与的系统增加human judgement。2）不会轻易受到worker个性差异的影响，因为每个worker都会给所有参与的系统进行评估。3）尽管DA判断是绝对的，但众所周知，判断者会根据观察到的总体翻译质量来“校准”他们使用量表的方式。 对于每个HIT（包括所有参与的系统），这种影响都是平均的。</p></li></ul><h4 id="Annotator-Agreement"><a href="#Annotator-Agreement" class="headerlink" title="Annotator Agreement"></a>Annotator Agreement</h4><ul><li><p><strong>【bad reference pairs】</strong> 由于 bad reference pairs 的质量应该是显著偏低的，通过查看人类在这类pairs 上的评分是否也是显著偏低。来过滤掉可信赖度低的human assessors。</p><p>set（A, bad reference） 与  set（A, translatin_B）这两个集合上的人类评估，计算一个p-value， 若p-value&gt;0.05 则说明该human assessor的可信度低。</p></li><li><p><strong>【repeat pairs】</strong> 对于 repeat pairs, 查看得到 repeat assessments的程度。</p></li></ul><h4 id="Producing-the-Human-Ranking"><a href="#Producing-the-Human-Ranking" class="headerlink" title="Producing the Human Ranking"></a>Producing the Human Ranking</h4><ul><li><p>Standardized </p><p>为了消除不同的人类评估者的评分策略的差异，首先根据每个人类评估者的总体平均得分和标准差得分对翻译的人类评估得分进行<strong>标准化</strong>。</p></li><li><p>system  score ……</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning</title>
      <link href="2020/05/10/Attacking-Visual-Language-Grounding-with-Adversarial-Examples-A-Case-Study-on-Neural-Image-Captioning/"/>
      <url>2020/05/10/Attacking-Visual-Language-Grounding-with-Adversarial-Examples-A-Case-Study-on-Neural-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<h4 id="对抗样本的影响"><a href="#对抗样本的影响" class="headerlink" title="对抗样本的影响"></a>对抗样本的影响</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1genh1kz7svj30i40dpag7.jpg" alt="Fig_stopsign_2_small.png"></p><p>图1，在image RGB 上添加了一些扰动，结果使得captioning model的输出也发生了很大的变化。基于此，发现了两个问题。（1）我们的结果指出了在tested image captioning systems中的致命问题。（2）captioning model 中的对抗性例子突出了 人与机器之间visual language grounding 的不一致，表明当前的机器视觉和感知机制可能存在缺陷。</p><h4 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h4><ul><li>本文提出了一种<strong>设计对抗样本</strong>的方法</li><li>本文的这种对抗样本可以拿去用来分析captioning model 的鲁棒性-</li><li>本文，还利用 对抗样本，来做什么了，有没有做一些对抗性的训练？？</li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning to Evaluate Image Captioning</title>
      <link href="2020/05/09/Learning-to-Evaluate-Image-Captioning/"/>
      <url>2020/05/09/Learning-to-Evaluate-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>本文提出一个可学习的图像描述评价指标。</p><p><strong>Motivation</strong>: 由于当前的评价指标不是很完美，不能处理句子中存在的所有的病理行为，或者是说，当遇到某些病理行为时，则不能正常工作，比如，SPICE对字幕的语义很敏感，但往往会忽略其句法质量，SPICE倾向于对带有重复子句的长句子给予高分。每个评估指标都有其众所周知的盲点，基于规则的指标通常不灵活，无法应对新的病理病例。</p><p>因此本文提出，使用几种数据增强的方式，来扩展出很多的存在特征几种病理问题的对抗样本，并纳入训练过程中，使得训练出来的评价指标对于这些对抗样本更加的鲁棒。（即，可以识别出这些对抗样本的能力）</p><h4 id="How-to-Use-the-Proposed-Metric-in-Practice"><a href="#How-to-Use-the-Proposed-Metric-in-Practice" class="headerlink" title="How to Use the Proposed Metric in Practice"></a>How to Use the Proposed Metric in Practice</h4><p>由于涉及到需要学习 ，则评价指标的训练的数据分布 与 被测试的captioning dataset 之间存在差异。</p><p>本文解决: 假设要评估 coco  <strong>test</strong> captioning, 则将该份submission 分成两半，一半用于scratch 训练该评价指标，另外一半则使用该训练好的评价指标得到得分；然后交替，则得到了所有的得分！</p><h4 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h4><ul><li>(1) One direction of future work could aim to capture the heterogeneous nature of human annotated captions and incorporate such information into captioning evaluation.  <strong>Human annotated captions 带有人的个性</strong></li><li>(2) Another direction for future work could be training a caption generator together with the proposed evaluation metric (discriminator) in a generative adversarial setting. <strong>captioning model 与提出的评价指标，一起生成对抗的训练</strong></li><li>(3) Finally, gameability is definitely a concern, not only for our learning based metric, but also for other rule-based metrics. Learning to be more robust to adversarial examples is also a future direction of learning based evaluation metrics.  <strong>对 对抗样本更加的鲁棒，是基于学习的评价指标的一个未来的方向</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cross-modal Coherence Modeling for Caption Generation</title>
      <link href="2020/04/22/Cross-modal-Coherence-Modeling-for-Caption-Generation/"/>
      <url>2020/04/22/Cross-modal-Coherence-Modeling-for-Caption-Generation/</url>
      
        <content type="html"><![CDATA[<h4 id="现在图像描述中存在的问题"><a href="#现在图像描述中存在的问题" class="headerlink" title="现在图像描述中存在的问题"></a>现在图像描述中存在的问题</h4><ul><li>标注方式上：让工作人员标注出image 对应的text。</li><li>这导致的问题：（1）Unfortunately, such dedicated annotation efforts cannot yield enough data for training robust generation models; the resulting generated captions are plagued by content<br>hallucinations (Rohrbach et al., 2018; Sharma et al., 2018) that effectively preclude them for being used in real-world applications. </li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</title>
      <link href="2020/04/16/Oscar-Object-Semantics-Aligned-Pre-training-for-Vision-Language-Tasks/"/>
      <url>2020/04/16/Oscar-Object-Semantics-Aligned-Pre-training-for-Vision-Language-Tasks/</url>
      
        <content type="html"><![CDATA[<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><ul><li>现在基于bert 来处理的vision-language task 存在的问题：现在的方将image region features 和 text features 拼起来，然后利用自我注意机制以暴力方式学习图像区域和文本之间的语义对齐。（1）<strong>由于没有显示的region 与 text poses之间的对齐监督，因此是一种弱监督的任务。</strong> （2）另外，vision region常常过采样(region之间有重叠)，从而带来噪声和歧义（由于重叠，导致region之间的特征区分性不大），这将会使得vision-language task任务更加具有挑战性。</li><li>本文通过引入从images中检测出的object tags 作为anchor points来减轻images 和 text 之间语义对齐的学习。</li><li>本文提出了一个新的vision-language pre-training method <strong>OSCAR</strong> ，设计训练样本是一个三元组：（word sequence, a set of object tags, and a set of image region features. ）</li><li>Motivated by: the salient objects in an image can be accurately detected by modern object detectors, and that these objects are often mentioned in the paired text.</li></ul><p><img src="https://i.loli.net/2021/03/24/4CDOtMWl29nxbef.jpg" alt="搜狗截图20200416190213.png"></p><h2 id="object-tags-对模型性能的影响"><a href="#object-tags-对模型性能的影响" class="headerlink" title="object tags 对模型性能的影响"></a>object tags 对模型性能的影响</h2><p><img src="https://i.loli.net/2021/03/24/pisT2h5IxnCwj1z.png" alt="image-20210324105618554"></p>]]></content>
      
      
      <categories>
          
          <category> cross-modal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-modal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A negative case analysis of visual grounding methods for VQA</title>
      <link href="2020/04/15/A-negative-case-analysis-of-visual-grounding-methods-for-VQA/"/>
      <url>2020/04/15/A-negative-case-analysis-of-visual-grounding-methods-for-VQA/</url>
      
        <content type="html"><![CDATA[<h4 id="yaya简述"><a href="#yaya简述" class="headerlink" title="yaya简述"></a>yaya简述</h4><p>在VQA任务中，现在的方法尝试希望模型在回答问题时，同时能够关注到相对应的正确的物体（出发点：当模型关注到正确的物体时，能够更好的帮助模型选择出正确的答案）。于是，基于这样的方式，提出了一些方法 [1] [2]. 但是本文发现即便在模型中给了vision grounding 的监督，但是模型的grounding 能力却未必很好。那么提升VQA性能的真正原因其实是这个监督，仅仅是一种正则化效果。</p><p>作者使用了Grounding using irrelevant cues；Grounding using fixed random cues；Grounding using variable random cues 来说明，即使是错误的监督信息，相比于正确的监督也不会使得性能下降很多。</p><p>作者使用Regularization by zeroing out answers  来说明，给损失函数中加一个正则化项，使得training accuracy下降，就会达到正则化的效果，其VQA的性能与用grounding 监督的效果差距也不大。这就证明了使用grounding来监督，其实仅仅是起到了正则化的效果。</p><h4 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h4><ul><li><p>未来的方法必须设法通过使用与本文中介绍的类似的实验设置来验证性能增益不是源于spurious source.</p></li><li><p>创建一个数据集，使得能够评估  if methods are able to focus on relevant information.</p></li><li>Use tasks  that explicitly test grounding, e.g., in visual query detection an agent must output boxes around any regions of a scene that match the natural language query .</li></ul><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p>[1] Ramprasaath R Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi Parikh. <strong>Taking a hint: Leveraging explanations to make vision and language models more grounded.</strong>  In ICCV 2019. </p><p>[2] Jialin Wu and Raymond Mooney. <strong>Self-critical reasoning for robust visual question answering.</strong> In NeurIPS 2019</p>]]></content>
      
      
      <categories>
          
          <category> Visual Grounding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Visual Grounding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models</title>
      <link href="2020/04/01/Egoshots-an-ego-vision-life-logging-dataset-and-semantic-fidelity-metric-to-evaluate-diversity-in-image-captioning-models/"/>
      <url>2020/04/01/Egoshots-an-ego-vision-life-logging-dataset-and-semantic-fidelity-metric-to-evaluate-diversity-in-image-captioning-models/</url>
      
        <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul><li><p>当前的image caption dataset 存在的问题</p><p>图像字幕模型已经能够生成语法正确且易于理解的句子。但是，大多数字幕传达的信息有限，因为所使用的模型是在数据集上训练的，而该数据集并未为日常生活中存在的所有可能的对象提供字幕。由于缺少先验信息，因此大多数字幕仅偏向场景中出现的少数几个对象，因此限制了它们在日常生活中的使用。在本文中，我们试图证明当前现有图像字幕模型的偏向性，并提出一个新的图像字幕数据集<em>Egoshots</em>，由978张不带字幕的现实生活图像组成。我们进一步利用最先进的预训练图像字幕和对象识别网络来注释我们的图像并显示现有作品的局限性。</p></li><li><p>当前的standard metric存在的问题</p><p>此外，为了评估所生成字幕的质量，我们提出了一种新的图像字幕度量标准，即基于对象的<em>语义保真度</em>（SF）。现有的图像字幕度量标准只能在存在其相应注释（reference captions）的情况下评估字幕。但是，SF允许评估为图像生成的字幕而没有注释，这对于现实生活中生成的字幕非常有用。</p></li></ul><h4 id="SF"><a href="#SF" class="headerlink" title="SF"></a>SF</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gdel85qhz8j30se0g97eb.jpg" alt="搜狗截图20200401212845.png"></p><h4 id="Annotation-Pipline-and-Sementic-Fidelity-Metric"><a href="#Annotation-Pipline-and-Sementic-Fidelity-Metric" class="headerlink" title="Annotation Pipline and Sementic Fidelity Metric"></a>Annotation Pipline and Sementic Fidelity Metric</h4><ul><li><p>annotation pipline</p><p>使用三个预训练好的caption model: Show Attend And Tell (SAT), nocaps: novel object captioning at scale (NOC), and Decoupled Novel Object Captioner (DNOC) 在新的数据集Egoshots上进行captioning 任务。</p></li><li><p>sementic fidelity metric</p><p>我们提出了一种称为<em>语义保真</em>度的新图像字幕指标。SF考虑了两个元素：1）生成的字幕与图像中检测到的对象的语义接近度； 2）相对于检测到的对象实例数量的对象多样性。假设有一个最新的准完美对象检测器，通过考虑这两组（带字幕和检测到的）实体（即对象）之间的语义亲密性，当一个模型输出的caption中包含了并没有出现在image scene中的objects时，将进行惩罚。</p><p>公式：<img src="http://ww1.sinaimg.cn/large/006uWRWVly1gdelh0afpej3055021a9x.jpg" alt="搜狗截图20200401213725.png"></p><p>对于图像i，si是其预测字幕c i中的名词词与OD检测到的对象名词之间的语义相似性，＃O是O O D的基数，＃N是名词的数量（表示对象in N i）存在于caption i中。SF的范围为[0，1]：SF接近1的字幕传达更多信息，并且在语义上更接近于要字幕的场景（就字幕所涉及的对象而言）。</p><p>关于si的计算：Recent works (Mikolov et al., 2013; Conneau et al., 2017) show the ability of word embeddings that is transforming a word into its vectored form efficiently capture the semantic closeness of two given words. The SF metric uses this approach to calculate such semantic similarity between the noun words and objects in an image.</p><p>上述公式存在一个假设：that #O ≥ #N (Assumption 1) for all images. This approach to compute SF will work only assuming robust object detectors satisfying enough scene annotation granularity.  </p><p>同时为保证分母不为0，还需要一个假设：Assumption 2: #O ！= 0 (i.e., the object detector can at least detect one object in the image). </p><h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4></li><li><p>在上述新提出的metric中，如果使用SF执行字幕评估，则良好通用化且鲁棒的对象检测模型将扮演最重要的角色。（a well generalized and robust object detection model plays the most important role if<br>the evaluation of captions is performed using SF. ）</p></li><li>在物体检测器发生故障的情况下，度量是不可靠的。由于SF将无法惩罚字幕模型，因为它不能依赖忠实（即足够鲁棒）的对象检测器（＃O = 0，假设2损坏），因此无法应用SF。</li></ul><h4 id="Appendix-指标限制"><a href="#Appendix-指标限制" class="headerlink" title="Appendix: 指标限制"></a>Appendix: 指标限制</h4><ul><li><p>我们必须注意到度量标准的一些局限性，应加以补充/扩展为（1）解释字幕的动词和其他句法元素（当前只考虑了名词）；（2）根据解释的质量对字幕进行评分，并考虑图像中相同类型的对象相对于字幕中存在的对象的数量。诸如（Cohen17）之类的特定计数模型是有关如何增强此处提出的无标签数据集注释管道的特定示例。</p><p>应该在更有针对性的应用程序使用案例中评估指标，例如，在诸如导航，对目标用户（如盲人）的有用性。</p></li></ul><h4 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h4><ul><li>其实本文尝试使用一个open-domain dataset 来测试在 in-domain 上训练的captioning model的泛化性能。但是这本身就存在问题！因为，model本身就会受限于训练数据，因此这里却希望它有很强的泛化性能，这本身就太难为model了。<code>eg: 不能要求一个学了小学课程的人来做高中生的题目</code></li></ul><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ul><li><p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. </p><p>Distributed Representations of Words and Phrases and their  Compositionality. In NIPS, 2013. </p></li><li><p>Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herve J ´ egou. ´<br>Word Translation Without Parallel Data. ArXiv, abs/1710.04087, 2017 </p></li><li><p>Joseph Paul Cohen, Genevieve Boucher, Craig A. Glastonbury, Henry Z. Lo, and Yoshua Bengio.<br>Count-ception: Counting by Fully Convolutional Redundant Counting. In The IEEE International<br>Conference on Computer Vision (ICCV) Workshops, Oct 2017. </p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
          <category> 评价指标 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(ACL 2019)Putting Evaluation in Context: Contextual Embeddings improve Machine Translation Evaluation</title>
      <link href="2020/04/01/ACL-2019-Putting-Evaluation-in-Context-Contextual-Embeddings-improve-Machine-Translation-Evaluation/"/>
      <url>2020/04/01/ACL-2019-Putting-Evaluation-in-Context-Contextual-Embeddings-improve-Machine-Translation-Evaluation/</url>
      
        <content type="html"><![CDATA[<ul><li><p>没有认真阅读本篇文章，但是其中提到了尝试去拟合Human judgements，这一训练方案。</p><p> （1）We treat the human reference translation and the MT output as the premise and hypothesis, respectively 。</p><p>（2）Using squared error as part of regression loss – being better suited to Pearson’s r — and might be resolved through a different loss. Using hinge loss over pairwise preferences which would better reflect Kendall’s Tau</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BERTScore: Evaluating Text Generation with BERT</title>
      <link href="2020/04/01/BERTScore-Evaluating-Text-Generation-with-BERT/"/>
      <url>2020/04/01/BERTScore-Evaluating-Text-Generation-with-BERT/</url>
      
        <content type="html"><![CDATA[<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>我们提出BERTScore，这是一种用于文本生成的自动评估指标。</p><p>类似于通用指标，BERTScore计算候选句子中每个token与参考中每个token的相似性得分。但是，我们不是使用精确匹配，而是使用上下文化的BERT embedding 来计算相似度。</p><p>我们对几种机器翻译和图像字幕基准进行了评估，结果表明BERTScore与人为判断的关联性比现有指标更好，通常甚至大大超过特定于任务的监督指标。</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>在本文中，我们将重点放在句子级别的生成评估上，并提出了：BERTScore，这是一种基于预训练的BERT上下文嵌入 （bert）的评估指标。 BERTScore将两个句子之间的相似度计算为它们的标记之间的余弦相似度的加权汇总。</p><p>基于n-gram matching metric 的常见缺陷：</p><ul><li><p>semantically-correct phrases are penalized because they differ from the surface form of the reference.</p><p>解决： In contrast to string matching (e.g., in BLEU) or matching heuristics (e.g., in METEOR), we compute similarity using contextualized token embeddings, which have been shown to be effective for paraphrase detection  </p></li><li><p>n-gram models fail to capture distant dependencies and penalize semantically-critical ordering changes.</p><p>解决： contextualized embeddings are trained to effectively capture distant dependencies and ordering  </p></li></ul><p>实验结果：（1）In machine translation, BERTSCORE shows stronger system-level and segment-level correlations<br>with human judgments than existing metrics on multiple common benchmarks.（2）BERTSCORE is well-correlated with human annotators for image captioning, surpassing SPICE.</p><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><ul><li>见论文，比较好理解</li></ul><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ul><li>Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance  <ul><li>同样尝试使用contextual word embeddings  来构建一个metric.</li></ul></li><li>Putting evaluation in context: Contextual embeddings improve machine translation evaluation. In ACL, 2019.  </li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Grounded Situation Recognition</title>
      <link href="2020/03/31/Grounded-Situation-Recognition/"/>
      <url>2020/03/31/Grounded-Situation-Recognition/</url>
      
        <content type="html"><![CDATA[<h4 id="Grounded-Situation-Recognition-Task"><a href="#Grounded-Situation-Recognition-Task" class="headerlink" title="Grounded Situation Recognition Task"></a>Grounded Situation Recognition <strong>Task</strong></h4><h5 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h5><ul><li><p>以前的situation recognition task: </p><p><strong>Situation Recognition</strong> is the task of recognizing the activity happening in an image, the actors and objects involved in this activity, and the roles they play. Semantic roles describe how objects in the image participate in the activity described by the verb. </p><p>While situation recognition addresses <strong><em>what</em></strong> is happening in an image, <strong><em>who</em></strong> is playing a part in this and <strong><em>what</em></strong> their roles are, it does not address a critical aspect of visual understanding: <strong>where</strong> the involved entities lie in the image. </p></li><li><p>本文：We address this shortcoming and present <strong>Grounded Situation Recognition (GSR)</strong>, a task that builds upon situation recognition and requires one to not just identify the situation observed in the image but also visually ground the identified roles within the corresponding image.</p></li></ul><h4 id="Challenge-of-Grounded-Situation-Recognition-GSR"><a href="#Challenge-of-Grounded-Situation-Recognition-GSR" class="headerlink" title="Challenge of Grounded Situation Recognition (GSR)"></a>Challenge of Grounded Situation Recognition (GSR)</h4><ul><li><em>语义显著性</em>：与识别图像中的所有实体不同，它需要在呈现的<strong>主要活动的背景下</strong>识别关键对象和参与者。</li><li><em>语义稀疏性</em>：GSR存在语义稀疏性问题，  在训练中很少见到role and groundings 的许多组合。这一挑战要求模型从有限的数据中学习。</li><li><em>Ambiguity</em>：将角色定位到图像中通常需要消除在同一类别下的多个观察到的实体之间的歧义。</li><li><em>Scale</em>：grounded entities 的比例尺变化很大，图像中也缺少某些实体（在这种情况下，模型负责检测这种缺失）。</li><li><em>Hallucination</em>：标记语义角色并grounding 通常需要弄清物体的存在，因为它们可能被完全遮挡或不在屏幕上。</li></ul><h4 id="Situations-With-Groundings-SWiG-dataset"><a href="#Situations-With-Groundings-SWiG-dataset" class="headerlink" title="Situations With Groundings (SWiG) dataset"></a>Situations With Groundings (SWiG) dataset</h4><p><a href="https://prior.allenai.org/assets/project-content/gsr/gsr_banner.png" target="_blank" rel="noopener"><img src="https://prior.allenai.org/assets/project-content/gsr/gsr_banner.png" alt="SWiG examples">A sample of images from the SWiG dataset</a></p><p>We present the Situations With Groundings (SWiG) Dataset for training and evalutation on the GSR task. This dataset builds upon the <a href="https://homes.cs.washington.edu/~ali/papers/SituationRecognition.pdf" target="_blank" rel="noopener">Situation Recognition dataset</a> presented by Yatskar et al. The SWiG dataset contains approximately 125,000 images. Each image is associated with one verb. Three different annotators then label each <strong>entity</strong> in the frame associated with that <strong>verb</strong> and mark the <strong>location</strong> of the entity in the image. All three labels for each role are given in the SWiG dataset as well as an average of the three localizations.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Multi-task Collaborative Network for Joint Referring Expression Comprehension and Segmentation</title>
      <link href="2020/03/30/Multi-task-Collaborative-Network-for-Joint-Referring-Expression-Comprehension-and-Segmentation/"/>
      <url>2020/03/30/Multi-task-Collaborative-Network-for-Joint-Referring-Expression-Comprehension-and-Segmentation/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>VIOLIN: A Large-Scale Dataset for Video-and-Language Inference</title>
      <link href="2020/03/28/VIOLIN-A-Large-Scale-Dataset-for-Video-and-Language-Inference/"/>
      <url>2020/03/28/VIOLIN-A-Large-Scale-Dataset-for-Video-and-Language-Inference/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">U2FsdGVkX1/D5ZRbj353PE4miIbbr8pWfvIPahUTR05wCvPdd7233sZHY4u7QR/AkU8NPEJoISHoxi9FLzYJJkj1/q+vYwolUhSHFDYF3mqZnMjeO8JM7ztTR0zjUvuXXolCfpqvB4WyOJRUyRcyaq5kcEFdceww6L0S90xNGrCmHEUzQf3FNM84LP5W9+yHYmfhL+TOaMD18SV0QrllZypmT99Hx58nlRUFnY/fvwmdbVAzbMw7lu2+pymKlDWS4niAsL2i0l/3G1xUBOH8lUohqo6JTWd1y0diDvu8FJvb/gaNgws7alQ5yu4OOFLjlFo6+XXLJAb67rYEPYRNe3zt9ni38+DRohCmzTRUf9KrpiWHiwRAlwBvBJeNWfh3pwM3izmNYef3841Ds89agkHnJSaO+MTCmldRWQycZ8BwZsyFTEM25ZLhB7cZ6Wi20YMBbGpeKC1f1jc1PEgOAUi5+EWEGKrC35R+ygrzmbvSnznlyQTht8EPl2FnvsiT5ZLQixJRr1nrQwY4NwmOzxYIswJQ7wspdMibsuflUFpoexn65+uvrAkyrElrDo86nAvnmtbQYdJfbLxb0Xi6RiyENekqifW/qbUm7NJ+8jCIvb5uC9Kp8TJNWfky7oUw4b/fYibE51li6f/TUwQpbiRXU+EDy/U5s1/o/V29LbYeJDxZtgXnM09B1P07G7CLESNqSS5UGfiInBz+IhWhAEmyn9Jtx2Nn5HPXJ+lyg6a4ubLQWY4gwuSnTviXLRaQeVwIbrXglCp8g06bI62I/HrlEumyj9dxLryTIBQe6UJoSwaB/XkQzMxG5FnETD2qs1AFpq4zHdRNRCvNuu+KoGt9KbksR4CUN/8+JzFtynOTEsoWGy4cAe554GH0x9fxjl1PsWvGKBBGP3DaVftidE7KzHco6fDlanYZuiaQmGDhSd44WvORpcNN8slnMxzjGMeTZK3VUTcpqDh51D8K/eoC9j2wpG2UptgvgQBDXwgIOmuillQ0WVFLGRa1nvwzVQ/MaWJgq/Ob8ECB7chTSLRkgk7DHDzhgcnrZW2qG1ggXHGlFOkBs4ZBip8wXns/P5pTgQJCTPLJcKerdZ8T6PwXu285+tGNBr7JeEDst4Zl8R4ROIrUa+LI+UqMRYBe2nYr2bHOqYz/Zlbwy8zglJrJQg/Uf15aMaK9LQdBdF4+5PymRV3Le276WpgzVBchOIXQWr43udjk8arAv+YbX99cqplkiHkmsefH/fMQ5vZ/ufXCse1jipSQ0+bGwLAJGrf0JAe8BYEnHCgAwV4lFKLNsqeggAQih6/mQ/nDb/+LpzE9ULqD4CLHmhag6ZkfcwCU9jiOKilAnX1FNe7QVA4XFPEIDMmMAWRzzw01JVj+Ka+FjHlWSiUpRh/vB3h2NDz8uTfJFmvKnsygCT4B6fasoMvX3eLLhsvwFHh5F06kl6CVlp45cKrcps47lh4uOgWA+ivIVN+Mg6tFQRpC+Hd2c1tIY4zXbUQylvD8qfW8ZwynTL5F6/jCOElHRABf5v0d2cVTw+OM6iJEBDpXbuh6saBN4SgZsU2hexq3IPAM1n7NEECV+dHfNFubs+o2d0zhCo0O8N7flN/MfzYjAny+MDYO3pEoPwYExDeS/H1kYY87itliUDo2zRRMOHbTcK9zAMEeA2KosPrG1alVFTlo1CZ8w5PhyfaHi8hUEzKnVX8LFCyKYOPFidQaVStKNaBUHkWvLepDjolE6+5L9H5i0yXMEc4K1z1LDbjqFof2Z5CvSSdMuMlomvgmcVHkzddAkS0koOjSQCf6JwnUwjpHUvlOHygqP5dEk9vTD5mabtbX/fGpngaIDsCt3nyZMyGqif17314DMTTSSPi3Zlz/v9pdpKxDZCXdt/4jqVXGoApQKT5H8883dB2m6a36e81qETs8Sqbzr0z89m6E2vYiMh+WBG4XzxhlLff7qQia9M5gg2P30Me5isx59E8/k0pP9DCbQc2mCawiqqnld9uk4ypHgD0xIjpuVgunr36VsfB7VFaEHD2R+bJesQQSKn7p+Lz4hhbWU1ttyAFH4yVygJ859zJ5iLTTRS54QSQOuozGreGSB/G8QDArH6NEU8EX8DmcyWZriEAGayNXrIYJPZRZia8WedoLx2hkh7MMZYMjeUslBi3CxLo/XJDW0gcKCOYAHOsuqJZbZssweQ9fjg236KuPgDQlvGruSIwfTvlBJwGBl0I3Oh7Sp19a2o++BVGuiX3kvfuZ8BIf6C5apOaHotgA9YM6jH0YU8bxq3R4ynibpVAGbCXJ5wy9KkO4bb4pYFQjpO/qxjjiBZu68/CAR798r4u66bt9Pxp4OnLFQI4jQ7/lCxlkPk8wyrmHSwI8APVfDUm5Nf4gRsn9edNiUpCDD67LJzlcM1+PMwVxV7hw/qsaChiLapZ1KYtg+26zOxIkRLp8rE/nC1b2JHmt5xkhDyMWfkVsd694Z1iG57BI+O1B1K3GCdELpIzPZiS3PDc7guVr4O+zMvEUdsOTmZATXmWl/N8hgltl15dgeFIoD/PX+QRXi9L6UcSM++8o7VYBspjh/hShxZcXW9b0xapy3+7EON8sqYfBJITJ+UMicDRjx1W7qpWnEy94bKsYKc1Wjd7+3JdPWUa75DvC/WAUw5wJRKv+O0Xo6+U+gBDZ15t6EA4sedBB58LDIKKgCd0hB7NIvGfRMJbXDLf1+AYyIQ9iVhBllgD3u6HDTn4DXo54n55kYd5Ar8UUOiOa8DpmtnXj1PQotKkE4R0+KStvFRp8BFJulgmz2ECIm5KbVwWJuzErGdQ75+zCdBfWJGetLqpBZg3+3QGvF+kPabnG0gYa+KBbnio2R7q/tS21/EG9yzZJk5XGhpJLzm4NQ1NGq1JOVcnuOoUFwEZfYMp2wfHzx9jqZ3inryIDfs8ErlhX+x2IrAIxAWNxuf/X9YuFF2BAt0hWgF62xVuyjIdsTGQXZWpQt00+YJI5O5dE+IOZezxWttfps17kcHG38/86wh9jw+j3EoCRThuvQjMtkDnczNqDuEW2d8R6hSt9yGEZt7aqS4wmNMAfJYEsmHGnV+ws59UEHSkRClyRnE+8w4uon98iOfuJgefUSdO+9HjzwtCEyJTycBpBcBEnAZsErvkKmiwsStL2crW6EcZOs+qM0GP+raqT5m0tFOscWMc881jn6kS6GmmaVdcYR0pa5vK0W7B3sVnYKcEk3saAQOarOyT25MKPY14YpzktHbuuP5StEw0dz3or44KJnM0KtGzLpyPgwAHZlW+ebjNgFd+6n1S5Ksu/5Kq7xyRdFubEqKAfUeWSh9WcxFPdkNxqtIzaK92ckA+vQuk17QtF8V96d6Pw2zlT3LS8rfgklKBxqWJXURQV5CEkcVkfou1QODwu4YkjcedaoUgZpiRcyFMWnVkIjSsP4GkH/zHHMLkmqPucZcIaccK2qXMIMLJzGtbRHvTf1Yfljh5kHSj0vNiQ5ehTsq6MfBfo39XUyzEg1JmPKdbImaj4MVCZNJwiHf/kbmx93rwoN5V0XA+kE/9coaAkp1Fxbwkl2Bpjwz0bWwbbHBo1JcRqCJrcLWvYa58fFgbNXDIFs9eB2q1001mgyR0AX+hqgWztU9QZLesBso9F0N7q5CA9GiJ/XVRUneCmZKzc1X9+wjqniRgwZYGDZs+Fziyab4aA7HTdWEKbCk4BBX+vSQ3psKmuA/zt0Yixcz7nfTjXcC1mASTQmmPG+kSCft24sGwTwKV3FmpK1vz2poA+LGdyU2LiP5Xxh3oycpFNEJgArwZxS7WjLszQMkPq2yLz11mkx/FfS20WYofVS6tO84nR70gzBsmSK9D7WYYP/yTcitgqA7KTlQNOIFigv6XrYm1qdUuBBsdxA32c21Ee/sWf7GrvLzEoofRhDpmjuGWFWBrVj6E4sbQX+8RWp5Ff9BylrpvdzpViK7dDMrHJ+UkWyYRy94L/uRgsyQvGLmuu0ICcU3Zc7z483D1vdU5dos+d7IQPWp9xk1/QlLuf7FuLRZqDBPaLU2Ns3GFrdatYvQFQ6MdDemUjFIIZxYcKCnVx8+n10O3t/8eynu9UvdWuzLMMldFsdqGJwth3SrQSaloUBErV+dpNW+FM9A4xjtptQVcxtch9wp7CBcr0cTS1vE+Z3whWJWzKv76xFnwCZTlaI+ybGFpNlc95FAOfKBs5pU/bgE3rQB0HS2EwVGB9m0hDvVadw/YVC5ufNZsUH3pxLwor/jjSW7z7sEK2b8K3r/Wy3xvG0sFkuVTXEIhLj2keUgvwqeZxGBH83/l+MxYDNkeq/+jYzeKhtuNzCqeWCkAvM0fQG9jYeJnCJhkZLNNN58nVfV8dgQhxQF0zYCbDtGreAJ1Y/H5xVc5NldnHQiCR45xqcXOkotl6RC+KFIu8USp2qFE7+3bZVCV3ZVC+WaIxcZHnbJfzQJ4LOv9woXD8JJD+F35n/THM5IPDivAhM9lSVR7iGvIBWlnytwjFD9XYpPg7rZhbqOkLMhj11XnOJmWMsQDyf3kTgKSymewXrSsX3tVOV0dRKEZgDBUB3yxSRzzgUM2z0RiZmxPKCTi0jjC5FKmFW0t6844OUzFf8xp6d4U6dEkazAyGebg1uTW0BAtWFAiJ33fl1RTe5C8olXaVMnV7TB4ZfTU83/ebla1EnDPcJnsIbN9Y1XL2OtFQ1+wKmvQAj9YjAF7hgthTN/mWHGTg1UFbZ60OcL1eRwKBBaVhJDhM3wn7r/BYIfHgbATmsEXwPx4Lcy30f0Q63GkrZ/j5NTDpRTQ2B3eQfJM01sAqccK0S19A5wQo1tFESu1skxXh1aDkxS3/ZkzQS0ijQb3g52YsuD4Xq++XeubsewRyVRcsvndNpMplh4kFTsfE/YX4cSXptNNRViRDS1XwffGIEjWt/JKUY+s5nfl+xBJwvmEUcFOg3/vnB3wWI1sE2UM9JHExC4GMKZ/nePGQlsDL10au+nWS8Yxkn0fuhlut83KXwJtj5GNS9amrCeNc1/i50yFtyDcDbrT0lpzSbU8DGdE1BgHzye2cOGFaeMGueA38QOqvidHQ6s1ryM86i4ZzkfKM3x8pCGMcofA1gHQ5febnNihQ5qWFuc8rixkSTrDSYicjYdNm11P3QGuyDIE2lmdE9MTV2s5MDtk4ioH2S1DPbBuarA4VeMsMnIyvbfDhfcqvgdSauYQeIX3vAWZu3ZtWG9cvPYlUvN2i6L4OcZ9MR5J9+DcJuPvvmC73RqWL2oGUJfVIRBaRcIV0Tnme15T0boJdCghlPj6l59qcWzcbmpA6bTXhqKM0U9+LHRW1foVsjYfGhE2QTPr1/QFi3xqQgnxQyGIi8+GH86ghqbi1vxKTq8AKpljSDCDg3+dBhg3jh4YK73wZ6Ew4JynrIV336zHue8osQN0HiDtHkempcoORep/3ks61hLUAvQ3xDZ8xfBs5GfHtAtwymnOjZlq5J3K5yIv5cvaWn8zv7+vbDowdVS43TtnXKvgejJL95C9JG/HZqu2RHlgxAdauTMMha1fruL3z6PYPcrIb1c2u+S7Q95n3YWgZooCAm1btaCXT7vBRaNEP+HQEsWCKANXG3Ics79m79PQKKSAs1pUJLxBA+1O328lsN2QPpnTjPew7Ws1lividzoV85qe7OUu4NreyP/5NGVYFEjkaw2uWb5JFpXS7J4yQqqKYrKdHzZvsca1dxfzqWtxZukNS715hMG+0ylVJ+DaxWQ5T7LBQ4V6IK9Fu2MtGthnrGOBE+CgGJbl5zj9Eg33Tq49SuzHwGXY77+vCOsfJSFYwq/RYvwVYk9vdln4kdk84tZ7+F45X6gClRSc5OxQ9KearFZ5xTZJgo9OKHi3AggePRHqEMIx+IdMIlQvqKMwNuHrsFfFk0MNjJcdgPy+HEewlyNlGperVLzQWOcthlYnHKS5DSXX2IpZ/jHG+z3BDyKjqug1LtfHWTH9PpOEJzp0jtHYEWodUlHz1DIwqtrOw8t6PjHeVVF7e4b8KPRUBCuTjOea4daasVTt7CirfuccD3CPWUs0s6jUVK0TFah4hqSSaNH56ruJT8piLm6Pag5u+HPP8z/r6TnI+TsWjAIA2wafhShHA8bnpHSWDdW/oI3ipLu0Mn+zyTpSPPZbu84oQBZBJaZ6j5z3ZoyYYKg/Acm/N5bnva72oU8uG4oszlvFTI9l4AegHdxFFJEPhI2ZW7pSWRkqwHdoQLZspSWvB+R6thEXWo43+qEOzVfbPkjfIcvnycqfnqtWC/DKE8WtXamLjIP2Ld64GEeAK1qPK/A4M8uny6mLjDQfKzaVGnsQC21HbpVCNYxnGJDTTnQcEkn1R+jj/Io79Uz9wicsJmXlHqPPAX6MlsPACwmuRi0NQkULV9EXcdbJolmmcSmNqvWIaYsNZoLGPQzF3I3HE35zb+UktS4YEh8oUbYc7CfOHogxTffJf+IZLvRvHjaCbvbijmDM1DZxZJ/W604JJR8YkrcNMounUZzdgRWi8kzgJ5nfboh4hHuU+d2qk4CBHe8Txy2LLNKGYLgY9imR8oDY5yX3HQAhMjGIzB/gcGtAMNXDkBV/hU6dw8sLl+YO6srNFg6haOzmoiqXSD6mYci8NahUUg/NrHx5Ewd5ThVQZh+btd9RpquipmuvKkbvyFDI0QsmEqCFxDkHo1DILN5hoIFwoLpIsODvwZbYUHabqJ2seM7C7TtDDT6pXVgTGE4sw5WxpEVIH0U+KA9ROueI0/0m5imjKW6lJqFDdacm4BkUGKhJPu2wRRtfdTZcM1ZYPbuRjf9qDOZhI8tmcYFKsbOzirHOWo/J6BRWZo/a9dcwEcvKP3KYJofi2qV79bbcAiNGw2Oz7dYb7SAho3p6GL2K6YkddjVV9+ym7ayhw/amCSBkcmMyDIeH/9Hdy0ZEtEyOnngsXN0GFVV2OdY1oxSRy/bco5bZnmbK6/8mXUAVMQMt/TpiAzuvXhhcGFsUX2A+t0zQVB/qxD0nS1og6sqKtPvCQ08aznv/bAzenn7lOShHHOJdvCKcBY6/9O4HZ/J/YMntZ3qXn9kRxghAE8wD6ATv8V+OrTsXRUQxaZMeTQ4hg2JduJSTJcyVUhTA7MVDtj1fobRuRf/f683J39E4ltHZIaVkKlHcsoXVNxN880c6IHRtzQgK+NlCnJMa6iWyXrSMENsZ4QkftwEn9b4uIoJuEtmC7c0hF2Ehv5daYrnFxgpIxBogZIP2oV2ZsaW1tft5ouhGjJQGyaLb1NcshGs6gNPx0Sf1jkoEBVzO4OXvNZeQlqmgllnDelyL4wnJdX+KAfrIskGTx0cjKMtevbLSIu0X4wLD3KbQWBvp3A68kYMMzvzGkK1VE5xFQugS4ajr0OerUWXM6tlqEn8g5RerRnORFSQlCdtdp7aAm3uxAi4N4yzpJbQBFIEa/2YrX2ZFG+FAuyD4dmgybXIo7tGnanuaev1bRZlVMaSQUROYmX1NhkLaGLUUHdGstTJC8dJUf2eZqGcGWtZIxdmcQHzUZ9YhDB0bbKq2B4tvrovfQwL95qy+d+x/zu5FCcvRiUo9+7c/1PfOW+EJEwxtnfoFWuPd1hAmSXzeP0AN7e24Q7kEIBothB9nL6A8yxei7uimb2QPE9y/BSgNBGLie29+kd2t+wxPMHidu5lJu7gdpxRrFAlI+oIT3gD0HIra9q8HdPH7/FPKQm4veaBPZj8ifQiciM3DPSOBYrlq41likaP/myQp2yubRrdiD21vXaUmu1SpAgJyAHvk6w2UVCo5IbO263AuSKZaXG0+gpJNljqgaklT7yXRDcZqhdbcSgPEcRNvpsGWxSIoZncJSQI/ofs7G5M/btpHmHOMccqEQHmBLOK667fL9Z1ZPuxYJ/lg0OP7uAT8yatwNT0XvY4EIk2cEa6cVtmSmpsaAMpaSyrKlATbfBN7oPWX83/v4bnamUdJJHWI3ohK8haMdRVWmMVukaKA5F99oN6vbFOoF7qjM6VlqcATwu8c7bHx3ITlngtUmkrT0C6S/F+RXEsnKv4g2WTYM6RLqDEcK71YytrYzTaLCR2hi3PrdnGrUvKifXKzOFKoTEdehaHoAA53vtY/fNrsHpDjCX5Q+s1A18/WkUqFeeOsZJKOLQhRj1MDwsMvzSBlQ26dLPx/932bF/R6NmeDLEcnAFw+D7DngvO9610vAkRS6Uv9UpCHS7kxOQ+CK0msMik0MMSonZ3Amakwp9NyBcw8fLnq/U4k1lzWMF986A08oEdfdnAd69xpj/3ZqFla3uFBJXlSa0ChOjdyus38b8PHrE6ebSpeR+PPEgXObdEWYpYCz6vRZrMQ8M0nSRK6rYEXAatRWVwD4a52TyOPcfVhJqcqKUbL4Txqvh69azoHlEQlDQUiKliuaaqwaCiFtMErqHJJeiKVBkq5plBp+AeemPCQubfhmbhO0OTq8P5fflRCps/RH9aCvw5+rXlDWTfqXA/yCZ5BMobqwYL1ExqqoC/NA6eyLjnPX3xjJLXF02jdJmbk+X/utZm6p6mW4YudanSmtL6pIGTpi3jZm7EMGdoioQtzQdKKkOKSb3JVmU5H9cLLZvknWONniedgt0PBMbI8gSJgQVb7wEIxywZokO24nWjWVLhsAg5LDrK8mHWSttET86eRiK7a+w0zVdwION/NZZhebwE5Y/l9N7CUyj9cSr0rmodE+9ucrmmhoQ9b8UWpIFWrRNAQdZozuxPnKhu0llks7el1K2j6uoo1tIXYjPa2VHotD8V5QkJvwxHriHCBixD6A6JD93VjHnqbhPLZYD71SrcKRtl+NXvN5hxg42ZB0zPCfSrc7K8FG6NdA1x1K/ctFzcP97h5sk4DRTuWnySUs+wNJTtHr2KILp/0PxFOlG0q8FCbUdSJfQpsvumt5YoZcP6fdtSyIPrwwd9IzmhU/M39QQ8fyP55BkikD3YC41CtLkgGFKIQH5qyCSk2cekAuk6glJ+ezxKy+Usx0gXm/UiUDQW/9WUSl5YeEAVjdMoQC7kBcnzkIm/rsy+zPEVQ37HDf67/BbcMzu1lwExzdB2Gzb0WT6hAXTMwEY6oiSWFA2Q8Jxk/n/D4i+E02i4ppCwtmMvksRz7jS213yUbZV69NJYWtAWXeG6mwoT6TV1pUqZBhmCiyERiDiI516aJwse+xV8IrsOU9Q3t3pF+pPzyAjHnpOQu5p9fLhTA3BbtDjRS8czbqlK4x57aPmxP+DDC7lzU5FqbysEkk+duP+6g2gBxpd/Q63vdMjAXQhNy81LK/PTc/xWMKhPC9ljb5btCLlOTmEDLij69TwIzpn1h8ncqdrkbnNfdyjJlMI7bbIFfBsUipKXjLmt7hAbmp7zWWAwCQUi2+9hIjB4/HU27oFlrWsQ2mRO3X5zPmJ3OaVk+iLkjOWSkDAGhg5k1hlEGpNNODDIfj5fQ6vHNWfFVMretxpv+pLwMM1LNKPZu/rOxoVuP86lQj8Fp+1YZ/beWU2G8LN4iJva8pOFG+3fOpx6rNBby5lta6OuPgKkL6DsT/GruARtmKvf7PiSTeQUykE6m5Ofz/Z0w59y6hTRG43mR0MxsOz1/oLzaD4sOAyDc+7dAgHIjZVaZnruMToS8+AFcpNF3EmRVfrYBexxkPp0Qt9N6wzsflPGKQQO49MTsebDXiDe15NnQ8yhUPRQ+IIifWLeQAJkXNu4huWFpXZ3WSFMoJ0u+Saxvp8pEcvKV6F5e4+qe1eIUewouMioZiX7+GwjjxjALsCoiRy0w/vtachys+japrCDqYuXg9odoqHVTYtrDEKtl/IzzYPt1Z/UZn/Il02hRV3FmK7cJsJ6pfPVVnO7akkHlWxbNmw9N0ARCwruLVLECJNnnjyPHUyzzhM8yrfLtvOom9YnRkfraEp83Y18vLsDuADz48FtxLhws9ea5TDqTuEbYaf4Oi2xXulXWeEVevoQyLlXLlFCn7ABJiLrqvl7Zm7A9e3TBzk47tpopscJWZRNGB2/F84lhprIwlf85aTsW+/bycABhUMlzQhIFxAFKs3hiDkN5Bj/lL/pdWB5S8nyT09uVgSCPzHVlqnmSB8cwJlZedhmnlgZk2KFHIlyo5iOUz2jt8hzsBBMhjkCqkHfrRnvsEocHdU7TSudMQBF4jjQ4XalE1cBjRwF5jkY/OarYglogAhMqul4eojPT5mPXOajVO7ZY+T6xLr6H2etbwE6LlBU87UeD+QZ09djU7Rv57C7CHKnJ0JIqezBMGP2Mpe38c7rs9WR5T71MaZQwwjbOs2k1Of5buD4XnNengbA0nANQx105Ui2PpcUuW7bD8Zu7xhRWoC5zfTxl9aQaBrOuxsWN2ShnWYVcLZ0YxKpMSLZiewjAarPjm5wRGnSgUsEx2A2ADmKOd5qPX0A5HxSHES2tcoBaCDYTbnmXbjRSqFdpt9n6PP8alD48SefBQjMGAN7TfivMS3nK2PAGw8VHjrw298ZjVQKcV3W7Ud+beH1UDxox6Tvf3+3vmcQflSaJshSUkb39Yg5MelhUkWolPMqw5JL6q5VhaQO5q9Ax2ovtAQW/FYcIPP+ds+pYiF6GLTZypsitbQKKpylf6zH6EVJ5skzopCtN8yo3htKyuDsmt7SdguIRDNG6iEsb4VK8NWth3l/DLBNDcypRDc5mIocDdW6HNMNaOdB3sDIl6y+uWQhLuJasCxVEkIqLVk2G/98P+bg5EsZWR8x/yJw9qFRu53UNIebd59192ImoWRsBPYHJgm2pV3s5M88qr3PPdPO+M8HHoGXDhufpLzFZIsKiou8ecSDUcgS2DIDn+GYx6SgBFUrQ8O4Omo+z/aH5TBB5L7bO2vw0WJu/YXuYRAkfNwDeOH0lV7E/iWKJLm+cyMT3Jfezvd00AHJFlJpc/rvuNHgFnYPxYa8XOB333RUz+SQe/qFh5pKYKYjJvFW0OIsOy/gVwFQe02MYd+3Aq+QX7qMluX1ZvHCyWzttv/cLqMKasND9pOjU5O4xC9TZyrd4yLfZc2fR4MSiOusEjfEAkkshQUGt1CFpJdKmAKChNB1qb66Jj1ogRd6Ojtx/4ApXlDZuEeXUuf1T18M1G9i16hqy9tzGvDE8zH2iY63rVeF6fO2SStcjhL1v+2Sf5TfB6gqWfraMbMllhv5kyyhpXrKNgSIQIfPvaAYHFXLj1HlMVu1nn3TCJM/JsM17IEEWHeAB5WjnWYOFOKkVPgn8p1O+bk3/lmHNRbudsEnun7Y3bPHP8f4JBUqOhfyr6Gq4it8m4T9u8N1cvNsKLv/71jWLSRIDrN3Ws+mH3xT6K5bh4y8hvOBF9XrX1HPv6Pcci3ug1jh8vUUVLnpdHiPHiGs7PnffVznLWnMMGGw49Mkp5NAkwTJLqcyjh508/kFMN0ukaKfxkI4l+x+51iFCOpZPsIdXioIpHS8ptz4S8vClOrW9uCAz36fSHCm4DKI7be2jLWRD5EftHD1CZoqxNR4hGuOufCUn3XdNhdBGex83chC7sq1aiPqYoY3niZdX7hpFh8QI9Bp65a3wXLITy8Rb9nDRMDp2PNjL2HY8jRIv1brtzAsvN/sxGaSusK3HLiiT8i0zYbtSsQxe0zRVfR/0HrBO9ZbxKTUKjZceNXyroddd/JIoHOlbp4Zlyaum2gJB0+Jm1xXidln8Je75dZFcAXZYveFu2cSG1yzgwLNdejkBpzJlOkoVUqlv9nI75tp0kqSC6nQW5RLnk5i6RHu0blwAGYrZA7fr32GAOdTq1sIX0N9+5KAp7rM6wkZK4LO8LG2ZUMtCPhZMmoycFzBqvna3tsYR8XL5JA2CG9GrBpIizzA4RrvSp7gfSPE5CJR3wTRluha1xp7kyRsQ4p6bSx1+hmJsO4w7d5LKwpJjaoY5wbaTIH2EwZMNKLPSA7fZOU2Bwn2oqacqoVO3mOVxTTiJbEuQVIrYEB6FHsWDWf2Z8++T2wpMbVqOUTNC2Tp43vzrfESPzDgKO2DVirjmhBZGZx2jVt+Xkg6V1hUrLHXb+ZYw6MUL3ZJkvMCrcYm/t/JL9/LDCC16kGfrB61gL/hQFf1jy63yY2qZh06g8isdlHFmEL/qIXlA49gyVoLMT+I+FfGvsVSeIYNoPi2StiDR97sZoPRY2eVUUunMA73x5cQ1N26LJM4I2MrKIiWnbpyG5l2CUrV4ZqZOKbi6gokW4CVKPjM8Kb9zXLkbD2gEuvBVtGvaDRS+7O/klqhronJvdNRKR6u30A+4RzDFEVpFF6sexPnuKWssXG0OtdGwCci8g1zHcQUCMwCJNu+EaAoliXW3ncXkiqqWDBTuR+Df4QMOR+X6wuAk21o7VDA3zicTRAqnV6JMd9tqaC2G9E601E/cLXptcsY4KSx56z7+a1aVJqaXnuuLfWwewC54wFCW8/WQ4PeOTWvPIZRaVWJx5uH1iUuznI6YKx8w/mDe/y04Hw8bhbBeFkBpfZ+noqhQjS2Tox/5DVRtbOzn76x5KELWvnX8J6nYxdaMX3DJ0MkE4ULZVKQ0RuUty+0nW4MeiDqXg5x+DxreCsJuQQA57Qo0xBqmy6BRmz5Yub9z7XHwDJpmjWUYUm6Amf85g5tR/+qVOEs0tk94f0GGKRK507ZrgR4xK5QBlfyHuq1pUCRc2RYT6R9V5B721hRgE85sYKsU/d2FUblFgIKPyiFte7rjqbWcrdvn2CY57hk+UK5gwWNFdh7jDPOauBUKf2l9/sHlzn5iCLUS2SgpyQomPN36HudHMGNeMKjrAAh5lKAgVAmtJAAinVp05gMJyA+VzZx7huCdCIgQPCPYqrQ4GYJ9h5tlgfe0Suqup9ditphNy2BEscC0F4mitgkRA5y75RR+3NRrnQl/cKeA/umPfommWHmAnsFEVakNgt7G5msOyG37V/3Iw+QZLprhC1ztA1ms0H7vsmRXYgF3UEAiCivAi4JytoJN3s0s/BGsA+Dh7WlpdwvqMiONlKHC6FX5kZdINo9hr5idoKg7MVlsAgqg7EA15moUjuSjRTdHUtCJl00PvJSiAcNZfM2wyi8wFJ9GqTx54ATHfQjOM3/ff9YmxHAAfcl8IpZQrJ5ZLsz+3DrlRs859uVfOrmX7lEay26HVoBpzJZuCFppXGhmWjidxl5Qqz4kKlcvTmDAokbcKr4cf38VZJuQ9stNTUdSF4xst8xdJz+MT5125tC1mX5wUFdXtoMckpjvO85b+HbVeKSe5GXgEJAOtPj/2imMnIJIBqjVC4uJNSP+945fbNzsd4kV570omyB8K/P+7lAncRqki5aLy5eD+RUL+kg+kAf7kk/ojmqT8Zqjop5e5qTxTBqtzFZizZoffj6Z5/PHuvdjqF7uMniIVQTKjGc8c0EdBqE/BqRPl7dAeEAfDAJxhSkOvjih6dgrL4LU90QkmhuWDfqdJYvnAms4FcIrWjfCn9p0bbZ9V/FuKlEwrP9kGsKh/HwysibAl6KJJxxJT0PN2nYkQ23r6VNZeLx2rkjRF3ZbYSxJcmtmpP3/oXaI3aZvPxqwq5i9QL0y0Lo/2sTcSq5Z+OavssH8TNNZ8s4VmUM0NsXI8oLPc2nnDuFDXKYzrRE4Ri0JIR6CnNHTEsSJvNuJ/aNXKqlQuqkFwYuqNwG26Szh2pfzkMAUgy+y2dPS3sOdTI2e40vECNVco3KuuFT8OR/oroBm0DTA01R6a1BW5U50TWYgmb+NwR9gZWmEgDJ6O3enq7ITElwS/Qn708cQYXkx3mo9WlX3ZNkQdX/0x86gh8pn7IM5TzKJ04jFzkjk6BrarkVf/okGkleFAwx+7OsIitnosCmXA2Y2OIGK0YDBMSAyLO3dYTWJCqmoMQ088658V3GHGKW6ByI/eOS3cF9fGPPyRKMTjvAYTOBHRLsKQIrJsxkHXO+yAODFMwXIHMmNKG2ttnR+WC1d4Ctzqhq6rTG2VXAudmn4hdRW7C49nCJmO1eIYW6MYqZAPSVSyVfLoLHmqfIT3biOGt9OYmB4rur3O4os8aQBBzMfj+Q+z83nvaTG/TOMT7pL08rXUyEX7vDCxf9EX90ByXnD88ywPH2fWBHRlduoRsdaJNdnp2YblkloVNEQQOajE0iXP9YXnDp5BQaU14quiaxfrA2F28RjgZKnz4wRe66Sny4he1G1SZ+HeKnTz4iECxmroqQwsEIJ+4TN3kBMmcQeEx6uQPTW+eRz5lvGymleZk07TwYuadO3pNYo52unKFDB4Nhrv0p/71I/oixJZsE2/wmn4d0E9xlVrSAS3DxVLtqhRyoEo1K6hb5E1bmsX6eCEBpzV/4Ll3GobRA3hNvJQZJrxOgU5JWIQEiISoCwnbcWPLSLohSgX2NWxT/ddfFHXhdI2reSiwI4BqFWjLoX+5v39ocgqvI601eKERrM3F1QM8ycYKfKCgfk5HvaDeDdFYBS3c2WxngJ8lV02La8ZicB9MrOHrxeS1eqCbbFTDwCAcBqtJ5nVXx7NxAwJMWy8m5COhukhYm94Cu9G6Fcvla+R+EIO3J0ZbNXYh7bz/ueC6HCjS7+3WOEVDVQrS9YbMI2UjReHcXO0DbqzvMdVbPBtJ6wZs3E0t2wEr5hbF+Y+cOrsTYbx8xDX0DZwtpNKfkqmTjbG3W/fGjI0+3u0kIu6a7h3JIAwAMdm2pp80Wo+8d1X/Sr6bxuK/ZJfYAqGhgIKsyuVebcjh3voRmW4HWdQxppnl3iYLj8vA30YErEIo6jpgfFDTPj+Ao3OuV6kXA8UT6CJHpCH0F1gMYosuemWtrrFLD5fUJgWemzc7uI4lZKjHRyh229FAj9NEZEl3aF9I6UKkanfdJv/LL9sqksP/pv/FxRbv1j5brYPWj4jEBP4oPWY8anongQQKHSjXpyZQhcaBHtXihbwQHK+s22e1rmP885+JbCZKe3Dj5pSq1jTmvb6phXFA57RKriABjFEu864zlkw3EvSLa4Wrrydi0KefIIzvdsMhWFqMnmdJFhekw2ldcEKJofvxrDRLym2ggavYe8mDJwceVvIe1zKulpdNrMMFrkrW/eVuczB4p8ioir8vFewW/gQ1zx+PluNIV3lt7jX5E8LoC+xMHuqK0UVgGNrrDbEw4fFJJhiDiCeAStlTmhIYJcB4P/BOhTeluwt3FeVgE6aPlyrwswrcUiBpPWVjEAJlRRZnuvXRb5cveKy8Fa3uoNJTnHHQoXaqwQCe8TT2Xy/X3Kkk5GYZ06tk/SmFC62RPILQVZngst7eoqFB3KsM5jsc+NNctGdK8ARlfGZpllmvc+R9AW3PX9rLKabX/FOioteuJj6/xi2s64bkauqT6An3lZj0a/wKlmUajyWvEgtWcHKmGmmUSMDOxADdC9YC1ZGxaOIxVneEODvqKAOr7HELU3HdLRsdOnIda92lTarFboTEr8zxkWuhE1IsvXB95U65y+dP68CaoIXlkOpi6/FeJWKULhvYsKk2QGGiAWJ6nCS8/7Dmfgo7aegD/N6vy/8RKGK7MUUEupGDWlitzRa5kmIGeimQrcK06CoplGIVaUsRsBiW3A/UX4ECoUI7nAVOk6jFyXCmbV4LD5jilMgjSc5NOue7Oz/kgT40Np6lfj8RLz5bIkl5fJQfAx2/cf0/OChgagmTGxWhcRzG4OuyoTg9zhgcIZPsqkUF+Q6jKfI19lJjj9g7YuDwYPo7VL1TurWObMrULTIB39fUB0kwcN3187fps8sZrmZNGy2u/VqI40KsKKaJ+/2BomsNeWbbFDSCAk/QQv62OnQ45h7fBrUde8HXL7SYqqb/fe1ajrvwcNvRtsTEU9nPyw/wML1M1QalaidLoabga78wHTS2WkDZxRkDSC6uU0E+su0BVYPW/36yAqw1nLs8BPoIPhREdzIIlQcVNrvNMwTUJlR7hvSJ+V6HK5Ocku6njB9hTOwQ+LNOXsw6CjSqGSUjCpRai8N8RuYolwkeqPJLhwUCg2ZcVBKNeDz32IQEg8UcG4H3rFnoFGMyajfT14VLZtlzK4NLvkTzIEPIG/6Y1lH6M/MzG+7Mle2mqFrQUZlde22tTaxi/VLGDaXNNGg1RyBT3cy9wlD/lD3E73CTmJyYvYbKgciEPw3ziIYXIWtOkoQ+Sgxinkh1igGhwVq59bCI4dRwWYSVU+LBnJc+gzA/a8RIJem+XmoSMzKUmR7SHF1ozdxZOajN+DY7+XqdwjUoYUCXgk0b21UQGFBqZKvikDszauzflSUJkh2eR6dQdWjQGu6zWOWYgPApecND0PTlDW3myC8j5ANBJfVz/ktUJ7F2r3RsRQem9OgtRu79Ij/7CycNBIwjCFddgO8UNlVvkIHrzxamebYsrefumxo3xbcCAmm7ddBsHWUsm80mPKiHF3zayXF2/nyqKTY/9XWdf825HZLmz7Mw5wq7cVcv6kp+zA4fDWHthjuI6qZ0YEjXk58TdGp7N93lazlJHUNlKvTlrx3Sp1qZyQ6K8fY4mbfH9MGj9RCSzRyGUAmH/fDWg2EZ7o6+yS5A/g0rNFe91+wAX05j0ZuZ/4fv350XOT3yPfmXQL1YkCsaWoKUJ9aa7sPBTOIqWxrMYEMornOrlwZv0qoY/le8T1A6jlQ7GPwtTs+6/XHiUvmiWggLSXl5GKEK9TffASaCrEQKIigsHv4PPlDyXWegBL49zOIBWcK1wHhiUMJCYNzQuihSF9zByHsIl5m1UcvlGLDoaf82WoV6KWXJDM4w0rzSS4M0O3w+9SLn8aksYLiQIu3i80Jm2QbYcJRVeNAuvTiopm8JicOP1rojfHSfPi/Ba0FhLuPyKKjxXuJ8BBaU2GB+StUnTqduzgVBM8eGf+jO506TjSJ3bKHSLmLMaL6IuOSRAs+7VJRqTN4clJVOFEGxiimXhzS8HbWSmXHP8ITVL8YT6h0qPeDyq4GQFVvo5DueMem62GYzTdbsBLyMBHl7k+kUk7dKisdxyZGRxSsmjK2Woe8BrycQBGCO0zhvLKqzpQHCv43x8MepkSE0ny+VVjJgDeZgM5pJlHFDNGO2YWWD4zB1Rwd58HPJXTiIOVU+KC+NihYWftt3GGyIhoQu9OmB2ZO2AnG/Yx5nRk85xeTTR3iaFae5CC7rmHd/ZhqjkP9pKJNqjkz8ZqLGjFzbVWMh1SViSsVH420BEXxKJYw1QY37rFAVOAJha+t47XpdHRacGP74wHUCPIuYLdasL0mENu0k76naGyuyZ3L+TQHR2pyp8/qpqzEgIUakwi0n4bbjZQBaN3OXdDHXeVtLlZdHwjuY1T6lnd9i0Zrb9xEbVI+xBC2r8IZl5oyIT1ZUMvW1FEgGtY7KozoqK7Y3mMdwApobWdBf5KFB1ssxHDxo7yx9obmUtd0enoXKKUQuDcW3FF1fhB59V14KuFSH5rFKjHhZIppKrSx555SVDaPh4GqD53u8AO5e/d2TPVKBEituGjHMCeq5tM6bv3ZEBwUDQVbuV+GRSM77K4kdJBPWaFTgSyh+tnP+2j3dvbpHrbv67X9kVE3u4kKzuxNUyQeD9e5luLh2JLpNNhed5AOR/KQQvONEwDE5TUvIOFVVlZ9svjcB5yTUUfOMcRo2Kj1lwV/LMvFkBIk1GJTgOhPHIrwMOAdfThXjgB9tS4e7LmC/TnhwcRLLE2KRBtfNy13qTxDbwfJZF7acRk1kHfhLCOFNfc/s+pxqBDN24AMO41N5jWPqWOf0eGDJrdpVr2BGSxDfa0q5kPQOTSzpME7SgZ3vAB20Ka5Ic/pJO7WjcBqr/a//CWv/VGfP1+tBK1Fgu1PoZ8wSnV9ACHuz6UWvjMPVxn5ufa9T7b5esBLzkRDNn/5+9hKSpL15Ln3B3tumfmRkdk5kj+mh2YqUXUk9TZHsedIVpzr2cGFNQVYHNqXQ4Nq72pdysrZxQnDyPhVv7DQ0VZ+YdZ9Eq0pvVoNFTD8K5F3wzOFBkLXMv8HAaEoSQ0A+FAiHVW/UBrRjoTWf8Wn6eVnoINXzpWK12xOnAAi0pguaG8wU4gc+vWpydjzEeioie82gND4oohNK/GO5IYQ7WxgHu4c6n3O2qibLUN8sybctYwVTgLOiPe7l6LGW98nBQR2wvAVBhCEI5N60WUpsIGBzdix7Pd0lzq7/jhBc8H+D8QOAHKLcoAcgmvS4hKsUzyqH+fREdcr/bvX1zMvf19wov4l5wRJfSNiIXN+UTVo4eywnALrXyxXDtWLMuSNcasLi6niwLtIoXmddesLkI33MrqeBK7StqqN+Jqrp8YR9tL+kfKhY3tzyWQlFzwovgeaefLEJBsB6Aiwapa60mcXWdshHkswg1Gp7bV1fZ5srkjd33J5onHE4YAOzgEUCbt9WBIHuVWusk0xUNFCcALfMXRYwRDtxONHB+Q3BOenhT9rXUw5FMYYFRgwL+XDmzuhZ/MszV9mCyQZehR+OGFQUnejszOjxfimLkt4JaWWTQPV0X3cR35U0SJRHplyd+Mdw/w622RUbKQWhLEw3kTEXmTdr5heh5D4C3M4kwPjpZaJf3xR4HzuY2GrivE8tDh8oXeph6Wsu3PjrkT9uPltrgcsgYFUPKPU1NiwZuc3fHoLDQ994HeTmFA7gQ10Ct/3g5q6ZME8lqwB2dwgkms7t1kdrl5YA+JPsTss5J3cSTOuUVbdMhZAia5fVse9XxhCZ9hvhfXALx3E2rZ9ppd/oDSQlacBsxBWyyB2FbEZ7O1xW6yaS/voak26+etouobFf9wesTzrRtMoWzWmC90CoRipl0Mgo5L1+k04uuvU0CaPFWMsMfEs90rlYrFdeTrOf4Su+xuzncKH52TFjBHCPUvNGniCfaeXaZxXvot9gSJFvuxqw3ah0tvVZe4dnDZlcpPlIlkTiJ6gDSJIH6q8YqhVvdkJMJIYsx23WJuNUZjsicf24l5jiFx079ipZ2oXxAqqUU2svP7CX9HrqZ+xKfZXu9P8gmkCGiNk+74+H2NsoOdyNPBgm3zo42K2bmhjuw6zCKCQiybwiV7YRb6WurLya9IZalylUJeJI/tWHnrgb3w+Z8X51DmRuZOfDQ2RllZX4lDIAJWl/v3d8dyXGUEVAgXFPEHN51TZKvk91LuijqP9MrPfpycrPzbbmXHF94sG1HqwAcScZwk6GUZ8IvYrwcWKc2JK0Wp9vEaQuoRi7JmFWSiHrwB9Aww1x7ITdnHxdRajhhHAgxdzAUX9Wwjtwg/htwn8E278DV2hr8RFvgv0hES7cTUrQ/s/4QPegrt6vQznRr4Uh4Z2xRxXgnOM1I/kkO1A8vm3eJo3Td2kL8GCz6FF1nSUp3X46ywDsc9lVvqV7s/G+aPMnJ2re6tPkiFi7vtpIAJcmCU3c8j1z8W4Q4XK52CeBF7pMIaJbqRNxxEzRkyzY1w135IUXoiWrfGePezXA77uxY6WnyUg8NbhTC8p453qLsJ/7Ic7X63EdJu8d2xLQzESc/QKUPj8MeQbNnFRU6+oqoa8KCphGd2I1Vudivf3BBOZoJtjej/vyUK4xagwf+jDyXvjJJ6wG6X//juD1mFqiRDFCA00ROm5mr+1zZHpZZYzZqD8e/3RG1f8b9j6W6xHijrcz13nbJL/zeR6rg45fox6wM2HuuTnOFgZgMOlAawOyFD89OoRAowdDtSwGnSkH0aIphxV5cK4BGEwpvmbFjdUfQI8Z9WenVb4j4ykwEnFKFkmTksWKhAqa0mbNJHpYFGvj9yNsAQ+6Q4GWHyDeDSDaQVhcj+9lMCUHLhFvQLVoLXXM0psrND+qLpv6lyt+4t+L9pj6aZOyS+/A2lZOKlzRLPamvpZAUSGwmr/zHOicIghz2S6KQScYXxGDY+Uyjl7l0YOqNU2t66zKTu4ZmOId9GcIzRzdHFeDGhQeivmRIZkIoFQartUhmjW3dO7jgvgqQon1tpEa7M9fWXGm79prHFRfmGogs/V4vJZlMUpZQsurTzQyadx6TsoCtL+AOvKxlT8kn8r34owVpi20KG5sSZ66IdEHxt+kSAil2D+kfGWV8NEcZh6+MaPAvA2L/kWQtUkJskEYhA/7bcL9BR1N8vaZ5nymwll1a3W9hjz1TRoji3j9+fQiiU9VY0KwUP8f6WwnNSCMaGWbd5so+A486wNfZX9D7sJFp77NPyjZeBruzywUSCeXULp9X2tWCYjm97NfHNZyZewTPX8AVB9L5KfY/zl+3/BYxDt2ZGgBrF7hIvMrdbJgloXFHrw4NJ3aJSoBaBQ/DyWgeiA/uzAq396UIbj/tNAbCnbESFDpka3r1ND5POoL0SB6iGj7Rx8XKprGyKg1ERuhSoOXuFxAj51bo/hgj2jUQOAwE/dyFrq5hUfEw03ke2UD8Tr6zA+graqQboTFzJ5Vp2c15funR9mlIvbMlEo28M8/xghhLIR91AdYRWp2PMroNbegKricrqQMSI+NkZKLwDzvhFGZiID3gdpfz/44apqZuFRoUwQp1x22eHWbfcjxr0pMmXT87i2MYf2jh9/irY1VcDjVYkDiOcwe9QPapCit5FVB+cd3J4m8AyTuiqNWxiTOhg9npi5OK/FqAL7/NE8zEcTq2sCzmODlFhAgcPEs+NuB/agTVknTtnGJnS7JJW7VTNISZDUOx3hQVgzjBuB7gohgsFE8gRbft8MGBR2v4NmcuVgDCIdMn2MFaF24aaotMKi2qAbYRzU6Y2Mr7vgm16sqFhBS5VkIHMOOgu4I+SqagVKYwgqyGj/y74eh55gzqVOLk+BymyhNZKK1sJpErJu9KPv9cB6UZVsHhxOoxCmMPSrQAg/K7QX8ao4YXwRjD5j6XrfIvFgH/ylxMAzdr2PRkEdqMQc+auTXz76uRLS2lgn+1mc2XMvKFPAxLkFgNSFP5ury/4qkQQunwzrruiO7sFNDSFJburZKCFNG7C+wZHdF9FSwhQavHCT7VQpwjGxE4v8nGcVKHtP1guCUafv560ToiBdvubvUM3v5FPdYGufYNyMzeTIj2l52wfNNuf3cLtZDxkbrhq0HZ8wXcLEfwRiPT4SsbuAR67uQ7sKdqADx7txdpImbCCJ16k5rmYqBZOAsyBELUl7OUuZjdBBqD1wW/pOEN/cR61gp61fyt1uXL7ySh6CX9E0lyyWOeUWTTiVLd/VJAYcaIRZK4nYLtll8Ktfm36CbBb7woKC9VYoOZQYI2MSOy+Pj0mqLONqtUhQhbtHF/QyuXfgsdhn4HN/LRoFTKXiDgxAzJDXkagLebj28UbmwundoC2z1dxklrB13tN8HVrUGWDdXU6N7KDmNHQHEGYcPrH9Od7XF+WuRAhd/j+LtKObK23VmlEXo5SDWlDzr4TLAtsL/QUAK+cafrrlP64CZ33Z8Q/+vwkqh7PJBAVUGFq8MAPsoqp3t/ozK/lNY2VpTmOlAg/BKu89Vw1Mh8RpnqLntAXJJ9a16SQNP9adR9/CS57ptpklux2nbsW9eYXME3LzRj3NTTiRWOE9zrQDyfE4hkLHAggCSaSwafVVvOm1K0Rk7ByO23U+39EUuc0TDl0UF0Q9/BBXGa9J24bDQtX+8XR+j1Ki2jN1CUSli+GfwqBYPW8SWKHf8rRz0CzIG2KAnLyyp4SHjxM7OwtWoYyzAppv2Rx209sc/g3jKE4lljt9ElqX6K3pExxgLm3XpABDUGYTffK8HiNK3HcUur69k3Y7vT2TER256l8H5o3TDPjK8yolOnWKjv7tKDgTZ7RhBSpG5kWXyKW+3oUy+23t6qyA3Sm4jY6IxhCRmy2Pbn4j6x5qR2ZRF7WdwTvXEpqVX3d+sqDGR1e8tsGUDJC3Vpd68zzPvwCY83W/UKNwCRQJU5ikf4cAph1vwRiXyZhxZbVMPG0lCx6A//lNSyA/tYr/w9og/n2K+xiqImetlcx2Nn71P9cX9RTM9XM6tyi4NayS1ONl2ickhUunIkCv8fG5yus2QzXCMBBKulXNe91FJxgi9Ie+59a97xCSPehJFs79bG8USbinwVyrNXvc0DabmECZJC5bDNYo5BGuRagd1xCVDQ+nBFB7NMnioaVCwcEf570VtvrhziRFugJLdMoZ1TTjY8zB2C0hE2Uq0VsOltLbuhSU1U2umC94Tf9qCr0pVyhQqhZi1WJMd2yf5aXMSBpHOwrIXUHnhPJwDulvIYfHajxy8yPgj0tXDoeR6ogXqmzmwwYxUsSEpJq487q0ag+YXHQExLZZUfWD5nnZgkcMEmn7jtMhbHOBHIAvuoK9xSFQFa8Ec06Ketq2XffXG4RtvjUhhhMjAJiiFhZ6/pg/XA9yw3RnrvlcbN2Nysj+xXeRVaX4SmZvzAqy1ey2zzx4LwePQyhbme7stZbvh1ELfBcTPlRnQwHUFSpXALZXKK5i3Hm0fxl6S1CKT/jKVo+1U2OmTdL/J32w/HQv1Bn25fFg9kYvwLbXReDhxpqokX3L0i6D4zTr24SqDCIAZU435ZQnzqPts3eZ9het67ZNhn6k4RnmsgXQduqZi3u6k6AdEu1znPoLIqc84f2qA4hhXgDeCN3DsnoGfPDCOslmnoDyO+W6SnJySZVTUUegxQGSg5IOBD8YEjIVmK3p5On6hhJtVgHR25XRFlnhNc2ouKkYfP/qKsqUbssSmu3/OLKtDRVpNhINb1XPGa9Uq5V+eKh0j4QkdIZxebv96kCVcqurr4EpkrjcHU9u8eZlOzqv+Vl5+PKD8zC5v/2nNy5zrvZfBhuuS5m1yzvhCOgiEmPMzLQmvOjc9HkPB0W2nGglYX+Ve1f+5sDA+822XEas3zKJkLsMA6DNJtxqTdDmF/0aDYbJVQ1WMV9lHijywI7kYNM34oDUsnpajafKWujQBtqnK5JCWALSpyI93oKV9V6E1bKrTuXxFRrbQQvTLfT3l5lU8dYMzxY0WnK9XCD4JAxZmpMWYDYZGJSe6G4oK8fzQndquhgqwvc3M5VVVCDn2GJwr2/L5oUqq5eNjTwTDOAIWAe4TgMDKi8nkUzI9jruobc/rNiIYNyoBKr8c32Cxy2fnDsc5DaqxgpM2Xn6VlrfUS2aSty2i+N4CFzrFOc5bTCPkIQQPHiVgKY6wwqb7v2kwdE3Vawizxt74dRS2tHqV4x0As2Gj1WoQ6ihCL+IlIQMZ3C1R//vO+CdFxgK7GdyZ3hg5Z29a+Ri0As8ociDYeU/hwtJ1YIEfRuw4dV/v987DFjNXlyT3GlKpHWH9a7snUpn3n0lDPYbV9Vk9pDwSKd3JDrPxMQQCGqAzx2EPIB4jeo+tVTVgbmhZe0pmc2sSmCVXnX7wQ3BKV+da8WDTN/gTsD3GH/2MJy5+KL4iylGSsmJ5EvoAGdghQzrkX8Gfqq2FEuJPMiQkW/ZJhgKYSFZSAsatdqaMD7Dj7oelCE/DgiXqL0Q1LFWDTJX5WJSVtJkovkfgddHk/Cs/DC1CB681sCYSGHsQs3bv9xrEIV3G9jkowofo6IuABMCNJqqLGINju3voqin9NpEqtmWN0lo2MeI4aimH976zc/gipffU9fG1iuL3CEv8VwiQeQSMf4s5RQFbcqkH078kUWD1Fa5D+rGdxycv1C3OSpVZtuQrGkfanQP4XJHZTTvO5ug9iMNEsVZgwFR4KXzrhhJQQng+4och6deMhvNOKC99nBwFTgpI7dGkxGlpQQPvbBGTsBvtZt66zpmetLqWokxACTvxKXqWkNjM7Pa7d9oBClqFVi/pUALYJjJ828SlLYUMqePKpvJ7biNQwCMhKBZTfuhsXFVWD3BmW2XUypiuc3nfR5zJNwHnw//CJtbNYO4Ndrk6YYDcRx0souZv6oIVGHir1GMxw+azEPxzbug8DZ4WQJXMLKEfH04HnlTD0AghK/xrMHwIKYBGheql1YYj+EIw1gV7OSf6Ym1huDbIyqpAadruh7+7I9/tLcVF32/PcwNmoVojw50iaa0ad8uWST3Lp8S5plCjE/jl7ySTzq5tpi6334AepnguHLx15HRCjyjah6iG8OpUv5YOUMFboPgrhSVbP6DaLBuNJTRHaiUlJaBCgCMcBgpAbaZb+SqHEc51YY0pwIb4JO3U73x8LS2aGG0EMho71MHleyM4S2gl0T1amr6rn1vZM8YWcpLIAhVo9CdUr2lzULW9iBE7v4BfwlVIj/bqELTwvka5ZDv/vBbM074OgQNnmL9NQD7cIPPs8lat26kdakVoRd0qtjloeqARhOjgfLO7FBZz3BRx6lLXm1DwKOEVHzVU03k0oGSLsFlmKblRdwZsxtcwm8PdMuGBrSiQ4QkCIbxfMQBQBBjlO50K/0gc7dPZxI1sfgfJ+czlKayOPbPhcldjANqt/5uXasV/n5nJkQ+aRlM5mqVPN56GiiDY2yz0pkZBBC/o3VpQPuIYPiU//McDV8hpwjAVd/v6oFhmt+qngHwoSf+/4VCtdpi8zFiqKLJrJZqBuzctTax/FZ8WiVyu7xStG3SS4AshJ/p2rD58ALji7fp8VyuKWoYzQkco+TNXL8RQPkQK2HmC2l+8v25iMPbPVKm+vezeNzTSYPoHRxSQ0t3jDvJkZDuzUOfjjggSxR275/ZtowJrwtwjxm3Anjhnc/zCVOiHgd7uAZHBuH3OTze+eJIaX7fT1Pb4qQAt6L5hKePawmpZ0+U0iPI67fVEf4Ow++FlPyi00LCIWxDWbK84hihA+Q/U0f6xn9+Tp027Ni+CQxqqfuH6f26az1P7pgM676kLyfx5mgxbQD4hD5kkp3RBdJjnvMMg4U6yF0eUaBCDys4SEcYDlwqlQWOZMDmz2V3wTXzyzGe3nPsEqN9kHzTJMvREAv27pQtZ9rm1M77nFT6iDIxasacVMitslg1qdq5ObRSugfsgDm/6vkXyjTXNJLKYhpsqWPnckxADqXcT8ra9o9Gm7S3BrnsD/WtFMGvV26X3WRyjovNxt824paY6km851VzrAXQenJraDpSdc8Bz67A7CbTGdCO8HF7R9486fI6TsXNdAFqL8tiAsBUGuyuWERRnbk4Fuh6T75b3GuchBaSOitvI2d9robchPT9sWkBUz/sr6zTLfAdRf+um5MyzmRzOF+OginL0QX91wjuRCdCD+O69jX2RrD9CQUjYNvdIoim4L1q1A8+Kh7fWTaBVuy9GYR9a0Hh3plWOQlDeCdgl4eRsZoEWDwblc/pDq3OIq1OQxSAAYLiim9xVvcZF4B3qlEVD2wZLZoVX014ba5P+fKvuw2WtniQeXKSYEy3eirTNzgUWjM8ne+zby+M3T7xW7HonRWaB/mb7wZoN5akGVTXx+ipOdRbxZ2vgjCK2svEDrWYWVzx/7W9Fgcyg/zlhpsysqFtuzlq3mNAY0vt5sloyNnj0x2jZbnv6gaS5hEO0BRPd7JncmY9RXoySyrq7WBzttV+b+xvh8+Sjrq5nskrvIYN7XU5TVk2Mm37o4rmYS10ODpuzrZgyKzhpIBd7vu93OfGavFjBdnsaJR8oJOaA2Jf8qFn1YOFe7obMMzJmiz9ajRS5oLILbhHIoYxkOyPAi8eTRIBZ2VTeuXB62YI1/JGLsL2eFNVv0F8r9i0eJxlxfhuK/bMUNRPq+lc9DtexszSVEWQ82RA0F7AvB4CZxUUCY2p1SBaoOJpdQ2TiSMVgc5AYhus3r5C2f5GOQnrBe5b5Z8l1yRYaawAKqVYyS5tmOI7p7RdF6fheOq0rUzWtlCO4z3dRHkvKleF8mIDKaxYgrEW720eDAOh3ORXRJai668prjyQceEpI/bttZgoUg3gI8hdACN7BYpuGFouopjxOU7opN+dtLLbS5Fia7+UcnX5HNhW6UyuobDa17PjGZwP6zuL9fxdzL2j8mieJ8r83eR2a4RFEIIjk6C8HPQmmCPdRzdza9a/DTUB6JiOFu/netMmAoHL4ySvOzJcevqJ4ZVbIN8/Od75AMep8Gl6GpP+3tibpPGm9dY36mcrMvgj/SOQhHobyaX+96kyyfxuyQ3TWsYvmWleChO3yQYteTrf1VlmYI0e2mViyGC7QxzpulCRHrgLFYYjMbkVHd4hS7jHj4Eo3WsjlxqypaEpUghdC4t0REnj9tWM46yrc1p8J14opX9QvmHnvUezUXq9S6RLlwdYcYk7ZK/kf2wG2Uzl0EjKKgC0LJFjKTP6ZQYIOV+8VemKso0Bs1JSXbcFacTE/QgO08KkGypKB/LqOpdhu8/94hrZQM36z3NprrKTRfFrc31OP5v9lEyrhbrUR06zw5h4GdVR+cZSSXoBbrgkqE7qPjQYKzuiv4WTRYOyPcAl3GrpBbsAcwr3wxOIPNmN5X5NPctersnz5A/eFTbBpz79iy3+M5lfP/YED42jtxoOX/2mLNjtnz8ZW8qruGYhpNeRd9zwNHAGtW7GdINn2ZSGatdunJDy1y+kPrzwk35Xc46ZNR+l3hTMetjkMHK/TjYNJdW7VIuVgbF8bEJ5zkp7APwhxgVz+tjMw1F/y6QCRfDH6jyc+IhgiYOx0mD9n9LXXVX/2/M594NzH5JydI4uMP5K0D+kM7SIn73+gf/qS75J6YV+tU/5w0ZTzw0BjXL41BBw7SrD4j7ZADZ1c2Arjt71TVbf65WIBbdmRkAL1EMOtNSIZQStlqQ8QwG0P2FLqbkVBW/RMQ6/qSyrigJIRwTNcVdkz5qPXOoXBUbzLXCNhRJD4sQaEJgs5YyLpau86aQS1AFxA0AQwd4pjcdXbaapp/SsOOAQ8DwPkouN0UtOwNb/rKFTpTsK8HZRc6kh8l2mXnaYBMsBYyghB9oLK8JRpfsf8+Gv1LNVO8YDXeNAOmDZp1VrXXQ5HecB6aV5qqSNyY3Lb1yxGJP4vsNezfUDI16xA/Vw+Nwzrli+h234/RSr1NPQi2HvSf7TVZnOnJykdFEawgYpwFjlfedd9TlwjRIQs9TZQJHwYSrCNpFuRRH1DIH6VtTakCPMQPEnws9cdAlMW1PUAMOS+64HT51huDBRVSWN0yB6Jj0olXQ/tujdx89A484lq1OJeTYEvC4YJsZp71oGhT28zlBI2WayeE3aRgy63h3RXxzpMCvdR1SwsgqJOxrqMFxs0t/wIf3NtNsKm52nz0Mo2lOHraDy2nxLmOLCUZwvgsww869z/EQVfIKjL03S+QLO+kR/EmkYEhDnM5empecAxkhqOYkvNTkTHAUnlXtwPsE+UWWUVz6srjs52kKu+XohvZZ3A/F7Rd3V+U4Pne0Xfg73MSV5a4HG23mLw0lgH8bgnDVCLf3kIWLjbqEff/yI/sx/ohLMDVJEI9Cy+97aMemLVGf9OMsFMndgct3J2s+4as5kyiGT5s1Kx4NM7Wa6XS+MK2Db6mUS+6JVVvbRieBxH2GIaERI6QuU3NNsl2Yqg1YqIz4oseJpFVO/xA6Yz7OhaznESxszbr1BLJrQyVQVW4/1n6JTICb9+dswYrGZMfz0lM3A94Anhnkxz2e3UzM35jA80TToZ8yFI79chY3kFKGKpShP5VatROXAK8Iyb96AaQ+YLvfXHiZLQGNw1L5RGRRJTWc4eo7XJlFQ4ghIZ9E2x7ea3BoxUV6aUv4YYToON9e8wf7e3HzN5S8mGA08pr7ziqqaxPb9x1F1fgAw</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Object Hallucination in Image Captioning </title>
      <link href="2020/03/25/Object-Hallucination-in-Image-Captioning/"/>
      <url>2020/03/25/Object-Hallucination-in-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul><li>当前captioning task 存在的问题<ul><li>当前的caption model 目前存在的问题是，生成的句子中出现的object 常常是在corresponding vision scene 中没有出现到的。</li><li>当前使用的评价指标只能评估 candidate caption 与 gt captions 之间的一个相似性，不能捕捉到candidate caption 与 image information之间的relevance. </li></ul></li></ul><ul><li>因此本文进行的一个工作：<ul><li>We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination.  </li></ul></li></ul><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><ul><li><p>（一）关于 object hallucination 的四点讨论</p><ul><li><p>从人类的角度</p><ul><li>丢失对显著物体的描述是一个 failure mode，但是captions is summaries，<strong>因此一般不期待其描述出场景中的所有的objects.</strong> 另外在人类的标注数据中，也不偏向于标注出所有出现在scene 中的objects</li><li>研究报告表明，human judgements 比较不待见那些caption中包含了image content中未出现的 object，<strong>Correctness is more important to human judges than specificity.   </strong></li><li>Many visually impaired who <strong>value correctness over coverage, </strong>hallucination is an obvious concern.  </li></ul></li><li>从模型的角度<ul><li>object hallucination  揭示了caption model 存在的一个问题，可能caption model并没有对视觉场景学习到一个很好的视觉表征，而是对损失函数过拟合。</li></ul></li></ul></li><li><p>（二）本文要研究的问题</p><ul><li>本文研究当前captioning models中存在的object hallucination现象</li><li>考虑了几个关键问题：<ul><li>(1) <strong>Which models are more prone to hallucination?</strong>  spanning different architectures and learning objectives.   <ul><li>一个新的评价指标来评估object hallucination：CHAIR (Caption Hallucination Assessment with Image Relevance)  </li></ul></li><li>(2) <strong>What are the likely causes of hallucination?   </strong><ul><li>造成object hallucination这一现象的原因主要有两点：visual misclassification and over-reliance on language priors  <ul><li>提出：image and language model consistency scores  </li></ul></li></ul></li><li>(3) <strong>How well do the standard metrics capture hallucination?</strong>  <ul><li>当前的评价指标并不能很好的捕捉到object hallucination 这一现象。</li></ul></li></ul></li></ul></li></ul><h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><h5 id="CHAIR-Metric"><a href="#CHAIR-Metric" class="headerlink" title="CHAIR Metric"></a>CHAIR Metric</h5><ul><li>同时使用GT sentence 和 coco image segmentation这两个信息 to measure object hallucination。</li></ul><p>  <img src="http://ww1.sinaimg.cn/large/006uWRWVly1gd7d25xmjcj30h605k0te.jpg" alt="搜狗截图20200326152719.png"></p><h5 id="Image-Consistency"><a href="#Image-Consistency" class="headerlink" title="Image Consistency"></a>Image Consistency</h5><ul><li>对比 <strong>caption model 与 image (alone) model</strong> 两个模型对于预测objects 之间的一致性误差。</li></ul><h5 id="Language-Consistency"><a href="#Language-Consistency" class="headerlink" title="Language Consistency"></a>Language Consistency</h5><ul><li>对比 <strong>caption model 与 sentence (alone) model</strong> 两个模型对于预测下一个word 之间的一致性误差。</li></ul><h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><h5 id="Which-Models-Are-More-Prone-To-Hallucination"><a href="#Which-Models-Are-More-Prone-To-Hallucination" class="headerlink" title="Which Models Are More Prone To Hallucination?"></a>Which Models Are More Prone To Hallucination?</h5><ul><li>一般情况下 ，在标准的evaluation metrics 上表现性能好的模型，在CHAIR metric 上也能表现的比较好，即object hallucination现象相对较弱。但是当模型基于 cider 进行强化学习的训练之后，则不是这种一致的现象。</li><li>（1）使用attention 的模型更加偏向于有较低的object hallucination；NBT模型在标准的evaluation metrics 上表现性能没有topdown-BB 好，但是CHAIR性能却更好，原因在于其使用的pre-trained  detector 与 captioning dataset is in a same domain 。</li><li>（2）当模型基于 cider 进行强化学习的训练之后，将会增加hallucination 的数量。</li><li>（3）LRCN Model 比 FC Model有更多的object hallucination，说明仅仅在第一个 time step 输入视觉特征，将会产生更少的object hallucination </li><li>（4）the GAN loss actually helps decrease hallucination.  the GAN loss encourages sentences to be human-like。</li><li>（5）CE loss: beam size 5, object hallucination 会比 lower beam size 小很多；self-critical loss: beam size sometimes leads to worse performance on CHAIR.   即object hallucination 数量会更多。</li></ul><h5 id="What-Are-The-Likely-Causes-Of-Hallucination"><a href="#What-Are-The-Likely-Causes-Of-Hallucination" class="headerlink" title="What Are The Likely Causes Of Hallucination?"></a>What Are The Likely Causes Of Hallucination?</h5><ul><li><p>We rely on the deconstructed TopDown models to analyze the impact of model components on hallucination  </p><ul><li>通过设计的 几个 deconstructed TopDown models 的分析可以看出，使得object hallucination 数量减少的原因是：due to access to feature maps with spatial locality, not the actual attention mechanism.  </li><li>LRCN Model 比 FC Model 有更多的object hallucination，说明仅仅在第一个 time step 输入视觉特征，将会产生更少的object hallucination 。作者在文中对这一现象给出的解释是，在每一步都输入视觉特征 fc_feature, 而不是 spatial feature, 这将导致对视觉特征的过拟合。</li></ul></li><li><p>Investigate what causes hallucination using the deconstructed TopDown models and the image consistency and language consistency scores. </p><ul><li>We note that models with less hallucination tend to make errors consistent with the image model, whereas models with more hallucination tend to make errors consistent with the  language model.  这说明有更少object hallutition 的models 有更强的能力从Image 中提取知识到句子生成过程中。</li><li>在Robust split 上进行实验发现，所有models之间的language consistency 差异度不大；相比于 Karpathy split，相对应下的models image consistency 有所下降。这是由于 Robust split 在测试集上，会出现 novel compositions of objects at test time. 使得所有的模型有很强的language prior.</li></ul></li><li><p>在训练过程中，分析FC model 的image/language consistency，结果发现在训练开始，与language model 的一致性更好，随着训练的结束，与 image model 的一致性更好。这说明，模型首先学习生成流畅的语言，而后再去学习结合视觉信息。</p><h5 id="How-Well-Do-The-Standard-Metrics-Capture-Hallucination"><a href="#How-Well-Do-The-Standard-Metrics-Capture-Hallucination" class="headerlink" title="How Well Do The Standard Metrics Capture Hallucination?"></a>How Well Do The Standard Metrics Capture Hallucination?</h5></li><li><p>作者分析了 standard metric 与 CHAIRs  之间的 pearsom correlation coefficient ，结果发现 SPICE 的一致性更好。</p></li><li><p>object hallucination can not be always predicted based on the traditional sentence metrics.  </p></li><li><p><strong>与当前使用的standard metrci 互为补充，可以提升与人类评分的一致程度。</strong></p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gd8997jm24j30gi08375g.jpg" alt="搜狗截图20200327100117.png"></p><p>这个意思是说，第一列，单独分析各个automatic metric 与 human judgement 的一致性。第二/三列，各个评价指标分别加上1-CHs/ 1-CHi 之后再与human judgement 计算一致性。可以发现，一致性得到提升。即 <strong>CHAIR is complementary to standard metrics</strong></p></li></ul><h5 id="Does-hallucination-impact-generation-of-other-words"><a href="#Does-hallucination-impact-generation-of-other-words" class="headerlink" title="Does hallucination impact generation of other words?"></a>Does hallucination impact generation of other words?</h5><ul><li>Hallucinating objects 影响句子生成的质量，不仅是由于 object 没有被正确的预测，也是由于hallucinated word 影响到了生成的其他的words.</li><li>通过比较TopDown 和 TD-Restrict 生成的句子可以分析这个现象。We find that after the hallucinated word is generated, the following words in the sentence are different 47.3% of  the time.  </li><li>一旦一个hallucination words 被生成，则其又会由于language prior(hallucinating a “cat” leading to hallucinating<br>a “chair”  )，产生更多的hallucination words 。</li></ul><h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4><ul><li><p>the popular self critical loss increases CIDEr score, but also the amount of hallucination. </p></li><li><p>CHAIR complements the standard sentence metrics in capturing human preference( judgements ).  </p></li><li><p>Models with stronger image consistency frequently hallucinate fewer objects, suggesting that strong visual processing is important for  avoiding hallucination.  </p></li><li><strong>Advises for captioning task:</strong> 仅使用CE-loss/ standard sentence metrics来优化，不太能解决object hallucination 这个问题，若同时以 image relevance 来优化，会更好。</li></ul><h4 id="yaya-总结"><a href="#yaya-总结" class="headerlink" title="yaya 总结"></a>yaya 总结</h4><ul><li>在设计评价指标上给我的几点启发<ul><li>(1) 不需要要求machine generated caption 可以概括所有的objects which have been occurred in the vision scene</li><li>(2) machine generated caption 进行评价时，正确性比全面性更加重要</li></ul></li><li><p>本文的一个巧妙的点</p><ul><li>本文为了查看object hallucination，使用COCO的80个类。对于candidata caption 首先将其token， 然后调整成单数形式，然后使用同义词的思想，去跟COCO 的80个类别进行匹配。</li><li>另外对于GT sentences，也提出一个list，这个地方不太知道它说的什么意思？？？？？  </li></ul></li><li>GVD 好像也类似的提到过类似的思想</li><li>关于the image consistency and language consistency scores.<ul><li>在这个得分的计算方式上，是否还有什么可以改进的地方？</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP: 自编码 and 自回归</title>
      <link href="2020/03/24/NLP-%E8%87%AA%E7%BC%96%E7%A0%81-and-%E8%87%AA%E5%9B%9E%E5%BD%92/"/>
      <url>2020/03/24/NLP-%E8%87%AA%E7%BC%96%E7%A0%81-and-%E8%87%AA%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<ul><li><p><a href="https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff" target="_blank" rel="noopener">https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff</a>      </p></li><li><p>这篇博文写的不错<br><a href="https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff" target="_blank" rel="noopener">https://www.infoq.cn/article/4SRM7UMVS4GdD9A90wff</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions</title>
      <link href="2020/01/16/VIFIDEL-Evaluating-the-Visual-Fidelity-of-Image-Descriptions/"/>
      <url>2020/01/16/VIFIDEL-Evaluating-the-Visual-Fidelity-of-Image-Descriptions/</url>
      
        <content type="html"><![CDATA[<h3 id="当前指标存在的问题"><a href="#当前指标存在的问题" class="headerlink" title="当前指标存在的问题"></a>当前指标存在的问题</h3><ul><li>BLEU, ROUGE, Meteor, CIDEr 这些指标， 他们依靠精确的字符串匹配来测量 condidate 文本和reference文献之间的surface-level 、 n-gram 重叠。当 references 有限的情况下，这会导致样本稀疏问题 （reference数量对 metric 得分有很大影响，因为reference 数量越多，多样性更好）。Meteor 通过匹配字典和释义表中的同义词来部分解决此问题，但受限于此类字典的可用性，也不能很好地适用于其他的 language。SPICE and BAST 通过计算语义级别的相似性来解决 exact string matching。但是这个方法严重的依赖于语言资源，例如 parsers, semantic role labellers, tailored rules, 使其很难适应到不同的语言和领域。</li></ul><h3 id="仅仅使用-reference-description-来-评估-image-description-的缺点"><a href="#仅仅使用-reference-description-来-评估-image-description-的缺点" class="headerlink" title="仅仅使用 reference description 来 评估 image description 的缺点"></a>仅仅使用 reference description 来 评估 image description 的缺点</h3><ul><li>受限于 reference 的数量，可能会造成<strong>样本稀缺</strong>的问题。</li><li>reference description 是<strong>主观的，有歧义的</strong>，可能不能涵盖 image 中所有的关键信息，可能只包含 image content 的一个子集。<strong>使用 object labels 可以解决这个问题</strong> </li><li>references 可能含有错误的信息。</li></ul><h3 id="基于-object-information-来-作为评价指标的优点"><a href="#基于-object-information-来-作为评价指标的优点" class="headerlink" title="基于 object information 来 作为评价指标的优点"></a>基于 object information 来 作为评价指标的优点</h3><ul><li><strong>少的标注时间消耗</strong>： 若仅使用 multiple descriptions 来作为参考，则必然需要人类为 每个 image 来标注 多个 descriptions，在标注数据上需要花费很多时间。且为每个 image 标注的description 数量越多，评估的越准确，则也需要更多的标注时间。</li><li>但是若使用基于 object imformation ， 则可以使用 predicted objects 或者 object annotations</li></ul><h3 id="本文的方法简述"><a href="#本文的方法简述" class="headerlink" title="本文的方法简述"></a>本文的方法简述</h3><p>我们认为，衡量特定标准的细粒度度量标准对于理解IDG系统如何比另一个系统更有用。 </p><p>我们专注于这样一种标准，即视觉保真度。 </p><p>该标准旨在衡量<strong>description相对于图像中所描述内容的真实性</strong>，对description中多余的信息进行惩罚，对正确的信息进行奖励</p><p>This criterion aims to measure how faithful a description is with respect to what is depicted in the image (i.e.<strong style="color:red;"> <strong>systems should be rewarded for describing elements depicted in the image and penalised for describing things that are not depicted</strong></strong>). </p><blockquote><p>Introduction</p></blockquote><h3 id="Modelling-object-importance-with-reference-descriptions"><a href="#Modelling-object-importance-with-reference-descriptions" class="headerlink" title="Modelling object importance with reference descriptions"></a>Modelling object importance with reference descriptions</h3><ul><li><p>human reference 可以作为一种guidance 提供信息 — 人类对该张图片关注的重点在哪里。</p></li><li><p>与 CIDEr 很相似，都是考虑与 human reference 之间的consensus 信息，但是有以下两点不同：  </p><p>（1）使用reference 来建模object的重要性，而不是直接将 候选与 参考进行比较。<br>（2）在语义空间使用word embedding来执行 word matching，而不是直接在计算表面的匹配度（eg: n-gram）   </p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
          <category> 评价指标 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TIGEr: Text-to-Image Grounding for Image Caption Evaluation</title>
      <link href="2020/01/15/TIGEr-Text-to-Image-Grounding-for-Image-Caption-Evaluation/"/>
      <url>2020/01/15/TIGEr-Text-to-Image-Grounding-for-Image-Caption-Evaluation/</url>
      
        <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>当前在图像描述领域使用的 automatic metric 仅仅考虑了 gt 与 pred sentence 之间的匹配度。这就存在问题：（1）给出的 references 可能不能  fully cover the image content。（2）自然语言本质上就是有歧义的（模棱两可的）</li><li><p>因此提出了 TIGEr，该指标，（1）不仅可以评估 pred caption 与  image content 之间的匹配度，（2）也能评估 pred caption 与 gt caption 之间的匹配度</p></li><li><p>（1） 对于  pred caption 与 gt caption， 使用预训练的 image-text grounding model 来 grounds the content of texts。然后分别比较 relevance ranking 和 distribution of grounding weights。</p></li><li>（2）计算 pred 与 gt caption 之间的匹配度时，不采用 n-gram matching，而是将他们映射到一个共同的语义空间，再对得到的映射向量进行比较。</li></ul><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul><li>现在方法的缺点<ul><li>Popular metrics, such as BLEU and CIDEr, are based solely on text matching between reference captions and machinegenerated captions, potentially leading to biased evaluations because references may not fully cover the image content and natural language is inherently ambiguous. </li></ul></li></ul><h4 id="本文提出的方法：TIGEr"><a href="#本文提出的方法：TIGEr" class="headerlink" title="本文提出的方法：TIGEr"></a>本文提出的方法：TIGEr</h4><ul><li>yaya:本文提出了一个learned-based 和 ruled-based 相结合的方法。（1）learned-based: 首先预训练一个 grounding model , 利用这个grounding model可以分别得到reference caption 和 candidate caption 对regions in image的关注度。（2）再对这两个关注度进行两个方面的比较：==<strong>a:</strong>==  The first one measures how similarly these image regions are ordered (by their grounding scores) in the two vectors. ==<strong>b:</strong>== The second one measures how similarly the<br>attention (indicated by grounding scores) is distributed among different regions of the image in the two vectors.(KL 散度) ==<strong>c:</strong>==The TIGEr score is the average score of the resulting two similarity scores.</li></ul><h4 id="关于实验部分，可以给人思考的点"><a href="#关于实验部分，可以给人思考的点" class="headerlink" title="关于实验部分，可以给人思考的点"></a>关于实验部分，可以给人思考的点</h4><ul><li>Changing Reference Number</li><li>Text Component Sensitivity</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gcmt5o6s5ij30do0b60vf.jpg" alt="搜狗截图20200308204541.png"><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gcmt5o7hyyj30b80eftc7.jpg" alt="搜狗截图20200308204629.png">    </p><h4 id="yaya-不赞同文中提到的观点"><a href="#yaya-不赞同文中提到的观点" class="headerlink" title="yaya 不赞同文中提到的观点"></a>yaya 不赞同文中提到的观点</h4><ul><li>For example, if a human judge wrote a caption solely based on a particular region of an image (e.g., a human face or a bottle as shown in Figure 1) while ignoring the rest of the image, then a good machine-generated caption would be expected to describe only the objects in the same region</li><li>yaya分析：<strong>不同的人</strong>当看到一张图片的时候，关注的点是不尽相同的，在这种情况下，由reference 对 region 的attention，当成machine generated caption <strong>target</strong> 本身就不是正确的！</li><li>yaya: 本文提出的方法，是希望 learning-based 和 ruled-based method 结合，但是显然，ruled-based metric 就会存在一些缺陷，比如，只能评价caption system performence 的某一方面，换句话说，评价的不够全面！</li><li>yaya认为该metric的一个缺点：该metric 同时依赖 reference caption 和 image content, 缺一不可。==对于那些没有reference captions的描述任务，就不可以适用。==</li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
          <category> 评价指标 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Graph Matching Networks for Learning the Similarity of Graph Structured Objects</title>
      <link href="2019/12/20/Graph-Matching-Networks-for-Learning-the-Similarity-of-Graph-Structured-Objects/"/>
      <url>2019/12/20/Graph-Matching-Networks-for-Learning-the-Similarity-of-Graph-Structured-Objects/</url>
      
        <content type="html"><![CDATA[<ul><li>ICML 2019</li></ul><h3 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h3><ul><li>本文主要是提出了一种 新的方法来计算图的相似度问题</li><li>普通的方法分别单独计算 graph vector，而后再计算graph 之间的相似性</li><li>新提出的方法在计算  graph vector 时考虑了 cross graph matching vector来得到 更具有判别性的 graph vector，从而更好的用于 计算 graph similarity.</li></ul><h3 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h3><ul><li>Graph Edit Distance（GED）：文中对问题进行了简化，两个graph（G1, G2），相同数量的节点数和边数, 如何变动一个图中的edge(i, j) 到 edge(i’, j’) 才能使两个graph 完全一样。以下是 GED=1 的例子。</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ga3ats0xyzj308p07jaaw.jpg" alt="搜狗截图20191220170651.png"></p><ul><li>该文主要是想解决graph 的相似性问题，而不是真正的要求解出来需要几步的 graph edit distance。因此对问题做了如下的设定：<code>positive pair:（原图G，对G变动一条边：G1）</code>，<code>negative pair: （原图G，对G变动两条边：G1）</code></li><li>positive pair 认为这两个 graph 是相似的，label=1; 而negative pair认为两个graph 是不相似的, label=-1。</li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ga3b3udt8bj30pe0bsgnq.jpg" alt="搜狗截图20191220171642.png"></p><ul><li><p>一般的计算两个graph之间的相似性问题采用上图中的左图的方法，分别单独计算出 graph vector，而后再计算 vector space similarity</p></li><li><p>而本文：计算两个graph之间的匹配，然后互相作为补充特征（cross-graph matching vector），得到更加 <strong>discriminative</strong>  graph representation， 从而更加有效的graph 之间的相似度问题。</p></li><li>yaya: 文中使用的是 <code>difference between node_i and its closest neighbor in the other graph</code>  来计算  <code>cross-graph matching vector</code> 。我认为还可以有其他的方法或许会更加有效。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 图卷积网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图卷积网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Quality Estimation for Image Captions Based on Large-scale Human Evaluations</title>
      <link href="2019/12/12/Quality-Estimation-for-Image-Captions-Based-on-Large-scale-Human-Evaluations/"/>
      <url>2019/12/12/Quality-Estimation-for-Image-Captions-Based-on-Large-scale-Human-Evaluations/</url>
      
        <content type="html"><![CDATA[<h3 id="Quality-Estimation-QE-of-image-captions"><a href="#Quality-Estimation-QE-of-image-captions" class="headerlink" title="Quality Estimation (QE) of image-captions"></a>Quality Estimation (QE) of image-captions</h3><ul><li>本文提出了在图像描述领域一个新的问题，Quality Estimation。由于当前的 automatic metric 非常依赖 ground-truth references，因此当一个模型训练好后，若是对一个 unseen images which don’t have gt sentence 进行描述，则无法对该描述进行评价。    </li></ul><h3 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h3><h4 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h4><ul><li>（1）首先在 Conceptual Captions dataset  上训练多个 image-captioning model （这个数据集会在比 coco上训练的captioning model 更好），captioning model的差异可以体现在 image feature extraction model，object detection提取的object 数量，caption decoder。    </li><li>（2）以上的 image-captioning model 可以为一个image 提供多个 sentence，作者对image 进行了脱敏处理    </li></ul><h4 id="数据打分及处理"><a href="#数据打分及处理" class="headerlink" title="数据打分及处理"></a>数据打分及处理</h4><ul><li><p>（1）这些 image-caption pairs 放到 crowdsource.google上让大家对这些 captioning，进行评价：好、坏或者跳过。每个image-captioning pair 被分配给10个人进行打分     </p></li><li><p>（2）得到收集的 rating image-captioning pairs 之后，对 unique image，将10个评分进行处理， using the equation y = round(mean(ri) ∗ 8)/8.     </p></li></ul><h4 id="QE-Model"><a href="#QE-Model" class="headerlink" title="QE Model"></a>QE Model</h4><ul><li>本文作者设计了两个模型（并把这两个模型进行融合）来处理 QE task。一个是使用到 image-captioning model，两一个是不使用   </li><li>（1）使用image-captioning model：<strong>Confidence-based Features QE Model</strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVgy1g9txhzr2swj30yc0i642k.jpg" alt="搜狗截图20191212143453.png"></li><li>（2） 不使用 image-captioning model：<strong>Generation-independent Bilinear QE model</strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVgy1g9txhzmt4rj30xs0apq5u.jpg" alt="搜狗截图20191212143515.png"></li></ul><h4 id="Spearman’s-ρ-Analysis"><a href="#Spearman’s-ρ-Analysis" class="headerlink" title="Spearman’s ρ Analysis"></a>Spearman’s ρ Analysis</h4><ul><li>该文的主要目的就是希望在 没有gt sentence 的情况下，对 unseen-image 进行描述时，可以给出一个caption 的评分。或者是说，该captioning与 人类的描述的相近程度。</li><li>该任务也是希望提出一个 machine learning metric similar to human evaluation （trained-metric），则对该模型好坏的一个的评判就是这个模型给出的评分与人类评分的相近程度。</li><li>predict： 模型对image-caption pair 的评分， Gt:  人类给出的评分</li><li>指标：Spearman’s correlation.  <a href="https://github.com/ShiYaya/spearman-rank" target="_blank" rel="noopener">my github explanation</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pth: save in py2, but load in py3</title>
      <link href="2019/12/11/pth-save-in-py2-but-load-in-py3/"/>
      <url>2019/12/11/pth-save-in-py2-but-load-in-py3/</url>
      
        <content type="html"><![CDATA[<h3 id="在torch-load-pth-时出现的问题：UnicodeDecodeError-39-utf-8-39-codec-can-39-t-decode-byte-0xba-in-position-0-invalid-start-byte"><a href="#在torch-load-pth-时出现的问题：UnicodeDecodeError-39-utf-8-39-codec-can-39-t-decode-byte-0xba-in-position-0-invalid-start-byte" class="headerlink" title="在torch.load(*.pth) 时出现的问题：UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xba in position 0: invalid start byte"></a>在torch.load(*.pth) 时出现的问题：<code>UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xba in position 0: invalid start byte</code></h3><ul><li><p>经过网络查询，发现是由于该文件是在 python2 下保存的，但是现在却是在python3下读取，而导致的错误</p></li><li><p>有的人给出了下面的解决方案(但是对于我是无效的)：    </p><p>来自：<a href="https://github.com/CSAILVision/places365/issues/25" target="_blank" rel="noopener">https://github.com/CSAILVision/places365/issues/25</a></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools import partial</span><br><span class="line">import pickle</span><br><span class="line">pickle.load = partial(pickle.load, <span class="attribute">encoding</span>=<span class="string">"latin1"</span>)</span><br><span class="line">pickle.Unpickler = partial(pickle.Unpickler, <span class="attribute">encoding</span>=<span class="string">"latin1"</span>)</span><br><span class="line">model = torch.load(model_file, <span class="attribute">map_location</span>=lambda storage, loc: storage, <span class="attribute">pickle_module</span>=pickle)</span><br></pre></td></tr></table></figure></li></ul><ul><li><p><strong>这里给出我的解决办法</strong>   </p><p>（1） 在python2 环境下读取该文件，然后用 pickle来保存   </p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tmp_data = torch.<span class="built_in">load</span>(model_file)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'tmp.pickle'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> <span class="built_in">file</span>:</span><br><span class="line">    pickle.dump(tmp_data, <span class="built_in">file</span>, protocol=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>（2）换到python3环境下，再读取pickle文件，再用torch.load来保存（这一点或可以省略）</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'tmp.pickle'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> <span class="built_in">file</span>:</span><br><span class="line">    tmp_data = pickle.<span class="built_in">load</span>(<span class="built_in">file</span>, encoding=<span class="string">'latin1'</span>)</span><br><span class="line">    </span><br><span class="line">torch.save(tmp_data, tmp_model.pth)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video</title>
      <link href="2019/12/02/Weakly-Supervised-Spatio-Temporally-Grounding-Natural-Sentence-in-Video/"/>
      <url>2019/12/02/Weakly-Supervised-Spatio-Temporally-Grounding-Natural-Sentence-in-Video/</url>
      
        <content type="html"><![CDATA[<ul><li><strong>ACL 2019</strong></li></ul><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li><p>image grounding取得了很大的进步，但是将该任务迁移到视频上，需要对视频中的每帧都标注region，这个工程量是巨大的。</p></li><li><p>为了避免这种标注工作的工程量，一些<strong>weakly-supervised</strong> video grouding工作【1】【2】被提出来，他们只提供了video-sentence pairs，没有提供 fine-grained regional annotations。在他们的 video grounding任务中，他们仅仅对名词和代词在 视频的静态帧进行grounding。</p></li><li><p>但是这种 grounding存在问题，比如sentence: A brown and white dog is lying on the grass and then it stands up. 但是帧中出现了多个狗，而我们给出的要搜索的对象仅仅是一个名词： ‘dog’，没有其他更多的信息，来进行更加具体地定位，那么就有可能定位错误。另外只对一张静态帧进行定位，也无法捕捉到object在时域上的动态变化。</p></li><li><p>基于上述的分析，本文提出了一个在video grounding上 weakly-supervised 的新任务：<strong>weakly-supervised spatio-temporally grounding sentence in video (WSSTG).</strong>    </p></li></ul><h3 id="Weakly-supervised-spatio-temporally-grounding-sentence-in-video"><a href="#Weakly-supervised-spatio-temporally-grounding-sentence-in-video" class="headerlink" title="Weakly-supervised spatio-temporally grounding sentence in video"></a>Weakly-supervised spatio-temporally grounding sentence in video</h3><ul><li>Specifically, given a natural sentence and a video, we aim to localize a spatio-temporal tube (i.e., a sequence of bounding boxes) ,（本文中作者将tube 称作 instance）</li><li>yaya: 相比于之前的video-grounding任务，同是 weakly-supervised，但是有两点不同：（1）是句子级别的描述，对要定位的对象的描述更加具体，而不是仅仅是个noun。（2）是要定位出一个 spatial-temporal tube，而不是仅在一张静态帧中定位出一个bbox。</li><li>这两点不同同时带来了优势和挑战</li><li>（1）细节性的描述可以消除歧义，但是如何捕捉句子中的语义并在video中定位出来是一个难题；（2）相比于在静态帧中定位一个bbox, 而是在video中定位一个tube,更能呈现出一个object在时域上的动态。但是，如何利用和建模tube的时空特性以及它们与句子的复杂关系提出了另一个挑战。</li><li>compared with 【2】: different from 【2】，whose text input consists of nouns/pronouns and output is a bounding box in a specific frame, we aim  to ground a natural sentence and output a spatio-temporal tube in the video. </li></ul><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul><li>提出了一个新任务：weakly-supervised spatio-temporally grounding sentence in video</li><li>针对该任务提出了一个method：提出了一个Attentive interactor利用 tube(instance) 与 sentence之间的细粒度的关系来计算 匹配度；提出了一个diversity loss来加强 reliable instance-sentence pairs 并惩罚 unreliable ones。</li><li>在VID object detection dataset 数据集的基础上，对tube(instance) 增加了description</li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><ul><li>该任务是 给出一个 a natural sentence query <strong>q</strong> and a video <strong>v</strong> 来定位一个spatial-temporal tube，作者也将这个tube 称作 instance。</li><li>由于是弱监督，因此仅仅只给出 video-sentence pair，细粒度的regional annotations不给出！</li><li>将该任务转为一个 Multiple instance learning problem。给定一个video，首先由instance generator【3】来生成一组instance proposals，然后再根据语义相似性来匹配 natural sentence query 和 instance。  </li></ul><h4 id="Instance-Extraction"><a href="#Instance-Extraction" class="headerlink" title="Instance Extraction"></a>Instance Extraction</h4><ul><li><strong>Instance Generation</strong> ：  先由faster rcnn提取object proposals，假设每帧提取N个proposal ， 然后根据【3】得到N个spatial-temporal tube</li><li><strong>Feature Representation</strong> ：I3D-RGB， I3D-Flow， frame-level RoI pooled feature   </li></ul><h4 id="Attentive-Interactor"><a href="#Attentive-Interactor" class="headerlink" title="Attentive Interactor"></a>Attentive Interactor</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g9io2sodynj30i40hk0v9.jpg" alt="搜狗截图20191202204720.png"></p><ul><li>（1）分别对 sequential visual features 和 sequential textual features 经过LSTM进行编码，LSTM每个step输出的隐层状态作为新的representation，得到新的visual feature 和 sentence representation</li><li>（2）依次以visual feature中的每个隐状态作为查询，以 sentence 所有隐状态作为key 和 value，输入Attention中，则得到了<strong>visual guided sentence feature</strong>。（直观的理解：在给定某一个视觉特征，用attention去分析要关注哪一个word）  </li></ul><h4 id="Matching-Behavior-Characterization"><a href="#Matching-Behavior-Characterization" class="headerlink" title="Matching Behavior Characterization"></a>Matching Behavior Characterization</h4><ul><li>用余弦函数计算 <code>i-th</code> visual feature 和 visual guided sentence features 之间的 匹配度</li><li>对所有的step 加和，则得到instance proposal 与 sentence 之间的匹配度</li></ul><h3 id="Training-Loss"><a href="#Training-Loss" class="headerlink" title="Training Loss"></a>Training Loss</h3><ul><li><p>论文对这里介绍的比较详细，参见论文。</p></li><li><p><strong>ranking loss</strong>： aiming at distinguishing aligned video-sentence pairs from the unaligned ones.  这个损失是希望不匹配的video-sentence之间计算出来的匹配度差一些，比如给网络输入不与该视频对应的句子。</p></li><li><strong>novel diversity loss</strong> ：to strengthen the matching behaviors between reliable instance-sentence pairs and penalize the unreliable ones from the aligned video-sentence pair.  这个损失主要是希望对一个video，在计算tube 与 sentence之间的匹配度时，希望不同的 tube之间的差异性（diversity）大一些！</li></ul><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul><li>一个video 给出了N个 tube proposal，当计算完匹配度之后，选取匹配度最大的那个proposal，然后计算与GT之间的 overlap【4】，若overlap 大于一个阈值，则任务预测正确。</li></ul><h3 id="Yaya-Analysis："><a href="#Yaya-Analysis：" class="headerlink" title="Yaya Analysis："></a>Yaya Analysis：</h3><ul><li><p><strong>此类任务可提升的point</strong></p></li><li><p>更好的 detector来获取 object proposal</p></li><li><p>更好的算法来获取 tube proposal</p></li><li><p>设计算法更好滴计算 sentence 与 tube proposal 匹配度！</p></li><li><p>对 rank loss 给予更多的约束，像此文：提出了一个novel  diversity loss</p></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li>【1】De-An Huang, Shyamal Buch, Lucio Dery, Animesh Garg, Li Fei-Fei, and Juan Carlos Niebles. 2018. <strong>Finding “it”: Weakly-supervised reference-aware visual grounding in instructional videos</strong>. In CVPR. </li><li>【2】Luowei Zhou, Nathan Louis, and Jason J Corso. 2018. <strong>Weakly-supervised video object grounding from text by loss weighting and object interaction</strong>. BMVC. </li><li>【3】Georgia Gkioxari and Jitendra Malik. 2015. <strong>Finding action tubes</strong>. In CVPR, pages 759–768. </li><li>【4】Masataka Yamaguchi, Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. 2017. <strong>Spatio-temporal person retrieval via natural language queries</strong>. In ICCV. </li></ul>]]></content>
      
      
      <categories>
          
          <category> Visual Grounding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Viusal Grounding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Finding It: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos</title>
      <link href="2019/12/02/Finding-It-Weakly-Supervised-Reference-Aware-Visual-Grounding-in-Instructional-Videos/"/>
      <url>2019/12/02/Finding-It-Weakly-Supervised-Reference-Aware-Visual-Grounding-in-Instructional-Videos/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Learning to Generate Grounded Visual Captions without Localization Supervision</title>
      <link href="2019/12/01/Learning-to-Generate-Grounded-Visual-Captions-without-Localization-Supervision/"/>
      <url>2019/12/01/Learning-to-Generate-Grounded-Visual-Captions-without-Localization-Supervision/</url>
      
        <content type="html"><![CDATA[<h3 id="ICLR-2020-under-view"><a href="#ICLR-2020-under-view" class="headerlink" title="ICLR 2020 under view"></a>ICLR 2020 under view</h3><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li><p>问题：在captioning 任务中，当前的评价指标并不能很好的反应生成的句子与该视频之间的契合度（Groud），有可能生成的句子只是基于在训练过程中学习到的priors（一种统计特性，而不是基于该视频本身）</p></li><li><p>当前模型对于 groud 这个任务，存在的困难：（1）由于当前的 language model 常使用 attention 机制来关注某一个 region，以此来预测下一个生成的单词。换句话说，就是在不知道将会生成什么单词的情况下，却要先定位region， 另外，一篇论文 [1] 提出，attention机制关注的region与人类所关注的并不一致（2）更难的是：传入 attention网络的是 RNN 的 hidden_state，由于 RNN 的记录历史的特性，这个输入包括的是过去所有的信息，而不是针对于某一个individual word。  </p></li></ul><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul><li>不同于 GVD，该文不使用 annotation bbox 作为监督信号，而是使用了 decoder + localizer + redecoder的结构来自我监督（self-supervision）</li><li>由于其自监督的特性，在一些infrequent word上该文的方法比监督的方法，效果更好</li><li>不仅使用一般的为每个 object class 计算 grounding accuracy， 还提出了一个新的指标：为每个sentence 计算grounding accuracy。</li></ul><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ul><li><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g9heo6n8glj318k0lp0ze.jpg" alt="搜狗截图20191201183637.png"></p></li><li><p><strong>分阶段训练</strong></p></li><li>（1）正常的 encoder-decoder先训练 ~30个epoch</li><li>（2）在正常的基础上进行添加。 （a）<strong>re-localize</strong>: language_lstm 会得到y1, y2, …, yT 个预测的序列，将这些序列作为attention机制中的 查询向量，赋给每个region一个attention系数，这样就可以在每一个step重构attention系数分配，这样也解决了motivation中提到的问题，即attention是由某一个individual word 而计算得来的。（b）每个step 有了attention对齐之后的attention_region， 再输入到language_LSTM中，得到<strong>再次预测的sequence of word</strong>。</li><li><p>在这第二阶段，就是两个loss 交叉熵损失进行权重加和来训练</p></li><li><p>可以发现一个问题，对于visual-words 和 non-visual-words都进行了re-localize。实际上对于，on a 等这类词汇，并不需要在image中找到 grounded region。 该文作者在补充材料里给出了一些额外的实验， eg, 将这些non-visual words 进行抑制，不计算reconstruction loss, 或者给这些localized region representation重新赋给invalid representaion。但是实验表明，在Flickr30 上性能（caption and ground）有提升，但是在 activity上（caption 没变化，ground下降）。</p></li><li>但是作者并没有给出分析，我个人总觉得实验设计的不完善，分析的也不多。</li></ul><h3 id="Measuring-grounding-per-generated-sentence"><a href="#Measuring-grounding-per-generated-sentence" class="headerlink" title="Measuring grounding per generated sentence"></a>Measuring grounding per generated sentence</h3><ul><li>提该指标的原因：（Such metrics （F1all, F1loc） are extremely stringent as captioning models are generally biased toward certain words in the vocabulary, given the long-tailed distribution of words. ）</li></ul><h3 id="Analysis-Grounding-performance-when-using-a-better-object-detector"><a href="#Analysis-Grounding-performance-when-using-a-better-object-detector" class="headerlink" title="Analysis:  Grounding performance when using a better object detector."></a>Analysis:  Grounding performance when using a better object detector.</h3><ul><li>在 Flickr30k Entities 上进行实验，分析 better detector 对 grounding性能的影响</li><li>（1）使用 GT box (ubrealistically) ，进行实验，发现 caption metric 和 grounding accuracy都有提升</li><li>（2）在 Flickr30k上训练一个detector（之前使用的是在 visual genome上训练好的），进行实验，发现，使得caption metirc下降，（作者分析：由于在本数据集上进行训练，得到的 the ROI features and their associated object predictions 更偏向于 该数据中的  the annotated object words 却不能很好地泛化以预测 diverse captions， 从而导致了captioning 指标下降）</li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv Batra.  <strong>Human attention in visual question answering: Do humans and deep networks look at the same regions?</strong>  Computer Vision and Image Understanding, 163:90–100, 2017. </p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vatex_challenge_solutions</title>
      <link href="2019/10/20/vatex-challenge-solutions/"/>
      <url>2019/10/20/vatex-challenge-solutions/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch 索引 切片</title>
      <link href="2019/10/10/pytorch-%E7%B4%A2%E5%BC%95-%E5%88%87%E7%89%87/"/>
      <url>2019/10/10/pytorch-%E7%B4%A2%E5%BC%95-%E5%88%87%E7%89%87/</url>
      
        <content type="html"><![CDATA[<h2 id="给出：四维张量，三维的索引。根据索引得到张量中对应的数据"><a href="#给出：四维张量，三维的索引。根据索引得到张量中对应的数据" class="headerlink" title="给出：四维张量，三维的索引。根据索引得到张量中对应的数据"></a>给出：四维张量，三维的索引。根据索引得到张量中对应的数据</h2><ul><li><p>四维张量，object_feats.shape = [bs, 28, 5, 1024]。一个video中截取 28帧，每帧提取5个object, 其特征向量维度为1024</p></li><li><p>三维索引，traj_idx.shape = [bs, 28, 5]。以第一帧 frame上的 5个object 作为anchor，找到以该anchor 作为开头的轨迹（即，在其余帧上的对应的objects的索引），</p></li><li><p>目标：由索引，提取对应的 object feature</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">traj_feats = object_feats.gather(<span class="number">2</span>, traj_idx.unsqueeze(<span class="number">3</span>).expand_as(object_feats))</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用 objects position 作为特征的论文</title>
      <link href="2019/10/10/%E4%BD%BF%E7%94%A8-objects-position-%E4%BD%9C%E4%B8%BA%E7%89%B9%E5%BE%81%E7%9A%84%E8%AE%BA%E6%96%87/"/>
      <url>2019/10/10/%E4%BD%BF%E7%94%A8-objects-position-%E4%BD%9C%E4%B8%BA%E7%89%B9%E5%BE%81%E7%9A%84%E8%AE%BA%E6%96%87/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对分类损失的优化</title>
      <link href="2019/09/23/%E5%AF%B9%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%98%E5%8C%96/"/>
      <url>2019/09/23/%E5%AF%B9%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>视频描述评价指标的分析</title>
      <link href="2019/09/21/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E7%9A%84%E5%88%86%E6%9E%90/"/>
      <url>2019/09/21/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E7%9A%84%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Motifs: Scene Graph Parsing with Global Context</title>
      <link href="2019/09/18/Neural-Motifs-Scene-Graph-Parsing-with-Global-Context/"/>
      <url>2019/09/18/Neural-Motifs-Scene-Graph-Parsing-with-Global-Context/</url>
      
        <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>给出了一个对 repeated structures 的量化分析</li><li>分析显示，（1）由object label对预测 relation label是有效的，但是反之却不成立。（2）在一个image graph中会出现重复的模板（eg：大象有耳朵，大象有鼻子，  XX has YY）</li></ul><h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><ul><li>给定 object feature 和 object label 来去预测 relation label</li></ul>]]></content>
      
      
      <categories>
          
          <category> 场景图解析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 场景图解析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>video captioning 任务的难点？</title>
      <link href="2019/09/07/video-captioning-%E4%BB%BB%E5%8A%A1%E7%9A%84%E9%9A%BE%E7%82%B9%EF%BC%9F/"/>
      <url>2019/09/07/video-captioning-%E4%BB%BB%E5%8A%A1%E7%9A%84%E9%9A%BE%E7%82%B9%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>视频描述任务中用到objects的论文总结</title>
      <link href="2019/09/01/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%94%A8%E5%88%B0objects%E7%9A%84%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/"/>
      <url>2019/09/01/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%94%A8%E5%88%B0objects%E7%9A%84%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<ul><li><p>CVPR 2018</p><ol><li>Fine-grained Video Captioning for Sports Narrative</li></ol></li><li><p>CVPR 2019</p><ol><li>Grounded Video Description</li><li>Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning</li><li>Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning 【再去读一遍】</li><li>Adversarial Inference for Multi-Sentence Video Description</li></ol></li><li><p>ACM 2019</p><ol><li>Hierarchical Global-Local Temporal Modeling for Video Captioning</li></ol></li></ul><h3 id="Grounded-Video-Description"><a href="#Grounded-Video-Description" class="headerlink" title="Grounded Video Description"></a>Grounded Video Description</h3><p><img src="https://i.loli.net/2019/09/02/Hvtk4BJVNQ2WwdM.png" alt="搜狗截图20190902104324.png"></p><ol><li><p>如何使用region feature？</p><p> 仅在language lstm 用到了 region featrue, attention 加权求和之后 与 cat[ fc, motion] features 对应元素相加（cat[fc, motion]也是在经过attention加权求和之后的）</p><p> 但我个人认为对应元素相加，并没有道理，相当于在 cat[ fc, motion] 的基础上增加了一个 bias，没有什么道理</p></li><li><p>region feature 的构成？</p><p>R：是 object detector  在 fc6 输出的 feature</p><p>Ms(R)：是 object detector 在 fc7 输出的feature (这里有一些细节的修改，具体见论文)</p><p>Ml： 是 position embedding</p></li></ol><p><img src="https://i.loli.net/2019/09/02/y4JkxlmLQpqaj5c.png" alt="搜狗截图20190902105022.png"></p><h3 id="Object-aware-Aggregation-with-Bidirectional-Temporal-Graph-for-Video-Captioning"><a href="#Object-aware-Aggregation-with-Bidirectional-Temporal-Graph-for-Video-Captioning" class="headerlink" title="Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning"></a>Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning</h3><p><img src="https://i.loli.net/2019/09/02/TIP7Ww3FnLNKzvu.png" alt="搜狗截图20190902144125.png"></p><ol><li><p>简要介绍本文的结构</p><p> 在encoder 部分，使用 object feature 和 frame feature，分别经过设计的VALD 得到更新的特征向量</p><p> 在 decoder 部分，对object feature 使用两层的attention, 先对 <strong>一个轨迹</strong>上的objects 进行attention 的加权求和，再对N different objects instances进行 attention 的加权求和，这样就可以得到对所有objects 的聚合表达</p><p> 轨迹：对于第一帧的ojects, 根据相似性分别去找其他帧与其对应的objects，而构成的时域轨迹。</p><p>  这里采用了前向轨迹，和后向轨迹两种，在decoder 输出预测的单词之后，进行融合。</p></li><li><p>如何使用region feature？<br> 仅有一个lstm ，在输入lstm前对objects features进行两层attention 加权求和后，与同样经过attention的frames feature进行加和（sum）。<br> 本文没有使用 motion feeture</p></li><li><p>region feature 的构成？<br> 非常简单，只有 appearance feature，但是经过了 obejct VLAD module！</p></li><li><font color="#0099ff" size="5" face="黑体">object feature 的 hierarchical attention 值得借鉴呢！<br>计算object 相似性的部分也不错</font></li></ol><h3 id="Spatio-Temporal-Dynamics-and-Semantic-Attribute-Enriched-Visual-Encoding-for-Video-Captioning"><a href="#Spatio-Temporal-Dynamics-and-Semantic-Attribute-Enriched-Visual-Encoding-for-Video-Captioning" class="headerlink" title="Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning"></a>Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning</h3><p><img src="https://i.loli.net/2019/09/02/T5AzpW8DHkVL2Oy.png" alt="搜狗截图20190902152617.png"></p><ul><li>此文没有太看懂</li></ul><ol><li><p>如何使用region feature？</p><p> 得到 obejcts sematics embeddding 一起其他三个信息，经过聚合之后得到特征向量v，再经过一个线性变换得到v，再送入decoder中</p></li><li><p>region feature 的构成？</p><p> 由 object detector 输出的特征，以及其他输出（objetcs 存现的频率、概率），来构建semantics</p></li></ol><h3 id="Hierarchical-Global-Local-Temporal-Modeling-for-Video-Captioning"><a href="#Hierarchical-Global-Local-Temporal-Modeling-for-Video-Captioning" class="headerlink" title="Hierarchical Global-Local Temporal Modeling for Video Captioning"></a>Hierarchical Global-Local Temporal Modeling for Video Captioning</h3><p><img src="https://i.loli.net/2019/09/02/m5xLQnzCJGsjWVc.png" alt="搜狗截图20190902161552.png"></p><ol><li><p>如何使用region features ?</p><p> encoder 部分由两层LSTM，第一层LSTM 构建 frames features 和 c3d features的 隐层状态，并送入第二层LSTM，</p><p>在第二层LSTM 的每一个step, 都对该step 对应帧上的 objetcs进行attention 加权求和，并送入LSTM中，得到该帧的objects 的聚合特征的隐层状态   </p><p> <img src="https://i.loli.net/2019/09/02/q6XNP8iSVzekyCE.png" alt="搜狗截图20190902165813.png"></p></li><li><p>region feature 的构成？</p><p>   每帧 objects features 的加权求和，再经过LSTM得到隐层状态</p></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>当前基于 objects feature 的论文，decoder 部分没有太大的新颖（一般都是Top-Down或者是 Soft-Attention），主要的新颖的地方是在 encoder 部分</li><li>encoder部分有的使用LSTM 以及attention 来更新 objects features；有的使用VLAD 来构建 行为特征，使用 objects 的时域轨迹和两层attention 来聚合特征；使用objetcs 的其他信息，比如 position 以及 label 等信息</li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>目标检测模型中的性能评估——MAP(Mean Average Precision))</title>
      <link href="2019/08/31/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E2%80%94%E2%80%94MAP-Mean-Average-Precision/"/>
      <url>2019/08/31/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E2%80%94%E2%80%94MAP-Mean-Average-Precision/</url>
      
        <content type="html"><![CDATA[<p>参考：<a href="https://blog.csdn.net/katherine_hsr/article/details/79266880" target="_blank" rel="noopener">https://blog.csdn.net/katherine_hsr/article/details/79266880</a></p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多标签图像分类任务的评价方法-mAP</title>
      <link href="2019/08/31/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95-mAP/"/>
      <url>2019/08/31/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95-mAP/</url>
      
        <content type="html"><![CDATA[<p>转载 from: <a href="http://blog.sina.com.cn/s/blog_9db078090102whzw.html" target="_blank" rel="noopener">http://blog.sina.com.cn/s/blog_9db078090102whzw.html</a></p><p>多标签图像分类（Multi-label   Image  Classification）任务中图片的标签不止一个，因此评价不能用普通单标签图像分类的标准，即mean  accuracy，该任务采用的是和信息检索中类似的方法—mAP（mean  Average  Precision）。mAP虽然字面意思和mean  accuracy看起来差不多，但是计算方法要繁琐得多，以下是mAP的计算方法：</p><p>首先用训练好的模型得到所有测试样本的confidence  score，每一类（如car）的confidence   score保存到一个文件中（如comp1_cls_test_car.txt）。假设共有20个测试样本，每个的id，confidence  score和ground  truth  label如下：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPgy6XQd58yJ15f" target="_blank" rel="noopener"><img src="http://s16.sinaimg.cn/mw690/002T2ChPgy6XQd58yJ15f" alt="img"></a> </p><p>接下来对confidence  score排序，得到：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPgy6XQd86isc4c" target="_blank" rel="noopener"><img src="http://s13.sinaimg.cn/mw690/002T2ChPgy6XQd86isc4c" alt="img"></a><em>这张表很重要，接下来的precision和recall都是依照这个表计算的</em>﻿</p><p>然后计算precision和recall，这两个标准的定义如下：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPgy6XQdjij4Ae8" target="_blank" rel="noopener"><img src="http://s9.sinaimg.cn/mw690/002T2ChPgy6XQdjij4Ae8" alt="img"></a></p><p>上图比较直观，圆圈内（true   positives + false  positives）是我们选出的元素,它对应于分类任务中我们取出的结果，比如对测试样本在训练好的car模型上分类，我们想得到top-5的结果，即：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPgy6XQdbTpla5c" target="_blank" rel="noopener"><img src="http://s13.sinaimg.cn/mw690/002T2ChPgy6XQdbTpla5c" alt="img"></a></p><p>在这个例子中，true   positives就是指第4和第2张图片，false   positives就是指第13，19，6张图片。方框内圆圈外的元素（false   negatives和true  negatives）是相对于方框内的元素而言，在这个例子中，是指confidence   score排在top-5之外的元素，即：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPgy6XQdcMwKCea" target="_blank" rel="noopener"><img src="http://s11.sinaimg.cn/mw690/002T2ChPgy6XQdcMwKCea" alt="img"></a> </p><p>其中，false   negatives是指第9，16，7，20张图片，true   negatives是指第1,18,5,15,10,17,12,14,8,11,3张图片。</p><p>那么，这个例子中Precision=2/5=40%，意思是对于car这一类别，我们选定了5个样本，其中正确的有2个，即准确率为40%；Recall=2/6=30%，意思是在所有测试样本中，共有6个car，但是因为我们只召回了2个，所以召回率为30%。</p><p>实际多类别分类任务中，我们通常不满足只通过top-5来衡量一个模型的好坏，而是需要知道从top-1到top-N（N是所有测试样本个数，本文中为20）对应的precision和recall。显然随着我们选定的样本越来也多，recall一定会越来越高，而precision整体上会呈下降趋势。把recall当成横坐标，precision当成纵坐标，即可得到常用的precision-recall曲线。这个例子的precision-recall曲线如下：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPgy6XQddBz7ze9" target="_blank" rel="noopener"><img src="http://s10.sinaimg.cn/mw690/002T2ChPgy6XQddBz7ze9" alt="img"></a></p><p>接下来说说AP的计算，此处参考的是PASCAL  VOC  CHALLENGE的计算方法。首先设定一组阈值，[0, 0.1, 0.2, …, 1]。然后对于recall大于每一个阈值（比如recall&gt;0.3），我们都会得到一个对应的最大precision。这样，我们就计算出了11个precision。AP即为这11个precision的平均值。这种方法英文叫做11-point interpolated average precision。</p><p>当然PASCAL VOC CHALLENGE自2010年后就换了另一种计算方法。新的计算方法假设这N个样本中有M个正例，那么我们会得到M个recall值（1/M, 2/M, …, M/M）,对于每个recall值r，我们可以计算出对应（r’ &gt; r）的最大precision，然后对这M个precision值取平均即得到最后的AP值。计算方法如下：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPzy76AuWjHOp29" target="_blank" rel="noopener"><img src="http://s10.sinaimg.cn/mw690/002T2ChPzy76AuWjHOp29" alt="img"></a></p><p>相应的Precision-Recall曲线（这条曲线是单调递减的）如下：</p><p><a href="http://blog.photo.sina.com.cn/showpic.html#url=http://album.sina.com.cn/pic/002T2ChPzy76AuH9Z6010" target="_blank" rel="noopener"><img src="http://s1.sinaimg.cn/mw690/002T2ChPzy76AuH9Z6010" alt="img"></a></p><p>AP衡量的是学出来的模型在每个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏，得到AP后mAP的计算就变得很简单了，就是取所有AP的平均值。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>到底ResNet在解决一个什么问题呢</title>
      <link href="2019/08/17/%E5%88%B0%E5%BA%95ResNet%E5%9C%A8%E8%A7%A3%E5%86%B3%E4%B8%80%E4%B8%AA%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%E5%91%A2/"/>
      <url>2019/08/17/%E5%88%B0%E5%BA%95ResNet%E5%9C%A8%E8%A7%A3%E5%86%B3%E4%B8%80%E4%B8%AA%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%E5%91%A2/</url>
      
        <content type="html"><![CDATA[<p>对知乎上回答的简单总结</p><hr><p><strong>一、引言：为什么会有ResNet？Why ResNet？</strong></p><ul><li>过拟合？<br>  不是！因为深层网络表现为训练误差和测试误差都比较高，所以不是过拟合</li><li><p>梯度消失？梯度爆炸？<br>  不是！因为已经使用了 batch normalization ，在很大程度上解决了梯度消失、爆炸的问题，（yaya：我个人认为对梯度消失问题有一定的帮助，毕竟梯度值为1）</p></li><li><p>深层网络退化的原因？</p><p>  由于非线性激活函数的存在，使得信息被丢失，而不能完整保留，所以，应该在网络中加入恒等映射</p></li></ul><p><strong>二、关于resnet网络结构 【没看懂为什么要有两层】  </strong></p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g62hrnrs6nj30h9048aax.jpg" alt></p><ul><li>yaya 分析：<br>一层：  relu(x +  w1 x)<br>两层：  relu(x +w2 relu(w1 x))</li></ul><p>​       既然非线性激活函数会把信息丢失，为什么不这样：relu(wx) + x ，因为这样是错误的，本身relu是需要放在输出后面，起到非线性的作用，但是这样，就不算作对输出的非线</p><p><strong>三、更多的理解    </strong></p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g62hpvudvxj30iu0cc3zi.jpg" alt></p><hr><p>yaya 的总结/理解</p><ol><li>resnet 解决的不是过拟合的问题，因为过拟合的现象是，train loss 小，但是val loss大，但是当前深层网络的问题是train loss大，val loss也大</li><li>resnet 提供了一个梯度为1的反向传播，在一定程度上解决了梯度消失的问题</li><li>FPN中指出，不同深度的网络的结合可以结合不同的分辨率，但是当前resnet 只跨越了一种分辨率，因此，没能很好地利用这一特点，因此desnet便被提出来</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>深入理解Batch Normalization批标准化</title>
      <link href="2019/08/15/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Batch-Normalization%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96/"/>
      <url>2019/08/15/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Batch-Normalization%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<ul><li>转载 from：<a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/8724433.html</a></li></ul><blockquote><p>这几天面试经常被问到BN层的原理，虽然回答上来了，但还是感觉答得不是很好，今天仔细研究了一下Batch Normalization的原理，以下为参考网上几篇文章总结得出。</p></blockquote><p>　　Batch Normalization作为最近一年来DL的重要成果，已经广泛被证明其有效性和重要性。虽然有些细节处理还解释不清其理论原因，但是实践证明好用才是真的好，别忘了DL从Hinton对深层网络做Pre-Train开始就是一个<strong>经验领先于理论分析</strong>的偏经验的一门学问。本文是对论文《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》的导读。</p><p>　　机器学习领域有个很重要的假设：<strong>IID独立同分布假设</strong>，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。那BatchNorm的作用是什么呢？<strong>BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。</strong></p><p>　　接下来一步一步的理解什么是BN。</p><p>　　为什么深度神经网络<strong>随着网络深度加深，训练起来越困难，收敛越来越慢？</strong>这是个在DL领域很接近本质的好问题。很多论文都是解决这个问题的，比如ReLU激活函数，再比如Residual Network，BN本质上也是解释并从某个不同的角度来解决这个问题的。</p><h2 id="一、“Internal-Covariate-Shift”问题"><a href="#一、“Internal-Covariate-Shift”问题" class="headerlink" title="一、“Internal Covariate Shift”问题"></a>一、“Internal Covariate Shift”问题</h2><p>　　从论文名字可以看出，BN是用来解决“Internal Covariate Shift”问题的，那么首先得理解什么是“Internal Covariate Shift”？</p><p>　　论文首先说明Mini-Batch SGD相对于One Example SGD的两个优势：梯度更新方向更准确；并行计算速度快；（为什么要说这些？因为BatchNorm是基于Mini-Batch SGD的，所以先夸下Mini-Batch SGD，当然也是大实话）；然后吐槽下SGD训练的缺点：超参数调起来很麻烦。（作者隐含意思是用BN就能解决很多SGD的缺点）</p><p>　　接着引入<strong>covariate shift的概念</strong>：<strong>如果ML系统实例集合<x,y>中的输入值X的分布老是变，这不符合IID假设</x,y></strong>，网络模型很难<strong>稳定的学规律</strong>，这不得引入迁移学习才能搞定吗，我们的ML系统还得去学习怎么迎合这种分布变化啊。对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是<strong>在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。</strong></p><p>　　然后提出了BatchNorm的基本思想：能不能<strong>让每个隐层节点的激活输入分布固定下来呢</strong>？这样就避免了“Internal Covariate Shift”问题了。</p><p>　　BN不是凭空拍脑袋拍出来的好点子，它是有启发来源的：之前的研究表明如果在图像处理中对输入图像进行白化（Whiten）操作的话——所谓<strong>白化</strong>，<strong>就是对输入数据分布变换到0均值，单位方差的正态分布</strong>——那么神经网络会较快收敛，那么BN作者就开始推论了：图像是深度神经网络的输入层，做白化能加快收敛，那么其实对于深度网络来说，其中某个隐层的神经元是下一层的输入，意思是其实深度神经网络的每一个隐层都是输入层，不过是相对下一层来说而已，那么能不能对每个隐层都做白化呢？这就是启发BN产生的原初想法，而BN也确实就是这么做的，<strong>可以理解为对深层神经网络每个隐层神经元的激活值做简化版本的白化操作。</strong></p><h2 id="二、BatchNorm的本质思想"><a href="#二、BatchNorm的本质思想" class="headerlink" title="二、BatchNorm的本质思想"></a><strong>二、</strong>BatchNorm的本质思想</h2><p>　　BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的<strong>激活输入值</strong>（就是那个x=WU+B，U是输入）<strong>随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近</strong>（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这<strong>导致反向传播时低层神经网络的梯度消失</strong>，这是训练深层神经网络收敛越来越慢的<strong>本质原因</strong>，<strong>而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</strong>，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是<strong>这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong></p><p>　　THAT’S IT。其实一句话就是：<strong>对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。</strong>因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。BN说到底就是这么个机制，方法很简单，道理很深刻。</p><p>　　上面说得还是显得抽象，下面更形象地表达下这种调整到底代表什么含义。</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180405225246905-37854887.png" alt="img"></p><p>  图1  几个正态分布</p><p>　　假设某个隐层神经元原先的激活输入x取值符合正态分布，正态分布均值是-2，方差是0.5，对应上图中最左端的浅蓝色曲线，通过BN后转换为均值为0，方差是1的正态分布（对应上图中的深蓝色图形），意味着什么，意味着输入x的取值正态分布整体右移2（均值的变化），图形曲线更平缓了（方差增大的变化）。这个图的意思是，BN其实就是把每个隐层神经元的激活输入分布从偏离均值为0方差为1的正态分布通过平移均值压缩或者扩大曲线尖锐程度，调整为均值为0方差为1的正态分布。</p><p>　　那么把激活输入x调整到这个正态分布有什么用？首先我们看下均值为0，方差为1的标准正态分布代表什么含义：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180405225314624-527885612.png" alt="img"></p><p>图2  均值为0方差为1的标准正态分布图</p><p>　　这意味着在一个标准差范围内，也就是说64%的概率x其值落在[-1,1]的范围内，在两个标准差范围内，也就是说95%的概率x其值落在了[-2,2]的范围内。那么这又意味着什么？我们知道，激活值x=WU+B,U是真正的输入，x是某个神经元的激活值，假设非线性函数是sigmoid，那么看下sigmoid(x)其图形：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407143109455-1460017374.png" alt="img"></p><p>图3. Sigmoid(x)</p><p>及sigmoid(x)的导数为：G’=f(x)*(1-f(x))，因为f(x)=sigmoid(x)在0到1之间，所以G’在0到0.25之间，其对应的图如下：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407142351924-124461667.png" alt="img"></p><p>图4  Sigmoid(x)导数图</p><p>　　假设没有经过BN调整前x的原先正态分布均值是-6，方差是1，那么意味着95%的值落在了[-8,-4]之间，那么对应的Sigmoid（x）函数的值明显接近于0，这是典型的梯度饱和区，在这个区域里梯度变化很慢，为什么是梯度饱和区？请看下sigmoid(x)如果取值接近0或者接近于1的时候对应导数函数取值，接近于0，意味着梯度变化很小甚至消失。而假设经过BN后，均值是0，方差是1，那么意味着95%的x值落在了[-2,2]区间内，很明显这一段是sigmoid(x)函数接近于线性变换的区域，意味着x的小变化会导致非线性函数值较大的变化，也即是梯度变化较大，对应导数函数图中明显大于0的区域，就是梯度非饱和区。</p><p>　　从上面几个图应该看出来BN在干什么了吧？其实就是把隐层神经元激活输入x=WU+B从变化不拘一格的正态分布通过BN操作拉回到了均值为0，方差为1的正态分布，即原始正态分布中心左移或者右移到以0为均值，拉伸或者缩减形态形成以1为方差的图形。什么意思？就是说<strong>经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。</strong></p><p>　　但是很明显，看到这里，稍微了解神经网络的读者一般会提出一个疑问：如果都通过BN，那么不就跟把非线性函数替换成线性函数效果相同了？这意味着什么？我们知道，如果是多层的线性函数变换其实这个深层是没有意义的，因为多层线性网络跟一层线性网络是等价的。这意味着网络的<strong>表达能力</strong>下降了，这也意味着深度的意义就没有了。<strong>所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale*x+shift)</strong>，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把这个值从标准正态分布左移或者右移一点并长胖一点或者变瘦一点，每个实例挪动的程度不一样，这样等价于非线性函数的值从正中心周围的线性区往非线性区动了动。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。当然，这是我的理解，论文作者并未明确这样说。但是很明显这里的scale和shift操作是会有争议的，因为按照论文作者论文里写的理想状态，就会又通过scale和shift操作把变换后的x调整回未变换的状态，那不是饶了一圈又绕回去原始的“Internal Covariate Shift”问题里去了吗，感觉论文作者并未能够清楚地解释scale和shift操作的理论原因。</p><h2 id="三、训练阶段如何做BatchNorm"><a href="#三、训练阶段如何做BatchNorm" class="headerlink" title="三、训练阶段如何做BatchNorm"></a>三、训练阶段如何做BatchNorm</h2><p>　　上面是对BN的抽象分析和解释，具体在Mini-Batch SGD下做BN怎么做？其实论文里面这块写得很清楚也容易理解。为了保证这篇文章完整性，这里简单说明下。</p><p>　　假设对于一个深层神经网络来说，其中两层结构如下：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180405213859690-1933561230.png" alt="img"></p><p>  图5  DNN其中两层</p><p>　　要对每个隐层神经元的激活值做BN，可以想象成每个隐层又加上了一层BN操作层，它位于X=WU+B激活值获得之后，非线性函数变换之前，其图示如下：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180405213955224-1791925244.png" alt="img"></p><p>  图6. BN操作</p><p>　　对于Mini-Batch SGD来说，一次训练过程里面包含m个训练实例，其具体BN操作就是对于隐层内每个神经元的激活值来说，进行如下变换：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407142802238-1209499294.png" alt="img"></p><p>　　要注意，这里t层某个神经元的x(k)不是指原始输入，就是说不是t-1层每个神经元的输出，而是t层这个神经元的线性激活x=WU+B，这里的U才是t-1层神经元的输出。变换的意思是：某个神经元对应的原始的激活x通过减去mini-Batch内m个实例获得的m个激活x求得的均值E(x)并除以求得的方差Var(x)来进行转换。</p><p>　　上文说过经过这个<strong>变换后某个神经元的激活x形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大导数值，增强反向传播信息流动性，加快训练收敛速度。**</strong>但是这样会导致网络表达能力下降，为了防止这一点，每个神经元增加两个调节参数（scale和shift），这两个参数是通过训练来学习到的，用来对变换后的激活反变换，使得网络表达能力增强，即对变换后的激活进行如下的scale和shift操作，这其实是变换的反操作：**</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407142923190-79595046.png" alt="img"></p><p>　　BN其具体操作流程，如论文中描述的一样：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407142956288-903484055.png" alt="img"></p><p>　　过程非常清楚，就是上述公式的流程化描述，这里不解释了，直接应该能看懂。</p><h2 id="四、BatchNorm的推理-Inference-过程"><a href="#四、BatchNorm的推理-Inference-过程" class="headerlink" title="四、BatchNorm的推理(Inference)过程"></a>四、BatchNorm的推理(Inference)过程</h2><p>　　BN在训练的时候可以根据Mini-Batch里的若干训练实例进行激活数值调整，但是在推理（inference）的过程中，很明显输入就只有一个实例，看不到Mini-Batch其它实例，那么这时候怎么对输入做BN呢？因为很明显一个实例是没法算实例集合求出的均值和方差的。这可如何是好？</p><p>　　既然没有从Mini-Batch数据里可以得到的统计量，那就想其它办法来获得这个统计量，就是均值和方差。可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量，因为本来就打算用全局的统计量，只是因为计算量等太大所以才会用Mini-Batch这种简化方式的，那么在推理的时候直接用全局统计量即可。</p><p>　　决定了获得统计量的数据范围，那么接下来的问题是如何获得均值和方差的问题。很简单，因为每次做Mini-Batch训练时，都会有那个Mini-Batch里m个训练实例获得的均值和方差，现在要全局统计量，只要把每个Mini-Batch的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量，即：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407143405654-1995556833.png" alt="img"></p><p>　　有了均值和方差，每个隐层神经元也已经有对应训练好的Scaling参数和Shift参数，就可以在推导的时候对每个神经元的激活数据计算NB进行变换了，在推理过程中进行BN采取如下方式：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407143658338-63450857.png" alt="img"></p><p>　　这个公式其实和训练时</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407143807788-1841864822.png" alt="img"></p><p>　　是等价的，通过简单的合并计算推导就可以得出这个结论。那么为啥要写成这个变换形式呢？我猜作者这么写的意思是：在实际运行的时候，按照这种变体形式可以减少计算量，为啥呢？因为对于每个隐层节点来说：</p><p>　　　　　　　　<img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407144519480-1024698421.png" alt="img">　　<img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407144549010-487189588.png" alt="img"></p><p>　　都是固定值，这样这两个值可以事先算好存起来，在推理的时候直接用就行了，这样比原始的公式每一步骤都现算少了除法的运算过程，乍一看也没少多少计算量，但是如果隐层节点个数多的话节省的计算量就比较多了。</p><h2 id="五、BatchNorm的好处"><a href="#五、BatchNorm的好处" class="headerlink" title="五、BatchNorm的好处"></a>五、BatchNorm的好处</h2><p>　　BatchNorm为什么NB呢，关键还是效果好。<strong>①**</strong>不仅仅极大提升了训练速度，收敛过程大大加快；②还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；③另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。**总而言之，经过这么简单的变换，带来的好处多得很，这也是为何现在BN这么快流行起来的原因。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux 文件名中有空格、括号 时如何操作</title>
      <link href="2019/08/14/linux-%E6%96%87%E4%BB%B6%E5%90%8D%E4%B8%AD%E6%9C%89%E7%A9%BA%E6%A0%BC%E3%80%81%E6%8B%AC%E5%8F%B7-%E6%97%B6%E5%A6%82%E4%BD%95%E6%93%8D%E4%BD%9C/"/>
      <url>2019/08/14/linux-%E6%96%87%E4%BB%B6%E5%90%8D%E4%B8%AD%E6%9C%89%E7%A9%BA%E6%A0%BC%E3%80%81%E6%8B%AC%E5%8F%B7-%E6%97%B6%E5%A6%82%E4%BD%95%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h3 id="如何处理-cd-cp"><a href="#如何处理-cd-cp" class="headerlink" title="如何处理 cd cp"></a>如何处理 <code>cd</code> <code>cp</code></h3><ul><li><p>将文件名用<strong>双引号</strong> 包起来</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmd = 'cp -r <span class="string">"&#123;&#125;"</span> <span class="string">"&#123;&#125;"</span>'.format(source_path, target_path)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python 3.3.3 字面量,正则,反斜杠和原始字符串</title>
      <link href="2019/08/14/python-3-3-3-%E5%AD%97%E9%9D%A2%E9%87%8F-%E6%AD%A3%E5%88%99-%E5%8F%8D%E6%96%9C%E6%9D%A0%E5%92%8C%E5%8E%9F%E5%A7%8B%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
      <url>2019/08/14/python-3-3-3-%E5%AD%97%E9%9D%A2%E9%87%8F-%E6%AD%A3%E5%88%99-%E5%8F%8D%E6%96%9C%E6%9D%A0%E5%92%8C%E5%8E%9F%E5%A7%8B%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
      
        <content type="html"><![CDATA[<ul><li>注明：转载 from <a href="https://www.cnblogs.com/xiangnan/p/3446904.html" target="_blank" rel="noopener">https://www.cnblogs.com/xiangnan/p/3446904.html</a></li></ul><h1 id="两个不起眼但是比较重要的设定"><a href="#两个不起眼但是比较重要的设定" class="headerlink" title="两个不起眼但是比较重要的设定"></a>两个不起眼但是比较重要的设定</h1><ul><li>Python str类型的字面量解释器</li></ul><p>当反斜杠及其紧接字符无法构成一个具有特殊含义的序列(‘recognized escape sequences’)时,Python选择保留全部字符.直接看例子:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\c'</span></span><br><span class="line"><span class="string">'\\c'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\d'</span></span><br><span class="line"><span class="string">'\\d'</span></span><br></pre></td></tr></table></figure><p>官方管’\c’这种序列叫’unrecognized escape sequences’.官方文档相应部分:</p><p>Unlike Standard C, all unrecognized escape sequences are left in the string unchanged, i.e., <em>the backslash is left in the string</em>. (This behavior is useful when debugging: if an escape sequence is mistyped, the resulting output is more easily recognized as broken.) </p><p>按这段英文的意思,估计C语言里面,’c’和’\c’是等同的.Python是’\\c’和’\c’等同.这个等以后学C语言再确定.</p><p>与上面对应的是,如果紧接字符能够和反斜杠构成’recognized escape sequences’的<strong>全部</strong>或者<strong>起始部分</strong>,中文就叫’被承认的转义序列’吧.比如:</p><p><a href="javascript:void(0" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>;)</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\b'</span></span><br><span class="line"><span class="string">'\x08'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\n'</span></span><br><span class="line"><span class="string">'\n'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\x'</span></span><br><span class="line"><span class="symbol">SyntaxError:</span> (unicode error) <span class="string">'unicodeescape'</span> codec can<span class="string">'t decode bytes in position 0-1: truncated \xXX escape</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; '</span>\N<span class="string">'</span></span><br><span class="line"><span class="string">SyntaxError: (unicode error) '</span>unicodeescape<span class="string">' codec can'</span>t decode bytes <span class="keyword">in</span> position <span class="number">0</span>-<span class="number">1</span>: malformed \N character escape</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\U'</span></span><br><span class="line"><span class="symbol">SyntaxError:</span> (unicode error) <span class="string">'unicodeescape'</span> codec can<span class="string">'t decode bytes in position 0-1: truncated \UXXXXXXXX escape</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; '</span>\u<span class="string">'</span></span><br><span class="line"><span class="string">SyntaxError: (unicode error) '</span>unicodeescape<span class="string">' codec can'</span>t decode bytes <span class="keyword">in</span> position <span class="number">0</span>-<span class="number">1</span>: truncated \uXXXX escape</span><br></pre></td></tr></table></figure><p><a href="javascript:void(0" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>;)</p><ul><li>Python re模块正则表达式解释器</li></ul><p>当反斜杠及其紧接字符无法构成一个具有特殊含义的序列(special sequences)时,re选择忽略反斜杠,例如:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\e'</span>,<span class="string">'eee'</span>)</span><br><span class="line">[<span class="string">'e'</span>, <span class="string">'e'</span>, <span class="string">'e'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'e'</span>,<span class="string">'eee'</span>)</span><br><span class="line">[<span class="string">'e'</span>, <span class="string">'e'</span>, <span class="string">'e'</span>]</span><br></pre></td></tr></table></figure><p>可见,’\e’和’e’起到了完全一样的效果.Python相关文档描述是:</p><p>If the ordinary character is not on the list, then the resulting RE will match the second character. For example, <code>\$</code> matches the character <code>&#39;$&#39;</code>.</p><p>与上面对应的是,如果能够构成special sequences,那么re会解释为相应含义.例如:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\w'</span>,<span class="string">'abcdefghijklmnopqrstuvwxyz'</span>)</span><br><span class="line">[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>, <span class="string">'g'</span>, <span class="string">'h'</span>, <span class="string">'i'</span>, <span class="string">'j'</span>, <span class="string">'k'</span>, <span class="string">'l'</span>, <span class="string">'m'</span>, <span class="string">'n'</span>, <span class="string">'o'</span>, <span class="string">'p'</span>, <span class="string">'q'</span>, <span class="string">'r'</span>, <span class="string">'s'</span>, <span class="string">'t'</span>, <span class="string">'u'</span>, <span class="string">'v'</span>, <span class="string">'w'</span>, <span class="string">'x'</span>, <span class="string">'y'</span>, <span class="string">'z'</span>]</span><br></pre></td></tr></table></figure><h1 id="字面量"><a href="#字面量" class="headerlink" title="字面量"></a>字面量</h1><p>字面量(Literals),是用于表示一些Python内建类型的常量的符号.最常见的字面量类型是str literals 和 bytes literals.</p><p>比如:</p><p><a href="javascript:void(0" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>;)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'abc'</span></span><br><span class="line"><span class="string">'abc'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">"abc"</span></span><br><span class="line"><span class="string">'abc'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'啊哦额'</span></span><br><span class="line"><span class="string">'啊哦额'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'abc'</span></span><br><span class="line"><span class="string">b'abc'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">r'\n'</span></span><br><span class="line"><span class="string">'\\n'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'啊哦额'</span></span><br><span class="line">SyntaxError: bytes can only contain ASCII literal characters.</span><br></pre></td></tr></table></figure><p><a href="javascript:void(0" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>;)</p><p>反斜杠\的用途按紧接其后的字符种类可划分为3类:</p><p>1.将特殊字符转换为字面量.这特殊字符包括(单引号,双引号,反斜杠):’”\</p><p>2.将普通字符转换为特殊序列.包括:abfNnrtuUvx0123456789.</p><p>(注意,bytes字面量中,NuU这三个普通字符无法被转义成特殊序列)</p><p>3.将”新行”和自身忽略掉.这个比较抽象,举例说明:py文件中,某个字符串太长了,以至于需要分两行写,那么你可以插个反斜杠,紧接着换行,然后写剩余字符串.</p><p>下面是官方文档归纳的表:</p><div class="table-container"><table><thead><tr><th>Escape Sequence</th><th>Meaning</th><th>Notes</th></tr></thead><tbody><tr><td><code>\newline</code></td><td>Backslash and newline ignored</td><td></td></tr><tr><td><code>\\</code></td><td>Backslash (<code>\</code>)</td><td></td></tr><tr><td><code>\&#39;</code></td><td>Single quote (<code>&#39;</code>)</td><td></td></tr><tr><td><code>\&quot;</code></td><td>Double quote (<code>&quot;</code>)</td><td></td></tr><tr><td><code>\a</code></td><td>ASCII Bell (BEL)</td><td></td></tr><tr><td><code>\b</code></td><td>ASCII Backspace (BS)</td><td></td></tr><tr><td><code>\f</code></td><td>ASCII Formfeed (FF)</td><td></td></tr><tr><td><code>\n</code></td><td>ASCII Linefeed (LF)</td><td></td></tr><tr><td><code>\r</code></td><td>ASCII Carriage Return (CR)</td><td></td></tr><tr><td><code>\t</code></td><td>ASCII Horizontal Tab (TAB)</td><td></td></tr><tr><td><code>\v</code></td><td>ASCII Vertical Tab (VT)</td><td></td></tr><tr><td><code>\ooo</code></td><td>Character with octal value <em>ooo</em></td><td>(1,3)</td></tr><tr><td><code>\xhh</code></td><td>Character with hex value <em>hh</em></td><td>(2,3)</td></tr></tbody></table></div><p>Escape sequences only recognized in string literals are:</p><div class="table-container"><table><thead><tr><th>Escape Sequence</th><th>Meaning</th><th>Notes</th></tr></thead><tbody><tr><td><code>\N{name}</code></td><td>Character named <em>name</em> in the Unicode database</td><td>(4)</td></tr><tr><td><code>\uxxxx</code></td><td>Character with 16-bit hex value <em>xxxx</em></td><td>(5)</td></tr><tr><td><code>\Uxxxxxxxx</code></td><td>Character with 32-bit hex value <em>xxxxxxxx</em></td><td>(6)</td></tr></tbody></table></div><p>举例:</p><p><a href="javascript:void(0" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>;)</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\N&#123;END OF LINE&#125;'</span></span><br><span class="line"><span class="string">'\n'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\N&#123;HORIZONTAL TABULATION&#125;'</span></span><br><span class="line"><span class="string">'\t'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\u9f6a'</span>==<span class="string">'齪'</span></span><br><span class="line">True</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\1'</span>==<span class="string">'\01'</span></span><br><span class="line">True</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\1'</span>==<span class="string">'\001'</span></span><br><span class="line">True</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="string">'\1'</span>==<span class="string">'\0000001'</span></span><br><span class="line">False</span><br></pre></td></tr></table></figure><p><a href="javascript:void(0" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>;)</p><h1 id="正则"><a href="#正则" class="headerlink" title="正则"></a>正则</h1><ul><li>正则表达式的反斜杠的作用</li></ul><p>一种是使紧跟在后面的元字符(special characters或metacharacters)失去特殊含义,变为字面量.这些元字符有14个:</p><p>.^$*+?{}<a href></a>|</p><p>另一种是使紧跟在后面的普通字符变得具有特殊含义.这些普通字符是:</p><p>AbBdDsSwWZ0123456789</p><p>以及在str字面量中能被反斜杠转义的字符:</p><p>\’”abfnrtuUvx0123456789</p><p>例如:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\"'</span>,<span class="string">'"'</span>)</span><br><span class="line">[<span class="string">'"'</span>]</span><br></pre></td></tr></table></figure><p>正则pattern的反斜杠的作用和Python字面量的反斜杠类似,这据说是带来”反斜杠灾难”的根源.最典型的莫过于你需要用正则’\\\\’才能匹配字面量反斜杠’\\’.</p><p>为方便说明,我们假设re.search(pattern,string)中,pattern表示正则表达式字符串,string表示待匹配的字符串.</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; re.search(<span class="string">'\\\\'</span>,<span class="string">'\\'</span>)</span><br><span class="line">&lt;_sre<span class="selector-class">.SRE_Match</span> <span class="selector-tag">object</span> at <span class="number">0</span>x02858528&gt;</span><br></pre></td></tr></table></figure><p>详细来说就是一个文本层级的反斜杠’\’(比如你在txt文件中看到的反斜杠),对应Python str 字面量的’\\’,对应正则pattern的’\\\\’.这个确实比较难以理解,实在不行就住这点就好:<strong>如果不是最简单的正则类型(比如’ab’),强烈推荐对pattern使用r前缀符</strong>.这样容易理解:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>re.search(<span class="string">r'\\'</span>,<span class="string">'\\'</span>)</span><br><span class="line">&lt;_sre.SRE_Match object at <span class="number">0x02858448</span>&gt;</span><br></pre></td></tr></table></figure><p>注意:</p><ul><li>1.多重含义的特殊序列处理机制</li></ul><p>b0123456789比较特殊,它们在Python字面量和re正则中都能和反斜杠构成作用不同的特殊序列.例如\b,在python 字面量中解释为”退格键”.re正则中解释为’单词边界’.<strong>python 字面量有优先解释权</strong>,如下可证:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\b'</span>,<span class="string">'\b'</span>)  <span class="comment">#'\b'被优先解释为退格键,而不是单词边界</span></span><br><span class="line">[<span class="string">'\x08'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\b'</span>,<span class="string">'\b'</span>)</span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\b'</span>,<span class="string">'b'</span>) </span><br><span class="line">[<span class="string">''</span>, <span class="string">''</span>]</span><br></pre></td></tr></table></figure><p>再比如:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'(a)\1\1'</span>,<span class="string">'aaa'</span>) <span class="comment">#\1按字面量优先解释为八进制字符串,因此无匹配结果</span></span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'(a)\\1\\1'</span>,<span class="string">'aaa'</span>)  <span class="comment">#\\1按正则引擎层级的反斜杠解释为第一个匹配组提取到的字符,相当于'(a)aa'</span></span><br><span class="line">[<span class="string">'a'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'a\1\1'</span>,<span class="string">'a\1\1'</span>) <span class="comment">#\1按字面量优先解释为八进制字符串,所以有匹配结果</span></span><br><span class="line">[<span class="string">'a\x01\x01'</span>]</span><br></pre></td></tr></table></figure><p>了解这个设置有什么用?</p><p>1.当你想使用正则层级的特殊序列\1时,如果你没有使用r作为前缀,那么你必须使用\\1才能如愿.</p><p>2.当你想使用字面量层级的特殊序列\1时,则不能使用r作为pattern前缀.</p><p>想想,你有可能在一个r前缀的字符串中写出能够匹配值为1的八进制字符串的pattern吗?</p><p>也许我太较真了,因为实践中好像从没遇到过需要匹配值为1的八进制字符串的情况,但理论上就是这样的.</p><ul><li><strong>2.正则表达式中特殊序列的准确定义的猜想</strong></li></ul><p>官方文档下面的一句话值得推敲:</p><p>Note that <code>\b</code> is used to represent word boundaries, and means “backspace” only inside character classes</p><p>意思是说\b只有在[…]里面时才表示退格键,这显然是错的.比如下面这个例子,\b没有在[]之内,但它是按”退格键”解释的,并非”单词边界”:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\b'</span>,<span class="string">'\b'</span>)</span><br><span class="line">[<span class="string">'\x08'</span>]</span><br></pre></td></tr></table></figure><p>除非官方文档描述的\b是指文本层面的数据(比如你在txt文档里看到的\b).</p><p>由此引出了一个猜想,re的正则pattern中”反斜杠+普通字符”构成特殊序列或”反斜杠+特殊字符”构成字面量—这种描述中的反斜杠准确来说是指两个反斜杠!</p><p>仍然是举例说明:</p><p><a href="javascript:void(0" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>;)</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\b\w+\\b'</span>,<span class="string">'one two three'</span>)  <span class="comment">#必须用\\b才能表示单词边界</span></span><br><span class="line">[<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\b\\w+\\b'</span>,<span class="string">'one two three'</span>)  <span class="comment">#想想,为什么\w和\\w都一样</span></span><br><span class="line">[<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\d'</span>,<span class="string">'123'</span>)</span><br><span class="line">[<span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\d'</span>,<span class="string">'123'</span>)</span><br><span class="line">[<span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>]</span><br></pre></td></tr></table></figure><p><a href="javascript:void(0" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>;)</p><ul><li>3.u和U只在str字面量中才能被转义,bytes字面量中是普通字符.</li></ul><p>以下是我猜测的正则表达式分析器和Python字面量分析器的传递规则表格:</p><div class="table-container"><table><thead><tr><th>Python string literal</th><th>values passed to regular expression</th><th>number of characters</th><th>what regular expression engine does</th><th>real meaning for regular expression</th></tr></thead><tbody><tr><td>\e</td><td>\e</td><td>2</td><td>ignore the backslash</td><td>e</td></tr><tr><td>\\e</td><td>\e</td><td>2</td><td>ignore the backslash</td><td>e</td></tr><tr><td>e</td><td>e</td><td>1</td><td>nothing spacial</td><td>e</td></tr><tr><td>\n</td><td>\n</td><td>1</td><td>nothing spacial</td><td>换行符</td></tr><tr><td>\\n</td><td>\n</td><td>2</td><td>\n is special</td><td>换行符</td></tr><tr><td>\b</td><td>\b</td><td>1</td><td>nothing spacial</td><td>退格键</td></tr><tr><td>\\b</td><td>\b</td><td>2</td><td>\b is special</td><td>word boundary</td></tr><tr><td>\s</td><td>\s</td><td>2</td><td>\s is special</td><td>Unicode whitespace characters</td></tr><tr><td>\\</td><td>\</td><td>1</td><td>must followed by a charcter</td><td>Can’t form any meaning</td></tr><tr><td>\\\\</td><td>\\</td><td>2</td><td>remove all special meanning of \</td><td>\</td></tr><tr><td>*</td><td>*</td><td>1</td><td>* is special</td><td>repeat the left characters 0 or more times</td></tr><tr><td>*</td><td>*</td><td>2</td><td>remove all special meanning of *</td><td>*</td></tr></tbody></table></div><p>最后是待探究的例子:</p><p><a href="javascript:void(0" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>;)</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\n'</span>,<span class="string">'\n\n'</span>)</span><br><span class="line">[<span class="string">'\n'</span>, <span class="string">'\n'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\n'</span>,<span class="string">'\n\n'</span>)</span><br><span class="line">[<span class="string">'\n'</span>, <span class="string">'\n'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\\n'</span>,<span class="string">'\n\n'</span>)</span><br><span class="line">[<span class="string">'\n'</span>, <span class="string">'\n'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\\\n'</span>,<span class="string">'\n\n'</span>)</span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\b'</span>,<span class="string">'\b\b'</span>)</span><br><span class="line">[<span class="string">'\x08'</span>, <span class="string">'\x08'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\b'</span>,<span class="string">'\b\b'</span>)</span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\\b'</span>,<span class="string">'\b\b'</span>)</span><br><span class="line">[<span class="string">'\x08'</span>, <span class="string">'\x08'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\\\b'</span>,<span class="string">'\b\b'</span>)</span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\c'</span>,<span class="string">'\c\c'</span>)</span><br><span class="line">[<span class="string">'c'</span>, <span class="string">'c'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\c'</span>,<span class="string">'\c\c'</span>)</span><br><span class="line">[<span class="string">'c'</span>, <span class="string">'c'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\\c'</span>,<span class="string">'\c\c'</span>)</span><br><span class="line">[<span class="string">'\\c'</span>, <span class="string">'\\c'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; re.findall(<span class="string">'\\\\c'</span>,<span class="string">'\c\c'</span>)</span><br><span class="line">[<span class="string">'\\c'</span>, <span class="string">'\\c'</span>]</span><br></pre></td></tr></table></figure><p><a href="javascript:void(0" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>;)</p><p>参考:</p><p>Python 3.3.3 官方文档</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>glob 之 **</title>
      <link href="2019/08/13/glob-%E4%B9%8B/"/>
      <url>2019/08/13/glob-%E4%B9%8B/</url>
      
        <content type="html"><![CDATA[<ul><li>该篇主要介绍glob的一些使用小技巧</li></ul><h3 id="想要获得某个文件目录下所有-指定文件格式-的所有文件"><a href="#想要获得某个文件目录下所有-指定文件格式-的所有文件" class="headerlink" title="想要获得某个文件目录下所有 指定文件格式 的所有文件"></a>想要获得某个文件目录下所有 <strong><em>指定文件格式</em></strong> 的所有文件</h3><ul><li><p>假设有一个文件环境如下图所示</p><p><img src="https://i.loli.net/2019/08/14/sjTANPfDuV6cord.png" alt="搜狗截图20190814100532.png"></p></li></ul><ul><li><p>比如想要获得<code>/userhome/dataset/MSVD/YouTubeClips/YouTubeClips</code> 下 <code>.avi</code>格式的所有文件</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">path</span> = <span class="string">'/userhome/dataset/MSVD/YouTubeClips/YouTubeClips/'</span></span><br><span class="line">glob.glob(<span class="built_in">path</span> + <span class="string">'*.avi'</span>)</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>想要获得某目录下的所有子目录中的所有指定文件格式的所有文件</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">path</span> = <span class="string">'/userhome/dataset/MSVD/YouTubeClips/'</span></span><br><span class="line">glob.glob(<span class="built_in">path</span> + <span class="string">'**/'</span> + <span class="string">'*.avi'</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">path</span> = <span class="string">'/userhome/dataset/MSVD/'</span></span><br><span class="line">glob.glob(<span class="built_in">path</span> + <span class="string">'**/'</span> + <span class="string">'**/'</span> + <span class="string">'*.avi'</span>)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title> pytorch clone() vs copy_()</title>
      <link href="2019/08/06/pytorch-clone-vs-copy/"/>
      <url>2019/08/06/pytorch-clone-vs-copy/</url>
      
        <content type="html"><![CDATA[<p><code>clone</code>() → Tensor</p><ul><li>反向传播时，将会返回到原来的变量上<br>Returns a copy of the <code>self</code> tensor. The copy has the same size and data type as <code>self</code>.</li><li>NOTE</li><li>Unlike copy_(), this function is recorded in the computation graph. Gradients propagating to the cloned tensor will propagate to the original tensor.</li></ul><hr><p><code>copy_</code>(<em>src</em>, <em>non_blocking=False</em>) → Tensor</p><ul><li><p>只是值得复制<br>Copies the elements from <code>src</code> into <code>self</code> tensor and returns <code>self</code>.</p></li><li><p>The <code>src</code> tensor must be <a href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics" target="_blank" rel="noopener">broadcastable</a> with the <code>self</code> tensor. It may be of a different data type or reside on a different device.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实验中遇到的问题及解决</title>
      <link href="2019/08/05/%E5%AE%9E%E9%AA%8C%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3/"/>
      <url>2019/08/05/%E5%AE%9E%E9%AA%8C%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3/</url>
      
        <content type="html"><![CDATA[<h3 id="问题-1"><a href="#问题-1" class="headerlink" title="问题 1"></a>问题 1</h3><ul><li>问题描述：首先表现为：在pycharm debug下和在running模式下的实验结果不一致，&lt;/br&gt;<br>后来，在训练阶段将预训练的模型保存下来，载入evaluate.py 文件中再次进行评估，得到的分数与在训练阶段评估的分数不一致</li><li>解决思路：由于第二个现象，更加容易解决，因此先解决他，师兄提出一个办法，将保存的模型再次载入，这样就可以有两个网络，然后比较两个网络的数据是在哪里出现差异的，这样可以找到问题。</li><li>解决办法：</li></ul><ol><li>在训练一个epoch 后，将模型保存了下来，然后用两个网络，一个时train.py中重新加载这个网络，一个是在evaluate.py中加载这个网络，将得到的结果，进行比较，（看输出的结果是否一致），然后发现，在一些video 输出的结果是一样的，在一些video是不一样的。&lt;/br&gt;</li><li>找到那些video对应的结果不一样的所对应的iteration，在该iteration打印出了网络中的部分变量的数据，发现，在dataloader的数据就是不一样的.&lt;/br&gt;</li><li>那么问题就是出现在数据加载上。通过对数据加载部分的代码进行调试，发现，仅在num_workers=0时，两个dataloader的数据才一样，而采用多线程的话，两个dataloader的数据不完全一样。而又在其他的代码上测试，多线程不会影响数据加载，那么问题就是出现在，自己设计的dataset上，&lt;/br&gt;</li><li>又发现在加载h5py文件时，没有取切片，而self.critical pytorch代码时加上了的，通过加上切片 <code>[:]</code> 发现在多线程时，是正常的。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 问题总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 问题总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-Label Image Recognition with Graph Convolutional Networks</title>
      <link href="2019/08/02/Multi-Label-Image-Recognition-with-Graph-Convolutional-Networks/"/>
      <url>2019/08/02/Multi-Label-Image-Recognition-with-Graph-Convolutional-Networks/</url>
      
        <content type="html"><![CDATA[<h3 id="Motivation：建模-label-之间的依赖"><a href="#Motivation：建模-label-之间的依赖" class="headerlink" title="Motivation：建模  label 之间的依赖"></a>Motivation：建模  label 之间的依赖</h3><ul><li>使用GCN来建模label之间的依赖</li><li>有向图</li><li>每个节点用 label 的词向量来表达</li></ul><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><h4 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h4><ul><li><p>GCN 的输入：GCN 的 输入是label的word embedding，使用预训练的glove vector，如果label 是含有多个词的，那么对这多个词的词向量取平均，</p></li><li><p>GCN的输出<code>C*D</code>是为了得到一个分类器，<code>C</code>是类别数，<code>D</code>是image representation的维度，</p></li><li><p>邻接矩阵：a<sub>ij</sub>用条件概率来表示：当label<sub>i</sub>出现时，label<sub>j</sub>出现的概率，因此这不是一个对称矩阵，具体地论文中还给出了更加细节的修改。</p></li></ul><h4 id="image-representation"><a href="#image-representation" class="headerlink" title="image representation"></a>image representation</h4><ul><li>使用 ResNet101 得到 conv5层的输出，再经过全局池化得到一个<code>D</code>维度的特征向量</li></ul><h4 id="multi-label-classifier"><a href="#multi-label-classifier" class="headerlink" title="multi-label classifier"></a>multi-label classifier</h4><ul><li>将上两步的输出进行矩阵相乘，就可以得到 计算的multi-label</li></ul><p><img src="https://i.loli.net/2019/08/03/cdwYEWSF9q6tk3p.png" alt="搜狗截图20190802221229.png"></p><h3 id="不同点-vs-semi-supervised-gcn"><a href="#不同点-vs-semi-supervised-gcn" class="headerlink" title="不同点 vs semi-supervised gcn"></a>不同点 vs semi-supervised gcn</h3><p>1.</p><ul><li>不同于一般的GCN，输入节点的特征，和边，经过GCN之后，得到的是更新后的节点特征</li><li>本文GCN的输出<code>C*D</code>是为了得到一个分类器，<code>C</code>是类别数，<code>D</code>是image representation的维度，</li><li>GCN 的 输入是label的word embedding，使用预训练的glove vector，如果label 是含有多个词的，那么对这多个词的词向量取平均</li></ul><p>2.</p><ul><li>一般的GCN的邻接矩阵是预先定义好的，</li><li>但是本文的邻接矩阵：need to construct the <code>A</code> from scrach</li></ul>]]></content>
      
      
      <categories>
          
          <category> 图卷积网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图卷积网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>word2vec</title>
      <link href="2019/08/02/word2vec-1/"/>
      <url>2019/08/02/word2vec-1/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>word2vec</title>
      <link href="2019/08/01/word2vec/"/>
      <url>2019/08/01/word2vec/</url>
      
        <content type="html"><![CDATA[<h3 id="使用one-hot-来作为词向量"><a href="#使用one-hot-来作为词向量" class="headerlink" title="使用one-hot 来作为词向量"></a>使用one-hot 来作为词向量</h3><ul><li>存在一个缺点，即，两个单词之间的余弦相似度为0，因为one-hot是两两正交的形式。</li><li>但是相似度为0，显然是不对的</li></ul><h3 id="word2vet"><a href="#word2vet" class="headerlink" title="word2vet"></a>word2vet</h3><ul><li>跳字模型：中心词生成背景词</li><li>连续词袋模型：背景词生成中心词</li><li><p>这两个模型存在的问题：在softmax中，由于分母是对整个vocab进行求和，导致反向传播的计算量非常大</p></li><li><p><a href="https://www.bilibili.com/video/av18512944/" target="_blank" rel="noopener">相关教程</a></p></li></ul><p>预训练模型</p><ul><li>glove</li><li>fasttext</li><li><a href="https://www.bilibili.com/video/av18795160/?spm_id_from=333.788.videocard.0" target="_blank" rel="noopener">相关教程</a></li><li>spacy</li><li><a href="https://shiyaya.github.io/2019/07/16/Spacy工具包/" target="_blank" rel="noopener">https://shiyaya.github.io/2019/07/16/Spacy%E5%B7%A5%E5%85%B7%E5%8C%85/</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>FPN</title>
      <link href="2019/08/01/FPN/"/>
      <url>2019/08/01/FPN/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="https://vision.cornell.edu/se3/wp-content/uploads/2017/07/fpn-poster.pdf" target="_blank" rel="noopener">poster</a></li><li><a href="https://blog.csdn.net/WZZ18191171661/article/details/79494534" target="_blank" rel="noopener">某篇博客</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>VideoGraph: Recognizing Minutes-Long Human Activities in Videos</title>
      <link href="2019/07/30/VideoGraph-Recognizing-Minutes-Long-Human-Activities-in-Videos/"/>
      <url>2019/07/30/VideoGraph-Recognizing-Minutes-Long-Human-Activities-in-Videos/</url>
      
        <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>当前基于CNN或者non-lcoal的方法，可以建模 temporal concepts，但是却不能建模分钟级长的时域依赖。</li><li>学习一个无向图，节点和边都是直接从video中得到，而不需要进行单独的节点标注。</li><li>这里的节点是：组成activity的一个unit-action，比如 “煎鸡蛋” 这个activity里的 “打破鸡蛋” 。</li><li>边，表示 (units-action) 运动单元之间的时域关系</li></ul><h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><ul><li>建模长范围的activity</li><li>捕捉到细节信息</li></ul><h3 id="Vs-Video-as-space-time-region-graph"><a href="#Vs-Video-as-space-time-region-graph" class="headerlink" title="Vs  Video as space-time region graph"></a>Vs  <code>Video as space-time region graph</code></h3><ul><li>Video as space-time region graph： 需要提取 key objects</li><li>Video graph：自动的从video中学到 nodes</li></ul>]]></content>
      
      
      <categories>
          
          <category> 行为识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 行为识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装pytorch_geometricc</title>
      <link href="2019/07/30/%E5%AE%89%E8%A3%85pytorch-geometricc/"/>
      <url>2019/07/30/%E5%AE%89%E8%A3%85pytorch-geometricc/</url>
      
        <content type="html"><![CDATA[<ul><li><p><a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html#frequently-asked-questions" target="_blank" rel="noopener">官方链接</a></p></li><li><p>下面是截取自官方</p></li></ul><h2 id="Directly-Installation"><a href="#Directly-Installation" class="headerlink" title="Directly Installation"></a>Directly Installation</h2><p>We have outsourced a lot of functionality of PyTorch Geometric to other packages, which needs to be installed in advance. These packages come with their own CPU and GPU kernel implementations based on the newly introduced <a href="https://github.com/pytorch/extension-cpp/" target="_blank" rel="noopener">C++/CUDA extensions</a> in PyTorch 0.4.0.</p><p>Note</p><p>We do not recommend installation as root user on your system python. Please setup an <a href="https://conda.io/docs/user-guide/install/index.html/" target="_blank" rel="noopener">Anaconda/Miniconda</a> environment or create a <a href="https://www.docker.com/" target="_blank" rel="noopener">Docker image</a>.</p><p>Please follow the steps below for a successful installation:</p><ol><li><p>Added  by yaya:</p><ul><li><p>may be you can select a conda environments, will be more fine</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3<span class="number">-5.0</span><span class="number">.0</span>-Linux-x86_64.sh</span><br><span class="line">conda create -n pytorch_geometric python=<span class="number">3.7</span> -y</span><br><span class="line">source activate pytorch_geometric</span><br></pre></td></tr></table></figure></li><li><p>after into env: pytorch_geometric</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">pip</span> <span class="selector-tag">install</span> <span class="selector-tag">torch-1</span><span class="selector-class">.1</span><span class="selector-class">.0-cp37-cp37m-manylinux1_x86_64</span><span class="selector-class">.whl</span>  # <span class="selector-tag">download</span> <span class="selector-tag">at</span> <span class="selector-tag">first</span></span><br><span class="line"><span class="selector-tag">pip</span> <span class="selector-tag">installl</span> <span class="selector-tag">numpy-1</span><span class="selector-class">.17</span><span class="selector-class">.0-cp37-cp37m-manylinux1_x86_64</span><span class="selector-class">.whl</span>  # <span class="selector-tag">download</span> <span class="selector-tag">at</span> <span class="selector-tag">first</span></span><br><span class="line"><span class="selector-tag">pip</span> <span class="selector-tag">install</span> <span class="selector-tag">scipy-1</span><span class="selector-class">.3</span><span class="selector-class">.0-cp37-cp37m-manylinux1_x86_64</span><span class="selector-class">.whl</span>  # <span class="selector-tag">download</span> <span class="selector-tag">at</span> <span class="selector-tag">first</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><ol><li><p>Ensure that at least PyTorch 1.1.0 is installed:</p><blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; $ python -c <span class="string">"import torch; print(torch.__version__)"</span></span><br><span class="line">&gt; <span class="meta">&gt;&gt;&gt; </span><span class="number">1.1</span>.<span class="number">0</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote></li><li><p>Ensure CUDA is setup correctly (optional):</p><blockquote><ol><li><p>Check if PyTorch is installed with CUDA support:</p><blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;    &gt; $ python -c <span class="string">"import torch; print(torch.cuda.is_available())"</span></span><br><span class="line">&gt;    &gt; <span class="meta">&gt;&gt;&gt; </span>True</span><br><span class="line">&gt;    &gt;</span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote><p>&gt;</p><blockquote><ol><li><p>Add CUDA to <code>$PATH</code> and <code>$CPATH</code> (note that your actual CUDA path may vary from <code>/usr/local/cuda</code>):</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda/bin:<span class="variable">$PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">echo</span> <span class="variable">$PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; &gt;&gt;&gt; /usr/<span class="built_in">local</span>/cuda/bin:...</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">export</span> CPATH=/usr/<span class="built_in">local</span>/cuda/include:<span class="variable">$CPATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">echo</span> <span class="variable">$CPATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; &gt;&gt;&gt; /usr/<span class="built_in">local</span>/cuda/include:...</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt;</span></span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote><p>&gt;</p><blockquote><ol><li><p>Add CUDA to <code>$LD_LIBRARY_PATH</code> on Linux and to <code>$DYLD_LIBRARY_PATH</code> on macOS (note that your actual CUDA path may vary from <code>/usr/local/cuda</code>):</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda/lib64:<span class="variable">$LD_LIBRARY_PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">echo</span> <span class="variable">$LD_LIBRARY_PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; &gt;&gt;&gt; /usr/<span class="built_in">local</span>/cuda/lib64:...</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">export</span> DYLD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda/lib:<span class="variable">$DYLD_LIBRARY_PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; $ <span class="built_in">echo</span> <span class="variable">$DYLD_LIBRARY_PATH</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt; &gt;&gt;&gt; /usr/<span class="built_in">local</span>/cuda/lib:...</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    &gt;</span></span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote><p>&gt;</p><blockquote><ol><li><p>Verify that <code>nvcc</code> is accessible from terminal:</p><blockquote><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="quote">&gt;    &gt; $ nvcc --version</span></span><br><span class="line"><span class="quote">&gt;    &gt; &gt;&gt;&gt; 10.0</span></span><br><span class="line"><span class="quote">&gt;    &gt;</span></span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote><p>&gt;</p><blockquote><ol><li><p>Ensure that PyTorch and system CUDA versions match:</p><blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;    &gt; $ python -c <span class="string">"import torch; print(torch.version.cuda)"</span></span><br><span class="line">&gt;    &gt; <span class="meta">&gt;&gt;&gt; </span><span class="number">10.0</span></span><br><span class="line">&gt;    &gt; </span><br><span class="line">&gt;    &gt; $ nvcc --version</span><br><span class="line">&gt;    &gt; <span class="meta">&gt;&gt;&gt; </span><span class="number">10.0</span></span><br><span class="line">&gt;    &gt;</span><br></pre></td></tr></table></figure></blockquote></li></ol></blockquote></li><li><p>Install all needed packages:</p><blockquote><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="symbol">$</span> you can see <span class="number">4.</span> first (optional)</span><br><span class="line">&gt; <span class="symbol">$</span> pip install --verbose --<span class="keyword">no</span>-cache-dir torch-scatter</span><br><span class="line">&gt; <span class="symbol">$</span> pip install --verbose --<span class="keyword">no</span>-cache-dir torch-sparse</span><br><span class="line">&gt; <span class="symbol">$</span> pip install --verbose --<span class="keyword">no</span>-cache-dir torch-cluster</span><br><span class="line">&gt; <span class="symbol">$</span> pip install --verbose --<span class="keyword">no</span>-cache-dir torch-spline-conv (optional)</span><br><span class="line">&gt; <span class="symbol">$</span> pip install torch-geometric</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote></li><li><p>added by yaya:<br>may be you can pip install scipy at first ,because above need it.</p></li></ol><h2 id="Docker-install"><a href="#Docker-install" class="headerlink" title="Docker install"></a>Docker install</h2><ul><li><a href="https://github.com/rusty1s/pytorch_geometric/tree/master/docker" target="_blank" rel="noopener">https://github.com/rusty1s/pytorch_geometric/tree/master/docker</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>GCN_LSTM  vs  SGAE</title>
      <link href="2019/07/30/gcn-on-captioning/"/>
      <url>2019/07/30/gcn-on-captioning/</url>
      
        <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Video Description: A Survey of Methods, Datasets and Evaluation Metrics</title>
      <link href="2019/07/29/Video-Description-A-Survey-of-Methods-Datasets-and-Evaluation-Metrics/"/>
      <url>2019/07/29/Video-Description-A-Survey-of-Methods-Datasets-and-Evaluation-Metrics/</url>
      
        <content type="html"><![CDATA[<h3 id="视频描述仍然处于起步阶段的原因"><a href="#视频描述仍然处于起步阶段的原因" class="headerlink" title="视频描述仍然处于起步阶段的原因"></a>视频描述仍然处于起步阶段的原因</h3><ul><li>对视频描述模型的分析是困难的，很难去判别是visual feature 亦或是 language model 哪个做的贡献大</li><li>当前的数据集，既没有包含足够的视觉多样性，也没有复杂的语言结构</li><li>当前的凭据指标并不能非常正确的去评估生成的句子与人类生成的句子之间的一致程度</li></ul><h3 id="the-difficulty-of-video-caption"><a href="#the-difficulty-of-video-caption" class="headerlink" title="the difficulty of video caption"></a>the difficulty of video caption</h3><ul><li>并不是在video中的所有object 都是与description相关的，可能其只是背景中的一个元素。    </li><li>此外，还需要objects的运动信息，以及 事件，动作，对象之间的因果关系。   </li><li>视频中的action可能有不同的长度，不同的action之间，可能有重叠。    </li></ul><h3 id="Sequence-Learning-based-Video-Captioning-Methods"><a href="#Sequence-Learning-based-Video-Captioning-Methods" class="headerlink" title="Sequence Learning based Video Captioning Methods"></a>Sequence Learning based Video Captioning Methods</h3><h4 id="CNN-RNN-based"><a href="#CNN-RNN-based" class="headerlink" title="CNN-RNN-based"></a>CNN-RNN-based</h4><ul><li><p>第一个 end-to-end：</p><p>S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. Mooney, and K. Saenko. 2014. Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729, (2014).<br><img src="https://i.loli.net/2019/07/29/5d3ea016090c918345.png" alt="图片1.png" title="图片1.png"></p></li><li><p>S2VT （变长输入，变长输出）</p><p>I. Sutskever, O. Vinyals, and Q. V. Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems. 3104-3112.<br><img src="https://i.loli.net/2019/07/29/5d3ea01536b3144846.png" alt="图片2.png" title="图片2.png">   </p></li><li><p>TA ( 加入C3D[1] )</p><p>L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and A.Courville. 2015. Describing videos by exploiting temporal structure. In IEEE ICCV<br><img src="https://i.loli.net/2019/07/29/5d3ea016a248c95582.png" alt="图片3.png" title="图片3.png">  </p></li><li><p>LSTM-E （making a common visual-semantic-embedding ）</p><p>Y. Pan, T. Mei, T. Yao, H. Li, and Y. Rui. 2016. Jointly modeling embedding and translation to bridge video and language. In IEEE CVPR.<br><img src="https://i.loli.net/2019/07/29/5d3ea421aaf9013065.png" alt="图片4.png" title="图片4.png"></p></li></ul><ul><li><p>GRU-EVE  ( short fourier transform)</p><p>N. Aafaq, N. Akhtar, W. Liu, S. Z. Gilani and A. Mian. 2019. Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning. In IEEE CVPR.<br><img src="https://i.loli.net/2019/07/29/5d3ea0163113561600.png" alt="搜狗截图20190729152752.png" title="搜狗截图20190729152752.png">   </p></li><li><p>h-RNN<br>H. Yu, J. Wang, Z. Huang, Y. Yang, and W. Xu. 2016. Video paragraph captioning using hierarchical recurrent neural networks. In IEEE CVPR.<br><img src="https://i.loli.net/2019/07/29/5d3ea63af2e0354548.png" alt="图片5.png" title="图片5.png"></p></li></ul><h4 id="RL-based"><a href="#RL-based" class="headerlink" title="RL-based"></a>RL-based</h4><ul><li><p>Z. Ren, X. Wang, N. Zhang, X. Lv, and L. Li. 2017. Deep reinforcement learning-based image captioning with embedding reward. arXiv preprint arXiv:1704.03899, (2017).</p></li><li><p>Y. Chen, S. Wang, W. Zhang, and Q. Huang. 2018.  ==Less Is More: Picking Informative Frames for Video Captioning.==  arXiv preprint arXiv:1803.01457, (2018).</p><p>提出了一个基于强化学习的方法，来选择 key informative frames 来表达一个 complete video ，希望这样的操作可以忽略掉噪声和不必要的计算。</p></li><li><p>L. Li and B. Gong. 2018. End-to-End Video Captioning with Multitask Reinforcement Learning. arXiv preprint arXiv:1803.07950,<br>(2018).</p></li><li><p>R. Pasunuru and M. Bansal. 2017. Reinforced video captioning with entailment rewards. arXiv preprint arXiv:1708.02300, (2017).</p></li><li><p>S. Phan, G. E. Henter, Y. Miyao, and S. Satoh. 2017. Consensusbased Sequence Training for Video Captioning. arXiv preprint arXiv:1712.09532, (2017).</p></li><li><p>X. Wang, W. Chen, J. Wu, Y. Wang, and W. Y. Wang. 2017.  ==Video Captioning via Hierarchical Reinforcement Learning.==  arXiv preprint arXiv:1711.11135, (2017).</p><p>在 decoder阶段，使用 深度强化学习，这个方法证明可以捕捉到视频内容中的细节，并生成细粒度的description，但是！这个方法相对于当前的baseline 没有多大的提高。（我自己还需要再看看， 使用DRL的motivation）</p></li></ul><h3 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h3><ul><li><p><a href="https://blog.csdn.net/joshuaxx316/article/details/58696552" target="_blank" rel="noopener">参考链接</a></p></li><li><p>BLEU、ROUGE、METEOR  来源于 机器翻译</p></li><li><p>CIDEr、SPICE 来源于图像描述   </p></li></ul><h4 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a>BLEU</h4><ul><li><a href="https://blog.csdn.net/allocator/article/details/79657792" target="_blank" rel="noopener">BLEU参考链接</a></li><li>==BLEU实质是对两个句子的共现词频率计算==，但计算过程中使用好些技巧，追求计算的数值可以衡量这两句话的一致程度。 </li><li>BLEU容易陷入常用词和短译句的陷阱中，而给出较高的评分值。本文主要是对解决BLEU的这两个弊端的优化方法介绍。</li><li>缺点</li></ul><ol><li>　不考虑语言表达（语法）上的准确性；<br>2.　 测评精度会受常用词的干扰；<br>3.　 短译句的测评精度有时会较高； </li><li>　没有考虑同义词或相似表达的情况，可能会导致合理翻译被否定；</li></ol><h4 id="ROUGE"><a href="#ROUGE" class="headerlink" title="ROUGE"></a>ROUGE</h4><p><img src="https://i.loli.net/2019/07/29/5d3ed71f2086769963.png" alt="20170228224903951.png" title="20170228224903951.png"></p><h4 id="METEOR"><a href="#METEOR" class="headerlink" title="METEOR"></a>METEOR</h4><p><img src="https://i.loli.net/2019/07/29/5d3edcce1761442736.png" alt="20170228225011405.png" title="20170228225011405.png">   </p><h4 id="CIDEr"><a href="#CIDEr" class="headerlink" title="CIDEr"></a>CIDEr</h4><p><img src="https://i.loli.net/2019/07/29/5d3edcce646d089162.png" alt="20170228225056046.png" title="20170228225056046.png"></p><h4 id="SPICE"><a href="#SPICE" class="headerlink" title="SPICE"></a>SPICE</h4><ul><li>基于 gt 和 pred 的场景图解析，来对预测结果进行评价，</li><li>不被广泛使用的原因是，当前sentence scene graph 的能力还比较若，很容易解析错误(eg:dog swimming through river”, the failure case could be the word “swimming” being parsed as “object” and the word “dog” parsed as “attribute” )</li><li>对句子解析错误了，那么给出的评价指标也不会很好！！！</li></ul><h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><p><img src="https://i.loli.net/2019/07/29/5d3edd503479c20027.png" alt="搜狗截图20190729194921.png" title="搜狗截图20190729194921.png">    </p><h3 id="当前的瓶颈："><a href="#当前的瓶颈：" class="headerlink" title="当前的瓶颈："></a>当前的瓶颈：</h3><h4 id="缺乏有效的评价指标"><a href="#缺乏有效的评价指标" class="headerlink" title="缺乏有效的评价指标"></a>缺乏有效的评价指标</h4><ul><li>我们的调查显示，阻碍这一研究进展的一个主要瓶颈是缺乏有效和有目的设计的视频描述评价指标。目前，无论是从机器翻译还是从图像字幕中，都采用了现有的度量标准，无法衡量机器生成的视频字幕的质量及其与人类判断的一致性。改进这些指标的一种方法是增加引用语句的数量。我们认为，从数据本身学习的目的构建的度量标准是推进视频描述研究的关键。    </li><li><p>王鑫也曾说：human evaluation在video captioning任务中是有必要的       </p><h4 id="视觉特征部分的瓶颈"><a href="#视觉特征部分的瓶颈" class="headerlink" title="视觉特征部分的瓶颈"></a>视觉特征部分的瓶颈</h4></li><li>在一个video中，可能出现多个activity，但是caption model只能检测出部分几个，导致性能下降。   </li><li>可能这个video中 action 的持续时间较长，但是，当前的video representation方法只能捕捉时域较短的运动信息（eg:C3D），因此不能很好地提取视频特征。   </li><li>大多数特征提取器只适用于静态或平稳变化的图像，因此难以处理突然的场景变化。目前的方法通过表示整体视频或帧来简化视觉编码部分。可能需要进一步探索注意力模型，以关注视频中具有重要意义的空间和时间部分。   </li><li>当前的encoder 与 decoder 部分，并 ==不是端到端的==，需要先提取 video representation再进行decoder，这样分布进行，而不是端到端的训练是不好的！    </li></ul><h3 id="captioning-model-的可解释性不足"><a href="#captioning-model-的可解释性不足" class="headerlink" title="captioning model 的可解释性不足"></a>captioning model 的可解释性不足</h3><ul><li>举个例子：当我们从包含“白色消防栓”的帧中看到视频描述模型生成的标题“红色消防栓”时，很难确定颜色特征是视觉特征提取器编码错误还是由于使用的语言模型bias( 由于有过多的训练数据是“红色消防栓)。<br><img src="https://i.loli.net/2019/07/29/5d3ee4996cf7480633.png" alt="搜狗截图20190729202028.png" title="搜狗截图20190729202028.png"></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li>[1] D. Tran, L. D. Bourdev, R. Fergus, L. Torresani, and M. Paluri. 2014. C3D: Generic Features for Video Analysis. CoRR abs/1412.0767, (2014). </li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Temporal Deformable Convolutional Encoder-Decoder Networks for Video Captioning</title>
      <link href="2019/07/28/Temporal-Deformable-Convolutional-Encoder-Decoder-Networks-for-Video-Captioning/"/>
      <url>2019/07/28/Temporal-Deformable-Convolutional-Encoder-Decoder-Networks-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="当前存在的问题"><a href="#当前存在的问题" class="headerlink" title="当前存在的问题"></a>当前存在的问题</h3><ul><li>RNN 存在梯度消失和梯度下降的问题</li><li>RNN 的本质的循环依赖，限制了其并行计算</li><li>因此本文提出了 ==Temporal Deformable Convolutional Encoder-Decoder Networks (dubbed as TDConvED) ==that fully employ convolutions in both encoder and decoder networks for video captioning. </li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Motion Guided Spatial Attention for Video Captioning</title>
      <link href="2019/07/28/Motion-Guided-Spatial-Attention-for-Video-Captioning/"/>
      <url>2019/07/28/Motion-Guided-Spatial-Attention-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="当前的问题"><a href="#当前的问题" class="headerlink" title="当前的问题"></a>当前的问题</h3><ul><li>spatial attention 很少有人去探索</li><li>motion information 被利用通常是使用3D-CNNs来作为另外一种模态</li></ul><h3 id="本文的工作"><a href="#本文的工作" class="headerlink" title="本文的工作"></a>本文的工作</h3><ul><li>两个贡献： MGSA、GARU</li><li>The proposed MGSA utilize motion information between consecutive frames by applying CNN to stacked optical flows. </li><li>In addition, a gated recurrent unit named GARU is designed to adaptively relate spatial attention maps across time.<br><img src="https://i.loli.net/2019/07/28/5d3d877e9c0d546970.png" alt="搜狗截图20190728193057.png" title="搜狗截图20190728193057.png">    </li></ul><h3 id="Encoder-部分我的理解"><a href="#Encoder-部分我的理解" class="headerlink" title="Encoder 部分我的理解"></a>Encoder 部分我的理解</h3><ul><li>对一个video 采取N帧，对这N帧提取appearences feature，得到<code>N*H*W*D</code>的特征向量</li><li>以每帧为中心，采取连续的M帧，这M帧计算optical flow，并将这个<code>N*M</code>帧的optical flow images送入CNN中，得到<code>N*H*W*1</code>的特征向量。</li><li>==构造一个长度为N的GRU时域序列，每次送入一帧==  appearence feature 和 optical flow cnn feature，并得到一个输出,维度为<code>H*W</code>，</li><li>该输出作为一个attention系数，并与 ==当前帧== frame feature 相乘。得到一个为该帧的每个像素点（<code>H*W</code>）分配的权重系数。即进行加权求和，则可以得到该帧的spatial attention</li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p><img src="https://i.loli.net/2019/07/28/5d3d87b91afaa36288.png" alt="搜狗截图20190728193156.png" title="搜狗截图20190728193156.png" width="440px" height="400px">    </p><h3 id="Experimental-results"><a href="#Experimental-results" class="headerlink" title="Experimental results"></a>Experimental results</h3><ul><li>这里只是想提一点，就是有一些论文在MSR-VTT上的实验结果，是使用了==音频信息==。<br><img src="https://i.loli.net/2019/07/28/5d3d80835814750581.png" alt="搜狗截图20190728190107.png" title="搜狗截图20190728190107.png"></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><p>基于 spatial attention的 video captioning model</p></li><li><p>Li, X.; Zhao, B.; and Lu, X. 2017. MAM-RNN: multi-level attention model based RNN for video captioning. In IJCAI, 2208–2214. </p></li><li><p>Yang, Z.; Han, Y.; and Wang, Z. 2017. Catching the temporal regions-of-interest for video captioning. In ACM MM, 146–153. attention, spatial. </p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning</title>
      <link href="2019/07/27/Object-aware-Aggregation-with-Bidirectional-Temporal-Graph-for-Video-Captioning/"/>
      <url>2019/07/27/Object-aware-Aggregation-with-Bidirectional-Temporal-Graph-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h3><ul><li>本文旨在于捕捉基于object的运动信息(capture object-based trajectory)，以前向流为例，以第一帧中的object regions 作为anchor， 来寻找在其他帧中相对应的regions， 计算该anchor 与 第i帧中的regions的相似性【相似性不仅考虑了特征相似性，还考虑了空间位置相似性】，然后相似性最大的那个region，认为是与anchor一致的objects， 然后将他们组成一组。反向流类似。  </li><li>这个捕捉运动信息的思想与 【Learning Video Representations from Correspondence Proposals】中的很相似。   </li></ul><h3 id="当前存在的问题"><a href="#当前存在的问题" class="headerlink" title="当前存在的问题"></a>当前存在的问题</h3><ul><li>当前的工作主要使用 global frame 或者是 salient regions而不是使用specific objects，那么将不能捕捉到每个object 的细节的时域动态。</li></ul><h3 id="文章的主要工作"><a href="#文章的主要工作" class="headerlink" title="文章的主要工作"></a>文章的主要工作</h3><h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><ul><li>constructs ==bidirectional temporal graph== to extract  the temporal trajectories for each object instance, which captures the detailed temporal dynamics in video content.    </li><li>==aggregation process on object regions==, which can capture the object-aware semantic information， 这里主要是得到了 VLAD[5, 6] representation   </li></ul><h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><ul><li><p>对object VLAD representation实施了temporal attention 和 object attention</p></li><li><p>对 frames VLAD representation 实施了 temporal attention</p></li><li><p>然后分别进行nn.linear 线性变换后，相加</p></li><li><p>再与word_embedding相加送入GRU</p></li><li></li><li><p><font color="#0099ff" size="5" face="黑体">前向流和后向流的融合：</font>在分别得到两流输出的word score 之后，进行 sum</p><p><img src="https://i.loli.net/2019/07/28/5d3d37324d6d283934.png" alt="搜狗截图20190728134820.png" title="搜狗截图20190728134820.png">   </p></li></ul><h3 id="本文的性能分析"><a href="#本文的性能分析" class="headerlink" title="本文的性能分析"></a>本文的性能分析</h3><ul><li>可以准确的描述video，比如关键的objects。</li><li><strong>但是！不能很好地去描述 objects 之间的交互</strong></li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li>【NetVLAD】Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: Cnn architecture for weakly supervised place recognition. In CVPR, pages 5297–5307, 2016.</li><li>【SeqVLAD】Youjiang Xu, Yahong Han, Richang Hong, and Qi Tian. Sequential video vlad: Training the aggregation locally and temporally. IEEE Transactions on Image Processing (TIP), 27(10):4933–4944, 2018</li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pointing Novel Objects in Image Captioning</title>
      <link href="2019/07/26/Pointing-Novel-Objects-in-Image-Captioning/"/>
      <url>2019/07/26/Pointing-Novel-Objects-in-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p><img src="https://i.loli.net/2019/07/27/5d3c1676f301a18995.png" alt="搜狗截图20190727171628.png" title="搜狗截图20190727171628.png"></p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><ul><li>当前的模型，都是以image-caption对来进行训练，因此训练模型只能输出in-domain objects，但是，在实际应用中有些图片含有丰富的信息，但是用现有的模型却不能充分的表达。   </li></ul><h4 id="解决1"><a href="#解决1" class="headerlink" title="解决1"></a>解决1</h4><ul><li>希望可以生成新的words,(没有出现在training dataset)   </li><li>本文提出了解决办法：用object learner 来扩增标准的deep caption 结构。即，由一个图像分类任务，则可以得到该图像中出现的obects。这可以作为一个补充信息，加入到当前现有的deep caption Model 中。   </li><li>具体地：（1）标准的LSTM decoder 会输出一个predicted word,  （2）objects learner 通过一个copying layer 也可以得到一个预测单词。那么该选谁，本文并不硬选择，而是软选择，即给一个系数，来给这两个分配个概率，然后加和。这个选择的过程称为 <strong>Pointing Mechanism</strong>   </li><li>loss:<br><img src="https://i.loli.net/2019/07/27/5d3c1c012cccc48971.png" alt="搜狗截图20190727173939.png" title="搜狗截图20190727173939.png">   </li></ul><h4 id="解决2"><a href="#解决2" class="headerlink" title="解决2:"></a>解决2:</h4><ul><li>希望将image中的所有信息，在句子中都可以覆盖到</li><li>提出了一个新的损失。target caption中含有 n词，即对应到image 中的 objects。那么希望生成的句子中含有的n词信息能够包含image中所有出现到的objects（即 target caption中的所有名词）</li><li>那么可以根据预测的单词是否生成了 target caption 中的名词，来计算损失（文章中这里在计算损失的时候忽略了语法结构，即不要求名词出现的在句子中的位置，只要求出现就可以）.   </li><li>loss:</li></ul><p><img src="https://i.loli.net/2019/07/27/5d3c1c754b56a83488.png" alt="搜狗截图20190727174134.png" title="搜狗截图20190727174134.png"><br><img src="https://i.loli.net/2019/07/27/5d3c1c7536e6674709.png" alt="搜狗截图20190727174147.png" title="搜狗截图20190727174147.png"> </p>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vision to language 大牛</title>
      <link href="2019/07/26/vision-to-language-%E5%A4%A7%E7%89%9B/"/>
      <url>2019/07/26/vision-to-language-%E5%A4%A7%E7%89%9B/</url>
      
        <content type="html"><![CDATA[<h3 id="王鑫"><a href="#王鑫" class="headerlink" title="王鑫"></a>王鑫</h3><p>Papers can be found at <a href="https://sites.cs.ucsb.edu/~xwang" target="_blank" rel="noopener">https://sites.cs.ucsb.edu/~xwang</a><br>Email: xwang@cs.ucsb.edu</p><h4 id="video-captioning-via-hierarchical-reinforcement-learning"><a href="#video-captioning-via-hierarchical-reinforcement-learning" class="headerlink" title="video captioning via hierarchical  reinforcement learning"></a>video captioning via hierarchical  reinforcement learning</h4><ol><li>强化学习</li><li>加入音频信号</li></ol><h4 id="zero-shot-video-captioning"><a href="#zero-shot-video-captioning" class="headerlink" title="zero-shot video captioning"></a>zero-shot video captioning</h4><ul><li>Topic-Aware Mixture of Experts (TAMoE)  <h4 id="evaluation"><a href="#evaluation" class="headerlink" title="evaluation"></a>evaluation</h4></li></ul><ol><li><p>如何去评判，本身就是一个问题，当前的评价指标并不是那么合理</p></li><li><p>human evaluation是一个必要的评测方法，尤其是对于生成story的</p></li></ol><h4 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h4><ul><li>利用强化学习直接对指标进行优化，很可能会造成，指标上去了，但是生成的句子语义并不好。所以提出了下篇论文</li></ul><ol><li>Adversarial REward Learning (AREL)</li></ol><h4 id="Connecting-Language-and-Vision-to-Actions"><a href="#Connecting-Language-and-Vision-to-Actions" class="headerlink" title="Connecting Language and Vision to Actions"></a>Connecting Language and Vision to Actions</h4><ul><li>Look Before You Leap: Model-based RL</li><li>Reinforced Cross-Modal Matching (RCM)</li></ul><h3 id="吴琦"><a href="#吴琦" class="headerlink" title="吴琦"></a>吴琦</h3><h4 id="从-Vision-到-Language-再到-Action，万字漫谈三年跨域信息融合研究"><a href="#从-Vision-到-Language-再到-Action，万字漫谈三年跨域信息融合研究" class="headerlink" title="从 Vision 到 Language 再到 Action，万字漫谈三年跨域信息融合研究"></a><a href="https://mp.weixin.qq.com/s/lnoL1TpKY8HQqCMaBqWA5Q" target="_blank" rel="noopener">从 Vision 到 Language 再到 Action，万字漫谈三年跨域信息融合研究</a></h4><h4 id="一文纵览-Vision-and-Language-领域最新研究与进展"><a href="#一文纵览-Vision-and-Language-领域最新研究与进展" class="headerlink" title="一文纵览 Vision-and-Language 领域最新研究与进展"></a><a href="https://mp.weixin.qq.com/s/dyY64QrvPWbjGvJw5H51OA" target="_blank" rel="noopener">一文纵览 Vision-and-Language 领域最新研究与进展</a></h4>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Video Representations from Correspondence Proposals</title>
      <link href="2019/07/26/Learning-Video-Representations-from-Correspondence-Proposals/"/>
      <url>2019/07/26/Learning-Video-Representations-from-Correspondence-Proposals/</url>
      
        <content type="html"><![CDATA[<h3 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h3><ul><li>与Non-local 类似，都是在现有CNN（2D， 3D）模型中加入一个设计的模块</li></ul><h3 id="CPNet-介绍"><a href="#CPNet-介绍" class="headerlink" title="CPNet 介绍"></a>CPNet 介绍</h3><p>（1）在CNN的某一层，得到了<code>T*H*W*d</code>的特征，这<code>T*H*W</code>个特征，是经过conv来的，即一个特征，返回到原图对应的是一个块（区域）的特征。   </p><p>（2）类似于graph 中的邻接矩阵的操作，计算这个<code>T*H*W</code>个节点之间的相似性，相似性近的前K个（且不在同一帧），认为他们之间存在对应关系，即找到了一个区域对应到其他帧的对应区域。   </p><p>（3）将原区域，与对应区域的特征，与他们之间的位置关系，输入到MLP中，得到了一个更新的特征。对每个对应区域都采取这样的操作，得到K个特征。取max，得到了一个鲁棒的特征（可以去掉不是对应块区域的特征，即去掉噪声）。   </p><p><img src="https://i.loli.net/2019/07/26/5d3a7afee6b0178187.png" alt="搜狗截图20190726120054.png" title="搜狗截图20190726120054.png"></p><ul><li>是不是跟Non-Local很像，==CP Module就是融合了相似区域的特征，对原区域的特征进行更新。==</li></ul><h3 id="Non-local-vs-CPNet"><a href="#Non-local-vs-CPNet" class="headerlink" title="Non-local  vs   CPNet"></a>Non-local  vs   CPNet</h3><ul><li><p>在toy dataset （figure4）上设计了toy model（两层 CNN）,将现有的三个SOTA model以及自己设计的CPNet上进行试验</p></li><li><p>可以看到 I3D，ARTNet ，TRN三个模型的效果都不是很好</p></li><li>ARTNet ，TRN 是由于只使用了两个卷积层，不能捕捉长范围的运动信息</li><li>Non-local 可以捕捉长范围的运动信息，但是为什么效果还是不好：==NL block 没有加进去位置信息==（作者这么说的原因，就是因为在他们的CP module中有position information）</li></ul><p><img src="https://i.loli.net/2019/07/26/5d3a6c44659ae30922.png" alt="搜狗截图20190726102546.png" title="搜狗截图20190726102546.png"></p><h3 id="CPNet-可以退化成这三个model-没看懂-之后可以结合代码再深入分析"><a href="#CPNet-可以退化成这三个model-没看懂-之后可以结合代码再深入分析" class="headerlink" title="CPNet 可以退化成这三个model: ( 没看懂 ,之后可以结合代码再深入分析)"></a>CPNet 可以退化成这三个model: ( 没看懂 ,之后可以结合代码再深入分析)</h3><p><img src="https://i.loli.net/2019/07/26/5d3a889e2ce0b69830.png" alt="搜狗截图20190726125811.png" title="搜狗截图20190726125811.png"></p>]]></content>
      
      
      <categories>
          
          <category> 行为识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 行为识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Memory-Attended Recurrent Network for Video Captioning</title>
      <link href="2019/07/25/Memory-Attended-Recurrent-Network-for-Video-Captioning/"/>
      <url>2019/07/25/Memory-Attended-Recurrent-Network-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="词频"><a href="#词频" class="headerlink" title="词频"></a>词频</h3><p>“&gt; =3”的保留</p><p>MSR-VTT :11K   MSVD:4K</p><h3 id="Attention-Decoder"><a href="#Attention-Decoder" class="headerlink" title="Attention Decoder"></a>Attention Decoder</h3><ul><li><p>采用SA-LSTM的结构</p></li><li><p>细节：</p><ul><li><p>==共享attention==<br>由于需要对frames_feature ==(L帧)==  与 C3D_feature ==（L帧 -&gt; L/16个特征向量）== 都进行attention，这里进行了共享attention，好处：   &lt;/br&gt;  </p><p>（1）将2D 和 3D 特征映射到相似的特征空间  &lt;/br&gt;</p><p>（2）像是一种正则化，减少了参数，避免过拟合  &lt;/br&gt;  </p></li><li><p>降维<br>将2D 和 3D 的2048维度的特征，降维到512</p></li></ul></li></ul><h3 id="Attended-Memory-Decoder"><a href="#Attended-Memory-Decoder" class="headerlink" title="Attended Memory Decoder"></a>Attended Memory Decoder</h3><ul><li><p>当前模型的不足：</p><ul><li>现有的模型在生成word的时候，只依赖于当前video的信息，而不能依赖于那些，出现过该单词的其他video的信息</li><li>生成下一个单词，仅依赖于video信息和当前单词，没有建模相邻两个单词之间的兼容性（没看懂）</li></ul></li><li><p>具体的memeory设计详见论文</p></li></ul><h3 id="Attention-Coherent-Loss-AC-Loss"><a href="#Attention-Coherent-Loss-AC-Loss" class="headerlink" title="Attention-Coherent Loss (AC Loss)"></a>Attention-Coherent Loss (AC Loss)</h3><ul><li>将C3D 输入的L帧作为1个time interval,希望对一个time interval 中的frames feature 的attention系数值相近</li><li>仅对frames_features 的attention 系数，计算这样的一个loss</li></ul><p><img src="https://i.loli.net/2019/07/25/5d397582d36f640160.png" alt="搜狗截图20190725172350.png" title="搜狗截图20190725172350.png"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>深度层次化图卷积神经网络</title>
      <link href="2019/07/25/%E6%B7%B1%E5%BA%A6%E5%B1%82%E6%AC%A1%E5%8C%96%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>2019/07/25/%E6%B7%B1%E5%BA%A6%E5%B1%82%E6%AC%A1%E5%8C%96%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h3 id="池化-可以扩大感受野"><a href="#池化-可以扩大感受野" class="headerlink" title="池化 可以扩大感受野"></a>池化 可以扩大感受野</h3><h3 id="GCN（两层）-node-classification"><a href="#GCN（两层）-node-classification" class="headerlink" title="GCN（两层）( node classification )"></a>GCN（两层）( node classification )</h3><ul><li>基于邻域聚合的</li><li><code>H= AXW</code><h3 id="deeper-insight-into-graph-convolutional-networks-for-semi-supervised-learning"><a href="#deeper-insight-into-graph-convolutional-networks-for-semi-supervised-learning" class="headerlink" title="deeper insight into graph convolutional networks for semi-supervised learning"></a>deeper insight into graph convolutional networks for semi-supervised learning</h3></li><li>GCN层数多效果不好：特征之间过于平滑<h3 id="GAT-（两层）-GraphSAGE"><a href="#GAT-（两层）-GraphSAGE" class="headerlink" title="GAT （两层）  GraphSAGE"></a>GAT （两层）  GraphSAGE</h3></li><li>两层，感受野小，2-hop</li></ul><h3 id="Hierarchical-Graph-Representation-Learning-with-Differentiable-Pooling-（graph-classification）"><a href="#Hierarchical-Graph-Representation-Learning-with-Differentiable-Pooling-（graph-classification）" class="headerlink" title="Hierarchical Graph Representation Learning with Differentiable Pooling （graph classification）"></a>Hierarchical Graph Representation Learning with Differentiable Pooling （graph classification）</h3><ol><li>优点</li></ol><ul><li>简单的两层GCN 的感受野只有2-hop</li><li>但是如果GCN- clusterpooling，把相同的节点聚类在一起，再进行GCN，那么感受野就会扩大，</li><li>捕捉到了graph 中的Hierarchical  structure</li></ul><ol><li>缺点</li></ol><ul><li>但是由于他自身网络设计的，一次池化，就需要一个全连接层，使得想要设计一个很深的网络，就需要很多的参数，容易过拟合</li><li>很难去训练pooling matrix，这是由于不能保证，经过这一个池化层，就可以把相似的objects聚类到一起。本文作者在每层都增加了两个正则项</li></ul><h3 id="Hierarchical-Graph-Convolutional-Networks-for-Semi-supervised-Node-Classification-9层-IJCAI-2019"><a href="#Hierarchical-Graph-Convolutional-Networks-for-Semi-supervised-Node-Classification-9层-IJCAI-2019" class="headerlink" title="Hierarchical Graph Convolutional Networks for Semi-supervised Node Classification ( 9层 ) ( IJCAI 2019)"></a>Hierarchical Graph Convolutional Networks for Semi-supervised Node Classification ( 9层 ) ( IJCAI 2019)</h3><ul><li>粗化 coarsening</li><li>结构一致粗化</li><li>结构相似粗化</li></ul>]]></content>
      
      
      <categories>
          
          <category> 图卷积网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图卷积网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hierarchical Global-Local Temporal Modeling for Video Captioning</title>
      <link href="2019/07/23/Hierarchical-Global-Local-Temporal-Modeling-for-Video-Captioning/"/>
      <url>2019/07/23/Hierarchical-Global-Local-Temporal-Modeling-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>使用object features能够更好地检测出action 和 关键的Object</li></ul><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul><li><p>两个LSTM层</p></li><li><p>global : frame features and C3D features</p></li><li><p>local : objects </p></li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul><li>Top Down decoder<ul><li>Bottom LSTM：mean of regions</li><li>Top LSTM : attention of  regions</li></ul></li><li><p>Grounded video description的decoder：</p><ul><li>Bottom LSTM：mean of  fc+motion</li><li>Top LSTM: attention of  regions and attention of  fc+motion</li></ul></li><li>==Hierarchical Global-Local Temporal Modeling（本文） ==<ul><li>Bottom LSTM：attention of fc+motion</li><li>Top LSTM: attention of regions</li><li>本文不一样的地方是在Bottom LSTM的输入也加入了attention</li></ul></li></ul><h3 id="提取特征"><a href="#提取特征" class="headerlink" title="提取特征"></a>提取特征</h3><ul><li>等间隔提取帧的特征</li><li>由于帧之间的间隔，会使得没有运动信息，所以再使用C3D来补充运动信息（以该该为中心，提取16帧，输入C3D中）</li><li>object features: faster rcnn 去掉rcnn部分的类别/分数预测，提取head_to_heal处的pooled_feats</li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VATEX: a video caption dataset</title>
      <link href="2019/07/23/VATEX-a-video-caption-dataset/"/>
      <url>2019/07/23/VATEX-a-video-caption-dataset/</url>
      
        <content type="html"><![CDATA[<h2 id="VATEX数据集"><a href="#VATEX数据集" class="headerlink" title="VATEX数据集"></a>VATEX数据集</h2><ul><li>一个新的数据集，41269个video， 时长大约10s, 每个video有10个中文，10个英文，同时这10个之中，中英文之间有5个是两两配对的</li><li>提出了两个新的任务：（1）一个encoder-decoder模型，在两种语言之间共享参数，即希望一个模型，可以得到两种语言的描述。（2）提出了一种新的机器翻译任务，即当进行中英文的机器翻译任务时，可以添加视频的视觉特征作为辅助信息</li><li>数据集的来源：来自于kinetics的validation dataset, 然后它们找人进行了caption的标注。它们将这41269个video 分成了4部分，train, validation, public test, secret test(不公开，用于比赛)</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g8e7blhuobj30js0lfwkf.jpg" alt="搜狗截图20191028204431.png"></p><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><ul><li>encoder-decoder 就是 TopDown的形式</li><li>视觉特征：通过I3D（在kinetics train上预训练且不再fine-tune）来提取视觉特征，应该是把video分成了很多segments，对每个segment都提取I3D的特征，每个特征作为vi。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>faster_rcnn various box head</title>
      <link href="2019/07/22/faster-rcnn-various-box-head/"/>
      <url>2019/07/22/faster-rcnn-various-box-head/</url>
      
        <content type="html"><![CDATA[<h4 id="Ground-video-description"><a href="#Ground-video-description" class="headerlink" title="Ground video description"></a>Ground video description</h4><ul><li>在阅读这篇论文的时候，由于作者提取了objects，说是提取的fc6的特征，但是不太懂是哪里，在issue中，他说是借鉴这里的代码，于是乎，我就来看了看<a href="https://github.com/facebookresearch/Detectron/blob/8170b25b425967f8f1c7d715bea3c5b8d9536cd8/detectron/modeling/fast_rcnn_heads.py" target="_blank" rel="noopener">box_head</a>，哈哈哈哈哈哈 </li><li>fc6 是 box_head里边的，box_head就是 类似于faster_rcnn中的_head_to_tail</li></ul><h4 id="那么box-head-是干嘛的？"><a href="#那么box-head-是干嘛的？" class="headerlink" title="那么box_head 是干嘛的？"></a>那么box_head 是干嘛的？</h4><ul><li>由于经过roi_pooling 之后得到的是 7*7的一个pooled_feats，还要 ==再进行池化或者拍平，或者再进行全连接层等== ，以便于后边的预测，分类任务。</li></ul><ul><li><p>faster_rcnn 中的box_head就是 resnet layer4</p></li><li><p>mmdetection 中的 faster_rcnn 现将7*7  排成49 ，再送入两个全连接层，可以将这两个全连接层命名为fc6, fc7.   完美!！!！</p></li><li><p>这里展示了各种各样的 box_head</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mmdetection的configs中的各项参数具体解释</title>
      <link href="2019/07/21/mmdetection%E7%9A%84configs%E4%B8%AD%E7%9A%84%E5%90%84%E9%A1%B9%E5%8F%82%E6%95%B0%E5%85%B7%E4%BD%93%E8%A7%A3%E9%87%8A/"/>
      <url>2019/07/21/mmdetection%E7%9A%84configs%E4%B8%AD%E7%9A%84%E5%90%84%E9%A1%B9%E5%8F%82%E6%95%B0%E5%85%B7%E4%BD%93%E8%A7%A3%E9%87%8A/</url>
      
        <content type="html"><![CDATA[<p>一、简介<br>在使用mmdetection对模型进行调优的过程中总会遇到很多参数的问题，不知道参数在代码中是什么作用，会对训练产生怎样的影响，这里我以faster_rcnn_r50_fpn_1x.py和cascade_rcnn_r50_fpn_1x.py为例，简单介绍一下mmdetection中的各项参数含义</p><p>二、faster_rcnn_r50_fpn_1x.py配置文件<br>首先介绍一下这个配置文件所描述的框架，它是基于resnet50的backbone，有着5个fpn特征层的faster-RCNN目标检测网络，训练迭代次数为标准的12次epoch，下面逐条解释其含义</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model settings</span></span><br><span class="line">model = dict(</span><br><span class="line">type=<span class="string">'FasterRCNN'</span>,                         <span class="comment"># model类型</span></span><br><span class="line">    pretrained=<span class="string">'modelzoo://resnet50'</span>,          <span class="comment"># 预训练模型：imagenet-resnet50</span></span><br><span class="line">    backbone=dict(</span><br><span class="line">        type=<span class="string">'ResNet'</span>,                         <span class="comment"># backbone类型</span></span><br><span class="line">        depth=<span class="number">50</span>,                              <span class="comment"># 网络层数</span></span><br><span class="line">        num_stages=<span class="number">4</span>,                          <span class="comment"># resnet的stage数量</span></span><br><span class="line">        out_indices=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>),              <span class="comment"># 输出的stage的序号</span></span><br><span class="line">        frozen_stages=<span class="number">1</span>,                       <span class="comment"># 冻结的stage数量，即该stage不更新参数，-1表示所有的stage都更新参数</span></span><br><span class="line">        style=<span class="string">'pytorch'</span>),                      <span class="comment"># 网络风格：如果设置pytorch，则stride为2的层是conv3x3的卷积层；如果设置caffe，则stride为2的层是第一个conv1x1的卷积层</span></span><br><span class="line">    neck=dict(</span><br><span class="line">        type=<span class="string">'FPN'</span>,                            <span class="comment"># neck类型</span></span><br><span class="line">        in_channels=[<span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>, <span class="number">2048</span>],    <span class="comment"># 输入的各个stage的通道数</span></span><br><span class="line">        out_channels=<span class="number">256</span>,                      <span class="comment"># 输出的特征层的通道数</span></span><br><span class="line">        num_outs=<span class="number">5</span>),                           <span class="comment"># 输出的特征层的数量</span></span><br><span class="line">    rpn_head=dict(</span><br><span class="line">        type=<span class="string">'RPNHead'</span>,                        <span class="comment"># RPN网络类型</span></span><br><span class="line">        in_channels=<span class="number">256</span>,                       <span class="comment"># RPN网络的输入通道数</span></span><br><span class="line">        feat_channels=<span class="number">256</span>,                     <span class="comment"># 特征层的通道数</span></span><br><span class="line">        anchor_scales=[<span class="number">8</span>],                     <span class="comment"># 生成的anchor的baselen，baselen = sqrt(w*h)，w和h为anchor的宽和高</span></span><br><span class="line">        anchor_ratios=[<span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span>],         <span class="comment"># anchor的宽高比</span></span><br><span class="line">        anchor_strides=[<span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>],     <span class="comment"># 在每个特征层上的anchor的步长（对应于原图）</span></span><br><span class="line">        target_means=[<span class="number">.0</span>, <span class="number">.0</span>, <span class="number">.0</span>, <span class="number">.0</span>],         <span class="comment"># 均值</span></span><br><span class="line">        target_stds=[<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>],      <span class="comment"># 方差</span></span><br><span class="line">        use_sigmoid_cls=<span class="literal">True</span>),                 <span class="comment"># 是否使用sigmoid来进行分类，如果False则使用softmax来分类</span></span><br><span class="line">    bbox_roi_extractor=dict(</span><br><span class="line">        type=<span class="string">'SingleRoIExtractor'</span>,                                   <span class="comment"># RoIExtractor类型</span></span><br><span class="line">        roi_layer=dict(type=<span class="string">'RoIAlign'</span>, out_size=<span class="number">7</span>, sample_num=<span class="number">2</span>),   <span class="comment"># ROI具体参数：ROI类型为ROIalign，输出尺寸为7，sample数为2</span></span><br><span class="line">        out_channels=<span class="number">256</span>,                                            <span class="comment"># 输出通道数</span></span><br><span class="line">        featmap_strides=[<span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>]),                             <span class="comment"># 特征图的步长</span></span><br><span class="line">    bbox_head=dict(</span><br><span class="line">        type=<span class="string">'SharedFCBBoxHead'</span>,                     <span class="comment"># 全连接层类型</span></span><br><span class="line">        num_fcs=<span class="number">2</span>,                                   <span class="comment"># 全连接层数量</span></span><br><span class="line">        in_channels=<span class="number">256</span>,                             <span class="comment"># 输入通道数</span></span><br><span class="line">        fc_out_channels=<span class="number">1024</span>,                        <span class="comment"># 输出通道数</span></span><br><span class="line">        roi_feat_size=<span class="number">7</span>,                             <span class="comment"># ROI特征层尺寸</span></span><br><span class="line">        num_classes=<span class="number">81</span>,                              <span class="comment"># 分类器的类别数量+1，+1是因为多了一个背景的类别</span></span><br><span class="line">        target_means=[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],               <span class="comment"># 均值</span></span><br><span class="line">        target_stds=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.2</span>],            <span class="comment"># 方差</span></span><br><span class="line">        reg_class_agnostic=<span class="literal">False</span>))                   <span class="comment"># 是否采用class_agnostic的方式来预测，class_agnostic表示输出bbox时只考虑其是否为前景，后续分类的时候再根据该bbox在网络中的类别得分来分类，也就是说一个框可以对应多个类别</span></span><br><span class="line"><span class="comment"># model training and testing settings</span></span><br><span class="line">train_cfg = dict(</span><br><span class="line">    rpn=dict(</span><br><span class="line">        assigner=dict(</span><br><span class="line">            type=<span class="string">'MaxIoUAssigner'</span>,            <span class="comment"># RPN网络的正负样本划分</span></span><br><span class="line">            pos_iou_thr=<span class="number">0.7</span>,                  <span class="comment"># 正样本的iou阈值</span></span><br><span class="line">            neg_iou_thr=<span class="number">0.3</span>,                  <span class="comment"># 负样本的iou阈值</span></span><br><span class="line">            min_pos_iou=<span class="number">0.3</span>,                  <span class="comment"># 正样本的iou最小值。如果assign给ground truth的anchors中最大的IOU低于0.3，则忽略所有的anchors，否则保留最大IOU的anchor</span></span><br><span class="line">            ignore_iof_thr=<span class="number">-1</span>),               <span class="comment"># 忽略bbox的阈值，当ground truth中包含需要忽略的bbox时使用，-1表示不忽略</span></span><br><span class="line">        sampler=dict(</span><br><span class="line">            type=<span class="string">'RandomSampler'</span>,             <span class="comment"># 正负样本提取器类型</span></span><br><span class="line">            num=<span class="number">256</span>,                          <span class="comment"># 需提取的正负样本数量</span></span><br><span class="line">            pos_fraction=<span class="number">0.5</span>,                 <span class="comment"># 正样本比例</span></span><br><span class="line">            neg_pos_ub=<span class="number">-1</span>,                    <span class="comment"># 最大负样本比例，大于该比例的负样本忽略，-1表示不忽略</span></span><br><span class="line">            add_gt_as_proposals=<span class="literal">False</span>),       <span class="comment"># 把ground truth加入proposal作为正样本</span></span><br><span class="line">        allowed_border=<span class="number">0</span>,                     <span class="comment"># 允许在bbox周围外扩一定的像素</span></span><br><span class="line">        pos_weight=<span class="number">-1</span>,                        <span class="comment"># 正样本权重，-1表示不改变原始的权重</span></span><br><span class="line">        smoothl1_beta=<span class="number">1</span> / <span class="number">9.0</span>,                <span class="comment"># 平滑L1系数</span></span><br><span class="line">        debug=<span class="literal">False</span>),                         <span class="comment"># debug模式</span></span><br><span class="line">    rcnn=dict(</span><br><span class="line">        assigner=dict(</span><br><span class="line">            type=<span class="string">'MaxIoUAssigner'</span>,            <span class="comment"># RCNN网络正负样本划分</span></span><br><span class="line">            pos_iou_thr=<span class="number">0.5</span>,                  <span class="comment"># 正样本的iou阈值</span></span><br><span class="line">            neg_iou_thr=<span class="number">0.5</span>,                  <span class="comment"># 负样本的iou阈值</span></span><br><span class="line">            min_pos_iou=<span class="number">0.5</span>,                  <span class="comment"># 正样本的iou最小值。如果assign给ground truth的anchors中最大的IOU低于0.3，则忽略所有的anchors，否则保留最大IOU的anchor</span></span><br><span class="line">            ignore_iof_thr=<span class="number">-1</span>),               <span class="comment"># 忽略bbox的阈值，当ground truth中包含需要忽略的bbox时使用，-1表示不忽略</span></span><br><span class="line">        sampler=dict(</span><br><span class="line">            type=<span class="string">'RandomSampler'</span>,             <span class="comment"># 正负样本提取器类型</span></span><br><span class="line">            num=<span class="number">512</span>,                          <span class="comment"># 需提取的正负样本数量</span></span><br><span class="line">            pos_fraction=<span class="number">0.25</span>,                <span class="comment"># 正样本比例</span></span><br><span class="line">            neg_pos_ub=<span class="number">-1</span>,                    <span class="comment"># 最大负样本比例，大于该比例的负样本忽略，-1表示不忽略</span></span><br><span class="line">            add_gt_as_proposals=<span class="literal">True</span>),        <span class="comment"># 把ground truth加入proposal作为正样本</span></span><br><span class="line">        pos_weight=<span class="number">-1</span>,                        <span class="comment"># 正样本权重，-1表示不改变原始的权重</span></span><br><span class="line">        debug=<span class="literal">False</span>))                         <span class="comment"># debug模式</span></span><br><span class="line">test_cfg = dict(</span><br><span class="line">    rpn=dict(                                 <span class="comment"># 推断时的RPN参数</span></span><br><span class="line">        nms_across_levels=<span class="literal">False</span>,              <span class="comment"># 在所有的fpn层内做nms</span></span><br><span class="line">        nms_pre=<span class="number">2000</span>,                         <span class="comment"># 在nms之前保留的的得分最高的proposal数量</span></span><br><span class="line">        nms_post=<span class="number">2000</span>,                        <span class="comment"># 在nms之后保留的的得分最高的proposal数量</span></span><br><span class="line">        max_num=<span class="number">2000</span>,                         <span class="comment"># 在后处理完成之后保留的proposal数量</span></span><br><span class="line">        nms_thr=<span class="number">0.7</span>,                          <span class="comment"># nms阈值</span></span><br><span class="line">        min_bbox_size=<span class="number">0</span>),                     <span class="comment"># 最小bbox尺寸</span></span><br><span class="line">    rcnn=dict(</span><br><span class="line">        score_thr=<span class="number">0.05</span>, nms=dict(type=<span class="string">'nms'</span>, iou_thr=<span class="number">0.5</span>), max_per_img=<span class="number">100</span>)   <span class="comment"># max_per_img表示最终输出的det bbox数量</span></span><br><span class="line">    <span class="comment"># soft-nms is also supported for rcnn testing</span></span><br><span class="line">    <span class="comment"># e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)            # soft_nms参数</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># dataset settings</span></span><br><span class="line">dataset_type = <span class="string">'CocoDataset'</span>                <span class="comment"># 数据集类型</span></span><br><span class="line">data_root = <span class="string">'data/coco/'</span>                    <span class="comment"># 数据集根目录</span></span><br><span class="line">img_norm_cfg = dict(</span><br><span class="line">    mean=[<span class="number">123.675</span>, <span class="number">116.28</span>, <span class="number">103.53</span>], std=[<span class="number">58.395</span>, <span class="number">57.12</span>, <span class="number">57.375</span>], to_rgb=<span class="literal">True</span>)   <span class="comment"># 输入图像初始化，减去均值mean并处以方差std，to_rgb表示将bgr转为rgb</span></span><br><span class="line">data = dict(</span><br><span class="line">    imgs_per_gpu=<span class="number">2</span>,                <span class="comment"># 每个gpu计算的图像数量</span></span><br><span class="line">    workers_per_gpu=<span class="number">2</span>,             <span class="comment"># 每个gpu分配的线程数</span></span><br><span class="line">    train=dict(</span><br><span class="line">        type=dataset_type,                                                 <span class="comment"># 数据集类型</span></span><br><span class="line">        ann_file=data_root + <span class="string">'annotations/instances_train2017.json'</span>,       <span class="comment"># 数据集annotation路径</span></span><br><span class="line">        img_prefix=data_root + <span class="string">'train2017/'</span>,                               <span class="comment"># 数据集的图片路径</span></span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),                                             <span class="comment"># 输入图像尺寸，最大边1333，最小边800</span></span><br><span class="line">        img_norm_cfg=img_norm_cfg,                                         <span class="comment"># 图像初始化参数</span></span><br><span class="line">        size_divisor=<span class="number">32</span>,                                                   <span class="comment"># 对图像进行resize时的最小单位，32表示所有的图像都会被resize成32的倍数</span></span><br><span class="line">        flip_ratio=<span class="number">0.5</span>,                                                    <span class="comment"># 图像的随机左右翻转的概率</span></span><br><span class="line">        with_mask=<span class="literal">False</span>,                                                   <span class="comment"># 训练时附带mask</span></span><br><span class="line">        with_crowd=<span class="literal">True</span>,                                                   <span class="comment"># 训练时附带difficult的样本</span></span><br><span class="line">        with_label=<span class="literal">True</span>),                                                  <span class="comment"># 训练时附带label</span></span><br><span class="line">    val=dict(</span><br><span class="line">        type=dataset_type,                                                 <span class="comment"># 同上</span></span><br><span class="line">        ann_file=data_root + <span class="string">'annotations/instances_val2017.json'</span>,         <span class="comment"># 同上</span></span><br><span class="line">        img_prefix=data_root + <span class="string">'val2017/'</span>,                                 <span class="comment"># 同上</span></span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),                                             <span class="comment"># 同上</span></span><br><span class="line">        img_norm_cfg=img_norm_cfg,                                         <span class="comment"># 同上</span></span><br><span class="line">        size_divisor=<span class="number">32</span>,                                                   <span class="comment"># 同上</span></span><br><span class="line">        flip_ratio=<span class="number">0</span>,                                                      <span class="comment"># 同上</span></span><br><span class="line">        with_mask=<span class="literal">False</span>,                                                   <span class="comment"># 同上</span></span><br><span class="line">        with_crowd=<span class="literal">True</span>,                                                   <span class="comment"># 同上</span></span><br><span class="line">        with_label=<span class="literal">True</span>),                                                  <span class="comment"># 同上</span></span><br><span class="line">    test=dict(</span><br><span class="line">        type=dataset_type,                                                 <span class="comment"># 同上</span></span><br><span class="line">        ann_file=data_root + <span class="string">'annotations/instances_val2017.json'</span>,         <span class="comment"># 同上</span></span><br><span class="line">        img_prefix=data_root + <span class="string">'val2017/'</span>,                                 <span class="comment"># 同上</span></span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),                                             <span class="comment"># 同上</span></span><br><span class="line">        img_norm_cfg=img_norm_cfg,                                         <span class="comment"># 同上</span></span><br><span class="line">        size_divisor=<span class="number">32</span>,                                                   <span class="comment"># 同上</span></span><br><span class="line">        flip_ratio=<span class="number">0</span>,                                                      <span class="comment"># 同上</span></span><br><span class="line">        with_mask=<span class="literal">False</span>,                                                   <span class="comment"># 同上</span></span><br><span class="line">        with_label=<span class="literal">False</span>,                                                  <span class="comment"># 同上</span></span><br><span class="line">        test_mode=<span class="literal">True</span>))                                                   <span class="comment"># 同上</span></span><br><span class="line"><span class="comment"># optimizer</span></span><br><span class="line">optimizer = dict(type=<span class="string">'SGD'</span>, lr=<span class="number">0.02</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.0001</span>)   <span class="comment"># 优化参数，lr为学习率，momentum为动量因子，weight_decay为权重衰减因子</span></span><br><span class="line">optimizer_config = dict(grad_clip=dict(max_norm=<span class="number">35</span>, norm_type=<span class="number">2</span>))          <span class="comment"># 梯度均衡参数</span></span><br><span class="line"><span class="comment"># learning policy</span></span><br><span class="line">lr_config = dict(</span><br><span class="line">    policy=<span class="string">'step'</span>,                        <span class="comment"># 优化策略</span></span><br><span class="line">    warmup=<span class="string">'linear'</span>,                      <span class="comment"># 初始的学习率增加的策略，linear为线性增加</span></span><br><span class="line">    warmup_iters=<span class="number">500</span>,                     <span class="comment"># 在初始的500次迭代中学习率逐渐增加</span></span><br><span class="line">    warmup_ratio=<span class="number">1.0</span> / <span class="number">3</span>,                 <span class="comment"># 起始的学习率</span></span><br><span class="line">    step=[<span class="number">8</span>, <span class="number">11</span>])                         <span class="comment"># 在第8和11个epoch时降低学习率</span></span><br><span class="line">checkpoint_config = dict(interval=<span class="number">1</span>)      <span class="comment"># 每1个epoch存储一次模型</span></span><br><span class="line"><span class="comment"># yapf:disable</span></span><br><span class="line">log_config = dict(</span><br><span class="line">    interval=<span class="number">50</span>,                          <span class="comment"># 每50个batch输出一次信息</span></span><br><span class="line">    hooks=[</span><br><span class="line">        dict(type=<span class="string">'TextLoggerHook'</span>),      <span class="comment"># 控制台输出信息的风格</span></span><br><span class="line">        <span class="comment"># dict(type='TensorboardLoggerHook')</span></span><br><span class="line">    ])</span><br><span class="line"><span class="comment"># yapf:enable</span></span><br><span class="line"><span class="comment"># runtime settings</span></span><br><span class="line">total_epochs = <span class="number">12</span>                               <span class="comment"># 最大epoch数</span></span><br><span class="line">dist_params = dict(backend=<span class="string">'nccl'</span>)              <span class="comment"># 分布式参数</span></span><br><span class="line">log_level = <span class="string">'INFO'</span>                              <span class="comment"># 输出信息的完整度级别</span></span><br><span class="line">work_dir = <span class="string">'./work_dirs/faster_rcnn_r50_fpn_1x'</span> <span class="comment"># log文件和模型文件存储路径</span></span><br><span class="line">load_from = <span class="literal">None</span>                                <span class="comment"># 加载模型的路径，None表示从预训练模型加载</span></span><br><span class="line">resume_from = <span class="literal">None</span>                              <span class="comment"># 恢复训练模型的路径</span></span><br><span class="line">workflow = [(<span class="string">'train'</span>, <span class="number">1</span>)]                       <span class="comment"># 当前工作区名称</span></span><br></pre></td></tr></table></figure><p> 三、cascade_rcnn_r50_fpn_1x.py配置文件<br>cascade-RCNN是cvpr2018的文章，相比于faster-RCNN的改进主要在于其RCNN有三个stage，这三个stage逐级refine检测的结果，使得结果达到更高的精度。下面逐条解释其config的含义，与faster-RCNN相同的部分就不再赘述。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model settings</span></span><br><span class="line">model = dict(</span><br><span class="line">    type=<span class="string">'CascadeRCNN'</span>,</span><br><span class="line">    num_stages=<span class="number">3</span>,                     <span class="comment"># RCNN网络的stage数量，在faster-RCNN中为1</span></span><br><span class="line">    pretrained=<span class="string">'modelzoo://resnet50'</span>,</span><br><span class="line">    backbone=dict(</span><br><span class="line">        type=<span class="string">'ResNet'</span>,</span><br><span class="line">        depth=<span class="number">50</span>,</span><br><span class="line">        num_stages=<span class="number">4</span>,</span><br><span class="line">        out_indices=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>),</span><br><span class="line">        frozen_stages=<span class="number">1</span>,</span><br><span class="line">        style=<span class="string">'pytorch'</span>),</span><br><span class="line">    neck=dict(</span><br><span class="line">        type=<span class="string">'FPN'</span>,</span><br><span class="line">        in_channels=[<span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>, <span class="number">2048</span>],</span><br><span class="line">        out_channels=<span class="number">256</span>,</span><br><span class="line">        num_outs=<span class="number">5</span>),</span><br><span class="line">    rpn_head=dict(</span><br><span class="line">        type=<span class="string">'RPNHead'</span>,</span><br><span class="line">        in_channels=<span class="number">256</span>,</span><br><span class="line">        feat_channels=<span class="number">256</span>,</span><br><span class="line">        anchor_scales=[<span class="number">8</span>],</span><br><span class="line">        anchor_ratios=[<span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span>],</span><br><span class="line">        anchor_strides=[<span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>],</span><br><span class="line">        target_means=[<span class="number">.0</span>, <span class="number">.0</span>, <span class="number">.0</span>, <span class="number">.0</span>],</span><br><span class="line">        target_stds=[<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>],</span><br><span class="line">        use_sigmoid_cls=<span class="literal">True</span>),</span><br><span class="line">    bbox_roi_extractor=dict(</span><br><span class="line">        type=<span class="string">'SingleRoIExtractor'</span>,</span><br><span class="line">        roi_layer=dict(type=<span class="string">'RoIAlign'</span>, out_size=<span class="number">7</span>, sample_num=<span class="number">2</span>),</span><br><span class="line">        out_channels=<span class="number">256</span>,</span><br><span class="line">        featmap_strides=[<span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>]),</span><br><span class="line">    bbox_head=[</span><br><span class="line">        dict(</span><br><span class="line">            type=<span class="string">'SharedFCBBoxHead'</span>,</span><br><span class="line">            num_fcs=<span class="number">2</span>,</span><br><span class="line">            in_channels=<span class="number">256</span>,</span><br><span class="line">            fc_out_channels=<span class="number">1024</span>,</span><br><span class="line">            roi_feat_size=<span class="number">7</span>,</span><br><span class="line">            num_classes=<span class="number">81</span>,</span><br><span class="line">            target_means=[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            target_stds=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.2</span>],</span><br><span class="line">            reg_class_agnostic=<span class="literal">True</span>),</span><br><span class="line">        dict(</span><br><span class="line">            type=<span class="string">'SharedFCBBoxHead'</span>,</span><br><span class="line">            num_fcs=<span class="number">2</span>,</span><br><span class="line">            in_channels=<span class="number">256</span>,</span><br><span class="line">            fc_out_channels=<span class="number">1024</span>,</span><br><span class="line">            roi_feat_size=<span class="number">7</span>,</span><br><span class="line">            num_classes=<span class="number">81</span>,</span><br><span class="line">            target_means=[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            target_stds=[<span class="number">0.05</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.1</span>],</span><br><span class="line">            reg_class_agnostic=<span class="literal">True</span>),</span><br><span class="line">        dict(</span><br><span class="line">            type=<span class="string">'SharedFCBBoxHead'</span>,</span><br><span class="line">            num_fcs=<span class="number">2</span>,</span><br><span class="line">            in_channels=<span class="number">256</span>,</span><br><span class="line">            fc_out_channels=<span class="number">1024</span>,</span><br><span class="line">            roi_feat_size=<span class="number">7</span>,</span><br><span class="line">            num_classes=<span class="number">81</span>,</span><br><span class="line">            target_means=[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            target_stds=[<span class="number">0.033</span>, <span class="number">0.033</span>, <span class="number">0.067</span>, <span class="number">0.067</span>],</span><br><span class="line">            reg_class_agnostic=<span class="literal">True</span>)</span><br><span class="line">    ])</span><br><span class="line"><span class="comment"># model training and testing settings</span></span><br><span class="line">train_cfg = dict(</span><br><span class="line">    rpn=dict(</span><br><span class="line">        assigner=dict(</span><br><span class="line">            type=<span class="string">'MaxIoUAssigner'</span>,</span><br><span class="line">            pos_iou_thr=<span class="number">0.7</span>,</span><br><span class="line">            neg_iou_thr=<span class="number">0.3</span>,</span><br><span class="line">            min_pos_iou=<span class="number">0.3</span>,</span><br><span class="line">            ignore_iof_thr=<span class="number">-1</span>),</span><br><span class="line">        sampler=dict(</span><br><span class="line">            type=<span class="string">'RandomSampler'</span>,</span><br><span class="line">            num=<span class="number">256</span>,</span><br><span class="line">            pos_fraction=<span class="number">0.5</span>,</span><br><span class="line">            neg_pos_ub=<span class="number">-1</span>,</span><br><span class="line">            add_gt_as_proposals=<span class="literal">False</span>),</span><br><span class="line">        allowed_border=<span class="number">0</span>,</span><br><span class="line">        pos_weight=<span class="number">-1</span>,</span><br><span class="line">        smoothl1_beta=<span class="number">1</span> / <span class="number">9.0</span>,</span><br><span class="line">        debug=<span class="literal">False</span>),</span><br><span class="line">    rcnn=[                    <span class="comment"># 注意，这里有3个RCNN的模块，对应开头的那个RCNN的stage数量</span></span><br><span class="line">        dict(</span><br><span class="line">            assigner=dict(</span><br><span class="line">                type=<span class="string">'MaxIoUAssigner'</span>,</span><br><span class="line">                pos_iou_thr=<span class="number">0.5</span>,</span><br><span class="line">                neg_iou_thr=<span class="number">0.5</span>,</span><br><span class="line">                min_pos_iou=<span class="number">0.5</span>,</span><br><span class="line">                ignore_iof_thr=<span class="number">-1</span>),</span><br><span class="line">            sampler=dict(</span><br><span class="line">                type=<span class="string">'RandomSampler'</span>,</span><br><span class="line">                num=<span class="number">512</span>,</span><br><span class="line">                pos_fraction=<span class="number">0.25</span>,</span><br><span class="line">                neg_pos_ub=<span class="number">-1</span>,</span><br><span class="line">                add_gt_as_proposals=<span class="literal">True</span>),</span><br><span class="line">            pos_weight=<span class="number">-1</span>,</span><br><span class="line">            debug=<span class="literal">False</span>),</span><br><span class="line">        dict(</span><br><span class="line">            assigner=dict(</span><br><span class="line">                type=<span class="string">'MaxIoUAssigner'</span>,</span><br><span class="line">                pos_iou_thr=<span class="number">0.6</span>,</span><br><span class="line">                neg_iou_thr=<span class="number">0.6</span>,</span><br><span class="line">                min_pos_iou=<span class="number">0.6</span>,</span><br><span class="line">                ignore_iof_thr=<span class="number">-1</span>),</span><br><span class="line">            sampler=dict(</span><br><span class="line">                type=<span class="string">'RandomSampler'</span>,</span><br><span class="line">                num=<span class="number">512</span>,</span><br><span class="line">                pos_fraction=<span class="number">0.25</span>,</span><br><span class="line">                neg_pos_ub=<span class="number">-1</span>,</span><br><span class="line">                add_gt_as_proposals=<span class="literal">True</span>),</span><br><span class="line">            pos_weight=<span class="number">-1</span>,</span><br><span class="line">            debug=<span class="literal">False</span>),</span><br><span class="line">        dict(</span><br><span class="line">            assigner=dict(</span><br><span class="line">                type=<span class="string">'MaxIoUAssigner'</span>,</span><br><span class="line">                pos_iou_thr=<span class="number">0.7</span>,</span><br><span class="line">                neg_iou_thr=<span class="number">0.7</span>,</span><br><span class="line">                min_pos_iou=<span class="number">0.7</span>,</span><br><span class="line">                ignore_iof_thr=<span class="number">-1</span>),</span><br><span class="line">            sampler=dict(</span><br><span class="line">                type=<span class="string">'RandomSampler'</span>,</span><br><span class="line">                num=<span class="number">512</span>,</span><br><span class="line">                pos_fraction=<span class="number">0.25</span>,</span><br><span class="line">                neg_pos_ub=<span class="number">-1</span>,</span><br><span class="line">                add_gt_as_proposals=<span class="literal">True</span>),</span><br><span class="line">            pos_weight=<span class="number">-1</span>,</span><br><span class="line">            debug=<span class="literal">False</span>)</span><br><span class="line">    ],</span><br><span class="line">    stage_loss_weights=[<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0.25</span>])     <span class="comment"># 3个RCNN的stage的loss权重</span></span><br><span class="line">test_cfg = dict(</span><br><span class="line">    rpn=dict(</span><br><span class="line">        nms_across_levels=<span class="literal">False</span>,</span><br><span class="line">        nms_pre=<span class="number">2000</span>,</span><br><span class="line">        nms_post=<span class="number">2000</span>,</span><br><span class="line">        max_num=<span class="number">2000</span>,</span><br><span class="line">        nms_thr=<span class="number">0.7</span>,</span><br><span class="line">        min_bbox_size=<span class="number">0</span>),</span><br><span class="line">    rcnn=dict(</span><br><span class="line">        score_thr=<span class="number">0.05</span>, nms=dict(type=<span class="string">'nms'</span>, iou_thr=<span class="number">0.5</span>), max_per_img=<span class="number">100</span>),</span><br><span class="line">    keep_all_stages=<span class="literal">False</span>)         <span class="comment"># 是否保留所有stage的结果</span></span><br><span class="line"><span class="comment"># dataset settings</span></span><br><span class="line">dataset_type = <span class="string">'CocoDataset'</span></span><br><span class="line">data_root = <span class="string">'data/coco/'</span></span><br><span class="line">img_norm_cfg = dict(</span><br><span class="line">    mean=[<span class="number">123.675</span>, <span class="number">116.28</span>, <span class="number">103.53</span>], std=[<span class="number">58.395</span>, <span class="number">57.12</span>, <span class="number">57.375</span>], to_rgb=<span class="literal">True</span>)</span><br><span class="line">data = dict(</span><br><span class="line">    imgs_per_gpu=<span class="number">2</span>,</span><br><span class="line">    workers_per_gpu=<span class="number">2</span>,</span><br><span class="line">    train=dict(</span><br><span class="line">        type=dataset_type,</span><br><span class="line">        ann_file=data_root + <span class="string">'annotations/instances_train2017.json'</span>,</span><br><span class="line">        img_prefix=data_root + <span class="string">'train2017/'</span>,</span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),</span><br><span class="line">        img_norm_cfg=img_norm_cfg,</span><br><span class="line">        size_divisor=<span class="number">32</span>,</span><br><span class="line">        flip_ratio=<span class="number">0.5</span>,</span><br><span class="line">        with_mask=<span class="literal">False</span>,</span><br><span class="line">        with_crowd=<span class="literal">True</span>,</span><br><span class="line">        with_label=<span class="literal">True</span>),</span><br><span class="line">    val=dict(</span><br><span class="line">        type=dataset_type,</span><br><span class="line">        ann_file=data_root + <span class="string">'annotations/instances_val2017.json'</span>,</span><br><span class="line">        img_prefix=data_root + <span class="string">'val2017/'</span>,</span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),</span><br><span class="line">        img_norm_cfg=img_norm_cfg,</span><br><span class="line">        size_divisor=<span class="number">32</span>,</span><br><span class="line">        flip_ratio=<span class="number">0</span>,</span><br><span class="line">        with_mask=<span class="literal">False</span>,</span><br><span class="line">        with_crowd=<span class="literal">True</span>,</span><br><span class="line">        with_label=<span class="literal">True</span>),</span><br><span class="line">    test=dict(</span><br><span class="line">        type=dataset_type,</span><br><span class="line">        ann_file=data_root + <span class="string">'annotations/instances_val2017.json'</span>,</span><br><span class="line">        img_prefix=data_root + <span class="string">'val2017/'</span>,</span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),</span><br><span class="line">        img_norm_cfg=img_norm_cfg,</span><br><span class="line">        size_divisor=<span class="number">32</span>,</span><br><span class="line">        flip_ratio=<span class="number">0</span>,</span><br><span class="line">        with_mask=<span class="literal">False</span>,</span><br><span class="line">        with_label=<span class="literal">False</span>,</span><br><span class="line">        test_mode=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># optimizer</span></span><br><span class="line">optimizer = dict(type=<span class="string">'SGD'</span>, lr=<span class="number">0.02</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.0001</span>)</span><br><span class="line">optimizer_config = dict(grad_clip=dict(max_norm=<span class="number">35</span>, norm_type=<span class="number">2</span>))</span><br><span class="line"><span class="comment"># learning policy</span></span><br><span class="line">lr_config = dict(</span><br><span class="line">    policy=<span class="string">'step'</span>,</span><br><span class="line">    warmup=<span class="string">'linear'</span>,</span><br><span class="line">    warmup_iters=<span class="number">500</span>,</span><br><span class="line">    warmup_ratio=<span class="number">1.0</span> / <span class="number">3</span>,</span><br><span class="line">    step=[<span class="number">8</span>, <span class="number">11</span>])</span><br><span class="line">checkpoint_config = dict(interval=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># yapf:disable</span></span><br><span class="line">log_config = dict(</span><br><span class="line">    interval=<span class="number">50</span>,</span><br><span class="line">    hooks=[</span><br><span class="line">        dict(type=<span class="string">'TextLoggerHook'</span>),</span><br><span class="line">        <span class="comment"># dict(type='TensorboardLoggerHook')</span></span><br><span class="line">    ])</span><br><span class="line"><span class="comment"># yapf:enable</span></span><br><span class="line"><span class="comment"># runtime settings</span></span><br><span class="line">total_epochs = <span class="number">12</span></span><br><span class="line">dist_params = dict(backend=<span class="string">'nccl'</span>)</span><br><span class="line">log_level = <span class="string">'INFO'</span></span><br><span class="line">work_dir = <span class="string">'./work_dirs/cascade_rcnn_r50_fpn_1x'</span></span><br><span class="line">load_from = <span class="literal">None</span></span><br><span class="line">resume_from = <span class="literal">None</span></span><br><span class="line">workflow = [(<span class="string">'train'</span>, <span class="number">1</span>)]</span><br></pre></td></tr></table></figure><p>参考链接：</p><p><a href="https://www.jiqizhixin.com/articles/2018-10-17-10" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2018-10-17-10</a></p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Visual Genome 数据集</title>
      <link href="2019/07/21/Visual-Genome-%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
      <url>2019/07/21/Visual-Genome-%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<ul><li>数据集介绍<br><a href="https://cloud.tencent.com/developer/article/1391855" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1391855</a></li></ul><p><a href="https://visualgenome.org/" target="_blank" rel="noopener">Visual Genome 主页</a></p><p><a href="https://visualgenome.org/api/v0/api_home.html" target="_blank" rel="noopener">Visual Genome Data</a></p><p><a href="https://visualgenome.org/api/v0/api_readme" target="_blank" rel="noopener">Visual Genome Readme</a></p><p>Visual Genome 数据集总览：</p><ul><li>108077 张图片</li><li>5.4 Million Region Descriptions</li><li>1.7 Million Visual Question Answers</li><li>3.8 Million Object Instances</li><li>2.8 Million Attributes</li><li>2.3 Million Relationships</li><li>Everything Mapped to Wordnet Synsets  </li><li>标注数据：  objects，attributes，图片内的 relationships</li><li>共 108K 张图片，每张图片平均有， 35 个 objects，26 个 attributes，21对 objects 见的成对 relationships.</li></ul><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/rex8eso6p5.png?imageView2/2/w/1620" alt="img"></p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/vtmiv1yyr6.png?imageView2/2/w/1620" alt="img"></p><h2 id="1-Visual-Genome-数据标注"><a href="#1-Visual-Genome-数据标注" class="headerlink" title="1. Visual Genome 数据标注"></a>1. Visual Genome 数据标注</h2><p>数据集主要包括七个主要部分：</p><ul><li>region descriptions</li><li>objects</li><li>attributes</li><li>relationships</li><li>region graphs</li><li>scene graphs</li><li>question answer pairs</li></ul><h3 id="1-1-Region-Descriptions"><a href="#1-1-Region-Descriptions" class="headerlink" title="1.1. Region Descriptions"></a>1.1. Region Descriptions</h3><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/8kgo0p0qim.png?imageView2/2/w/1620" alt="img"></p><p>数据集标注了图片的 regions descriptions，每个 region 有一个 bounding box. </p><p>如上图中，图片有三个 regions descriptions： “man jumping over a fire hydrant,”，“yellow fire hydrant,” 和   “woman in shorts is standing behind the man.”.</p><h3 id="1-2-Objects"><a href="#1-2-Objects" class="headerlink" title="1.2. Objects"></a>1.2. Objects</h3><p>数据集中每张图片平均有 35 个 objects，每个 object 采用 bounding box 标注.</p><p>如图：   </p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/ih1qpz1p3s.png?imageView2/2/w/1620" alt="img"></p><p><a href="http://blog.csdn.net/zziahgf/article/details/72819043" target="_blank" rel="noopener">MS-COCO 数据集</a> 只标注了 80 个 object categories，没有描述图片中的所有 objects. 实际场景中，可能有更多的 objects 类别.</p><p>Visual Genome 数据集旨在对图片里出现的所有视觉 objects 进行标注，objects categories 类别达到 33877 种.</p><h3 id="1-3-Attributes"><a href="#1-3-Attributes" class="headerlink" title="1.3. Attributes"></a>1.3. Attributes</h3><p>数据集中每张图片平均有 26 个 attributes. Objects 可能没有或者有更多的相关 attributes. </p><p>Attributes 可以是 color(如 yellow)，states(如 standing) 等，如图：   </p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/k1n26a1mdt.png?imageView2/2/w/1620" alt="img"></p><p>Attributes 能够对 objects 进行更容易的描述、对比与分类. 即使以前未见到某 object，根据 attributes 仍能推断出与 object 相关的东西. 如，“yellow and brown spotted with long neck(长脖子上有黄色和棕色的斑点)”，很可能推断出 object 是 giraffe(长颈鹿).</p><p>关于 attributes 的研究：</p><ul><li>采用examplar SVMs，利用相似特征来寻找 objects；</li><li>采用纹理(textures) 研究 objects，或者预测颜色.</li><li>采用 attributes 来提高目标分类结果. 如 fine-grained 识别.</li></ul><p>Attributes 一般被定义为 parts(如 has legs)、shapes(如，spherical球形的)、materials(如 furry毛皮的)；用于对新的 objects 类别进行分类.</p><p>Visual Genome 数据集对于 attributes 进行扩展，其 attributes 不是 image-specific 的，而是真实场景中 object-specific 的. attributes 类型包括：size(如 small), pose(如bent), state (如 transparent), emotion (如 happy)等等.</p><ul><li>基于 VGG16 的 attributes 预测结果：   </li></ul><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/373ih7qquc.png?imageView2/2/w/1620" alt="img"></p><h3 id="1-4-Relationships"><a href="#1-4-Relationships" class="headerlink" title="1.4. Relationships"></a>1.4. Relationships</h3><p>Relationships 是两个 objects 的连接关系.</p><p>Relationships 可以是 actions(如 jumping over)，spatial(如 is build)，comparative(如 taller than)，prepositional phrases (如 drive on). 如图：   </p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/5ljbd3m2av.png?imageView2/2/w/1620" alt="img"></p><ul><li>Relationship 预测结果：   </li></ul><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/pgbhzj5ui4.png?imageView2/2/w/1620" alt="img"></p><h3 id="1-5-Region-Graphs"><a href="#1-5-Region-Graphs" class="headerlink" title="1.5. Region Graphs"></a>1.5. Region Graphs</h3><p>结合 objects、attributes 以及  region descriptions 提取的 relationships，创建每个 regions 的 graph representation. </p><h3 id="1-6-Scene-Graphs"><a href="#1-6-Scene-Graphs" class="headerlink" title="1.6. Scene Graphs"></a>1.6. Scene Graphs</h3><p>Region graphs 是图片的局部区域表示，将 region graphs 结合，生成单个 scene graph来表示整张图片.</p><p>Scene graph 是全部 region graphs 的统一，包含了全部的 objects、attributes以及每个 region description 的 relationships.</p><p>Scene Graph 将多种不同层次的 scene 信息以更加一致的方式结合在一起.</p><h3 id="1-7-Question-Answer-QA-Pairs"><a href="#1-7-Question-Answer-QA-Pairs" class="headerlink" title="1.7. Question Answer(QA) Pairs"></a>1.7. Question Answer(QA) Pairs</h3><p>数据集中每张图片有两种类型的 QA pairs：</p><ul><li>freeform QAs - 基于整张图片；</li><li>region-based QAs - 基于图片的选择区域. </li></ul><p>每张图片标注了 6 中不同类型的问题：what, where, how, when, who, why.</p><p>如图：   </p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/vbxbgpfi92.png?imageView2/2/w/1620" alt="img"></p><p>Figure . Visual Genome 数据集. 每张图片包括：region descriptions - 描述了图像的局部信息；两种类型的 question answer pairs(QAs) - free form QAs 和 region-based QAs. 每个 region 转化为 objects、attributes 和 pairwise relationships region 构成的 region graph 表示. 最终， 结合 region graphs 以形成图片内全部 objects 的 scene graph.</p><h2 id="2-Visual-Genome-数据集应用"><a href="#2-Visual-Genome-数据集应用" class="headerlink" title="2. Visual Genome 数据集应用"></a>2. Visual Genome 数据集应用</h2><p>基本应用：</p><ul><li>attribute classification 属性分类</li><li>relationship classification 关系分类</li><li>description generation 描述生成</li><li>question answering QA</li></ul><p>更多应用：</p><ul><li>Dense image captioning</li><li>Visual question answering</li><li>Image understanding</li><li>Relationship extraction</li><li>Semantic image retrieval</li><li>Completing the Set of Annotations</li></ul><p>注 - 与其它数据集对比：   </p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1682974/cjqe5v7i44.png?imageView2/2/w/1620" alt="img"></p><h2 id="3-Reference"><a href="#3-Reference" class="headerlink" title="3. Reference"></a>3. Reference</h2><p>[1] - <a href="https://visualgenome.org/" target="_blank" rel="noopener">Visual Genome Home</a></p><p>[1] - <a href="https://visualgenome.org/static/paper/Visual_Genome.pdf" target="_blank" rel="noopener">Visual Genome Doc</a></p><p>[2] - <a href="https://arxiv.org/pdf/1701.02426.pdf" target="_blank" rel="noopener">Scene Graph Generation by Iterative Message Passing</a></p><p>本文参与<a href="https://cloud.tencent.com/developer/support-plan" target="_blank" rel="noopener">腾讯云自媒体分享计划</a>，欢迎正在阅读的你也加入，一起分享。</p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>设置随机种子</title>
      <link href="2019/07/21/%E8%AE%BE%E7%BD%AE%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90/"/>
      <url>2019/07/21/%E8%AE%BE%E7%BD%AE%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90/</url>
      
        <content type="html"><![CDATA[<p>def set_random_seed(seed):<br>    random.seed(seed)<br>    np.random.seed(seed)<br>    torch.manual_seed(seed)<br>    torch.cuda.manual_seed_all(seed)</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>mmdetection的安装</title>
      <link href="2019/07/20/mmdetection%E7%9A%84%E5%AE%89%E8%A3%85/"/>
      <url>2019/07/20/mmdetection%E7%9A%84%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><ul><li>选择镜像：py36-pytorch0.4.0-cu90-ctc （即，准备一个cuda9.0的环境）</li><li>note: 需要一个pytorch1.1.0（pytorch1.2测试不可以用，推荐使用1.1.0），后文有讲如何在anaconda下安装pytorch</li><li>进入容器，安装anaconda</li></ul><h3 id="按着Github-install的步骤进行安装如下："><a href="#按着Github-install的步骤进行安装如下：" class="headerlink" title="按着Github install的步骤进行安装如下："></a>按着<a href="https://github.com/open-mmlab/mmdetection/blob/master/INSTALL.md" target="_blank" rel="noopener">Github install</a>的步骤进行安装如下：</h3><ul><li>Create a conda virtual environment and activate it. Then install Cython.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n open-mmlab python=<span class="number">3.7</span> -y</span><br><span class="line">source activate open-mmlab</span><br></pre></td></tr></table></figure></li></ul><ul><li><p><strong> 以下的操作都是在进入open-mmlab环境之后进行的</strong></p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda <span class="keyword">install</span> cython</span><br></pre></td></tr></table></figure></li><li><p>安装 numpy</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> numpy</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>安装<a href="https://github.com/open-mmlab/mmcv" target="_blank" rel="noopener">mmcv</a> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install mmcv</span><br></pre></td></tr></table></figure></li><li><p>安装pytorch<br>最好是离线下载，然后再安装，因为conda install 或者 pip install 可能连接不上（细节：pip install torch  就会出现下载链接，然后自己复制链接去网页下载即可），下载之后：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">pip</span> <span class="selector-tag">install</span> <span class="selector-tag">torch-1</span><span class="selector-class">.1</span><span class="selector-class">.0-cp37-cp37m-manylinux1_x86_64</span><span class="selector-class">.whl</span></span><br></pre></td></tr></table></figure></li><li><p>安装opencv</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda <span class="keyword">install</span> -c menpo opencv</span><br></pre></td></tr></table></figure></li></ul><ul><li>安装matplotlib<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> matplotlib</span><br></pre></td></tr></table></figure></li></ul><ul><li>安装 terminaltables<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> terminaltables</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>安装 pip install pycocotools</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> pycocotools</span><br></pre></td></tr></table></figure></li><li><p>选择一个看的顺眼的位置：Clone the mmdetection repository.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/open-mmlab/mmdetection.git</span><br><span class="line">cd mmdetection</span><br></pre></td></tr></table></figure></li><li><p>Install mmdetection</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python setup.py develop</span><br></pre></td></tr></table></figure></li><li><p>大功告成</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>torch.no_grad</title>
      <link href="2019/07/17/torch-no-grad/"/>
      <url>2019/07/17/torch-no-grad/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/21" target="_blank" rel="noopener">https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/21</a></li><li>with torch.no_grad()</li><li>可以减少内存，加快运行速度，同时可以使得batch_size 增大</li><li>但不是说非得必要</li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spacy工具包</title>
      <link href="2019/07/16/Spacy%E5%B7%A5%E5%85%B7%E5%8C%85/"/>
      <url>2019/07/16/Spacy%E5%B7%A5%E5%85%B7%E5%8C%85/</url>
      
        <content type="html"><![CDATA[<h2 id="spacy的主要操作："><a href="#spacy的主要操作：" class="headerlink" title="spacy的主要操作："></a>spacy的主要操作：</h2><h3 id="1、分词断句"><a href="#1、分词断句" class="headerlink" title="1、分词断句"></a>1、分词断句</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import spacy</span><br><span class="line">nlp = spacy.<span class="built_in">load</span>(<span class="string">'en'</span>)</span><br><span class="line">doc = nlp(<span class="string">'Hello World! My name is HanXiaoyang'</span>)</span><br><span class="line"><span class="comment"># 分词</span></span><br><span class="line"><span class="keyword">for</span> <span class="keyword">token</span> <span class="keyword">in</span> doc:</span><br><span class="line">    print(<span class="string">'"'</span> + <span class="keyword">token</span>.<span class="keyword">text</span> + <span class="string">'"'</span>)</span><br><span class="line"><span class="comment"># 断句</span></span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> doc.sents:</span><br><span class="line">    print(sent)</span><br></pre></td></tr></table></figure><p>每个token对象有着非常丰富的属性，如下的方式可以取出其中的部分属性。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">doc</span> <span class="string">=</span> <span class="string">nlp("Next</span> <span class="string">week</span> <span class="string">I'll</span>   <span class="string">be</span> <span class="string">in</span> <span class="string">Shanghai.")</span></span><br><span class="line"><span class="string">for</span> <span class="string">token</span> <span class="string">in</span> <span class="attr">doc:</span></span><br><span class="line">    <span class="string">print("&#123;0&#125;\t&#123;1&#125;\t&#123;2&#125;\t&#123;3&#125;\t&#123;4&#125;\t&#123;5&#125;\t&#123;6&#125;\t&#123;7&#125;".format(</span></span><br><span class="line">        <span class="string">token.text,</span></span><br><span class="line">        <span class="string">token.idx,</span></span><br><span class="line">        <span class="string">token.lemma_,</span></span><br><span class="line">        <span class="string">token.is_punct,</span></span><br><span class="line">        <span class="string">token.is_space,</span></span><br><span class="line">        <span class="string">token.shape_,</span></span><br><span class="line">        <span class="string">token.pos_,</span></span><br><span class="line">        <span class="string">token.tag_</span></span><br><span class="line">    <span class="string">))</span></span><br><span class="line"><span class="string">输出结果如下：</span></span><br><span class="line"><span class="string">Next</span>    <span class="number">0</span>   <span class="string">next</span>    <span class="literal">False</span>   <span class="literal">False</span>   <span class="string">Xxxx</span>    <span class="string">ADJ</span> <span class="string">JJ</span></span><br><span class="line"><span class="string">week</span>    <span class="number">5</span>   <span class="string">week</span>    <span class="literal">False</span>   <span class="literal">False</span>   <span class="string">xxxx</span>    <span class="string">NOUN</span>    <span class="string">NN</span></span><br><span class="line"><span class="string">I</span>   <span class="number">10</span>  <span class="bullet">-PRON-</span>  <span class="literal">False</span>   <span class="literal">False</span>   <span class="string">X</span>   <span class="string">PRON</span>    <span class="string">PRP</span></span><br><span class="line"><span class="string">'ll 11  will    False   False   '</span><span class="string">xx</span> <span class="string">VERB</span>    <span class="string">MD</span></span><br><span class="line">    <span class="number">15</span>      <span class="literal">False</span>   <span class="literal">True</span>        <span class="string">SPACE</span>   <span class="string">_SP</span></span><br><span class="line"><span class="string">be</span>  <span class="number">17</span>  <span class="string">be</span>  <span class="literal">False</span>   <span class="literal">False</span>   <span class="string">xx</span>  <span class="string">VERB</span>    <span class="string">VB</span></span><br><span class="line"><span class="string">in</span>  <span class="number">20</span>  <span class="string">in</span>  <span class="literal">False</span>   <span class="literal">False</span>   <span class="string">xx</span>  <span class="string">ADP</span> <span class="string">IN</span></span><br><span class="line"><span class="string">Shanghai</span>    <span class="number">23</span>  <span class="string">shanghai</span>    <span class="literal">False</span>   <span class="literal">False</span>   <span class="string">Xxxxx</span>   <span class="string">PROPN</span>   <span class="string">NNP</span></span><br><span class="line"><span class="string">.</span>   <span class="number">31</span>  <span class="string">.</span>   <span class="literal">True</span>    <span class="literal">False</span>   <span class="string">.</span>   <span class="string">PUNCT</span>   <span class="string">.</span></span><br></pre></td></tr></table></figure><h3 id="2、词性标注"><a href="#2、词性标注" class="headerlink" title="2、词性标注"></a>2、词性标注</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 词性标注</span></span><br><span class="line">doc = nlp(<span class="string">"Next week I'll be in Shanghai."</span>)</span><br><span class="line">print([(<span class="keyword">token</span>.<span class="keyword">text</span>, <span class="keyword">token</span>.tag_) <span class="keyword">for</span> <span class="keyword">token</span> <span class="keyword">in</span> doc])</span><br></pre></td></tr></table></figure><p>[(‘Next’, ‘JJ’), (‘week’, ‘NN’), (‘I’, ‘PRP’), (“‘ll”, ‘MD’), (‘be’, ‘VB’), (‘in’, ‘IN’), (‘Shanghai’, ‘NNP’), (‘.’, ‘.’)]</p><h3 id="3、组块分析"><a href="#3、组块分析" class="headerlink" title="3、组块分析"></a>3、组块分析</h3><p>spaCy可以自动检测名词短语，并输出根(root)词，比如下面的”Journal”,”piece”,”currencies”</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">doc = nlp(<span class="string">"Wall Street Journal just published an interesting piece on crypto currencies"</span>)</span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> doc<span class="selector-class">.noun_chunks</span>:</span><br><span class="line">    print(chunk<span class="selector-class">.text</span>, chunk<span class="selector-class">.label_</span>, chunk<span class="selector-class">.root</span><span class="selector-class">.text</span>)</span><br></pre></td></tr></table></figure><p>输出结果：<br>Wall Street Journal NP Journal<br>an interesting piece NP piece<br>crypto currencies NP currencies</p><h3 id="4、命名实体识别"><a href="#4、命名实体识别" class="headerlink" title="4、命名实体识别"></a>4、命名实体识别</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">doc = nlp(<span class="string">"Two years ago, I lived in my Beijing."</span>)</span><br><span class="line"><span class="keyword">for</span> ent <span class="keyword">in</span> doc<span class="selector-class">.ents</span>:</span><br><span class="line">    print(ent<span class="selector-class">.text</span>, ent.label_)</span><br></pre></td></tr></table></figure><p>输出结果：<br>Two years ago DATE<br>BeijingGPE</p><p>还可以用非常漂亮的可视化做显示：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> spacy import displacy</span><br><span class="line">displacy.render(doc, <span class="attribute">style</span>=<span class="string">'ent'</span>, <span class="attribute">jupyter</span>=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="//upload-images.jianshu.io/upload_images/11681023-77f9837fa7e661dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/454/format/webp" alt></p><p>输出结果.png</p><h3 id="5、句法依存解析"><a href="#5、句法依存解析" class="headerlink" title="5、句法依存解析"></a>5、句法依存解析</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">doc = nlp(<span class="string">'Wall Street Journal just published an interesting piece on crypto currencies'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    print(<span class="string">"&#123;0&#125;/&#123;1&#125; &lt;--&#123;2&#125;-- &#123;3&#125;/&#123;4&#125;"</span>.format(</span><br><span class="line">        token<span class="selector-class">.text</span>, token<span class="selector-class">.tag_</span>, token<span class="selector-class">.dep_</span>, token<span class="selector-class">.head</span><span class="selector-class">.text</span>, token<span class="selector-class">.head</span><span class="selector-class">.tag_</span>))</span><br></pre></td></tr></table></figure><p>输出结果：<br>Wall/NNP &lt;—compound— Street/NNP<br>Street/NNP &lt;—compound— Journal/NNP<br>Journal/NNP &lt;—nsubj— published/VBD<br>just/RB &lt;—advmod— published/VBD<br>published/VBD &lt;—ROOT— published/VBD<br>an/DT &lt;—det— piece/NN<br>interesting/JJ &lt;—amod— piece/NN<br>piece/NN &lt;—dobj— published/VBD<br>on/IN &lt;—prep— piece/NN<br>crypto/JJ &lt;—compound— currencies/NNS<br>currencies/NNS &lt;—pobj— on/IN</p><h3 id="6、-词向量"><a href="#6、-词向量" class="headerlink" title="6、==词向量=="></a>6、==词向量==</h3><p>NLP中有一个非常强大的文本表示学习方法叫做==word2vec==，通过词的上下文学习到词语的稠密向量化表示，同时在这个表示形态下，语义相关的词在向量空间中会比较接近。也有类似v(爷爷)-v(奶奶) ≈ v(男人)-v(女人)的关系。<br>在spaCy中，要使用英文的词向量，需先下载预先训练好的结果。</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">python3</span> -m spacy download en_core_web_lg</span><br></pre></td></tr></table></figure><p>词向量的应用：</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">nlp = spacy.<span class="built_in">load</span>(<span class="string">'en_core_web_lg'</span>)</span><br><span class="line"><span class="built_in">from</span> scipy import spatial</span><br><span class="line"></span><br><span class="line"><span class="comment"># 余弦相似度计算</span></span><br><span class="line">cosine_similarity = lambda x, y: <span class="number">1</span> - spatial.distance.cosine(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 男人、女人、国王、女王 的词向量</span></span><br><span class="line">man = nlp.vocab[<span class="string">'man'</span>].vector</span><br><span class="line">woman = nlp.vocab[<span class="string">'woman'</span>].vector</span><br><span class="line">queen = nlp.vocab[<span class="string">'queen'</span>].vector</span><br><span class="line">king = nlp.vocab[<span class="string">'king'</span>].vector</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 我们对向量做一个简单的计算，"man" - "woman" + "queen"</span></span><br><span class="line">maybe_king = man - woman + queen</span><br><span class="line">computed_similarities = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 扫描整个词库的词向量做比对，召回最接近的词向量</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">word</span> <span class="keyword">in</span> nlp.vocab:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">word</span>.has_vector:</span><br><span class="line">        continue</span><br><span class="line"> </span><br><span class="line">    similarity = cosine_similarity(maybe_king, <span class="built_in">word</span>.vector)</span><br><span class="line">    computed_similarities.append((<span class="built_in">word</span>, similarity))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 排序与最接近结果展示</span></span><br><span class="line">computed_similarities = sorted(computed_similarities, key=lambda <span class="keyword">item</span>: -<span class="keyword">item</span>[<span class="number">1</span>])</span><br><span class="line">print([w[<span class="number">0</span>].<span class="keyword">text</span> <span class="keyword">for</span> w <span class="keyword">in</span> computed_similarities[:<span class="number">10</span>]])</span><br></pre></td></tr></table></figure><p>输出结果：<br>[‘Queen’, ‘QUEEN’, ‘queen’, ‘King’, ‘KING’, ‘king’, ‘KIng’, ‘Kings’, ‘KINGS’, ‘kings’]</p><h3 id="6、词汇与文本相似度"><a href="#6、词汇与文本相似度" class="headerlink" title="6、词汇与文本相似度"></a>6、词汇与文本相似度</h3><p>在词向量的基础上，spaCy提供了从词到文档的相似度计算的方法，下面的例子是它的使用方法。</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 词汇语义相似度(关联性)</span></span><br><span class="line">banana = nlp.vocab['banana']</span><br><span class="line">dog = nlp.vocab['dog']</span><br><span class="line">fruit = nlp.vocab['fruit']</span><br><span class="line">animal = nlp.vocab['animal']</span><br><span class="line"> </span><br><span class="line">print(dog.similarity(animal), dog.similarity(fruit)) <span class="comment"># 0.6618534 0.23552845</span></span><br><span class="line">print(banana.similarity(fruit), banana.similarity(animal)) <span class="comment"># 0.67148364 0.2427285</span></span><br></pre></td></tr></table></figure><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文本语义相似度(关联性)</span></span><br><span class="line">target = nlp(<span class="string">"Cats are beautiful animals."</span>)</span><br><span class="line"> </span><br><span class="line">doc1 = nlp(<span class="string">"Dogs are awesome."</span>)</span><br><span class="line">doc2 = nlp(<span class="string">"Some gorgeous creatures are felines."</span>)</span><br><span class="line">doc3 = nlp(<span class="string">"Dolphins are swimming mammals."</span>)</span><br><span class="line"> </span><br><span class="line">print(target.similarity(doc1))  <span class="comment"># 0.8901765218466683</span></span><br><span class="line">print(target.similarity(doc2))  <span class="comment"># 0.9115828449161616</span></span><br><span class="line">print(target.similarity(doc3))  <span class="comment"># 0.7822956752876101</span></span><br></pre></td></tr></table></figure><p>作者：还是那个没头脑<br>链接：<a href="https://www.jianshu.com/p/74e6c5376bc0" target="_blank" rel="noopener">https://www.jianshu.com/p/74e6c5376bc0</a><br>来源：简书<br>简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch-gather</title>
      <link href="2019/07/12/pytorch-gather/"/>
      <url>2019/07/12/pytorch-gather/</url>
      
        <content type="html"><![CDATA[<p>函数<code>torch.gather(input, dim, index, out=None) → Tensor</code><br> 沿给定轴 dim ,将输入索引张量 index 指定位置的值进行聚合.<br> 对一个 3 维张量,输出可以定义为:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out[<span class="string">i</span>][<span class="symbol">j</span>][<span class="string">k</span>] = input[<span class="string">index[i</span>][<span class="symbol">j</span>][<span class="string">k</span>]][<span class="string">j</span>][<span class="symbol">k</span>]  # if dim == 0</span><br><span class="line">out[<span class="string">i</span>][<span class="symbol">j</span>][<span class="string">k</span>] = input[<span class="string">i</span>][<span class="symbol">index[i</span>][<span class="string">j</span>][<span class="symbol">k</span>]][<span class="symbol">k</span>]  # if dim == 1</span><br><span class="line">out[<span class="string">i</span>][<span class="symbol">j</span>][<span class="string">k</span>] = input[<span class="string">i</span>][<span class="symbol">j</span>][<span class="string">index[i</span>][<span class="symbol">j</span>][<span class="string">k</span>]]  # if dim == 2</span><br></pre></td></tr></table></figure><p>Parameters:</p><ul><li>input (Tensor) – 源张量</li><li>dim (int) – 索引的轴</li><li>index (LongTensor) – 聚合元素的下标(index需要是torch.longTensor类型)</li><li>out (Tensor, optional) – 目标张量</li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2019/07/11/hello-world/"/>
      <url>2019/07/11/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>numpy 函数</title>
      <link href="2019/06/16/numpy-%E5%87%BD%E6%95%B0/"/>
      <url>2019/06/16/numpy-%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="np-round"><a href="#np-round" class="headerlink" title="np.round "></a><font color="#0099ff" size="7" face="黑体">np.round </font></h2><h3 id="round函数概念："><a href="#round函数概念：" class="headerlink" title="round函数概念："></a>round函数概念：</h3><p>英文：圆，四舍五入<br>是python内置函数，它在哪都能用，对数字取四舍五入。<br>round(number[, ndigits])<br>round 对传入的数据进行四舍五入，如果ngigits不传，默认是0（就是说保留整数部分）.ngigits&lt;0 的时候是来对整数部分进行四舍五入，返回的结果是浮点数.</p><h3 id="round-负数"><a href="#round-负数" class="headerlink" title="round 负数"></a>round 负数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 四舍五入是围绕着0来计算的，  </span></span><br><span class="line">round(<span class="number">0.5</span>) <span class="comment"># 1.0  </span></span><br><span class="line">round(<span class="number">-0.5</span>) <span class="comment">#-1.0</span></span><br></pre></td></tr></table></figure><h3 id="round-的陷阱"><a href="#round-的陷阱" class="headerlink" title="round 的陷阱"></a>round 的陷阱</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">round(<span class="number">1.675</span>, <span class="number">2</span>) <span class="comment">#1.68  </span></span><br><span class="line">round(<span class="number">2.675</span>, <span class="number">2</span>) <span class="comment">#2.67</span></span><br></pre></td></tr></table></figure><h3 id="举例："><a href="#举例：" class="headerlink" title="举例："></a>举例：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">round(<span class="number">3.4</span>) <span class="comment"># 3.0  </span></span><br><span class="line">round(<span class="number">3.5</span>) <span class="comment"># 4.0  </span></span><br><span class="line">round(<span class="number">3.6</span>) <span class="comment"># 4.0  </span></span><br><span class="line">round(<span class="number">3.6</span>, <span class="number">0</span>) <span class="comment"># 4.0  </span></span><br><span class="line">round(<span class="number">1.95583</span>, <span class="number">2</span>) <span class="comment"># 1.96  </span></span><br><span class="line">round(<span class="number">1241757</span>, <span class="number">-3</span>) <span class="comment"># 1242000.0  </span></span><br><span class="line">round(<span class="number">5.045</span>, <span class="number">2</span>) <span class="comment"># 5.05  </span></span><br><span class="line">round(<span class="number">5.055</span>, <span class="number">2</span>) <span class="comment"># 5.06</span></span><br></pre></td></tr></table></figure><h2 id="np-clip"><a href="#np-clip" class="headerlink" title="np.clip"></a><font color="#0099ff" size="7" face="黑体">np.clip</font></h2><p>numpy.clip(a, a_min, a_max, out=None)[source]<br>其中a是一个数组，后面两个参数分别表示最小和最大值，怎么用呢，老规矩，我们看代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x=np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>])</span><br><span class="line">np.clip(x,<span class="number">3</span>,<span class="number">8</span>)</span><br><span class="line">Out[<span class="number">88</span>]:</span><br><span class="line">array([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>self-attention</title>
      <link href="2019/06/16/self-attention/"/>
      <url>2019/06/16/self-attention/</url>
      
        <content type="html"><![CDATA[<ul><li>一篇解读：<a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">https://kexue.fm/archives/4765</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>优先级队列式分支限界法---最小重量机器设计问题--python实现</title>
      <link href="2019/05/22/%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97%E5%BC%8F%E5%88%86%E6%94%AF%E9%99%90%E7%95%8C%E6%B3%95-%E6%9C%80%E5%B0%8F%E9%87%8D%E9%87%8F%E6%9C%BA%E5%99%A8%E8%AE%BE%E8%AE%A1%E9%97%AE%E9%A2%98-python%E5%AE%9E%E7%8E%B0/"/>
      <url>2019/05/22/%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97%E5%BC%8F%E5%88%86%E6%94%AF%E9%99%90%E7%95%8C%E6%B3%95-%E6%9C%80%E5%B0%8F%E9%87%8D%E9%87%8F%E6%9C%BA%E5%99%A8%E8%AE%BE%E8%AE%A1%E9%97%AE%E9%A2%98-python%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<p>这里给出两个解决方案：</p><p>1）不使用优先级，简单使用队列式分支限界法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 普通的FIFO 队列式分支限界法</span></span><br><span class="line"><span class="comment">## 当 不满足总价格不超过d的要求时，则剪枝</span></span><br><span class="line"><span class="comment">## 当搜索到深度n时，即搜索到了叶节点，不再进行扩展节点的操作，而是针对于叶节点所对应的最小值，</span></span><br><span class="line"><span class="comment"># 反向求得该节点所对应的的路径</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># coding : utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> queue</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">n = <span class="number">3</span></span><br><span class="line">m = <span class="number">3</span></span><br><span class="line">d = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">price = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]]</span><br><span class="line">weight = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到当前扩展节点所在的Level</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getlevel</span><span class="params">(m, currrent)</span>:</span></span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    level = <span class="number">0</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> currrent == <span class="number">0</span>:</span><br><span class="line">        level = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> level</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        level = level+<span class="number">1</span></span><br><span class="line">        sum = m**level + sum  <span class="comment"># sum=m</span></span><br><span class="line">        <span class="keyword">if</span> sum-m**level &lt; currrent &lt;= sum:  <span class="comment"># m-m^0 = m-1</span></span><br><span class="line">            <span class="keyword">return</span> level</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到当前扩展节点，所对应的首个子节点所在的索引</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_idx</span><span class="params">(m, current)</span>:</span></span><br><span class="line">    level = getlevel(m, current)</span><br><span class="line">    <span class="comment"># 求current 在该level中的相对索引，即 相对于该level第一个元素的位置</span></span><br><span class="line">    current_level_idx = current - sum([m**i <span class="keyword">for</span> i <span class="keyword">in</span> range(level)])</span><br><span class="line">    <span class="comment"># 子节点所在层的开始绝对索引</span></span><br><span class="line">    start_idx  = sum([m**i <span class="keyword">for</span> i <span class="keyword">in</span> range(level+<span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> start_idx + current_level_idx*m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到最优解之后，反向查找其路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_path</span><span class="params">(m, current)</span>:</span></span><br><span class="line">    path = []</span><br><span class="line">    path.append(current%m)  <span class="comment"># from 1, not from 0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        level = getlevel(m, current)</span><br><span class="line">        <span class="keyword">if</span> level == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> path[::<span class="number">-1</span>]</span><br><span class="line">        current_level_idx = current - sum([m ** i <span class="keyword">for</span> i <span class="keyword">in</span> range(level)])  <span class="comment"># # 求current 在该level中的相对索引，即 相对于该level第一个元素的位置</span></span><br><span class="line">        path.append(current_level_idx // m + <span class="number">1</span>)  <span class="comment"># 得到上一级的索引位置</span></span><br><span class="line">        current = sum([m ** i <span class="keyword">for</span> i <span class="keyword">in</span> range(level<span class="number">-1</span>)]) + current_level_idx // m  <span class="comment">#得到上一级的值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MinWighet</span><span class="params">(n,m,d,price,weight)</span>:</span></span><br><span class="line">    minweight = float(<span class="string">"inf"</span>)</span><br><span class="line">    <span class="comment"># 子集树中的节点数</span></span><br><span class="line">    vec_len = sum([m ** i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>)]) + <span class="number">1</span></span><br><span class="line">    que = queue.Queue()</span><br><span class="line">    que.put(<span class="number">0</span>)</span><br><span class="line">    vec_price = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(vec_len)]</span><br><span class="line">    vec_weight = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(vec_len)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(<span class="keyword">not</span> que.empty()):</span><br><span class="line">        current = que.get()  <span class="comment"># 得到当前扩展节点（索引号）</span></span><br><span class="line">        level = getlevel(m, current)  <span class="comment"># 当前 扩展节点所在的level</span></span><br><span class="line"></span><br><span class="line">        idx = get_idx(m, current)   <span class="comment"># 得到当前扩展节点，所对应的首个子节点所在的索引</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 若搜索完了整棵树</span></span><br><span class="line">        <span class="keyword">if</span> getlevel(m, current) == getlevel(m, vec_len)<span class="number">-1</span>:</span><br><span class="line">            minweight = vec_price[current]</span><br><span class="line">            min_at_idx = current</span><br><span class="line">            <span class="keyword">while</span> (<span class="keyword">not</span> que.empty()):</span><br><span class="line">                <span class="comment"># minweight = min(minweight, vec_price[que.get()])</span></span><br><span class="line">                tmp = que.get()</span><br><span class="line">                <span class="keyword">if</span> minweight &gt; vec_price[tmp]:</span><br><span class="line">                    minweight = vec_price[tmp]</span><br><span class="line">                    min_at_idx = tmp</span><br><span class="line">            path = get_path(m, min_at_idx)</span><br><span class="line">            <span class="keyword">return</span> minweight, path</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断当前的扩展结点下的所有子节点是否可以加入活结点队列中</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            vec_price[idx] = int(vec_price[current] + price[level][i])</span><br><span class="line">            <span class="keyword">if</span> vec_price[idx] &lt;= d:</span><br><span class="line">                vec_weight[idx] = int(vec_weight[current] + weight[level][i])</span><br><span class="line">                que.put(idx)</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(MinWighet(n,m,d,price,weight))</span><br></pre></td></tr></table></figure><p><img src="data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" alt="点击并拖拽以移动"></p><p>2） 优先级队列式分支限界法</p><p>解空间：子集树，每个分支节点的分支数为m<br>解向量：x[1:n]  n为部件数量， x[i] 表示第i个部件使用哪个供应商。</p><p>算法：采用优先队列式分支限界法。<br>类似于单源最短路径，使用当前节点所确定下的采购方案对应的机器重量和最为优先级。<br>由于wij不是负值，当前节点所对应的当前机器重量和是解空间中以该节点为根的子树的中所有节点所对应的重量和的下界。</p><p>算法代码实现：</p><p>1）使用列表来代表队列，通过对列表中的活结点按照其当前重量和进行从小到大排序（实现了最小堆的维护）<br>2）定义一个节点类，属性有：节点所在的索引，以及节点当前的重量和<br>3）取出一个扩展节点：由于对活结点表进行了某种规则的排序，则直接取出列表的第一个元素即可<br>4）加入活结点表：将满足条件的子节点加入到活结点表中</p><p>失活当前扩展节点：删掉列表中的第一个元素即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 普通的FIFO 队列式分支限界法</span></span><br><span class="line"><span class="comment">## 当 不满足总价格不超过d的要求时，则剪枝</span></span><br><span class="line"><span class="comment">## 当搜索到深度n时，即搜索到了叶节点，不再进行扩展节点的操作，而是针对于叶节点所对应的最小值，</span></span><br><span class="line"><span class="comment"># 反向求得该节点所对应的的路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入优先级--使用当前节点的重量作为优先级，重量小优先级高</span></span><br><span class="line"><span class="comment"># 将队列改成列表，以append的方式加入到列表中，再以排序的方式维护当前列表的首个元素为最小权值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># coding : utf-8</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到当前扩展节点所在的Level</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getlevel</span><span class="params">(m, currrent)</span>:</span></span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    level = <span class="number">0</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> currrent == <span class="number">0</span>:</span><br><span class="line">        level = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> level</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        level = level+<span class="number">1</span></span><br><span class="line">        sum = m**level + sum  <span class="comment"># sum=m</span></span><br><span class="line">        <span class="keyword">if</span> sum-m**level &lt; currrent &lt;= sum:  <span class="comment"># m-m^0 = m-1</span></span><br><span class="line">            <span class="keyword">return</span> level</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到当前扩展节点，所对应的首个子节点所在的索引</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_idx</span><span class="params">(m, current)</span>:</span></span><br><span class="line">    level = getlevel(m, current)</span><br><span class="line">    <span class="comment"># 求current 在该level中的相对索引，即 相对于该level第一个元素的位置</span></span><br><span class="line">    current_level_idx = current - sum([m**i <span class="keyword">for</span> i <span class="keyword">in</span> range(level)])</span><br><span class="line">    <span class="comment"># 子节点所在层的开始绝对索引</span></span><br><span class="line">    start_idx  = sum([m**i <span class="keyword">for</span> i <span class="keyword">in</span> range(level+<span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> start_idx + current_level_idx*m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到最优解之后，反向查找其路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_path</span><span class="params">(m, current)</span>:</span></span><br><span class="line">    path = []</span><br><span class="line">    path.append(current%m)  <span class="comment"># from 1, not from 0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        level = getlevel(m, current)</span><br><span class="line">        <span class="keyword">if</span> level == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> path[::<span class="number">-1</span>]</span><br><span class="line">        current_level_idx = current - sum([m ** i <span class="keyword">for</span> i <span class="keyword">in</span> range(level)])  <span class="comment"># # 求current 在该level中的相对索引，即 相对于该level第一个元素的位置</span></span><br><span class="line">        path.append(current_level_idx // m + <span class="number">1</span>)  <span class="comment"># 得到上一级的索引位置</span></span><br><span class="line">        current = sum([m ** i <span class="keyword">for</span> i <span class="keyword">in</span> range(level<span class="number">-1</span>)]) + current_level_idx // m  <span class="comment">#得到上一级的值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为活结点表中的节点 定义了一个类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, idx, weight)</span>:</span></span><br><span class="line">        self.idx = idx</span><br><span class="line">        self.weight = weight</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MinWighet</span><span class="params">(n,m,d,price,weight)</span>:</span></span><br><span class="line">    <span class="comment"># 子集树中的节点数</span></span><br><span class="line">    vec_len = sum([m ** i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>)]) + <span class="number">1</span></span><br><span class="line">    que = []</span><br><span class="line">    que.append(Node(<span class="number">0</span>,<span class="number">0</span>))  <span class="comment"># 在活结点表中加入根节点</span></span><br><span class="line">    <span class="comment"># vec_price = [0 for _ in range(vec_len)]</span></span><br><span class="line">    <span class="comment"># vec_weight = [0 for _ in range(vec_len)]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(que):  <span class="comment"># 当活结点表非空时</span></span><br><span class="line">        que = sorted(que, key=<span class="keyword">lambda</span> node: node.weight)  <span class="comment"># 类似于最小堆的维护</span></span><br><span class="line">        current = que[<span class="number">0</span>]  <span class="comment"># 得到当前扩展节点（索引号）</span></span><br><span class="line">        level = getlevel(m, current.idx)  <span class="comment"># 当前 扩展节点所在的level</span></span><br><span class="line"></span><br><span class="line">        new_node_idx = get_idx(m, current.idx)   <span class="comment"># 得到当前扩展节点，所对应的首个子节点所在的索引</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 若搜索完了整棵树</span></span><br><span class="line">        <span class="keyword">if</span> getlevel(m, current.idx) == getlevel(m, vec_len)<span class="number">-1</span>:</span><br><span class="line">            minweight = current.weight</span><br><span class="line">            min_at_idx = current.idx</span><br><span class="line"></span><br><span class="line">            path = get_path(m, min_at_idx)</span><br><span class="line">            <span class="keyword">return</span> minweight, path</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断当前的扩展结点下的所有子节点是否可以加入活结点队列中</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="keyword">if</span> int(current.weight + price[level][i]) &lt;= d:</span><br><span class="line">                new_node = Node(new_node_idx, int(current.weight + weight[level][i]))</span><br><span class="line">                que.append(new_node)</span><br><span class="line">            new_node_idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将当前的扩展节点失活</span></span><br><span class="line">        <span class="keyword">del</span> que[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    n = <span class="number">3</span></span><br><span class="line">    m = <span class="number">3</span></span><br><span class="line">    d = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    price = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]]</span><br><span class="line">    weight = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]]</span><br><span class="line">    result = MinWighet(n,m,d,price,weight)</span><br><span class="line">    print(MinWighet(n,m,d,price,weight))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Grounded Video Description</title>
      <link href="2019/05/10/Grounded-Video-Description/"/>
      <url>2019/05/10/Grounded-Video-Description/</url>
      
        <content type="html"><![CDATA[<h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li>这是CVPR 2019 做视频描述的一篇文章，该文的主要贡献是对已有的ActivityNet dense caption数据集进行加强—-对其中的帧进行了object bbox的标注，这就为视频描述任务增加了非常有用的信息。</li><li>总的来说，<font color="#dd00dd">该文的出发点是：1. 利用object 信息来生成句子. 2. 希望生成的句子中的名词，在video中可以找到相对应的证据(object)。</font><br></li><li>grounded-based video description model ：联合生成的单词，并微调在description中生成的object。可以探索这种显式的监督对视频描述带来的益处，并与无监督（可能利用region feature，但是没有 penalize grounding）的方法进行对比。</li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ul><li>video输出的句子中提到的object，并没有在该video中实际存在。那么为什么有这种现象，是因为前的视频描述模型是基于先验知识，在之前的训练过程中，存在一个视频与该视频有 similar contexts，从而使得输出的单词中涵盖了训练video中的obejct，而不是该video本身的object，导致了该现象。</li><li>因此本文的工作： 将句子中的名词与视频中的object一 一对应起来，这样就可以建立sentence 与 evidence of video 之间的联系。<br>这样可以<font color="#0099ff" size="5">在视频描述模型中使用这些object 信息</font>，同时，<font color="#0099ff" size="5">还可以评估模型</font>（“teaching models to explicitly rely on the corresponding evidence in the video frame  when generating words and evaluating how well models   are doing in grounding individual words or phrases they  generated.”）。<br>如：该模型生成的句子中的名词与已经标注的object是否是一 一对应的（即便一个模型说出争取的sentence，比如一个男孩正在玩耍，但是如果video中有多个个男孩，那么该sentence输出的一个男孩是指向哪一个呢？）。</li></ul><h2 id="标注bbox时的细节"><a href="#标注bbox时的细节" class="headerlink" title="标注bbox时的细节"></a>标注bbox时的细节</h2><ul><li>“we collect ActivityNet-Entities (short as ANet-Entities) which  grounds or links noun phrases in sentences with bounding  boxes in the video frames.”<br>“we only  annotate a single frame of the video for each noun phrase” 。<br>即， 是对照着已有的sentence中的名词对其进行加框(bbox)，而不是对该video中的所有示例进行标注。对于sentence中的一个名词只在一帧上进行标注（稀疏标注） </li></ul><h2 id="调研工作"><a href="#调研工作" class="headerlink" title="调研工作"></a>调研工作</h2><h3 id="1-结合object-feature-做captioning任务"><a href="#1-结合object-feature-做captioning任务" class="headerlink" title="1. 结合object feature 做captioning任务"></a>1. 结合object feature 做captioning任务</h3><ul><li><strong>当前的方法</strong>，主要是两步：（1）使用off-the-shelf 或者是fine-tuned 的 object detector 来得到 object proposals （2）对object features采用动态attention，或者是对region进行分类，然后送入 decoder中。</li><li><strong>存在的问题</strong>，使用现成的object detector 将会使得到的object proposals 更偏向于 source dataset , 而不是偏向于当前的视频描述target dataset。一种解决方案是针对于target dataset 来fine-tune object detector。但是这种那个方案需要大量的标注，尤其是对于video，数据量会更大，</li><li>因此提出了<font color="#0099ff" size="5" face="黑体">本文的方法(给出了fine-tune obejct detector的改进方案)</font>：“Instead of  fine-tuning a general detector, we transfer the object classification knowledge from off-the-shelf object detectors to  our model and then fine-tune this representation as part of  our generation model with sparse box annotations. ”。</li><li>在文章中的4.4节开头给出了具体的实现：已经得到了bbox，现在的目的是想要得到the class probability distribution for each region. 将在visual genome上预训练的detector迁移到我们的<strong>object classifier</strong> 任务上，另外关于classes集合，假定我们已经有了K个类别，则我们在Visual Genome中根据最近距离找到与其对应的classes。  定义一个softmax( Wx+b )的分类层，W 和 b 的初始化是预训练的detector的最后一个线性层（分类层）的参数值（W应该是根据找出的K个类别按照索引抽出的一个矩阵）。<h3 id="2-object-attention"><a href="#2-object-attention" class="headerlink" title="2. object attention"></a>2. object attention</h3></li><li>某些作者指出，attention model关注的region和人类的关注点并不一致，增加attention supervision几乎不能提高性能。另一方面，在feature map attention 上增加监督，是有益处的。</li><li>在该文作者的实验中，region attention with supervision 并不能带来性能的提升，作者分析，这可能是由于缺少object context 信息，因此<font color="#0099ff" size="5" face="黑体">该文作者在attention model中引入了基于context encoding 的self-attention</font>，这将会使得信息能够在被采样的视频帧中的regions 传递(我理解的是，region feature 不仅仅是单纯的从fc层中提取到的信息，同时也结合了其余信息来得到 grounding-aware region encoding， 在文章的4.3 以及 4.4节有关于<strong>R<sup>~</sup></strong> 的定义)。 </li></ul><h2 id="Description-with-Grounding-Supervision"><a href="#Description-with-Grounding-Supervision" class="headerlink" title="Description with Grounding Supervision"></a>Description with Grounding Supervision</h2><ul><li>这个框架包括三个模块: <strong>grounding</strong>, <strong>region attention</strong> and <strong>language generation</strong>.<br>grounding： 对于生成的word， 从video中检测到对应的visual clue。<br>region attention: 动态的将visual clue 形成一个high-level的视觉内容的表达，并将其送入decoder。   </li><li>这里包括三种方式来结合object-level supervision: <strong>region classification</strong>,  <strong>object grounding (localization)</strong>, and <strong>supervised attention</strong>.  </li><li><strong>（我的理解，supervised attention直接针对attention中的系数，查看与真实的对应关系，设计的这个loss对于视频描述生成由益处；object grounding 涉及到了region attention中的系数，因此与描述生成有一定的关联，反向传播可能是有益处的；region classification中设计的loss： 它的反向传播会更新M<sub>s</sub>( R ), 进一步作用于region encoding, 进一步作用于region attention 和 language generate；）</strong></li></ul><h3 id="Language-Generation-Module"><a href="#Language-Generation-Module" class="headerlink" title="Language Generation Module"></a>Language Generation Module</h3><ul><li>本文的decoder 部分采用 [1] 中提到的decoder，与bottom up[3] 的结构大致近似，但是<strong>在[1]中</strong>第二层 language lstm的输入部分，不仅包括attention of region feature， 还包括attention of 最后一个卷积层k girds的特征。对应到本文的视频描述任务上，第二层language lstm的输入，不仅包括attention of region features ，还包括 attention of frames features。即region attention 和 temporal attention</li><li>需要注意的是该文中使用的<font color="#0099ff" size="5" face="黑体">temporal attention</font>是[2]中提到的self-attention context encoder with Bidirectional GRU (Bi-GRU)，而不是[1]中使用的attention机制。</li><li><font color="#0099ff" size="5" face="黑体">region attention</font>采用的就是bottom up 中的attention结构</li><li>下面将[1] 中的原图贴一下：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2x5hlq801j30l20ezq4v.jpg">   </li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Region-proposal-and-features"><a href="#Region-proposal-and-features" class="headerlink" title="Region proposal and features"></a>Region proposal and features</h3><p>For each frame, we use a Faster  R-CNN detector [24] with ResNeXt-101 backbone [30] for  region proposal and feature extraction (fc6). The detector is  pretrained on Visual Genome。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Neural baby talk. In CVPR 2018.<br>[2] End-to-end dense video captioning with masked transformer. In CVPR 2018.<br>[3] Bottom-up and top-down attention for image captioning and  visual question answering. In CVPR 2018.</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning（CVPR2019）</title>
      <link href="2019/05/10/Spatio-Temporal-Dynamics-and-Semantic-Attribute-Enriched-Visual-Encoding-for-Video-Captioning-1/"/>
      <url>2019/05/10/Spatio-Temporal-Dynamics-and-Semantic-Attribute-Enriched-Visual-Encoding-for-Video-Captioning-1/</url>
      
        <content type="html"><![CDATA[<p><img src="https://i.loli.net/2019/09/02/T5AzpW8DHkVL2Oy.png" alt="搜狗截图20190902152617.png"></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>我们的方法丰富视觉特征的<strong>时域动态temporal dynamics</strong>，通过在整个video上分层对CNN特征应用短的fourier 变换</li><li>从object detector 中提取高层语义，来丰富被检测object 的<strong>空间动态 spatial dynamics</strong></li><li>最终的表达映射到一个压缩的空间</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>object detector YOLO[1]</li><li>目标检测和C3D的输出层被用来得到高层语义属性，</li><li>提出的视觉特征包含检测的目标属性、目标发生的频率</li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>现有的video captioning model 一种使用平均池化得到特征，一种使用attention得到high level特征，但是这些视觉特征都是直接的被使用，则，这些方法没有充分利用CNN在视频字幕框架中的最新特性。我们的模型丰富了视觉特征，实验结果证明，该视觉特征与任意一个简单的语言模型相结合，可以提高其性能。</p><h2 id="Visual-Representation"><a href="#Visual-Representation" class="headerlink" title="Visual Representation"></a>Visual Representation</h2><ul><li>the visual representation of a  video V as v = [α; β; γ; η]</li><li>α; β; γ; η 是四个列向量，下面具体介绍如何得到这四个列向量<h3 id="Encoding-temporal-dynamics"><a href="#Encoding-temporal-dynamics" class="headerlink" title="Encoding temporal dynamics"></a>Encoding temporal dynamics</h3></li><li>首先已经有 f 帧 对应的CNN[2]特征向量，和c个clip对应的C3D[3]得到的特征向量</li><li>对某个video而言，其所有帧再某一个维度的神经元，组成了一个特征向量a，利用<strong>傅里叶变换</strong>得到一个p维度的特征向量，将a分成两半，分别进行傅里叶变换，得到一个p维度的特征向量，再次进行分半，等等一系列操作，可以得到 p×7的矩阵。则对于所有的神经元m 则得到m×p×7的张量。至此得到<strong>α</strong></li><li><strong>β</strong>同理，只是对clips对应的C3D特征进行处理</li><li>目前已经有将傅里叶变换应用在行为识别上的文章吗，但是本文是第一篇将傅里叶变换应用在视频描述上的文章。</li><li><font color="#0099ff" size="4" face="黑体">但是需要注意的是，该文并没有说明使用傅里叶变换的动机（rich temporal dynamics?），但是为什么使用傅里叶变换可以丰富？？</font></li></ul><h3 id="Encoding-Semantics-and-Spatial-Evolution"><a href="#Encoding-Semantics-and-Spatial-Evolution" class="headerlink" title="Encoding Semantics and Spatial Evolution"></a>Encoding Semantics and Spatial Evolution</h3><ul><li>比较复杂，利用object detector YOLO 来提取Object 以及C3D来加强语义信息，具体看论文吧</li></ul><h2 id="Experimental-Results-on-MSVD"><a href="#Experimental-Results-on-MSVD" class="headerlink" title="Experimental Results on MSVD"></a>Experimental Results on MSVD</h2><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2wd8q6kegj30dh0ig0wc.jpg">    </p><ul><li>GRU-MP - (C3D) 与 使用了傅里叶变换的GRU-EVEhft - (C3D)  相比，可知，使用傅里叶变换是有小鬼的</li><li>GRU-EVEhft - (CI) 与GRU-EVEhft+sem - (CI)相比，可得增加的senmatic 效果是不显著的。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Yolo9000: better, faster, stronger.  In IEEE CVPR, 2017<br>[2] Inception-v4, inception-resnet and the impact of residual  connections on learning. In AAAI, volume 4, page 12, 2017.<br>[3] Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference  on computer vision, pages 4489–4497, 2015.</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python-dict</title>
      <link href="2019/05/08/python-dict/"/>
      <url>2019/05/08/python-dict/</url>
      
        <content type="html"><![CDATA[<h1 id="dict-的get-函数"><a href="#dict-的get-函数" class="headerlink" title="dict 的get 函数"></a>dict 的get 函数</h1><h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Python 字典(Dictionary) get() 函数返回指定键的值，如果值不在字典中返回默认值。</p><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><p>get()方法语法：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dict.<span class="builtin-name">get</span>(key, <span class="attribute">default</span>=None)</span><br></pre></td></tr></table></figure><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><ul><li>key — 字典中要查找的键。</li><li>default — 如果指定键的值不存在时，返回该默认值值。</li></ul><h2 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h2><p>返回指定键的值，如果值不在字典中返回默认值None。</p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><p>get函数的作用是返回指定key的值，若key不存在，则返回default值，default值，默认为None，也可以自己指定</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
      <link href="2019/05/06/Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention/"/>
      <url>2019/05/06/Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention/</url>
      
        <content type="html"><![CDATA[<ul><li>encoder  attention<br>本文的出发点是利用低层次的特征，并结合了attention 机制<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2re7nfbn3j30q70430sx.jpg"></li></ul><p>参考链接：<a href="https://blog.csdn.net/shenxiaolu1984/article/details/51493673" target="_blank" rel="noopener">https://blog.csdn.net/shenxiaolu1984/article/details/51493673</a></p>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>池化层的反向传播</title>
      <link href="2019/04/21/%E6%B1%A0%E5%8C%96%E5%B1%82%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
      <url>2019/04/21/%E6%B1%A0%E5%8C%96%E5%B1%82%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
      
        <content type="html"><![CDATA[<ul><li>参考： <a href="https://blog.csdn.net/qq_21190081/article/details/72871704" target="_blank" rel="noopener">https://blog.csdn.net/qq_21190081/article/details/72871704</a></li><li>总结，<br>（1）对于平均池化层，比如2×2-&gt;1， 梯度的反向传递，是1-&gt;2×2，若梯度为x, 则，反向传播的梯度为4个 1/4<br>（1）对于最大池化层，比如2×2-&gt;1， 梯度的反向传递，是1-&gt;2×2，若梯度为x, 则，反向传播只赋值给最大值所在的元素，其余三个元素的梯度为0</li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An End-to-End Baseline for Video Captioning</title>
      <link href="2019/04/20/An-End-to-End-Baseline-for-Video-Captioning/"/>
      <url>2019/04/20/An-End-to-End-Baseline-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<h3 id="积累梯度那里没有看明白-—-解决内存占用多的问题"><a href="#积累梯度那里没有看明白-—-解决内存占用多的问题" class="headerlink" title="积累梯度那里没有看明白 — 解决内存占用多的问题"></a>积累梯度那里没有看明白 — 解决内存占用多的问题</h3><p>我认为可能是积累 loss, 直到达到某一个batch size才进行反向传播<br>说是为了解决内存占用多的问题，可是这样，就解决了吗？？？</p><h3 id="灵魂反问"><a href="#灵魂反问" class="headerlink" title="灵魂反问"></a>灵魂反问</h3><p>为什么要是end-to-end，我最终需要的是一个效果比较好的模型，但是为了只得到这样的一点提升，反而会需要很多的GPU计算资源。这个end-to-end fine-tune 是否有必要。</p><h3 id="目前方法存在的问题"><a href="#目前方法存在的问题" class="headerlink" title="目前方法存在的问题"></a>目前方法存在的问题</h3><p>encoder：比如 CNN， 一般是在不同任务上的其他数据集上进行预训练的，之后，在训练video captioning任务时，得到的video/image feature就不再fine tune。这样得到的结果是次优的。<br>目前改进这一缺陷的文章有：[1][2][3]，他们尝试捕捉不同帧之间的动态时域，但是，他们没有从根本上改变一个事实：视频描述任务需要一个与该任务相关的特征。</p><h4 id="当前没有人去fine-tune-encoder的原因"><a href="#当前没有人去fine-tune-encoder的原因" class="headerlink" title="当前没有人去fine-tune encoder的原因"></a>当前没有人去fine-tune encoder的原因</h4><p>（1）because of the amount of memory required to process  video data for each batch。是因为每个批次都要处理视频数据，所需要的存储空间会很大。<br>（2）batch sizes for video captioning can become very high (e.g. 512), making training  prohibitive on a small number of GPUs。同时，视频描述的批次一般都比较大，使得所需要的GPU数量会很多。</p><h4 id="本文提取出的解决方案——即训练过程"><a href="#本文提取出的解决方案——即训练过程" class="headerlink" title="本文提取出的解决方案——即训练过程"></a>本文提取出的解决方案——即训练过程</h4><p>In this paper we address this issue by accumulating gradients over multiple steps, to update parameters only after  the required effective batch size is achieved.<br>在多步积累梯度，并只在达到有效的批次大小之后(当神经网络训练完512个examples)，才进行梯度更新。<br>这种训练方案相比于分别训练两部分收敛速度会慢，因为所需的迭代次数增加了。但是这里采用了一个加速训练过程的方案，先分别训练encoder和decoder，然后再end-to-end进行fine-tune。</p><h4 id="主要的贡献"><a href="#主要的贡献" class="headerlink" title="主要的贡献"></a>主要的贡献</h4><ol><li>可以得到与具体任务（视频描述）相关的特征</li><li>积累梯度来限制GPU存储消耗，因此可以处理大批次，这是基于RNN的decoders所需的。</li><li>使用了两阶段的训练来加速训练</li><li>为未来的工作，创建了一个简单的baseline</li></ol><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><ol><li>先预训练encoder，例如利用图像识别、行为识别等</li><li>冻结encoder的参数，在视频描述任务上训练decoder，直到在验证数据集上表现出比较好的效果</li><li>整个网络，端到端的训练，冻结Inception-ResNet-v2中的BN层，由于该过程中占用的内存较多，作者采用了一种方法：accumulating gradients over multiple steps, to update parameters only after  the required effective batch size is achieved. </li></ol><ul><li>需要注意的是，在2. 3. 阶段的训练过程中，SA-LSTM都是使用target words(ground truth)作为输入，而不是使用之前的预测。</li></ul><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p> Inception-ResNet-v2[5] as an encoder，and a modified version of  Soft-Attention LSTM as a decoder.<br> <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2ad637ykjj30hy0h677d.jpg">    </p><ul><li>decoder<ul><li>输入LSTM的input: <strong>x<sup>t</sup></strong> ：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2aeagi7o3j30a30280sl.jpg">    <ul><li>该step 生成的word：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2aeiriwanj30jd014mx7.jpg"></li></ul></li></ul></li></ul><p>soft-attention这里，原文是采用attention机制进行加权求和 ，这里与原soft-attention[2] 略有不同。</p><ul><li><p>原soft-attention论文：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2aeagheogj308o03ct8l.jpg">     </p></li><li><p>现修改为：（增加了β<sub>t</sub>）</p></li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2aeagka0rj30h504dq37.jpg"><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2aeaggzpdj309302xdfq.jpg">   </p><ul><li><font color="#0099ff" size="5" face="黑体">这里与Figure2 图中显示的结构并不一致，这里是全权求和，但是在图中却是concatenate !</font></li></ul><ul><li>还有一些其他的修改<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2aes6w62rj30il09xwgp.jpg"></li></ul><h3 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h3><ul><li>step1 : encoder先训练，然后固定encoder的参数，训练decoder的参数，不进行联合训练</li><li>step2：在step1的基础上，联合训练encoder-decoder。</li><li>MSR-VTT的实验结果<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2ajqfodpkj30kl0b9acc.jpg">  </li><li>MSVD的实验结果<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2ajqfqhbjj30ks0icn0q.jpg"></li></ul><h4 id="yaya：-本文step1训练效果比较好的原因（相较于soft-attention-lstm-原论文）"><a href="#yaya：-本文step1训练效果比较好的原因（相较于soft-attention-lstm-原论文）" class="headerlink" title=" yaya： 本文step1训练效果比较好的原因（相较于soft-attention lstm 原论文）"></a><font color="#0099ff" size="5" face="黑体"> yaya： 本文step1训练效果比较好的原因（相较于soft-attention lstm 原论文）</font></h4><p>这里step1的意思是：encoder、decoder 分开训练，并不进行联合训练</p><ol><li>该文使用的 Inception-ResNet-v2作为encoder来提取特征。会比其他论文中使用的encoder更复杂。</li><li>初始化的细节<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2ak8as8u4j309703pglo.jpg"></li><li>对SA-LSTM[2]而言，进行了一些修改：<br>（1）frame features 转为一个特征向量，使用的注意力机制，但是该文在soft-attention的基础上，还增加了一个系数β<sub>t</sub><br>（2）LSTM的内部结构的计算公式增加了一项，如下图<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2ajg7vn8rj31fi0ci427.jpg"><br>（3）在生成word时，主要差别就是E[y<sub>t-1</sub>]前边是否有权重的问题<ul><li>soft-attention 使用的公式：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2aeiriwanj30jd014mx7.jpg"></li><li>但是在本文中使用的公式为：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g2ajjxtsplj30d103v74i.jpg"><ul><li>但是就该文3.3.2节中说：These changes are inspired by  the original code repository by Yao et al [2]，也就是有可能人家的源代码和在论文中提到的不一致。</li></ul></li></ul></li></ol><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Video captioning with  transferred semantic attributes. CVPR, 2017<br>[2] Describing videos by exploiting temporal  structure. ICCV, 2015.<br>[3] Task-driven dynamic fusion: Reducing ambiguity in video description. CVPR, 2017.<br>[4] Show and  tell: Lessons learned from the 2015 mscoco image captioning challenge. TPAMI, 2016.<br>[5] Inception-v4, inception-resnet and the impact of residual  connections on learning. AAAI, 2017</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python 关于list的操作</title>
      <link href="2019/04/18/python-%E5%85%B3%E4%BA%8Elist%E7%9A%84%E6%93%8D%E4%BD%9C/"/>
      <url>2019/04/18/python-%E5%85%B3%E4%BA%8Elist%E7%9A%84%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h3 id="list-的置换"><a href="#list-的置换" class="headerlink" title="list 的置换"></a>list 的置换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">nums[<span class="number">0</span>],nums[<span class="number">1</span>] = nums[<span class="number">1</span>],nums[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># nums = [2,1,3]</span></span><br></pre></td></tr></table></figure><h3 id="判断是否为空列表"><a href="#判断是否为空列表" class="headerlink" title="判断是否为空列表"></a>判断是否为空列表</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = []</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> a:</span><br><span class="line">    print(<span class="string">"a is a null list"</span>)</span><br><span class="line"><span class="comment"># 输出：a is a null list</span></span><br></pre></td></tr></table></figure><h3 id="列表的连接"><a href="#列表的连接" class="headerlink" title="列表的连接"></a>列表的连接</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b = [<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]</span><br><span class="line">c = a + b</span><br><span class="line">print(c)  <span class="comment"># [1, 2, 3, 6, 7, 8]</span></span><br><span class="line">d = a.extend(b)  <span class="comment"># extend()传入的参数需要是一个迭代对象 ：列表、元组、集合</span></span><br><span class="line">print(d) </span><br><span class="line"><span class="comment"># d 输出为None ,因为extend 无返回值，但是此时 a更改为[1, 2, 3, 6, 7, 8]</span></span><br></pre></td></tr></table></figure><h3 id="列表的排序函数"><a href="#列表的排序函数" class="headerlink" title="列表的排序函数"></a>列表的排序函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = [2,4,6,5]</span><br><span class="line">b = a.sort()  # a = [2,4,5,6] , b = None</span><br><span class="line">a= [2,4,6,5]  </span><br><span class="line">b = sorted(a)  # a = [2, 4, 6, 5] , b = [2, 4, 5, 6]</span><br><span class="line"></span><br><span class="line">## sorted() 输出的是排序的结果，但是不更改传入的列表</span><br><span class="line">## sort() 直接对列表进行排序操作，并更改列表值</span><br></pre></td></tr></table></figure><h3 id="列表的置换顺序函数"><a href="#列表的置换顺序函数" class="headerlink" title="列表的置换顺序函数"></a>列表的置换顺序函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">b = a.reverse() <span class="comment"># a=[4,3,2,1] b = None</span></span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对于列表 nums<br>凡是可以进行 nums.function()，这样的函数，一般是无返回值的，直接对列表本身进行操作。<br>比如<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">nums.extend([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])     <span class="comment"># [1, 2, 3, 4, 1, 2, 3]</span></span><br><span class="line">nums.append(<span class="number">2</span>)           <span class="comment"># [1, 2, 3, 4, 1, 2, 3, 2]</span></span><br><span class="line">nums.sort()              <span class="comment"># [1, 1, 2, 2, 2, 3, 3, 4]</span></span><br><span class="line">nums.reverse()           <span class="comment"># [4, 3, 3, 2, 2, 2, 1, 1]</span></span><br><span class="line">nums.insert(<span class="number">5</span>,<span class="number">100</span>)       <span class="comment"># [4, 3, 3, 2, 2, 100, 2, 1, 1]    在索引为5的位置插入元素100</span></span><br></pre></td></tr></table></figure></p><ul><li>注意<br>nums.index(100)          # <strong>有返回值5</strong>, 查找对应元素<strong>首次出现</strong>所在位置的索引</li></ul><h3 id="数字转为列表-123-gt-“1”-“2”-“3”"><a href="#数字转为列表-123-gt-“1”-“2”-“3”" class="headerlink" title="数字转为列表  123 -&gt; [“1”, “2”, “3”]"></a>数字转为列表  123 -&gt; [“1”, “2”, “3”]</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num = <span class="number">123</span></span><br><span class="line">A = list(str(num))  <span class="comment"># A = ['1', '2', '3']</span></span><br><span class="line">B = int(<span class="string">""</span>.join(A))  <span class="comment"># B = 123</span></span><br></pre></td></tr></table></figure><h3 id="中括号-for循环生成列表，，并使用判断"><a href="#中括号-for循环生成列表，，并使用判断" class="headerlink" title="中括号 for循环生成列表，，并使用判断"></a>中括号 for循环生成列表，，并使用判断</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">2</span>,<span class="number">3</span>,<span class="number">-7</span>,<span class="number">0</span>,]</span><br><span class="line">a = [<span class="number">1</span> <span class="keyword">if</span> num &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> num <span class="keyword">in</span> nums]</span><br><span class="line">print(a)   <span class="comment"># [1, 1, 0, 0]</span></span><br></pre></td></tr></table></figure><h2 id="python内置函数的复杂度"><a href="#python内置函数的复杂度" class="headerlink" title="python内置函数的复杂度"></a>python内置函数的复杂度</h2><p><a href="https://wiki.python.org/moin/TimeComplexity" target="_blank" rel="noopener">https://wiki.python.org/moin/TimeComplexity</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>双指针(two pointers)</title>
      <link href="2019/04/18/%E5%8F%8C%E6%8C%87%E9%92%88-two-pointers/"/>
      <url>2019/04/18/%E5%8F%8C%E6%8C%87%E9%92%88-two-pointers/</url>
      
        <content type="html"><![CDATA[<ul><li>荷兰分区问题<br>可参考 <a href="https://blog.csdn.net/sylar_d/article/details/52742598" target="_blank" rel="noopener">https://blog.csdn.net/sylar_d/article/details/52742598</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>pytorch0.4.0学习</title>
      <link href="2019/04/18/pytorch0-4-0%E5%AD%A6%E4%B9%A0/"/>
      <url>2019/04/18/pytorch0-4-0%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="不会的"><a href="#不会的" class="headerlink" title="不会的"></a>不会的</h2><ol><li>SGD 、 Adam中 的weight_decay 是干嘛的</li><li>SGD 中的 momentum是干嘛的，一般设置为多大</li><li>pytorch 中的初始化函数 </li><li>y = y.permute(0, 2, 1).contiguous()<br>这是干嘛的</li></ol><h3 id="学会的"><a href="#学会的" class="headerlink" title="学会的"></a>学会的</h3><ul><li>累加loss<br>以前（0.3.0）了累加loss(为了看loss的大小)一般是用total_loss+=loss.data[0] , 比较诡异的是, 为啥是.data[0]? 这是因为, 这是因为loss是一个Variable,<br>所以以后累加loss, 用loss.item().这个是必须的, 如果直接加, 那么随着训练的进行, 会导致后来的loss具有非常大的graph, 可能会超内存.<br>然而total_loss只是用来看的, 所以没必要进行维持这个graph!</li></ul><h3 id="pytorch-中-对tensor的一些函数"><a href="#pytorch-中-对tensor的一些函数" class="headerlink" title="pytorch 中 对tensor的一些函数"></a>pytorch 中 对tensor的一些函数</h3><ul><li><p>生成正态分布的随机张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)  <span class="comment"># 正态分布的随机张量</span></span><br><span class="line">a.sum()                 <span class="comment"># 对a中的元素求和</span></span><br></pre></td></tr></table></figure></li><li><p>对tensor 求最大值</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">a</span> = torch.randn(<span class="number">10</span>,<span class="number">8</span>)</span><br><span class="line">max_value, max_index = <span class="keyword">a</span>.<span class="built_in">max</span>(<span class="number">1</span>)   </span><br><span class="line"><span class="comment"># 按照维度对a求最大值 ，此处为1，即得到（10,1）的张量，</span></span><br><span class="line"><span class="comment"># 有两个返回值，第一个返回值为具体的最大值为多少，第二个返回值为该最大值所在的索引</span></span><br></pre></td></tr></table></figure></li><li><p>判断两数有多少个元素相等</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>])</span><br><span class="line">a.eq(b)   # 输出 tensor([ <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">0</span>], dtype=torch.uint8)</span><br></pre></td></tr></table></figure></li><li><p>批矩阵相乘</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 批矩阵相乘 pytorch <span class="number">0.3</span><span class="number">.0</span></span><br><span class="line">output = torch.bmm(W, x)</span><br><span class="line"></span><br><span class="line"># 批矩阵相乘 pytorch <span class="number">0.4</span><span class="number">.0</span></span><br><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = torch.rand(<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">c = torch.matmul(a, b)</span><br><span class="line">print(c.shape)  # torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"># 在pytorch <span class="number">0.4</span><span class="number">.0</span>中使用torch.matmul 输入的参数是两个<span class="number">3</span>d的tensor ,tensor的首个维度是batch_size</span><br></pre></td></tr></table></figure></li><li><p>tensor 两个维度转置</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># tensor 两个维度转置</span><br><span class="line">x = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">x = x.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">print(x.shape)  # torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure></li><li><p>chunk  cat </p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.cat(inputs, <span class="attribute">dimension</span>=0) → Tensor</span><br><span class="line"><span class="comment"># cat 是将多个tensor按照指定的维度拼接起来</span></span><br><span class="line">torch.chunk(tensor, chunks, <span class="attribute">dim</span>=0)</span><br><span class="line"><span class="comment"># chunk是将某个tensor按照指定的维度进行拆分成指定的块数</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="初始化函数"><a href="#初始化函数" class="headerlink" title="初始化函数"></a>初始化函数</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">self.W = nn.Conv2d(in_channels=<span class="number">512</span>, out_channels=<span class="number">1024</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>), \</span><br><span class="line">                   padding=(<span class="number">1</span>,<span class="number">1</span>), stride=(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">nn.init.kaiming.normal(self.W.weight)</span><br><span class="line">nn.init.kaiming.uniform(self.W.weight)</span><br><span class="line">nn.init.constant(self.W.bias, <span class="number">0</span>)</span><br><span class="line"># 输出 tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>动态规划(dynamic programming)</title>
      <link href="2019/04/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-dynamic-programming/"/>
      <url>2019/04/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-dynamic-programming/</url>
      
        <content type="html"><![CDATA[<h3 id="从起始点，走到终点"><a href="#从起始点，走到终点" class="headerlink" title="从起始点，走到终点"></a>从起始点，走到终点</h3><p>（1）共有多少路径<br>（2）哪条路径最短<br>对于grid 走路，只有两种走走法，这类问题，需要：   </p><ul><li>分析最后终点的结果，是怎么得来的：是由左邻和上邻的结果，进行某种运算得来的   </li><li>先将第一行、第一列进行初始化（结合具体问题）   </li><li>分析，递推公式，并采用自底向上的方式，因此，需要先高度的想，最后终点的递推公式，在结合这个公式，分析，在初始化之后，接下来的点，如何根据初始化的值，以及递推公式来计算得到。   </li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>分治递归： 一步一步的化解为小问题，最终由小问题再反向计算各大问题。自上而下   </li><li>动态规划：也需要得到递推公式，但是需要先将小问题的值写出来（初始化阶段），再根据递推公式，写for循环   </li></ul><h3 id="对于求解任意两个节点之间的值，并求全局最优解的问题，还需要再总结"><a href="#对于求解任意两个节点之间的值，并求全局最优解的问题，还需要再总结" class="headerlink" title="对于求解任意两个节点之间的值，并求全局最优解的问题，还需要再总结"></a>对于求解任意两个节点之间的值，并求全局最优解的问题，还需要再总结</h3><ul><li><a href="https://leetcode.com/problems/maximum-subarray/" target="_blank" rel="noopener">https://leetcode.com/problems/maximum-subarray/</a>  </li><li><a href="https://leetcode.com/problems/maximum-product-subarray/" target="_blank" rel="noopener">https://leetcode.com/problems/maximum-product-subarray/</a>  </li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>排序(sort)</title>
      <link href="2019/04/18/%E6%8E%92%E5%BA%8F-sort/"/>
      <url>2019/04/18/%E6%8E%92%E5%BA%8F-sort/</url>
      
        <content type="html"><![CDATA[<ul><li>快速排序与冒泡排序均是进行交换操作，使用的空间复杂度为O(1)，而插入排序的空间复杂度为O(n)</li><li>快速排序的平均时间复杂度为O(nlogn)，最坏情况复杂度为O(n^2)</li><li>冒泡排序的时间复杂度为O(n^2)</li><li>插入排序的时间复杂度为O(n^2)</li></ul><h2 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h2><blockquote><p>待补充</p></blockquote><h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partition</span><span class="params">(nums, left, right)</span>:</span></span><br><span class="line">    tmp = left</span><br><span class="line">    reference = nums[left]  <span class="comment"># 以最左端的nums[left] 作为中位数</span></span><br><span class="line">    left = left</span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> nums[left] &lt;= reference:</span><br><span class="line">            left = left + <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> nums[right] &gt; reference:</span><br><span class="line">            right = right - <span class="number">1</span></span><br><span class="line">        nums[left], nums[right] = nums[right], nums[left]</span><br><span class="line">    <span class="keyword">if</span> nums[left] &lt; reference:</span><br><span class="line">        nums[left], nums[tmp] = nums[tmp], nums[left]</span><br><span class="line">        <span class="keyword">return</span> left</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        nums[left<span class="number">-1</span>], nums[tmp] = nums[tmp], nums[left<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">return</span> left<span class="number">-1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">QuickSort</span><span class="params">(nums , left, right)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> left &lt; right:</span><br><span class="line">        index = partition(nums, left, right)</span><br><span class="line">        QuickSort(nums, left, index<span class="number">-1</span>)</span><br><span class="line">        QuickSort(nums, index+<span class="number">1</span>, right)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">nums = [<span class="number">6</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>]</span><br><span class="line"><span class="comment"># nums = [1,2,3,4]</span></span><br><span class="line"><span class="comment"># nums = [3,2,5,6,4,4,4,5,6]</span></span><br><span class="line">left = <span class="number">0</span></span><br><span class="line">right = len(nums)<span class="number">-1</span></span><br><span class="line">QuickSort(nums, left, right)</span><br><span class="line">print(nums)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法</title>
      <link href="2019/04/18/%E7%AE%97%E6%B3%95/"/>
      <url>2019/04/18/%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<ul><li><p><a href="https://blog.csdn.net/v_JULY_v/article/details/19131887" target="_blank" rel="noopener">别人的博客</a></p></li><li><p><a href="https://shiyaya.github.io/2019/04/18/%E5%8F%8C%E6%8C%87%E9%92%88-two-pointers/" target="_blank" rel="noopener">双指针</a></p></li><li><p><a href="https://shiyaya.github.io/2019/04/18/%E6%8E%92%E5%BA%8F-sort/" target="_blank" rel="noopener">排序</a></p></li><li><p><a href="https://shiyaya.github.io/2019/04/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-dynamic-programming/" target="_blank" rel="noopener">动态规划</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python 函数</title>
      <link href="2019/04/10/python-%E5%87%BD%E6%95%B0/"/>
      <url>2019/04/10/python-%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>numpy广播</title>
      <link href="2019/04/09/numpy%E5%B9%BF%E6%92%AD/"/>
      <url>2019/04/09/numpy%E5%B9%BF%E6%92%AD/</url>
      
        <content type="html"><![CDATA[<ul><li>末尾有彩蛋</li></ul><h1 id="NumPy-广播-Broadcast"><a href="#NumPy-广播-Broadcast" class="headerlink" title="NumPy 广播(Broadcast)"></a>NumPy 广播(Broadcast)</h1><p>广播(Broadcast)是 numpy 对不同形状(shape)的数组进行数值计算的方式， 对数组的算术运算通常在相应的元素上进行。</p><p>如果两个数组 a 和 b 形状相同，即满足 <strong>a.shape == b.shape</strong>，那么 a*b 的结果就是 a 与 b 数组对应位相乘。这要求维数相同，且各维度的长度相同。</p><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>import numpy as np    a = np.array([1,2,3,4])  b = np.array([10,20,30,40])  c = a * b  print (c)</p><p>输出结果为：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ <span class="number">10</span>  <span class="number">40</span>  <span class="number">90</span> <span class="number">160</span>]</span><br></pre></td></tr></table></figure></p><p>当运算中的 2 个数组的形状不同时，numpy 将自动触发广播机制。如：</p><h2 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np    </span><br><span class="line">a = np.array([[ <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],            [<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>],            [<span class="number">20</span>,<span class="number">20</span>,<span class="number">20</span>],            [<span class="number">30</span>,<span class="number">30</span>,<span class="number">30</span>]]) </span><br><span class="line">b = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) print(a + b)</span><br></pre></td></tr></table></figure><p>输出结果为：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[ <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line"> [<span class="number">11</span> <span class="number">12</span> <span class="number">13</span>]</span><br><span class="line"> [<span class="number">21</span> <span class="number">22</span> <span class="number">23</span>]</span><br><span class="line"> [<span class="number">31</span> <span class="number">32</span> <span class="number">33</span>]]</span><br></pre></td></tr></table></figure></p><p>下面的图片展示了数组 b 如何通过广播来与数组 a 兼容。</p><p><img src="http://www.runoob.com/wp-content/uploads/2018/10/image0020619.gif" alt="img"></p><p>4x3 的二维数组与长为 3 的一维数组相加，等效于把数组 b 在二维上重复 4 次再运算。</p><h2 id="yayay实例"><a href="#yayay实例" class="headerlink" title="yayay实例"></a>yayay实例</h2><p>在few-shot gnn任务中，想要计算邻接矩阵A，其公式为：<strong>a<sub>ij</sub> = fc(v<sub>i</sub>-v<sub>j</sub>)</strong><br>那么问题来了得到的邻接矩阵是N×N的，则计算的差值矩阵也应该是N×N的。那么该如何高效的计算出来这个差值矩阵。<br>```python<br>import numpy as np<br>N = 10<br>D = 7<br>X = np.ones((N,D))<br>X1 = np.expand_dims(X, axis=0)<br>X2 = np.expand_dims(X, axis=1)<br>X_abs = np.abs(X1-X2)<br>X_abs = np.reshape(X_abs, (N,N,D))<br>X_T = X_abs </p><p>?????????????????需要考虑一下这个转置问题</p>]]></content>
      
      
      <categories>
          
          <category> numpy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>少样本学习(few-shot learning)</title>
      <link href="2019/04/09/%E5%B0%91%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0-few-shot-learning/"/>
      <url>2019/04/09/%E5%B0%91%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0-few-shot-learning/</url>
      
        <content type="html"><![CDATA[<h2 id="小样本学习的概念"><a href="#小样本学习的概念" class="headerlink" title="小样本学习的概念"></a>小样本学习的概念</h2><ul><li>少样本学习(few-shot learning)的目标是在<strong>已知类别(Seen Class)训练一个分类模型，使它能够在只有少量数据的未知类别(Unseen Class)上面具有很好的泛化性能</strong>。</li><li>少样本学习面临两个重要的问题：<br>（1）已知类别和未知类别之间没有交集，导致它们的数据分布差别很大，不能直接通过训练分类器和微调(finetune)的方式得到很好的性能；<br>（2）未知类别只有极少量数据(每个类别1或者5个训练样本)，导致分类器学习不可靠。</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>指数加权平均</title>
      <link href="2019/04/09/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87/"/>
      <url>2019/04/09/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87/</url>
      
        <content type="html"><![CDATA[<p>参考：<a href="https://zhuanlan.zhihu.com/p/29895933" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29895933</a><br><a href="https://www.jianshu.com/p/41218cb5e099" target="_blank" rel="noopener">https://www.jianshu.com/p/41218cb5e099</a></p><h2 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h2><p><strong>v<sub>t</sub></strong> 是要代替  θ_t  的估计值，代表第t天的指数平均温度值<br><strong>θ<sub>t</sub></strong> 代表第t天的实际温度值<br><strong>β</strong> 代表可调节的超参数值  </p><p>则第t天的指数平均温度，可用如下公式表示<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1wnugwmkrj30ne0ggjsp.jpg" width="50%" height="50%">    </p><p>将<strong>v<sub>100</sub></strong> 展开可得:<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1wonisescj30j201o0sr.jpg"><br>v<sub>t</sub> 是对每天温度的加权平均，之所以称之为指数加权，是因为加权系数是随着时间以指数形式递减的，<strong>时间越靠近，权重越大</strong>，越靠前，权重越小。</p><p><img src="https://upload-images.jianshu.io/upload_images/1667471-485da343fbd96353.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/665/format/webp" alt><br>再来看下面三种情况：</p><p>当 β = 0.9 时，指数加权平均最后的结果如图<strong>红色线</strong>所示，代表的是最近 10 天的平均温度值；<br>当 β = 0.98 时，指结果如图<strong>绿色线</strong>所示，代表的是最近 50 天的平均温度值；<br>当 β = 0.5 时，结果如下图<strong>黄色线</strong>所示，代表的是最近 2 天的平均温度值；</p><p><img src="//upload-images.jianshu.io/upload_images/1667471-7d82e7b89e860299.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/473/format/webp" alt></p><p><img src="//upload-images.jianshu.io/upload_images/1667471-6fd989467bcb6121.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/475/format/webp" alt></p><p><strong>β 越小，噪音越多</strong>，虽然能够很快的适应温度的变化，但是更容易出现奇异值。</p><p><strong>β 越大，得到的曲线越平坦</strong>，因为多平均了几天的温度，这个曲线的波动更小。<br>但有个缺点是，因为只有 0.02 的权重给了当天的值，而之前的数值权重占了 0.98 ，<br>曲线进一步右移，在温度变化时就会适应地更缓慢一些，会出现一定延迟。</p><p>通过上面的内容可知，β 也是一个很重要的超参数，不同的值有不同的效果，需要调节来达到最佳效果，<strong>一般 0.9 的效果就很好</strong>。</p><p>作者：不会停的蜗牛<br>链接：<a href="https://www.jianshu.com/p/41218cb5e099" target="_blank" rel="noopener">https://www.jianshu.com/p/41218cb5e099</a><br>来源：简书  </p><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><p>我们可以看到指数加权平均的求解过程实际上是一个递推的过程，那么这样就会有一个非常大的好处，每当我要求从0到某一时刻（n）的平均值的时候，我并不需要像普通求解平均值的作为，保留所有的时刻值，类和然后除以n。</p><p>而是只需要保留0-(n-1)时刻的平均值和n时刻的温度值即可。也就是每次只需要保留常数值，然后进行运算即可，这对于深度学习中的海量数据来说，是一个很好的减少内存和空间的做法。</p><h2 id="为什么在优化算法中使用指数加权平均"><a href="#为什么在优化算法中使用指数加权平均" class="headerlink" title="为什么在优化算法中使用指数加权平均"></a>为什么在优化算法中使用指数加权平均</h2><p>上面提到了一些 指数加权平均 的应用，这里我们着重看一下在优化算法中的作用。</p><p>以 Momentum 梯度下降法为例，</p><p><strong>Momentum 梯度下降法</strong>，就是计算了梯度的指数加权平均数，并以此来更新权重，它的运行<strong>速度几乎总是快于标准的梯度下降算法</strong>。</p><p><strong>这是为什么呢？</strong></p><p>让我们来看一下这个图，</p><p><img src="//upload-images.jianshu.io/upload_images/1667471-07d825d3e2624537.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/745/format/webp" alt></p><p>例如这就是我们要优化的成本函数的形状，图中红点就代表我们要达到的最小值的位置，<br>假设我们<strong>从左下角这里出发开始用梯度下降法</strong>，那么蓝色曲线就是一步一步迭代，一步一步向最小值靠近的轨迹。</p><p>可以看出<strong>这种上下波动，减慢了梯度下降法的速度</strong>，而且无法使用更大的学习率，因为如果用较大的学习率，可能会偏离函数的范围。</p><p>如果有一种方法，可以使得在纵轴上，学习得慢一点，减少这些摆动，但是在横轴上，学习得快一些，快速地从左向右移移向红点最小值，那么训练的速度就可以加快很多。</p><p>这个方法就是动量 Momentum 梯度下降法，它<strong>在每次计算梯度的迭代中，对 dw 和 db 使用了指数加权平均法的思想</strong>，</p><p><img src="//upload-images.jianshu.io/upload_images/1667471-eedf9342a4bce813.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/272/format/webp" alt></p><p>这样我们就可以得到如图红色线的轨迹：</p><p><img src="//upload-images.jianshu.io/upload_images/1667471-f9e70b57daae0359.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/718/format/webp" alt></p><p>可以看到：<br><strong>纵轴方向</strong>，平均过程中正负摆动相互抵消，平均值接近于零，摆动变小，学习放慢。<br><strong>横轴方向</strong>，因为所有的微分都指向横轴方向，因此平均值仍然较大，向最小值运动更快了。<br>在抵达最小值的路上减少了摆动，加快了训练速度。</p><p>作者：不会停的蜗牛<br>链接：<a href="https://www.jianshu.com/p/41218cb5e099" target="_blank" rel="noopener">https://www.jianshu.com/p/41218cb5e099</a><br>来源：简书  </p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>模型剪枝 Model Pruning</title>
      <link href="2019/04/09/%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D-Model-Pruning/"/>
      <url>2019/04/09/%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D-Model-Pruning/</url>
      
        <content type="html"><![CDATA[<p>参考：<a href="https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/" target="_blank" rel="noopener">https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/</a><br>L1正则化项，加入损失函数中，可以对特征进行选择。<br>L1也可以应用到模型压缩任务中，选择某个filter是否有存在的必要，从而决定是否要剪掉。</p>]]></content>
      
      
      <categories>
          
          <category> 杂类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>视频分类总结</title>
      <link href="2019/04/09/%E8%A7%86%E9%A2%91%E5%88%86%E7%B1%BB%E6%80%BB%E7%BB%93/"/>
      <url>2019/04/09/%E8%A7%86%E9%A2%91%E5%88%86%E7%B1%BB%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h3 id="Two-Stream"><a href="#Two-Stream" class="headerlink" title="Two-Stream"></a>Two-Stream</h3><ul><li>训练<br>（1）spatial net：每个video中随机提取1帧：224<em>224</em>3，进行训练，<br>（2）temporal net：每个video中选取L=10帧光流，224<em>224</em>2L，进行训练<br>由于含有x,y 方向，因此10帧光流，对应的5帧图像，该5帧是连续的</li><li>测试：每个video中平均采25帧，并通过crop and flip等操作扩增10倍，整个video的得分，是这250帧的平均得分。</li></ul><h3 id="TSN"><a href="#TSN" class="headerlink" title="TSN"></a>TSN</h3><ul><li>训练，将video分段，默认为3段<br>（1）spatial net：每个video分成N段，每段随机提取1帧，则得到N帧，这N帧共享参数，一起训练，N帧分别得到的score进行平均，作为video的分数，并反向传播，训练。<br>（2）temporal net ,同理，每个video分成N段，每段随机提取L=10帧，这N段共享参数，一起训练，N段分别得到的score进行平均，作为video的分数，并反向传播，训练。</li><li>测试，将video分段，默认为25段</li><li>同训练过程</li></ul><h3 id="C3D"><a href="#C3D" class="headerlink" title="C3D"></a>C3D</h3><ul><li>在Sports-1M上进行训练，训练完成之后便得到一个video feature extractor </li><li>训练：在每个video中随机剪切5个2s长的clip，对clip进行训练 </li><li>测试：对于一个video，含有N帧，则将这N帧分成16帧的clips，每相邻的两个clips重叠8帧，然后将这些clips得到的fc6 activations 进行<strong>平均得到video feature</strong>，进而送入分类层得到video class label</li></ul><h3 id="I3D"><a href="#I3D" class="headerlink" title="I3D"></a>I3D</h3><ul><li>video 以25帧/秒的帧率来提取关键帧</li><li>训练：以64帧组成的snippets进行训练。</li><li>测试：同样以64帧组成的snippets进行测试，但是对于该video上的所有的snippet的<strong>预测结果进行取平均</strong>作为该video的预测结果</li></ul>]]></content>
      
      
      <categories>
          
          <category> 行为识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 行为识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>I3D</title>
      <link href="2019/04/09/I3D/"/>
      <url>2019/04/09/I3D/</url>
      
        <content type="html"><![CDATA[<ul><li>参考:<a href="https://zhuanlan.zhihu.com/p/34919655" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34919655</a></li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1wgkib11aj310b0ew77m.jpg">  </p><h3 id="训练和预测"><a href="#训练和预测" class="headerlink" title="训练和预测"></a>训练和预测</h3><ul><li>写在前面，方便看，哈哈哈哈</li><li>video 以25帧/秒的帧率来提取关键帧</li><li>在训练时，以64帧组成的snippets进行训练。</li><li>在测试时，同样以64帧组成的snippets进行测试，但是对于该video上的所有的snippet的预测结果进行取平均作为该video的预测结果</li></ul><h3 id="方法1：ConvNet-LSTM"><a href="#方法1：ConvNet-LSTM" class="headerlink" title="方法1：ConvNet+LSTM"></a><strong>方法1：ConvNet+LSTM</strong></h3><ul><li>该方案主要考虑到cnn在图像分类领域的成功以及LSTM对于序列建模的能力，很自然提出将两者结合起来的方案。如果只是用cnn的话，需要对每一帧都提取特征，然后将视频的所有帧特征进行总汇，以此来表达对视频的表示，显然这样做忽略的时间结构特征。而LSTM可以处理长时间依赖的问题，可以对视频从第一帧开始建模直到最后一帧，使用cnn网络对每帧提取特征，然后将特征送入LSTM来捕捉时间特征，最后一帧的输出用来对视频特征表示。<br>  LSTM往往依赖cnn的最后一层特征最为输入，因此不能够捕捉到<code>low-level motion</code>的特征，而且对于遍历整个视频也很难去训练。<h3 id="方法2：3D-ConvNets"><a href="#方法2：3D-ConvNets" class="headerlink" title="方法2：3D ConvNets"></a><strong>方法2：3D ConvNets</strong></h3></li><li><code>3D ConvNets</code>是对视频建模最自然的方式，和标准cnn区别在于由<code>2d conv</code>变为<code>3d conv</code>，来捕捉<code>spatio-temporal feature</code>。想法很好，但目前遇到一些问题，问题一：<code>3D ConvNets</code>的<code>3d conv</code>多了一个维度，参数量有较大增加，这将会很难去训练。问题二：没有利用那些Imagenet上成功的预训练模型来迁移学习，往往使用层数少的cnn在小数据集上从头训练。简要说就是要利用已有预训练模型，要减少参数或增大数据集。<br>  论文中实现了C3D（与原版略有差异）有8 conv layer、5 pooling layer 和 2 fc layer，并在所有圈卷积层和fc层加bn。输入是16×112×112（通过crop方法），将第一个pooling layer对时间的stride由1变成2，为了可以减少memory和允许更大batch。<h3 id="方法3：Two-Stream-Networks"><a href="#方法3：Two-Stream-Networks" class="headerlink" title="方法3：Two-Stream Networks"></a><strong>方法3：Two-Stream Networks</strong></h3></li><li>该方案利用短的视频段来建模，用每个clip的预测分数平均的方式（其实C3D也是类似），但不同的是输入，包括一张RGB和10张<code>optical flow</code>(其实是5张，x/y两个方向，运动特征)。模型能使用<code>two-branch</code>方式，利用预训练的imagenet模型，最后将预测结果平均下（最原始的，或者在最后softmax做融合），这样建模的模型比较好训练，同时也能获得更高的分数。  </li><li>模型的两个输入流也可以在后面的cnn层来进行融合，以提升相同，同时可以<code>end-to-end</code>训练。论文实现一个类似的two-stream方案，在最后一层用<code>3d conv</code>将spatial和flow特征进行融合。<h3 id="方法4：Two-Stream-Inflated-3D-ConvNets"><a href="#方法4：Two-Stream-Inflated-3D-ConvNets" class="headerlink" title="方法4：Two-Stream Inflated 3D ConvNets"></a><strong>方法4：Two-Stream Inflated 3D ConvNets</strong></h3></li><li>该方案是论文提出的，出发点是要利用imagenet的预训练模型，同时利用<code>3d conv</code>来提取<code>RGB stream</code>的<code>temporal feature</code>，最后再利用<code>optical-flow stream</code>提升网络性能，也就大融合的方案（把有效的技巧都用上）。  </li><li>通过对预训练的<code>2D conv</code>增加temporal维度，把N×N的filter变为N×N×N。简单的办法就是对N×N的filter重复复制N遍，并归一化，这样多的出发点是短期内时间不变性的假设，姑且把这当成<code>3D filter</code>初始化的一种策略吧。  </li><li>池化操作怎么膨胀？stride怎么选？主要依赖感受野尺寸，如果图像水平方向和竖直方向相等，那么stride也相等，而且越深的特征感受野越大。但是考虑到时间因素，对称感受野不是必须的，这主要还是依赖帧率和图片大小。时间相对于空间变化过快，将合并不同object的边信息，过慢将不能捕捉场景变化。  </li><li>虽然3D conv能够捕捉motion信息，但是与光流优化的方式和效果还是不一样，因此使用<code>two-stream</code>的方式构建，并分开训练两个网络。</li></ul><p><img src="https://pic2.zhimg.com/80/v2-34f1d3ac14884d5c9114d4e9383c2e89_hd.jpg" alt></p><hr><ul><li><p>数据集不同，评测结果也不同。flow在UCF-101上效果比HMDB-51、kinetics上好（有更多camera运动的原因）。<br><img src="https://pic2.zhimg.com/80/v2-e719a0a3a022e348838d4b6a5c0b8a55_hd.jpg" alt></p></li><li><p>在imagenet上训练后迁移到kinetics和直接在kinetics上的对比，迁移后的效果好，说明RGB流起的作用大。整体上I3D模型参数更少，更深，训练输入在时间和空间维度上都比C3D大。<br><img src="https://pic3.zhimg.com/80/v2-b358535638c000de801577fc84296252_hd.jpg" alt></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 行为识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 行为识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C3D</title>
      <link href="2019/04/09/C3D/"/>
      <url>2019/04/09/C3D/</url>
      
        <content type="html"><![CDATA[<h2 id="pytorch-中"><a href="#pytorch-中" class="headerlink" title="pytorch 中"></a>pytorch 中</h2><h3 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h3><p><a href="https://pytorch.org/docs/stable/nn.html#conv2d" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html#conv2d</a><br>the shape of input:  <font size="5," color="#0099ff">batch×channel×height×width</font></p><h2 id="Conv3d"><a href="#Conv3d" class="headerlink" title="Conv3d"></a>Conv3d</h2><p><a href="https://pytorch.org/docs/stable/nn.html#conv3d" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html#conv3d</a><br>the shape of input: <font size="5," color="#0099ff">batch×channel×depth×height×width</font></p><h2 id="C3D-用于行为识别-1-2"><a href="#C3D-用于行为识别-1-2" class="headerlink" title="C3D 用于行为识别[1][2]"></a>C3D 用于行为识别[1][2]</h2><p>[1] C3D 的网络结构</p><ul><li>输入： bs×3×<font size="5," color="#0099ff">16</font>×H×W，即输入一个长为16的视频序列clip， 实际是 bs×3×16×12×12</li><li>输出： bs×feature_size</li></ul><p>对于一个video，含有N帧，则将这N帧分成16帧的clips，每相邻的两个clips重叠8帧，然后将这些clips得到的fc6 activations 进行平均得到video feature，进而送入分类层得到video class label</p><h2 id="解读论文Learning-Spatio-Temporal-Features-with-3D-Residual-Networks-for-Action-Recognition"><a href="#解读论文Learning-Spatio-Temporal-Features-with-3D-Residual-Networks-for-Action-Recognition" class="headerlink" title="解读论文Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition"></a>解读论文<code>Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition</code></h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>当前行为识别的方法，参数量不能过多的原因是，现在可以用于训练的数据集较小，一旦参数量过大，使得模型过拟合</li><li>但是，现在有了kinetics 这样的大型数据集，这时，便可以提出一个参数量大的model</li></ul><h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><ul><li>生成一个用于 spatio-temporal recognition的标准预训练模型</li></ul><h3 id="关于image-size-部分的介绍"><a href="#关于image-size-部分的介绍" class="headerlink" title="关于image size 部分的介绍"></a>关于image size 部分的介绍</h3><ul><li>截取帧时，固定长宽比，并设置高为360（resneXt是240），</li></ul><h4 id="train"><a href="#train" class="headerlink" title="train"></a>train</h4><ul><li>以一个 16 连续帧组成的clip来代表 video</li><li>先以最小边为尺寸进行裁剪（裁剪为一个正方形），然后再进行resize 112</li><li>为了数据增强，这里有三个trick  (1) 最小边=min(height, weight) * sacle (2) 选取哪个区域进行裁剪，有五种选择 四个角和中心 (3)以50%的概率进行随机水平翻转</li></ul><h4 id="test"><a href="#test" class="headerlink" title="test"></a>test</h4><ul><li>一个video 去选择 没有重叠的所有 16 连续帧，然后对这所有的clip的得分进行取平均作为video的得分</li><li>不再进行数据增强</li><li>先按照最小边，固定长宽比进行缩放</li><li>再 中心裁剪成指定的 <code>112*112</code></li></ul><h3 id="分析-resnet34-c3d-比-rgb-i3d-差的原因"><a href="#分析-resnet34-c3d-比-rgb-i3d-差的原因" class="headerlink" title="分析 resnet34-c3d 比 rgb-i3d 差的原因"></a>分析 resnet34-c3d 比 rgb-i3d 差的原因</h3><ol><li><p>rgb-i3d 使用了64个GPU，可能他们使用的batch size 也比较大。而 resnet34-c3d 仅使用了4个GPU，256 batch size。</p></li><li><p>rgb-i3d 使用的 clip 分辨率为：3 × 64 × 224 × 224.  而 resnet34-c3d 的 clip 分辨率为 3 × 16 × 112 × 112 。    </p><p>即，i3d 使用连续的 <strong>64</strong> 帧组成一个clip，并且 image 分辨率为 <strong>224</strong>，</p><p>而， resnet c3d 使用连续的 <strong>16</strong> 帧组成一个clip，并且 image 分辨率为 <strong>112</strong>，</p></li></ol><h2 id="解读论文Can-Spatiotemporal-3D-CNNs-Retrace-the-History-of-2D-CNNs-and-ImageNet"><a href="#解读论文Can-Spatiotemporal-3D-CNNs-Retrace-the-History-of-2D-CNNs-and-ImageNet" class="headerlink" title="解读论文Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?"></a>解读论文<code>Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?</code></h2><h3 id="Goal-1"><a href="#Goal-1" class="headerlink" title="Goal"></a>Goal</h3><ul><li>本文的目的是为了验证当前存在的数据集是否足够支撑训练一个很深的3d网络</li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition .  ICCV 2017<br>[2] Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?  CVPR 2018</p>]]></content>
      
      
      <categories>
          
          <category> 行为识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 行为识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>视频解码</title>
      <link href="2019/04/09/%E8%A7%86%E9%A2%91%E8%A7%A3%E7%A0%81/"/>
      <url>2019/04/09/%E8%A7%86%E9%A2%91%E8%A7%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<ul><li>opencv提取关键帧（banet）</li><li><p>ffmpeg提取关键帧 （video to text）</p></li><li><p>i帧、P帧、B帧</p></li><li>参考：<a href="https://blog.csdn.net/huangblog/article/details/8739876" target="_blank" rel="noopener">https://blog.csdn.net/huangblog/article/details/8739876</a><br>I帧，Intra-coded frame：是一张完整的图片<br>P帧，predictive frame: 记录了与之前真的差别，在解码P帧之前需要参考之前的图片帧<br>B帧，Bi-Predictive frame: 不仅需要参考之前的图片帧，还需要参考之后的图片帧，才能完整解码。<br>因此解码P帧、B帧的速度相对较慢，直接解码I帧可以获得更快的速度。<br><strong>简单地讲，I帧是一个完整的画面，而P帧和B帧记录的是相对于I帧的变化。没有I帧，P帧和B帧就无法解码</strong>  </li></ul><h3 id="GOP"><a href="#GOP" class="headerlink" title="GOP"></a>GOP</h3><p>所谓GOP，意思是画面组，一个GOP就是一组连续的画面。从一个I帧到下一个I帧之间的所有帧的组合称为一个GOP。</p><ul><li><strong>I帧</strong><br>I帧是参考帧，一个GOP中必须含有I帧，它是一个全帧压缩编码帧。它将全帧图像信息进行JPEG压缩编码及传输。</li><li><strong>P帧的预测与重构:</strong><br>P帧是以I帧为参考帧,在I帧中找出P帧“某点”的预测值和运动矢量,取预测差值和运动矢量一起传送。在接收端根据运动矢量从I帧中找出P帧“某点”的预测值并与差值相加以得到P帧“某点”样值,从而可得到完整的P帧。</li><li><p><strong>B帧的预测与重构：</strong><br>B帧以前面的I或P帧和后面的P帧为参考帧,“找出”B帧“某点”的预测值和两个运动矢量,并取预测差值和运动矢量传送。接收端根据运动矢量在两个参考帧中“找出(算出)”预测值并与差值求和,得到B帧“某点”样值,从而可得到完整的B帧。</p></li><li><p><strong>用下图中的1234567帧来表达</strong><br>首先由1：I帧，<br>再由1、4帧得到第4帧所在位置处的图像信息<br>最后由1、4、2得到第2帧所在位置处的图像信息，由1、4、3得到第3帧所在位置处的图像信息</p></li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1w7ki6x02j30ra0ge0st.jpg">  </p><ul><li>另外一张图  </li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1w7ki8pfvj30nm0fk77r.jpg"></p><hr><h3 id="视频压缩：I帧、P帧、B帧"><a href="#视频压缩：I帧、P帧、B帧" class="headerlink" title="视频压缩：I帧、P帧、B帧"></a>视频压缩：I帧、P帧、B帧</h3><ul><li>来源： <a href="https://blog.csdn.net/huangblog/article/details/8739876" target="_blank" rel="noopener">https://blog.csdn.net/huangblog/article/details/8739876</a></li></ul><p>视频压缩中，每帧代表一幅静止的图像。而在实际压缩时，会采取各种算法减少数据的容量，其中IPB就是最常见的。</p><pre><code>简单地说，I帧是关键帧，属于帧内压缩。就是和AVI的压缩是一样的。P是向前搜索的意思。B是双向搜索。他们都是基于I帧来压缩数据。</code></pre><p>   I帧表示关键帧，你可以理解为这一帧画面的完整保留；解码时只需要本帧数据就可以完成（因为包含完整画面）</p><p>   P帧表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。（也就是差别帧，P帧没有完整画面数据，只有与前一帧的画面差别的数据）</p><p>   B帧是双向差别帧，也就是B帧记录的是本帧与前后帧的差别（具体比较复杂，有4种情况），换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的与本帧数据的叠加取得最终的画面。B帧压缩率高，但是解码时CPU会比较累~。</p><pre><code>采用的压缩方法: 分组:把几帧图像分为一组(GOP),为防止运动变化,帧数不宜取多。    1.定义帧:将每组内各帧图像定义为三种类型,即I帧、B帧和P帧;    2.预测帧:以I帧做为基础帧,以I帧预测P帧,再由I帧和P帧预测B帧;    3.数据传输:最后将I帧数据与预测的差值信息进行存储和传输。</code></pre><p>一、I帧  </p><pre><code>I图像（帧）是靠尽可能去除图像空间冗余信息来压缩传输数据量的帧内编码图像。I帧又称为内部画面 (intra picture)，I 帧通常是每个 GOP（MPEG 所使用的一种视频压缩技术）的第一个帧，经过适度地压缩（做为随机访问的参考点）可以当成图象。在MPEG编码的过程中部分视频帧序列压缩成为I帧，部分压缩成P帧，还有部分压缩成B帧。I帧法是帧内压缩法（P、B为帧间），也称为“关键帧”压缩法。I帧法是基于离散余弦变换DCT（Discrete Cosine Transform）的压缩技术，这种算法与JPEG压缩算法类似。采用I帧压缩可达到1/6的压缩比而无明显的压缩痕迹。I帧特点：    1.它是一个全帧压缩编码帧。它将全帧图像信息进行JPEG压缩编码及传输;    2.解码时仅用I帧的数据就可重构完整图像;    3.I帧描述了图像背景和运动主体的详情;    4.I帧不需要参考其他画面而生成;    5.I帧是P帧和B帧的参考帧(其质量直接影响到同组中以后各帧的质量);    6.I帧是帧组GOP的基础帧(第一帧),在一组中只有一个I帧;    7.I帧不需要考虑运动矢量;    8.I帧所占数据的信息量比较大。I帧编码流程：    (1)进行帧内预测，决定所采用的帧内预测模式。    (2)像素值减去预测值，得到残差。    (3)对残差进行变换和量化。    (4)变长编码和算术编码。    (5)重构图像并滤波，得到的图像作为其它帧的参考帧。</code></pre><p>二、P帧</p><pre><code> P图像（帧）是通过充分降低于图像序列中前面已编码帧的时间冗余信息来压缩传输数据量的编码图像，也叫预测帧。在针对连续动态图像编码时，将连续若干幅图像分成P,B,I三种类型，P帧由在它前面的P帧或者I帧预测而来，它比较与它前面的P帧或者I帧之间的相同信息或数据，也即考虑运动的特性进行帧间压缩。P帧法是根据本帧与相邻的前一帧（I帧或P帧）的不同点来压缩本帧数据。采取P帧和I帧联合压缩的方法可达到更高的压缩且无明显的压缩痕迹。P帧的预测与重构:    P帧是以I帧为参考帧,在I帧中找出P帧“某点”的预测值和运动矢量,取预测差值和运动矢量一起传送。在接收端根据运动矢量从I帧中找出P帧“某点”的预测值并与差值相加以得到P帧“某点”样值,从而可得到完整的P帧。P帧特点：    ①P帧是I帧后面相隔1-2帧的编码帧。      ②P帧采用运动补偿的方法传送它与前面的I或P帧的差值及运动矢量（预测误差）。      ③解码时必须将I帧中的预测值与预测误差求和后才能重构完整的P帧图像。      ④P帧属于前向预测的帧间编码。它只参考前面最靠近它的I帧或P帧。      ⑤P帧可以是其后面P帧的参考帧，也可以是其前后的B帧的参考帧。    ⑥由于P帧是参考帧，它可能造成解码错误的扩散。     ⑦由于是差值传送，P帧的压缩比较高。</code></pre><p>三、B帧</p><pre><code>B图像（帧）是既考虑与源图像序列前面已编码帧，也顾及源图像序列后面已编码帧之间的时间冗余信息来压缩传输数据量的编码图像，也叫双向预测帧。   B帧法是双向预测的帧间压缩算法。当把一帧压缩成B帧时，它根据相邻的前一帧、本帧以及后一帧数据的不同点来压缩本帧，也即仅记录本帧与前后帧的差值。只有采用B帧压缩才能达到200：1的高压缩。一般地，I帧压缩效率最低，P帧较高，B帧最高。B帧的预测与重构：    B帧以前面的I或P帧和后面的P帧为参考帧,“找出”B帧“某点”的预测值和两个运动矢量,并取预测差值和运动矢量传送。接收端根据运动矢量在两个参考帧中“找出(算出)”预测值并与差值求和,得到B帧“某点”样值,从而可得到完整的B帧。B帧特点：    1.B帧是由前面的I或P帧和后面的P帧来进行预测的;    2.B帧传送的是它与前面的I或P帧和后面的P帧之间的预测误差及运动矢量;    3.B帧是双向预测编码帧;    4.B帧压缩比最高,因为它只反映2参考帧间运动主体的变化情况,预测比较准确;    5.B帧不是参考帧,不会造成解码错误的扩散。 P 帧和 B 帧编码的基本流程为：    (1)进行运动估计，计算采用帧间编码模式的率失真函数(节)值。P 帧 只参考前面的帧，B 帧可参考后面的帧。    (2)进行帧内预测，选取率失真函数值最小的帧内模式与帧间模式比较，确定采用哪种编码模式。    (3)计算实际值和预测值的差值。    (4)对残差进行变换和量化。    (5)若编码，如果是帧间编码模式，编码运动矢量。注:I、B、P各帧是根据压缩算法的需要,是人为定义的,它们都是实实在在的物理帧,至于图像中的哪一帧是I帧,是随机的,一但确定了I帧,以后的各帧就严格按规定顺序排列。 </code></pre><p>四、实际应用</p><pre><code>从上面的解释看，我们知道I和P的解码算法比较简单，资源占用也比较少，I只要自己完成就行了，P呢，也只需要解码器把前一个画面缓存一下，遇到P时就使用之前缓存的画面就好了，如果视频流只有I和P，解码器可以不管后面的数据，边读边解码，线性前进，大家很舒服。但网络上的电影很多都采用了B帧，因为B帧记录的是前后帧的差别，比P帧能节约更多的空间，但这样一来，文件小了，解码器就麻烦了，因为在解码时，不仅要用之前缓存的画面，还要知道下一个I或者P的画面（也就是说要预读预解码），而且，B帧不能简单地丢掉，因为B帧其实也包含了画面信息，如果简单丢掉，并用之前的画面简单重复，就会造成画面卡（其实就是丢帧了），并且由于网络上的电影为了节约空间，往往使用相当多的B帧，B帧用的多，对不支持B帧的播放器就造成更大的困扰，画面也就越卡。一般平均来说，I的压缩率是7（跟JPG差不多），P是20，B可以达到50，可见使用B帧能节省大量空间，节省出来的空间可以用来保存多一些I帧，这样在相同码率下，可以提供更好的画质。</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>python类的继承</title>
      <link href="2019/04/08/python%E7%B1%BB%E7%9A%84%E7%BB%A7%E6%89%BF/"/>
      <url>2019/04/08/python%E7%B1%BB%E7%9A%84%E7%BB%A7%E6%89%BF/</url>
      
        <content type="html"><![CDATA[<ul><li>参考：<a href="https://www.cnblogs.com/bigberg/p/7182741.html" target="_blank" rel="noopener">https://www.cnblogs.com/bigberg/p/7182741.html</a>  </li></ul><h2 id="类的继承"><a href="#类的继承" class="headerlink" title="类的继承"></a>类的继承</h2><h3 id="1-继承的定义"><a href="#1-继承的定义" class="headerlink" title="1. 继承的定义"></a>1. 继承的定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">(object)</span>:</span>   <span class="comment"># 定义一个父类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">talk</span><span class="params">(self)</span>:</span>    <span class="comment"># 父类中的方法</span></span><br><span class="line">        print(<span class="string">"person is talking...."</span>)  </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Chinese</span><span class="params">(Person)</span>:</span>    <span class="comment"># 定义一个子类， 继承Person类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">walk</span><span class="params">(self)</span>:</span>      <span class="comment"># 在子类中定义其自身的方法</span></span><br><span class="line">        print(<span class="string">'is walking...'</span>)</span><br><span class="line"> </span><br><span class="line">c = Chinese()</span><br><span class="line">c.talk()      <span class="comment"># 调用继承的Person类的方法</span></span><br><span class="line">c.walk()     <span class="comment"># 调用本身的方法</span></span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>person is talking….<br>is walking…</p><h2 id="2-构造函数的继承"><a href="#2-构造函数的继承" class="headerlink" title="2. 构造函数的继承"></a>2. 构造函数的继承</h2><p>如果我们要给实例 c 传参，我们就要使用到构造函数，那么构造函数该如何继承，同时子类中又如何定义自己的属性？</p></blockquote><p>继承类的构造方法：<br>1.经典类的写法： 父类名称.<strong>init</strong>(self,参数1，参数2，…)  </p><ol><li><font color="#0059ff" size="5" face="黑体"> 新式类的写法：super(子类，self).__init__(参数1，参数2，....)</font></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">(object)</span>:</span>  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span>  </span><br><span class="line">        self.name = name  </span><br><span class="line">        self.age = age  </span><br><span class="line">        self.weight = <span class="string">'weight'</span>  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">talk</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        print(<span class="string">"person is talking...."</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Chinese</span><span class="params">(Person)</span>:</span>  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age, language)</span>:</span>  <span class="comment"># 先继承，在重构  </span></span><br><span class="line">Person.__init__(self, name, age)  </span><br><span class="line"><span class="comment"># 继承父类的构造方法，也可以写成：</span></span><br><span class="line"><span class="comment"># super(Chinese,self).__init__(name,age)  </span></span><br><span class="line">        self.language = language  <span class="comment"># 定义类的本身属性  </span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">walk</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        print(<span class="string">'is walking...'</span>)  </span><br><span class="line">        </span><br><span class="line">c = Chinese(<span class="string">'bigberg'</span>, <span class="number">22</span>, <span class="string">'Chinese'</span>)  </span><br><span class="line">print(c.name)  </span><br><span class="line">print(c.language)</span><br><span class="line">c.talk()</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>bigberg<br>Chinese<br>person is talking….</p></blockquote><h3 id="3-子类对父类方法的重写"><a href="#3-子类对父类方法的重写" class="headerlink" title="3.子类对父类方法的重写"></a>3.子类对父类方法的重写</h3><p>如果我们对基类/父类的方法需要修改，可以在子类中重构该方法。如下的talk()方法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">(object)</span>:</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span>  </span><br><span class="line">        self.name = name  </span><br><span class="line">        self.age = age  </span><br><span class="line">        self.weight = <span class="string">'weight'</span>  </span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">talk</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        print(<span class="string">"person is talking...."</span>)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Chinese</span><span class="params">(Person)</span>:</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age, language)</span>:</span>  </span><br><span class="line">        Person.__init__(self, name, age)  </span><br><span class="line">        self.language = language  </span><br><span class="line">        print(self.name, self.age, self.weight, self.language)  </span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">talk</span><span class="params">(self)</span>:</span>  <span class="comment"># 子类 重构方法  </span></span><br><span class="line">  print(<span class="string">'%s is speaking chinese'</span> % self.name)  </span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">walk</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        print(<span class="string">'is walking...'</span>)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">c = Chinese(<span class="string">'bigberg'</span>, <span class="number">22</span>, <span class="string">'Chinese'</span>)  </span><br><span class="line">c.talk()</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>bigberg 22 weight Chinese<br>bigberg is speaking chinese</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谱聚类</title>
      <link href="2019/04/08/%E8%B0%B1%E8%81%9A%E7%B1%BB/"/>
      <url>2019/04/08/%E8%B0%B1%E8%81%9A%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<ul><li>先占位置</li><li><a href="https://www.cnblogs.com/pinard/p/6221564.html" target="_blank" rel="noopener">参考某博客</a>  </li><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" target="_blank" rel="noopener">sklearn.cluster.SpectralClustering</a></p><ul><li>References<br>Normalized cuts and image segmentation, 2000 Jianbo Shi, Jitendra Malik <a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324" target="_blank" rel="noopener">http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324</a><br>A Tutorial on Spectral Clustering, 2007 Ulrike von Luxburg <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323" target="_blank" rel="noopener">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323</a><br>Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi <a href="http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf" target="_blank" rel="noopener">http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 图卷积网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图卷积网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spectral Networks and Deep Locally Connected Networks on Graphs</title>
      <link href="2019/04/08/Spectral-Networks-and-Deep-Locally-Connected-Networks-on-Graphs/"/>
      <url>2019/04/08/Spectral-Networks-and-Deep-Locally-Connected-Networks-on-Graphs/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>(SCST)Self-critical Sequence Training for Image Captioning</title>
      <link href="2019/04/08/SCST-Self-critical-Sequence-Training-for-Image-Captioning/"/>
      <url>2019/04/08/SCST-Self-critical-Sequence-Training-for-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<p>强化学习中的策略梯度法可以针对那些不可微分的度量进行优化，<br>本文中，使用强化学习的方法来优化图像描述任务，将这个新的优化方法称为self-critical sequence training (SCST)。</p><p><strong>sequence models for image captioning的理想训练过程， 应该是避免 exposure bias 并且可以直接优化任务中的度量</strong></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="图像描述方面的现状"><a href="#图像描述方面的现状" class="headerlink" title="图像描述方面的现状"></a><strong>图像描述方面的现状</strong></h3><p>[1] <strong>show attend and tell</strong> 证明在caption任务中，使用attention机制是有益处的。<br>[2] <strong>Teacher-Forcing</strong> 用于文本的deep generative models 的训练方法一般是：给定上一步word的ground truth 来最大化该步生成word的最大似然，来反向传播。这个方法称为“Teacher-Forcing”  。但是这种方法导致在训练和测试时很不匹配。因为在测试时，该步生成的单词是在给定上一步预测出的单词的前提下。  </p><ul><li><p>这个exposure  bias [2]导致在测试时产生误差累积，因为模型从未暴露于其自己的预测中。<br>This exposure  bias [2], results in error accumulation during generation at  test time,<br>since the model has never been exposed to its own  predictions</p><p>一些克服exposure bias的方法[3] [4]<br>[3] <strong>Scheduled sampling</strong> 他们表明，反馈模型自己的预测，并在训练过程中缓慢地增加反馈概率p，可以显着地提高测试时间的性能。<br>[4] <strong>Professor forcing</strong> a  technique that uses adversarial training to encourage the dynamics of the recurrent network to be the same when training conditioned on ground truth previous words and when  sampling freely from the network</p></li></ul><h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><h3 id="实验结果证明"><a href="#实验结果证明" class="headerlink" title="实验结果证明"></a>实验结果证明</h3><p>we find that directly optimizing the CIDEr metric with  SCST and greedy decoding at test-time is highly effective.</p><h2 id="Image-Features"><a href="#Image-Features" class="headerlink" title="Image Features"></a>Image Features</h2><ul><li>FC Models<br>由CNN+FC得到image 的 特征向量，并送入LSTM中，来生成caption<br>但是需要注意的是仅在first step 输入该特征向量，其余步输入上一步生成的word (embedding)   </li><li>Spatial CNN features for Attention models<br>在不缩放也不裁剪图片的基础上，使用resnet-101来提取最后一个卷积层的特征，并应用apply spatially adaptive max-pooling来得到一个固定的尺寸14 × 14 × 2048。在每一个time step，attention model在这14 × 14=196个位置上计算一个<font color="#0099ff" size="6" face="黑体"> attention mask</font>（注意力系数/权重）。由这个mask 计算所有位置的加权求和，以此得到image feature。  </li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Show, attend and tell: Neural image caption generation with visual attention. In ICML,  2015.<br>[2] Sequence level training with recurrent neural networks. ICLR, 2015.<br>[3] Scheduled sampling for sequence prediction with recurrent neural networks. In NIPS, 2015.<br>[4] Professor forcing: A  new algorithm for training recurrent networks. Neural Information Processing Systems. (NIPS) 2016.  </p>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>self.training in pytorch</title>
      <link href="2019/04/07/self-training-in-pytorch/"/>
      <url>2019/04/07/self-training-in-pytorch/</url>
      
        <content type="html"><![CDATA[<ul><li>代码来源于：<a href="https://zhuanlan.zhihu.com/p/26893755" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26893755</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable, Function</span><br><span class="line"></span><br><span class="line">x_train = np.array([[<span class="number">3.3</span>], [<span class="number">4.4</span>], [<span class="number">5.5</span>], [<span class="number">6.71</span>], [<span class="number">6.93</span>], [<span class="number">4.168</span>],</span><br><span class="line">                    [<span class="number">9.779</span>], [<span class="number">6.182</span>], [<span class="number">7.59</span>], [<span class="number">2.167</span>], [<span class="number">7.042</span>],</span><br><span class="line">                    [<span class="number">10.791</span>], [<span class="number">5.313</span>], [<span class="number">7.997</span>], [<span class="number">3.1</span>]], dtype=np.float32)</span><br><span class="line"></span><br><span class="line">y_train = np.array([[<span class="number">1.7</span>], [<span class="number">2.76</span>], [<span class="number">2.09</span>], [<span class="number">3.19</span>], [<span class="number">1.694</span>], [<span class="number">1.573</span>],</span><br><span class="line">                    [<span class="number">3.366</span>], [<span class="number">2.596</span>], [<span class="number">2.53</span>], [<span class="number">1.221</span>], [<span class="number">2.827</span>],</span><br><span class="line">                    [<span class="number">3.465</span>], [<span class="number">1.65</span>], [<span class="number">2.904</span>], [<span class="number">1.3</span>]], dtype=np.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x_train = torch.from_numpy(x_train)</span><br><span class="line">y_train = torch.from_numpy(y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"---------------------------------------"</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LinearRegression, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># input and output is 1 dimension</span></span><br><span class="line">        print(<span class="string">"self.training: "</span> + str(self.training))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.linear(x)</span><br><span class="line">        print(<span class="string">"self.training；"</span> + str(self.training))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">print(<span class="string">"initialize"</span>)</span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"---------------------------------------"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"model.eval()"</span>)</span><br><span class="line">model.eval()</span><br><span class="line">inputs = Variable(x_train)</span><br><span class="line">target = Variable(y_train)</span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line">out = model(inputs) <span class="comment"># 前向传播</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"---------------------------------------"</span>)</span><br><span class="line">print(<span class="string">"model.train()"</span>)</span><br><span class="line">model.train()</span><br><span class="line">inputs = Variable(x_train)</span><br><span class="line">target = Variable(y_train)</span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line">out = model(inputs) <span class="comment"># 前向传播</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorboard_logger</title>
      <link href="2019/04/06/tensorboard-logger/"/>
      <url>2019/04/06/tensorboard-logger/</url>
      
        <content type="html"><![CDATA[<p>使用tensorboard_logger记录训练过程中的数据<br>（1）首先需要安装tensorflow</p><ul><li>可参考<a href="https://blog.csdn.net/love666666shen/article/details/77099843" target="_blank" rel="noopener">https://blog.csdn.net/love666666shen/article/details/77099843</a></li><li>不需要单独设置一个tensorflow的环境，直接pip install 一个CPU 版本的即可</li><li>pip install —ignore-installed —upgrade <a href="https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.1.0-cp35-cp35m-win_amd64.whl" target="_blank" rel="noopener">https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.1.0-cp35-cp35m-win_amd64.whl</a></li></ul><p>（2）安装tensorboard</p><ul><li>pip install tensorboard</li></ul><p>（3）No scalar data was found的解决<br>只需将cmd目录cd进入日志文件存放的目录，再加载日志文件便可解决：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">e:</span><br><span class="line">cd logdir</span><br><span class="line">tensorboard <span class="attribute">--logdir</span>=E:\logdir <span class="attribute">--host</span>=127.0.0.1</span><br></pre></td></tr></table></figure></p><p><img src="https://i.loli.net/2019/09/05/7YODLiJAZ6aUTG4.png" alt="搜狗截图20190905112015.png"></p>]]></content>
      
      
      
        <tags>
            
            <tag> tensorboard </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>range xrange np.arange np.linspace</title>
      <link href="2019/04/05/range-xrange-np-arange-np-linspace/"/>
      <url>2019/04/05/range-xrange-np-arange-np-linspace/</url>
      
        <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1rmpadk75j309a07saa2.jpg"></p><p>python</p><ul><li>xrange 得到一个迭代器，（仅可以在python2中使用）</li><li>range 得到一个列表，（python2/python3均可）</li></ul><p>numpy</p><ul><li>numpy.arange 得到一份数组</li><li>numpy.linspace <strong>得到固定数量的等间隔数组，注意包含指定的尾部</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
          <category> numpy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>glob.glob vs os.listdir</title>
      <link href="2019/04/05/glob-glob-vs-os-listdir/"/>
      <url>2019/04/05/glob-glob-vs-os-listdir/</url>
      
        <content type="html"><![CDATA[<ul><li>现在想要得到某个文件夹下的一些图片，并按照顺序排列，如下图所示：  </li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1rl9d9xq9j30ia0cbjri.jpg">  </p><ul><li>第一种方法：（得到的frames_list是不包含路径的）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frames_list = sorted(os.listdir(video_path))</span><br></pre></td></tr></table></figure></li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1rlb8uq8aj309h0a8q2z.jpg"></p><ul><li>第二种方法：（得到的frames_list包含路径的）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frames_list = sorted(glob.glob(os.path.join(video_path, <span class="string">'*.jpg'</span>)))</span><br></pre></td></tr></table></figure></li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1rlbroiqmj30nt0b40u9.jpg"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>os.listdir 仅可以得到对当前路径下文件名称，但是不包含路径信息<br>glob.glob 可以得到对当前路径下文件名称，并包含路径信息</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python list sort方法</title>
      <link href="2019/04/04/python-list-sort%E6%96%B9%E6%B3%95/"/>
      <url>2019/04/04/python-list-sort%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p><strong>sort()</strong> 函数用于对原列表进行排序，如果指定参数，则使用比较函数指定的比较函数。</p><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><p>sort()方法语法：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list.sort(<span class="attribute">cmp</span>=None, <span class="attribute">key</span>=None, <span class="attribute">reverse</span>=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><ul><li>cmp — 可选参数, 如果指定了该参数会使用该参数的方法进行排序。</li><li>key — 主要是用来进行比较的元素，只有一个参数，具体的函数的参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序。</li><li>reverse — 排序规则，<strong>reverse = True</strong> 降序， <strong>reverse = False</strong> 升序（默认）。</li></ul><h2 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h2><p>该方法没有返回值，但是会对列表的对象进行排序。</p><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">video_sort_lambda = <span class="keyword">lambda</span> x: int(x[<span class="number">3</span>:<span class="number">-4</span>]) <span class="comment"># 定义一个函数对元素x进行操作，并得到一个整数Int</span></span><br><span class="line">video_root = <span class="string">"/userhome/dataset/MSVD/Video-Description-with-Spatial-Temporal-Attention/youtube"</span></span><br><span class="line">videos = sorted(os.listdir(video_root), key=video_sort_lambda) <span class="comment"># 按得到的整数，对list进行排序</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>史雅雅的收藏夹</title>
      <link href="2019/04/04/%E5%8F%B2%E9%9B%85%E9%9B%85%E7%9A%84%E6%94%B6%E8%97%8F%E5%A4%B9/"/>
      <url>2019/04/04/%E5%8F%B2%E9%9B%85%E9%9B%85%E7%9A%84%E6%94%B6%E8%97%8F%E5%A4%B9/</url>
      
        <content type="html"><![CDATA[<p><a href="http://pygments.org/" target="_blank" rel="noopener">http://pygments.org/</a></p><p><a href="https://202.38.95.226:7443/view.html" target="_blank" rel="noopener">https://202.38.95.226:7443/view.html</a></p><p><a href="https://aideadlin.es/?sub=ML,RO,CV,SP,NLP,DM" target="_blank" rel="noopener">https://aideadlin.es/?sub=ML,RO,CV,SP,NLP,DM</a></p><p><a href="https://yjs.ustc.edu.cn/" target="_blank" rel="noopener">https://yjs.ustc.edu.cn/</a></p><p><a href="https://www.json.cn/" target="_blank" rel="noopener">https://www.json.cn/</a></p><p><a href="https://kevinj-huang.github.io/" target="_blank" rel="noopener">https://kevinj-huang.github.io/</a></p><p><a href="https://shiyaya.github.io/" target="_blank" rel="noopener">https://shiyaya.github.io/</a></p><p><a href="https://stackedit.io/app#" target="_blank" rel="noopener">https://stackedit.io/app#</a></p><p><a href="http://jsonviewer.stack.hu/" target="_blank" rel="noopener">http://jsonviewer.stack.hu/</a></p><p><a href="http://www.nlpjob.com/" target="_blank" rel="noopener">http://www.nlpjob.com/</a></p><p><a href="https://nlp.stanford.edu/software/scenegraph-parser.shtml#Usage" target="_blank" rel="noopener">https://nlp.stanford.edu/software/scenegraph-parser.shtml#Usage</a></p><p><a href="https://paperswithcode.com/sota" target="_blank" rel="noopener">https://paperswithcode.com/sota</a></p><p><a href="https://sm.ms/" target="_blank" rel="noopener">https://sm.ms/</a></p><p><a href="http://www.arxiv-sanity.com/" target="_blank" rel="noopener">http://www.arxiv-sanity.com/</a></p><p><a href="http://www.cvpapers.com/" target="_blank" rel="noopener">http://www.cvpapers.com/</a></p><h3 id="论文搜索"><a href="#论文搜索" class="headerlink" title="论文搜索"></a>论文搜索</h3><ul><li><p><a href="https://dblp.uni-trier.de/db/" target="_blank" rel="noopener">https://dblp.uni-trier.de/db/</a></p><ul><li>（可以进行筛选，eg:nips, iccv, cvpr）; (也可以对某些作者进行查询)</li></ul></li><li><p><a href="http://openaccess.thecvf.com/menu.py" target="_blank" rel="noopener">http://openaccess.thecvf.com/menu.py</a></p></li><li><a href="http://actionrecognition.net/files/paper.php" target="_blank" rel="noopener">http://actionrecognition.net/files/paper.php</a></li><li><a href="http://www.aaai.org/Library/AAAI/aaai19contents.php" target="_blank" rel="noopener">http://www.aaai.org/Library/AAAI/aaai19contents.php</a></li><li><a href="https://dl.acm.org/results.cfm?within=owners.owner%3DHOSTED&amp;srt=_score&amp;query=&amp;Go.x=26&amp;Go.y=1" target="_blank" rel="noopener">https://dl.acm.org/results.cfm?within=owners.owner%3DHOSTED&amp;srt=_score&amp;query=&amp;Go.x=26&amp;Go.y=1</a></li></ul><h3 id="翻墙"><a href="#翻墙" class="headerlink" title="翻墙"></a>翻墙</h3><ul><li><p><a href="https://github.com/vpncn/vpncn.github.io" target="_blank" rel="noopener">https://github.com/vpncn/vpncn.github.io</a></p></li><li><p><a href="https://flyzyblog.com/install-ss-ssr-bbr-in-one-command/#ss" target="_blank" rel="noopener">https://flyzyblog.com/install-ss-ssr-bbr-in-one-command/#ss</a></p></li><li><p><a href="https://www.banpie.info/shadowsocks-pac-gfw/" target="_blank" rel="noopener">https://www.banpie.info/shadowsocks-pac-gfw/</a></p></li><li><p>Vultr搭建SS</p></li><li><p><a href="https://github.com/sirzdy/shadowsocks/wiki/Vultr搭建SS（VPS搭建SS）" target="_blank" rel="noopener">http://wuzhangyang.com/2019/03/06/vultr-ss/</a></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget https:<span class="comment">//raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh</span></span><br><span class="line"></span><br><span class="line">chmod +x shadowsocks.sh</span><br><span class="line"></span><br><span class="line">./shadowsocks<span class="selector-class">.sh</span> <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>搬瓦工</p></li><li><p>推荐：<a href="https://www.bandwagonhost.net/1967.html" target="_blank" rel="noopener">https://www.bandwagonhost.net/1967.html</a></p></li></ul><h2 id="会议搜索"><a href="#会议搜索" class="headerlink" title="会议搜索"></a>会议搜索</h2><ul><li><a href="http://www.searchconf.net/conf/searchresule/" target="_blank" rel="noopener">http://www.searchconf.net/conf/searchresule/</a></li></ul><h3 id="iccv-2019-challenge"><a href="#iccv-2019-challenge" class="headerlink" title="iccv 2019 challenge"></a>iccv 2019 challenge</h3><ul><li><a href="https://sites.google.com/site/iccv19clvllsmdc/home" target="_blank" rel="noopener">https://sites.google.com/site/iccv19clvllsmdc/home</a></li></ul><h3 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h3><ul><li><a href="https://github.com/rusty1s/pytorch_geometric" target="_blank" rel="noopener">https://github.com/rusty1s/pytorch_geometric</a></li></ul><p>深度学习课程</p><ul><li><a href="https://discuss.gluon.ai/c/5-category" target="_blank" rel="noopener">https://discuss.gluon.ai/c/5-category</a></li><li><a href="http://zh.d2l.ai/chapter_preface/preface.html" target="_blank" rel="noopener">http://zh.d2l.ai/chapter_preface/preface.html</a></li></ul><h3 id="tensorboard-可视化"><a href="#tensorboard-可视化" class="headerlink" title="tensorboard 可视化"></a>tensorboard 可视化</h3><ul><li><a href="https://www.aiuai.cn/aifarm646.html" target="_blank" rel="noopener">https://www.aiuai.cn/aifarm646.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 杂类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch 减小显存消耗，优化显存使用，避免out of memory</title>
      <link href="2019/04/03/pytorch-%E5%87%8F%E5%B0%8F%E6%98%BE%E5%AD%98%E6%B6%88%E8%80%97%EF%BC%8C%E4%BC%98%E5%8C%96%E6%98%BE%E5%AD%98%E4%BD%BF%E7%94%A8%EF%BC%8C%E9%81%BF%E5%85%8Dout-of-memory/"/>
      <url>2019/04/03/pytorch-%E5%87%8F%E5%B0%8F%E6%98%BE%E5%AD%98%E6%B6%88%E8%80%97%EF%BC%8C%E4%BC%98%E5%8C%96%E6%98%BE%E5%AD%98%E4%BD%BF%E7%94%A8%EF%BC%8C%E9%81%BF%E5%85%8Dout-of-memory/</url>
      
        <content type="html"><![CDATA[<h3 id="本文是整理了大神的两篇博客："><a href="#本文是整理了大神的两篇博客：" class="headerlink" title="本文是整理了大神的两篇博客："></a>本文是整理了大神的两篇博客：</h3><ul><li><p>如何计算模型以及中间变量的显存占用大小：<br><a href="https://oldpan.me/archives/how-to-calculate-gpu-memory" target="_blank" rel="noopener">https://oldpan.me/archives/how-to-calculate-gpu-memory</a></p></li><li><p>如何在Pytorch中精细化利用显存：<br><a href="https://oldpan.me/archives/how-to-use-memory-pytorch" target="_blank" rel="noopener">https://oldpan.me/archives/how-to-use-memory-pytorch</a></p></li><li><p>还有知乎中大神的解答：<br><a href="https://zhuanlan.zhihu.com/p/31558973" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31558973</a></p></li><li><p>ppt<br><a href="https://www.zhihu.com/question/67209417" target="_blank" rel="noopener">https://www.zhihu.com/question/67209417</a></p></li><li><p>在说之前先推荐一个实时监控内存显存使用的小工具：</p></li></ul><blockquote><p>sudo apt-get install htop</p></blockquote><ul><li>监控内存（-d为更新频率，下为每0.1s更新一次）：</li></ul><blockquote><p>htop -d=0.1</p></blockquote><ul><li>监控显存（-n为更新频率，下为每0.1s更新一次）：</li></ul><blockquote><p>watch -n 0.1 nvidia-smi</p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>torch.backends.cudnn.benchmark = true 使用情形</title>
      <link href="2019/04/03/torch-backends-cudnn-benchmark-true-%E4%BD%BF%E7%94%A8%E6%83%85%E5%BD%A2/"/>
      <url>2019/04/03/torch-backends-cudnn-benchmark-true-%E4%BD%BF%E7%94%A8%E6%83%85%E5%BD%A2/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.pytorchtutorial.com/when-should-we-set-cudnn-benchmark-to-true/" target="_blank" rel="noopener">pytorch-torch.backends.cudnn.benchmark文档</a></p><ul><li>torch.backends.cudnn.benchmark<br>设置这个 flag 可以让内置的 cuDNN 的 auto-tuner 自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。</li></ul><p>应该遵循以下准则：</p><ul><li>如果网络的输入数据维度或类型上变化不大，设置  torch.backends.cudnn.benchmark = true  可以增加运行效率；</li><li>如果网络的输入数据在每次 iteration 都变化的话，会导致 cnDNN 每次都会去寻找一遍最优配置，这样反而会降低运行效率。</li><li>在程序刚开始加这条语句可以提升一点训练速度，没什么额外开销。</li><li>有时候可能是因为每次迭代都会引入点临时变量，会导致训练速度越来越慢，基本呈线性增长。<br>开发人员还不清楚原因，但如果周期性的使用torch.cuda.empty_cache()的话就可以解决这个问题。这个命令是清除没用的临时变量的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stanford Scene Graph Parser</title>
      <link href="2019/03/26/Stanford-Scene-Graph-Parser/"/>
      <url>2019/03/26/Stanford-Scene-Graph-Parser/</url>
      
        <content type="html"><![CDATA[<ul><li>官网：<a href="https://nlp.stanford.edu/software/scenegraph-parser.shtml#Usage" target="_blank" rel="noopener">https://nlp.stanford.edu/software/scenegraph-parser.shtml#Usage</a></li></ul><h2 id="下载相应的文件（官网有）"><a href="#下载相应的文件（官网有）" class="headerlink" title="下载相应的文件（官网有）"></a>下载相应的文件（官网有）</h2><ul><li>stanford-corenlp-full-2015-12-09.zip</li><li>scenegraph-1.0.jar<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2></li></ul><ol><li>将stanford-corenlp-full-2015-12-09.zip解压，然后按照博客<a href="https://shiyaya.github.io/2019/03/26/ubuntu-%E5%AE%89%E8%A3%85-Stanford-CoreNLP/" target="_blank" rel="noopener">ubuntu 安装 Stanford CoreNLP</a>来安装corenlp</li><li>需要将 scenegraph-1.0.jar 放入解压之后的文件夹stanford-corenlp-full-2015-12-09中，</li></ol><ul><li>需要注意版本</li><li>java  idk 1.8+ 按照博客来就可以</li><li><p>corenlp 使用人家给定的2015的，不要升级</p><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>法1：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -mx2g -cp <span class="string">"*"</span> edu.stanford.nlp.scenegraph.RuleBasedParser</span><br></pre></td></tr></table></figure></li><li><p>注意该命令是在stanford-corenlp-full-2015-12-09文件夹下执行的<br>该方法是交互式的，提示你输入句子，他给出相对应的解析出的scene graph</p></li></ul><p>法2：</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ubuntu 安装 Stanford CoreNLP</title>
      <link href="2019/03/26/ubuntu-%E5%AE%89%E8%A3%85-Stanford-CoreNLP/"/>
      <url>2019/03/26/ubuntu-%E5%AE%89%E8%A3%85-Stanford-CoreNLP/</url>
      
        <content type="html"><![CDATA[<h3 id="安装java-jdk"><a href="#安装java-jdk" class="headerlink" title="安装java jdk"></a>安装java jdk</h3><ul><li><p>更新软件包列表：</p><figure class="highlight q"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-<span class="built_in">get</span> <span class="keyword">update</span></span><br></pre></td></tr></table></figure></li><li><p>安装openjdk-8-jdk：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-<span class="builtin-name">get</span> install openjdk-8-jdk</span><br></pre></td></tr></table></figure></li><li><p>查看java版本：</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -<span class="built_in">version</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="下载并解压Stanford-coreNLP-包："><a href="#下载并解压Stanford-coreNLP-包：" class="headerlink" title="下载并解压Stanford coreNLP 包："></a>下载并解压Stanford coreNLP 包：</h3><ul><li>从这里下载<br><a href="https://stanfordnlp.github.io/CoreNLP/download.html" target="_blank" rel="noopener">https://stanfordnlp.github.io/CoreNLP/download.html</a><br>或者以命令行方式下载<blockquote><p>wget <a href="http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip" target="_blank" rel="noopener">http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip</a>  </p></blockquote></li></ul><ul><li><p>解压</p><blockquote><p>unzip stanford-corenlp-full-2018-02-27.zip</p></blockquote></li><li><p>转到文件目录</p></li></ul><blockquote><p>cd stanford-corenlp-full-2018-02-27/</p></blockquote><h3 id="配置环境变量："><a href="#配置环境变量：" class="headerlink" title="配置环境变量："></a>配置环境变量：</h3><p>把下列这行代码加到你的.bashrc里面(vim .bashrc)</p><blockquote><p> cd ~<br>vim .bashrc<br>export CLASSPATH=/path/to/stanford-corenlp-full-2018-02-27/stanford-corenlp-3.9.1.jar<br>source ~/.bashrc  ## 使之生效<br>把/path/to/替换为你保存stanford-corenlp-full-2016-10-31的地方的路径</p></blockquote><h3 id="安装："><a href="#安装：" class="headerlink" title="安装："></a>安装：</h3><blockquote><p>pip install stanfordcorenlp</p></blockquote><p>处理中文还需要下载中文的模型jar文件，然后放到stanford-corenlp-full-2018-02-27根目录下即可</p><p>wget <a href="http://nlp.stanford.edu/software/stanford-chinese-corenlp-2018-02-27-models.jar" target="_blank" rel="noopener">http://nlp.stanford.edu/software/stanford-chinese-corenlp-2018-02-27-models.jar</a></p><h3 id="检查自己是否装好了stanfordcorenlp"><a href="#检查自己是否装好了stanfordcorenlp" class="headerlink" title="检查自己是否装好了stanfordcorenlp"></a>检查自己是否装好了stanfordcorenlp</h3><p>进入python2或者python3</p><p>命令行下输入：</p><blockquote><p>python<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> stanfordcorenlp <span class="keyword">import</span> StanfordCoreNLP</span><br></pre></td></tr></table></figure></p></blockquote><p>能成功导入不报错，就是安装成功了。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>java</title>
      <link href="2019/03/26/java%20%E7%9A%84%E7%BC%96%E8%AF%91%E4%B8%8E%E6%89%A7%E8%A1%8C/"/>
      <url>2019/03/26/java%20%E7%9A%84%E7%BC%96%E8%AF%91%E4%B8%8E%E6%89%A7%E8%A1%8C/</url>
      
        <content type="html"><![CDATA[<h2 id="java-的编译与执行"><a href="#java-的编译与执行" class="headerlink" title="java 的编译与执行"></a>java 的编译与执行</h2><ol><li><p>用文本编辑器新建一个yumhtest.java文件，在其中输入以下代码并保存：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">yumhtest</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"hello world !"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>编译：在shell终端执行命令 <strong>javac yumhtest.java</strong></p></li><li><p>运行：在shell终端执行命令 <strong>java yumhtest</strong><br>当shell下出现“hello world !”字样</p></li></ol><p>注意事项：类名应和文件名相同。</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java 的编译与执行</title>
      <link href="2019/03/26/java/"/>
      <url>2019/03/26/java/</url>
      
        <content type="html"><![CDATA[<h2 id="java-的编译与执行"><a href="#java-的编译与执行" class="headerlink" title="java 的编译与执行"></a>java 的编译与执行</h2><ol><li><p>用文本编辑器新建一个yumhtest.java文件，在其中输入以下代码并保存：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">yumhtest</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"hello world !"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>编译：在shell终端执行命令 <strong>javac yumhtest.java</strong></p></li><li><p>运行：在shell终端执行命令 <strong>java yumhtest</strong><br>当shell下出现“hello world !”字样</p></li></ol><p>注意事项：类名应和文件名相同。</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SPICE</title>
      <link href="2019/03/25/SPICE/"/>
      <url>2019/03/25/SPICE/</url>
      
        <content type="html"><![CDATA[<h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><h3 id="Ubuntu下安装Stanford-CoreNLP"><a href="#Ubuntu下安装Stanford-CoreNLP" class="headerlink" title="Ubuntu下安装Stanford CoreNLP"></a><a href="https://blog.csdn.net/Hay54/article/details/82313535" target="_blank" rel="noopener">Ubuntu下安装Stanford CoreNLP</a></h3>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>faster r-cnn 解读</title>
      <link href="2019/03/24/faster-r-cnn-%E8%A7%A3%E8%AF%BB/"/>
      <url>2019/03/24/faster-r-cnn-%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>来自<a href="https://mp.weixin.qq.com/s/M_i38L2brq69BYzmaPeJ9w" target="_blank" rel="noopener">机器之心</a><br>可能机器之心的那个链接无法转到，<a href="http://tech.ifeng.com/a/20180223/44884976_0.shtml" target="_blank" rel="noopener">看这个</a><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e0ltm2y7j30u0083wev.jpg"><br>by yaya:<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e10a6tguj31fj0mw0vw.jpg"></p><h2 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h2><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e0p68j95j30t80gaaar.jpg" style="zoom:60%"></p><p>anchor: 定义anchor的长: scale=[4,8,16], 长宽比: ratio=[0.5, 1, 1.5, 2]，则在each position of conv feature 将会有k=len(scale)×len(ratio)=12个anchor</p><p>(1)对于分类层，我们对每个锚点输出两个预测值：它是背景（不是目标）的分数，和它是前景（实际的目标）的分数.&lt;/br&gt;<br>则经过该1×1的卷积层，输出的shape=N×2k×H×W  &lt;/br&gt;</p><p>(2)对于回归或边框调整层，我们输出四个预测值(偏移值)：<font color="#0099ff" size="5" face="黑体">Δxcenter、Δycenter、Δwidth、Δheight</font>，我们将会把这些值用到锚点中来得到最终的建议：(x1, y1, x2, y2)分别为左下角和右上角的坐标，即area=(x2-x1)*(y2-y1).&lt;/br&gt;<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e0p66yxyj312w066t91.jpg"></p><p>(3) 补充，<font color="#0099ff" size="5" face="黑体">RoI Pooling</font>  得到 pooled feats，输入的是base feats, 得到的pred proposals 以及 <font color="#0099ff" size="5" face="黑体">1/scale</font><br>因为pred proposals得到的坐标是在<del>原始的image上的</del> 输入到网络中的image，而当前的base feats 是相对于原图有尺度变化的，为了对应.</p><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e0p6ekybj30u00c175l.jpg"></p><p>有两个不同的目标：&lt;/br&gt;<br>(1) 将建议分到一个类中，加上一个背景类（用于删除不好的建议）。&lt;/br&gt;<br>(2) 根据预测的类别更好地调整建议的边框。&lt;/br&gt;<br>在最初的 Faster R-CNN 论文中，R-CNN 对每个建议采用特征图，将它平坦化并使用两个大小为 4096 的有 ReLU 激活函数的全连接层。然后，它对每个不同的目标使用两种不同的全连接层：&lt;/br&gt;<br>一个有 N+1 个单元的全连接层，其中 N 是类的总数，另外一个是背景类。&lt;/br&gt;<br>一个有 4N 个单元的全连接层。我们希望有一个回归预测，因此对 N 个类别中的每一个可能的类别，我们都需要 <font color="#0099ff" size="5" face="黑体">Δxcenter、Δycenter、Δwidth、Δheight</font>。&lt;/br&gt;<br>训练和目标&lt;/br&gt;<br>R-CNN 的目标与 RPN 的目标的计算方法几乎相同，但是考虑的是不同的可能类别。我们采用建议和真实边框，并计算它们之间的 IoU。&lt;/br&gt;</p><p>那些有任何真实边框的建议，只要其 IoU 大于 0.5，都被分配给那个真实数据。那些 IoU 在 0.1 和 0.5 之间的被标记为背景。与我们在为 RPN 分配目标时相反的是，我们忽略了没有任何交集的建议。这是因为在这个阶段，我们假设已经有好的建议并且我们对解决更困难的情况更有兴趣。当然，这些所有的值都是可以为了更好的拟合你想找的目标类型而做调整的超参数。&lt;/br&gt;</p><p>边框回归的目标是计算建议和与其对应的真实框之间的偏移量，仅针对那些基于 IoU 阈值分配了类别的建议。&lt;/br&gt;</p><p>我们随机抽样了一个尺寸为 64 的 balanced mini batch，其中我们有高达 25% 的前景建议（有类别）和 75% 的背景。&lt;/br&gt;</p><p>按照我们对 RPN 损失所做的相同处理方式，现在的分类损失是一个多类别的交叉熵损失，使用所有选定的建议和用于与真实框匹配的 25% 建议的 Smooth L1 loss。由于 R-CNN 边框回归的全连接网络的输出对于每个类都有一个预测，所以当我们得到这种损失时必须小心。在计算损失时，我们只需要考虑正确的类。&lt;/br&gt;<br>这里若假定类别可知，则每个类都有预测，若类别不可知，则仅有一个预测即可，代码如下：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e0p66uafj30nq03vt8o.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Image Caption 常用评价指标</title>
      <link href="2019/03/24/Image-Caption-%E5%B8%B8%E7%94%A8%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
      <url>2019/03/24/Image-Caption-%E5%B8%B8%E7%94%A8%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Graph R-CNN for Scene Graph Generation</title>
      <link href="2019/03/24/Graph-R-CNN-for-Scene-Graph-Generation/"/>
      <url>2019/03/24/Graph-R-CNN-for-Scene-Graph-Generation/</url>
      
        <content type="html"><![CDATA[<p>这是ECCV 2018 场景图生成 的一篇文章。<br>写在前面，本文使用的GCN网络与“Graph Attention Networks”一致，都是计算两个节点之间的attention来计算邻接矩阵中的元素值，更新节点特征的公式是AXW。</p><ul><li><strong>查看本文的原因，主要是想看，其是如何提取relation feature的，但是文中仅使用了union box feature 作为relation feature。较为朴素！</strong>————————-不好</li><li><strong>同时也将relation 作为node放入graph 中，但是是object feature 与 realtion feature之间的混合graph，与“Auto-Encoding Scene Graphs for Image Captioning”一样采用的是异构图</strong>  </li><li><strong>文中对W<sup>sr</sup>Z<sup>r</sup>α<sup>sr</sup>， 为该node<sub>i</sub>与所有的relation nodes之间计算的注意力系数，并不合理，因为object 可能仅有一两个relation，怎么可能与所有的relation有关系呢</strong>—————————————不好（后又考虑了一下，可能没有关系的直接算0，就不再计算attention了）</li></ul><p>思索良久，终于发现是哪里不对了，一般的情况下，都是X’=AXW，这样的形式，以 X 为中心，更新X的特征，而Z<sub>i</sub><sup>r</sup>的更新公式中是以Z<sup>o</sup>为中心，因此是不是有些不对头呢？？？</p><p>本文的<strong>两个主要的贡献</strong>：</p><ol><li><strong>GCN</strong> with attention 用于scene graph generate 任务。Updating each object and relationship representation based on its neighbors</li><li>对于N个object ,若两两配对，则会产生N×N个relation，数量是N的二次，数量很多，但是很多又是没有必要的，以前的工作采用随机采样的方式，但是本文提出了<strong>RePN 网络来采样relation</strong>。</li><li>提出了一个新的用于scene graph generate 的评价指标，SGGen+（不是笔者关注的内容，因此此处忽略了对SGGen+的介绍）</li></ol><h2 id="场景图生成任务的主要步骤"><a href="#场景图生成任务的主要步骤" class="headerlink" title="场景图生成任务的主要步骤"></a>场景图生成任务的主要步骤</h2><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1dxanmhpfj312o08p75l.jpg">    </p><ol><li>P( V|I )  指：在给定image的情况下，去得到 <strong>object proposals</strong><br>使用pytorch 版本[1]的faster R-CNN来得到 bbox，类似于[2]，采用分段训练的方式，先对faster R-CNN进行预训练，然后，固定faster r-cnn参数，训练整个场景图生成网络。  </li><li>P( E|V, I ) 指：在给定image 和 bbox的情况下来得到 <strong>relation proposals</strong><br>如果假设每个object proposals 之间都会有一个relation，则有N×N个relation，或者是说，有N×N个object pairs。但是含有很多不合适的relation（本身这object pairs 之间不存在关系，但是却指定了某种关系），因此本文提出使用ReRN 网络来采样得到 relatedness relations。</li><li>P( R,O|V,E,I ) 指： 在给定image，object proposals以及relation proposal之后，得到object label 和 relation label。<br>一般的方法是采用iterative refinement process[2]，本文使用的是用GCN来迭代。</li></ol><ul><li>overview<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1dy02tb1zj31du0crajb.jpg"></li></ul><h2 id="Object-Proposal"><a href="#Object-Proposal" class="headerlink" title="Object Proposal"></a>Object Proposal</h2><p>使用faster r-cnn来提取<strong>object proposals</strong>，并得到相对应的一维特征向量（<strong>pooled feat</strong>），faster r-cnn 使用类别可知，则可以得到每个object 对应的<strong>label</strong><br>使用</p><h2 id="Relation-Proposal-Network"><a href="#Relation-Proposal-Network" class="headerlink" title="Relation Proposal Network"></a>Relation Proposal Network</h2><p>输入： <strong>labels</strong> of object pairs<br>输出：relatedness relations/ m 个object pairs<br>主要的步骤见下图：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e8ud0p66j32v311qjyg.jpg"></p><h2 id="Attention-GCN"><a href="#Attention-GCN" class="headerlink" title="Attention GCN"></a>Attention GCN</h2><h3 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h3><ul><li>与GAT的公式是一致的，具体可以参看论文GAT[ 3]，<strong>α<sub>i</sub></strong> 是注意力系数<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e8wl8yubj30f60360sr.jpg" style="zoom:60%"><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e8wl8m92j30gn04sjrp.jpg" style="zoom:60%"></li></ul><h3 id="aGCN-for-Scene-Graph-Generation"><a href="#aGCN-for-Scene-Graph-Generation" class="headerlink" title="aGCN for Scene Graph Generation"></a>aGCN for Scene Graph Generation</h3><ul><li>只构建一个graph，在这个graph中，object是node，relation也是node。<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e98pvy7aj30dh0cq0t3.jpg" style="zoom:60%">  </li><li><p>用skip代表object node之间的连接；构建的是有向边；捕捉了三中类型的连接：<br><strong>object &lt;—&gt; relationship</strong>， <strong>relationship &lt;—&gt; subject</strong> and <strong>object &lt;—&gt; object</strong><br><strong>s</strong>=subjects, <strong>o</strong>=objects, and <strong>r</strong>=relationships<br>object and relationship features as  <strong>Z<sup>o</sup></strong> and <strong>Z<sup>r</sup></strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1e9gaiumnj30vp0dg0v2.jpg" style="zoom:60%"><br>对上图的解读：<br>（1）虽说是一个图，但是进行了两个aGCN的计算，使用的object and relationship node representation是什么？文中说，visual aGCN 使用visual feature 来进行计算，semantic aGCN 使用pre-softmax outputs来进行计算。（没看懂）<br>（2）WZα公式是GCN的计算公式，</p><ul><li>以 <strong>W<sup>skip</sup> Z<sup>o</sup> α<sup>skip</sup></strong> 为例，<strong>W<sup>skip</sup></strong> 是可学习参数，<strong>Z<sup>o</sup></strong> 是object nodes feature 组成的矩阵（d,N），<strong>α<sup>skip</sup></strong> 是一个向量，为该 node<sub>i</sub>与所有的object nodes之间计算的注意力系数，维度为(1,N）（N个objects）</li><li>以 <strong>W<sup>sr</sup> Z<sup>r</sup> α<sup>sr</sup></strong> 为例，<strong>W<sup>sr</sup></strong> 是可学习参数，<strong>Z<sup>r</sup></strong> 是realtion nodes feature 组成的矩阵（d,m），<strong>α<sup>sr</sup></strong> 是一个向量，为该node<sub>i</sub>与所有的relation nodes之间计算的注意力系数，维度为(1,m）（m个realtion）</li><li>以 <strong>W<sup>rs</sup> Z<sup>o</sup> α<sup>rs</sup></strong> 为例，<strong>W<sup>rs</sup></strong> 是可学习参数，<strong>Z<sup>o</sup></strong> 是object nodes feature 组成的矩阵（d,N），<strong>α<sup>rs</sup></strong> 是一个向量，为该relation<sub>i</sub>与所有的object nodes之间计算的注意力系数，维度为(1,N）（N个objects）  </li></ul><p>（3）需要注意的是，<strong>α<sub>ii</sub></strong>=1，这将使得，每一行想加不为1</p></li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1ea7p889dj313l0dzwj3.jpg"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] A faster pytorch implementation of faster  r-cnn. <a href="https://github.com/jwyang/faster-rcnn.pytorch" target="_blank" rel="noopener">https://github.com/jwyang/faster-rcnn.pytorch</a><br>[2] Scene graph generation by iterative message passing<br>[3] Graph Attention Networks<br>[3] Graph Attention Networks</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Batch Normalization(BN层)</title>
      <link href="2019/03/23/Batch-Normalization-BN%E5%B1%82/"/>
      <url>2019/03/23/Batch-Normalization-BN%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<p>参看：<a href="https://blog.csdn.net/donkey_1993/article/details/81871132" target="_blank" rel="noopener">https://blog.csdn.net/donkey_1993/article/details/81871132</a><br>参看：<a href="https://zhuanlan.zhihu.com/p/34879333" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34879333</a></p><h2 id="BN层的原理"><a href="#BN层的原理" class="headerlink" title="BN层的原理"></a>BN层的原理</h2><ul><li>在训练阶段，输入到网络中的是mini batch<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1chneox3hj30dv0bddgx.jpg"><br>解析：</li><li><p>Normalization操作我们虽然缓解了ICS问题，让每一层网络的输入数据分布都变得稳定，但却导致了数据表达能力的缺失。也就是我们通过变换操作改变了原有数据的信息表达（representation ability of the network），使得底层网络学习到的参数信息丢失。另一方面，通过让每一层的输入分布均值为0，方差为1，会使得输入在经过sigmoid或tanh激活函数时，容易陷入非线性激活函数的线性区域。  </p></li><li><p>因此，BN又引入了两个可学习（learnable）的参数  γ与β  。这两个参数的引入是为了恢复数据本身的表达能力，对规范化后的数据进行线性变换，即<img src="https://www.zhihu.com/equation?tex=%5Ctilde%7BZ_j%7D%3D%5Cgamma_j+%5Chat%7BZ%7D_j%2B%5Cbeta_j" alt="\tilde{Z_j}=\gamma_j \hat{Z}_j+\beta_j">。特别地，当  γ<sup>2</sup>=σ<sup>2</sup> ， β=μ 时，可以实现等价变换（identity transform）并且保留了原始输入特征的分布信息。</p></li></ul><h2 id="测试阶段如何使用Batch-Normalization？"><a href="#测试阶段如何使用Batch-Normalization？" class="headerlink" title="测试阶段如何使用Batch Normalization？"></a>测试阶段如何使用Batch Normalization？</h2><p>我们知道BN在每一层计算的  μ与σ<sup>2</sup>都是基于当前batch中的训练数据，但是这就带来了一个问题：我们在预测阶段，有可能只需要预测一个样本或很少的样本，没有像训练样本中那么多的数据，此时 μ与σ<sup>2</sup>的计算一定是有偏估计，这个时候我们该如何进行计算呢？</p><p>利用BN训练好模型后，我们保留了每组mini-batch训练数据在网络中每一层的<br> μ<sub>batch</sub>与σ<sup>2</sup><sub>batch</sub> 。此时我们使用整个样本的统计量来对Test数据进行归一化，具体来说使用均值与方差的无偏估计：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmu_%7Btest%7D%3D%5Cmathbb%7BE%7D+%28%5Cmu_%7Bbatch%7D%29" alt="\mu_{test}=\mathbb{E} (\mu_{batch})"></p><p><img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2_%7Btest%7D%3D%5Cfrac%7Bm%7D%7Bm-1%7D%5Cmathbb%7BE%7D%28%5Csigma%5E2_%7Bbatch%7D%29" alt="\sigma^2_{test}=\frac{m}{m-1}\mathbb{E}(\sigma^2_{batch})"></p><p>得到每个特征的均值与方差的无偏估计后，我们对test数据采用同样的normalization方法：</p><p><img src="https://www.zhihu.com/equation?tex=BN%28X_%7Btest%7D%29%3D%5Cgamma%5Ccdot+%5Cfrac%7BX_%7Btest%7D-%5Cmu_%7Btest%7D%7D%7B%5Csqrt%7B%5Csigma%5E2_%7Btest%7D%2B%5Cepsilon%7D%7D%2B%5Cbeta" alt="BN(X_{test})=\gamma\cdot \frac{X_{test}-\mu_{test}}{\sqrt{\sigma^2_{test}+\epsilon}}+\beta"></p><p>另外，除了采用整体样本的无偏估计外。吴恩达在Coursera上的Deep Learning课程指出可以对train阶段每个batch计算的mean/variance采用<a href="[https://zhuanlan.zhihu.com/p/29895933](https://zhuanlan.zhihu.com/p/29895933"><font color="#0099ff" size="5" face="楷体"> 指数加权平均</font></a>)来得到test阶段mean/variance的估计。</p><h2 id="Batch-Normalization的优势"><a href="#Batch-Normalization的优势" class="headerlink" title="Batch Normalization的优势"></a>Batch Normalization的优势</h2><p>Batch Normalization在实际工程中被证明了能够缓解神经网络难以训练的问题，BN具有的有事可以总结为以下三点：</p><p><strong>（1）BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度</strong></p><p>BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。</p><p><strong>（2）BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定</strong></p><p>在神经网络中，我们经常会谨慎地采用一些权重初始化方法（例如Xavier）或者合适的学习率来保证网络稳定训练。<br>当学习率设置太高时，会使得参数更新步伐过大，容易出现震荡和不收敛。但是使用BN的网络将不会受到参数数值大小的影响。<br>在使用Batch Normalization之后，抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强，此时我们可以设置较大的学习率而不用过于担心模型divergence的风险。</p><p><strong>（3）BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题</strong></p><p>在不使用BN层的时候，由于网络的深度与复杂性，很容易使得底层网络变化累积到上层网络中，导致模型的训练很容易进入到激活函数的梯度饱和区；通过normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；另外通过自适应学习 γ与β又让数据保留更多的原始信息。</p><p><strong>（4）BN具有一定的正则化效果</strong></p><p>在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。</p><p>另外，原作者通过也证明了网络加入BN后，可以丢弃Dropout，模型也同样具有很好的泛化效果。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>optical flow(光流)</title>
      <link href="2019/03/23/optical-flow-%E5%85%89%E6%B5%81/"/>
      <url>2019/03/23/optical-flow-%E5%85%89%E6%B5%81/</url>
      
        <content type="html"><![CDATA[<p>参考：<a href="https://blog.csdn.net/zouxy09/article/details/8683859" target="_blank" rel="noopener">https://blog.csdn.net/zouxy09/article/details/8683859</a></p><h2 id="光流的定义"><a href="#光流的定义" class="headerlink" title="光流的定义"></a>光流的定义</h2><p>在人的眼睛在观察物体时，物体的景象在人的视网膜上形成一系列连续变化的图像，这一系列连续变化的信息不断“流过”视网膜，好像一种光的流，故称之为光流。<br>一般，光流是由于场景中前景目标本身的移动、相机的移动，或者两者的共同运动所产生的。<br>定义：它是空间运动物体在观察成像平面上的像素运动的瞬时速度，是利用图像序列中<strong>像素在时间域上的变化以及相邻帧之间的相关性</strong>来找到上一帧跟当前帧之间存在的<strong>对应关系</strong>，从而计算出<strong>相邻帧</strong>之间物体的运动信息的一种方法。</p><h2 id="如何计算光流"><a href="#如何计算光流" class="headerlink" title="如何计算光流"></a>如何计算光流</h2><ul><li>那通俗的讲就是通过一个图片序列，把每张图像中每个像素的运动速度和运动方向找出来就是光流场。那怎么找呢？咱们直观理解肯定是：第t帧的时候A点的位置是(x1, y1)，那么我们在第t+1帧的时候再找到A点，假如它的位置是(x2,y2)，那么我们就可以确定A点的运动了：(ux, vy) = (x2, y2) - (x1,y1)。</li><li>那怎么知道第t+1帧的时候A点的位置呢？ 这就存在很多的光流计算方法了。</li><li>1981年，Horn和Schunck创造性地将二维速度场与灰度相联系，引入光流约束方程，得到光流计算的基本算法。人们基于不同的理论基础提出各种光流计算方法，算法性能各有不同。Barron等人对多种光流计算技术进行了总结，按照理论基础与数学方法的区别把它们分成四种：<strong>基于梯度的方法、基于匹配的方法、基于能量的方法、基于相位的方法</strong>。近年来神经动力学方法也颇受学者重视。</li><li><p>yaya: 即光流法计算的是：相邻两帧之间的对应像素点之间的<strong>速度矢量</strong>，但是如何得到相邻帧对应的像素点是一个问题。<br>光流法主要依赖于三个假设：</p><p>  [亮度恒定] 图像中目标的像素强度在连续帧之间不会发生变化。<br>  [时间规律] 相邻帧之间的时间足够短，以至于在考虑运行变化时可以忽略它们之间的差异。该假设用于导出下面的核心方程。<br>  [空间一致性] 相邻像素具有相似的运动。  </p></li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1cgxrjz19j30hi0h90uo.jpg">  </p><p>上式中，I<sub>x</sub>,  I<sub>y</sub>可以通过图像沿x方向和y方向的导数计算，I<sub>t</sub>可以通过I(x,y,t)−I(x,y,t−1)计算。未知数是(u,v)， 正是我们想要求解的每个像素在前后相邻两帧的位移。</p><p>这里只有一个方程，却有两个未知数（实际是NN个方程，2N2N个未知数，NN是图像中待估计的像素点的个数，但是我们通过矩阵表示，将它们写成了如上式所述的紧凑形式），所以是一个不定方程。我们需要找出其它的约束求解方程。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1cgxrjo7dj30g00e6ta4.jpg"><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1cgxrhmkgj30gh05swej.jpg">  </p><p>如上图所示，H中的像素点(x,y)在I中的移动到了(x+u,y+v)的位置，偏移量为(u,v)。速度=位移在极短时间你内的位移量。  </p><p>参看：<a href="https://xmfbit.github.io/2017/05/03/cs131-opticalflow/" target="_blank" rel="noopener">https://xmfbit.github.io/2017/05/03/cs131-opticalflow/</a><br>参看：<a href="https://blog.csdn.net/carson2005/article/details/7581642" target="_blank" rel="noopener">https://blog.csdn.net/carson2005/article/details/7581642</a></p>]]></content>
      
      
      <categories>
          
          <category> 行为识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 行为识别 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从bagging到dropout</title>
      <link href="2019/03/22/%E4%BB%8Ebagging%E5%88%B0dropout/"/>
      <url>2019/03/22/%E4%BB%8Ebagging%E5%88%B0dropout/</url>
      
        <content type="html"><![CDATA[<ul><li>转载 from: <a href="https://blog.csdn.net/m0_37477175/article/details/77145459" target="_blank" rel="noopener">https://blog.csdn.net/m0_37477175/article/details/77145459</a></li></ul><p>dropout的思想继承自bagging方法，学习dropout先了解一下bagging方法。</p><h2 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h2><ul><li>bagging是一种集成方法（ensemble methods）,可以通过集成来减小泛化误差（generalization error）。 </li><li>bagging的<strong>最基本的思想</strong>是通过分别训练几个不同分类器，最后对测试的样本，每个分类器对其进行投票。在机器学习上这种策略叫model averaging。 </li><li>model averaging 之所以有效，是因为并非所有的分类器都会产生相同的误差，只要有不同的分类器产生的误差不同就会对减小泛化误差非常有效。 </li><li>对于bagging方法，允许采用相同的分类器，相同的训练算法，相同的目标函数。但是在<strong>数据集方面</strong>，<a href="https://www.baidu.com/s?wd=%E6%96%B0%E6%95%B0%E6%8D%AE&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd" target="_blank" rel="noopener">新数据</a>集与原始数据集的大小是相等的。每个数据集都是通过在原始数据集中随机选择一个样本进行替换而得到的。意味着，每个新数据集中会<strong>存在重复</strong>的样本。 </li><li>在数据集建好之后，用<strong>相同的学习算法</strong>分别作用于每个数据集就得到了几个分类器。 </li><li>下面这幅图片很好的解释了bagging的工作方式：我们想实现一个对数字8进行分类的分类器。此时构造了两个数据集，使用相同的学习算法，第一个分类器学习到的是8的上面那部分而第二个分类器学习的是8的下面那个部分。当我们把两个分类器集合起来的时候，此时的分类才是比较好的。 </li><li>Each of these individual classification ruls is brittle, but if we average there output then the detector is robust.<br><img src="https://img-blog.csdn.net/20170813153102572?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbTBfMzc0NzcxNzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></li></ul><h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><ul><li>我们可以把dropout类比成将许多大的神经网络进行集成的一种bagging方法。 </li><li>但是每一个神经网络的训练是非常耗时和占用很多内存的，训练很多的神经网络进行集合分类就显得太不实际了。 </li><li>但是，dropout可以训练所有子网络的集合，这些子网络通过去除整个网络中的一些<a href="https://www.baidu.com/s?wd=%E7%A5%9E%E7%BB%8F%E5%85%83&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd" target="_blank" rel="noopener">神经元</a>来获得。 </li><li><p>如下图所示：<br><img src="https://img-blog.csdn.net/20170813154717429?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbTBfMzc0NzcxNzU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p></li><li><p>可能有些人会问上图的有些子网络，从输入到不了最终的输出，怎么办？其实对于比较宽的层（wider layers）从输入到输出都切断的概率是非常小的，多以影响不是很大。</p></li><li><p>如何移除一个神经元呢，我们通过仿射和非线性变换，试神经元的输出乘以0。</p></li><li><p>每次我们加载一个样本到minibatch，然后随机的采样一个不同的二进制掩膜作用在所有的输出，输入，隐藏节点上。每个节点的掩膜都是独立采样的。采样一个掩膜值为1的概率是固定的超参数。</p></li></ul><h2 id="bagging与dropout训练的对比"><a href="#bagging与dropout训练的对比" class="headerlink" title="bagging与dropout训练的对比"></a>bagging与dropout训练的对比</h2><ul><li>在bagging中，所有的分类器都是独立的，而在dropout中，所有的模型都是共享参数的。</li><li>在bagging中，所有的分类器都是在特定的数据集下训练至收敛，而在dropout中没有明确的模型训练过程。网络都是在一步中训练一次（输入一个样本，随机训练一个子网络）</li><li>（相同点）对于训练集来说，每一个子网络的训练数据是通过原始数据的替代采样得到的子集。<strong>？？？</strong>（自己的理解：每一个输入一个样本初始化某一个子网络）</li></ul><h2 id="dropout的优势"><a href="#dropout的优势" class="headerlink" title="dropout的优势"></a>dropout的优势</h2><ul><li>very computationally cheap在dropout训练阶段，每一个样本每一次更新只需要O(n)<br>，同时要生成n个二进制数字与每个状态相乘。除此之外，还需要O(n)的额外空间存储这些二进制数字，直到反向传播阶段。</li><li>没有很显著的限制模型的大小和训练的过程。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>梯度消失与梯度爆炸的原因以及解决方案</title>
      <link href="2019/03/22/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E5%8E%9F%E5%9B%A0%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
      <url>2019/03/22/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E5%8E%9F%E5%9B%A0%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
      
        <content type="html"><![CDATA[<p>转载from: <a href="https://blog.csdn.net/raojunyang/article/details/79962665" target="_blank" rel="noopener">https://blog.csdn.net/raojunyang/article/details/79962665</a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文主要深入介绍深度学习中的梯度消失和梯度爆炸的问题以及解决方案。本文分为三部分，第一部分主要直观的介绍深度学习中为什么使用梯度更新，第二部分主要介绍深度学习中梯度消失及爆炸的原因，第三部分对提出梯度消失及爆炸的解决方案。有基础的同鞋可以跳着阅读。<br>其中，梯度消失爆炸的解决方案主要包括以下几个部分。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">- </span>预训练加微调</span><br><span class="line"><span class="bullet">- </span>梯度剪切、权重正则（针对梯度爆炸）</span><br><span class="line"><span class="bullet">- </span>使用不同的激活函数</span><br><span class="line"><span class="bullet">- </span>使用batchnorm</span><br><span class="line"><span class="bullet">- </span>使用残差结构</span><br><span class="line"><span class="bullet">- </span>使用LSTM网络</span><br></pre></td></tr></table></figure><h1 id="第一部分：为什么要使用梯度更新规则"><a href="#第一部分：为什么要使用梯度更新规则" class="headerlink" title="第一部分：为什么要使用梯度更新规则"></a>第一部分：为什么要使用梯度更新规则</h1><hr><ul><li><p>在介绍梯度消失以及爆炸之前，先简单说一说梯度消失的根源—–深度神经网络和反向传播。目前深度学习方法中，深度神经网络的发展造就了我们可以构建更深层的网络完成更复杂的任务，深层网络比如深度卷积网络，LSTM等等，而且最终结果表明，在处理复杂任务上，深度网络比浅层的网络具有更好的效果。但是，目前优化神经网络的方法都是基于反向传播的思想，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。这样做是有一定原因的，首先，深层网络由许多非线性层堆叠而来，每一层非线性层都可以视为是一个非线性函数 (非线性来自于非线性激活函数），因此整个深度网络可以视为是一个复合的非线性多元函数 </p></li><li><p>我们最终的目的是希望这个多元函数可以很好的完成输入到输出之间的映射，那么，优化深度网络就是为了寻找到合适的权值，满足取得极小值点，比如最简单的损失函数 </p></li><li>假设损失函数的数据空间是下图这样的，我们最优的权值就是为了寻找下图中的最小值点，对于这种数学寻找最小值问题，采用梯度下降的方法再适合不过了。 </li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1baz0q65tj30e809edj1.jpg"></p><h1 id="第二部分：梯度消失、爆炸"><a href="#第二部分：梯度消失、爆炸" class="headerlink" title="第二部分：梯度消失、爆炸"></a>第二部分：梯度消失、爆炸</h1><p>梯度消失与梯度爆炸其实是一种情况，看接下来的文章就知道了。两种情况下梯度消失经常出现，一是在<strong>深层网络</strong>中，二是采用了<strong>不合适的损失函数</strong>，比如sigmoid。梯度爆炸一般出现在深层网络和<strong>权值初始化值太大</strong>的情况下，下面分别从这两个角度分析梯度消失和爆炸的原因。</p><h3 id="1-深层网络角度"><a href="#1-深层网络角度" class="headerlink" title="1.深层网络角度"></a>1.深层网络角度</h3><p>比较简单的深层网络如下：<br><img src="https://img-blog.csdn.net/20171219215626301?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>图中是一个四层的全连接网络，假设每一层网络激活后的输出为,其中为第层, 代表第层的输入，也就是第层的输出，是激活函数，那么，得出，简单记为。<br>BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整，参数的更新为，给定学习率，得出。如果要更新第二隐藏层的权值信息，根据链式求导法则，更新梯度信息：<br>，很容易看出来，即第二隐藏层的输入。<br>所以说，就是对激活函数进行求导，如果此部分大于1，那么层数增多的时候，最终的求出的梯度更新将以指数形式增加，即发生<strong>梯度爆炸</strong>，如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了<strong>梯度消失</strong>。如果说从数学上看不够直观的话，下面几个图可以很直观的说明深层网络的梯度问题（图片内容来自参考文献1）：</p><p>注：下图中的隐层标号和第一张全连接图隐层标号刚好相反。<br>图中的曲线表示权值更新的速度，对于下图两个隐层的网络来说，已经可以发现隐藏层2的权值更新速度要比隐藏层1更新的速度慢</p><p><img src="https://img-blog.csdn.net/20171220110058983?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那么对于四个隐层的网络来说，就更明显了，第四隐藏层比第一隐藏层的更新速度慢了两个数量级：</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/20171220110732927?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p><strong>总结：</strong>从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。因此，梯度消失、爆炸，其<strong>根本原因</strong>在于反向传播训练法则，属于<a href="https://www.baidu.com/s?wd=%E5%85%88%E5%A4%A9%E4%B8%8D%E8%B6%B3&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd" target="_blank" rel="noopener">先天不足</a>，另外多说一句，Hinton提出capsule的原因就是为了彻底抛弃反向传播，如果真能大范围普及，那真是一个革命。</p><h3 id="2-激活函数角度"><a href="#2-激活函数角度" class="headerlink" title="2.激活函数角度"></a>2.激活函数角度</h3><p>其实也注意到了，上文中提到计算权值更新信息的时候需要计算前层偏导信息，因此如果激活函数选择不合适，比如使用sigmoid，梯度消失就会很明显了，原因看下图，左图是sigmoid的损失函数图，右边是其倒数的图像，如果使用sigmoid作为损失函数，其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失，sigmoid函数数学表达式为：<br><img src="https://img-blog.csdn.net/20171220113129230?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="sigmoid函数"><img src="https://img-blog.csdn.net/20171220113422675?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="sigmoid函数导数"></p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">同理，<span class="built-in">tanh</span>作为损失函数，它的导数图如下，可以看出，<span class="built-in">tanh</span>比<span class="built-in">sigmoid</span>要好一些，但是它的倒数仍然是小于<span class="number">1</span>的。<span class="built-in">tanh</span>数学表达为：</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/20171220114016270?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><h1 id="第三部分：梯度消失、爆炸的解决方案"><a href="#第三部分：梯度消失、爆炸的解决方案" class="headerlink" title="第三部分：梯度消失、爆炸的解决方案"></a>第三部分：梯度消失、爆炸的解决方案</h1><hr><h3 id="2-1-方案1-预训练加微调"><a href="#2-1-方案1-预训练加微调" class="headerlink" title="2.1 方案1-预训练加微调"></a>2.1 方案1-预训练加微调</h3><p>此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。</p><h3 id="2-2-方案2-梯度剪切、正则"><a href="#2-2-方案2-梯度剪切、正则" class="headerlink" title="2.2 方案2-梯度剪切、正则"></a>2.2 方案2-梯度剪切、正则</h3><p><strong>梯度剪切</strong>这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注：在WGAN中也有梯度剪切限制操作，但是和这个是不一样的，WGAN限制梯度更新信息是为了保证lipchitz条件。</span><br></pre></td></tr></table></figure><p>另外一种解决梯度爆炸的手段是采用<strong>权重正则化</strong>（weithts regularization）比较常见的是正则，和正则，在各个深度框架中都有相应的API可以使用正则化，比如在中，若搭建网络的时候已经设置了正则化参数，则调用以下代码可以直接计算出正则损失：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regularization_loss = tf.add_n(tf<span class="selector-class">.losses</span><span class="selector-class">.get_regularization_losses</span>(scope=<span class="string">'my_resnet_50'</span>))</span><br></pre></td></tr></table></figure><p>如果没有设置初始化参数，也可以使用以下代码计算正则损失：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l2_loss = tf.add_n([tf<span class="selector-class">.nn</span><span class="selector-class">.l2_loss</span>(var) <span class="keyword">for</span> <span class="selector-tag">var</span> <span class="keyword">in</span> tf.trainable_variables() <span class="keyword">if</span> <span class="string">'weights'</span> <span class="keyword">in</span> <span class="selector-tag">var</span>.name])</span><br></pre></td></tr></table></figure><p>正则化是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式： </p><p>其中，是指正则项系数，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注：事实上，在深度神经网络中，往往是梯度消失出现的更多一些。</span><br></pre></td></tr></table></figure><h3 id="2-3-方案3-relu、leakrelu、elu等激活函数"><a href="#2-3-方案3-relu、leakrelu、elu等激活函数" class="headerlink" title="2.3 方案3-relu、leakrelu、elu等激活函数"></a>2.3 方案3-relu、leakrelu、elu等激活函数</h3><p><strong>Relu:</strong>思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。先看一下relu的数学表达式：</p><p><img src="https://img-blog.csdn.net/20171220115642365?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>其函数图像：</p><p><img src="https://img-blog.csdn.net/20171220115719332?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>从上图中，我们可以很容易看出，relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。</p><p><strong>relu</strong>的主要贡献在于：</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 解决了梯度消失、爆炸的问题</span></span><br><span class="line"><span class="comment">-- 计算方便，计算速度快</span></span><br><span class="line"><span class="comment">-- 加速了网络的训练</span></span><br></pre></td></tr></table></figure><p>同时也存在一些<strong>缺点</strong>：</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）</span></span><br><span class="line"> <span class="comment">-- 输出不是以0为中心的</span></span><br></pre></td></tr></table></figure><p>尽管relu也有缺点，但是仍然是目前使用最多的激活函数</p><p><strong>leakrelu</strong><br>leakrelu就是为了解决relu的0区间带来的影响，其数学表达为：其中k是leak系数，一般选择0.01或者0.02，或者通过学习而来</p><p><img src="https://img-blog.csdn.net/20170702211001517?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2FpY2FpYXRuYnU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>leakrelu解决了0区间带来的影响，而且包含了relu的所有优点<br><strong>elu</strong><br>elu激活函数也是为了解决relu的0区间带来的影响，其数学表达为：<img src="https://img-blog.csdn.net/20171220134603079?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>其函数及其导数数学形式为：</p><p><img src="https://img-blog.csdn.net/20171220134614121?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>但是elu相对于leakrelu来说，计算要更耗时间一些</p><h3 id="2-4-解决方案4-batchnorm"><a href="#2-4-解决方案4-batchnorm" class="headerlink" title="2.4 解决方案4-batchnorm"></a>2.4 解决方案4-batchnorm</h3><p><strong>Batchnorm</strong>是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。batchnorm全名是batch normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。<br>具体的batchnorm原理非常复杂，在这里不做详细展开，此部分大概讲一下batchnorm解决梯度的问题上。具体来说就是反向传播中，经过每一层的梯度会乘以该层的权重，举个简单例子：<br>正向传播中，那么反向传播中，，反向传播式子中有的存在，所以的大小影响了梯度的消失和爆炸，batchnorm就是通过对每一层的输出规范为均值和方差一致的方法，消除了带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。<br>有关batch norm详细的内容可以参考我的另一篇博客：<br><a href="http://blog.csdn.net/qq_25737169/article/details/79048516" target="_blank" rel="noopener">http://blog.csdn.net/qq_25737169/article/details/79048516</a></p><h3 id="2-5-解决方案5-残差结构"><a href="#2-5-解决方案5-残差结构" class="headerlink" title="2.5 解决方案5-残差结构"></a>2.5 解决方案5-残差结构</h3><p><strong>残差结构</strong>说起残差的话，不得不提这篇论文了：Deep Residual Learning for Image Recognition，关于这篇论文的解读，可以参考知乎链接：<a href="https://zhuanlan.zhihu.com/p/31852747" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31852747</a>这里只简单介绍残差如何解决梯度的问题。<br>事实上，就是残差网络的出现导致了image net比赛的终结，自从残差提出后，几乎所有的深度网络都离不开残差的身影，相比较之前的几层，几十层的深度网络，在残差网络面前都不值一提，残差可以很轻松的构建几百层，一千多层的网络而不用担心梯度消失过快的问题，原因就在于残差的捷径（shortcut）部分，其中残差单元如下图所示：<br><img src="https://img-blog.csdn.net/20171220144105760?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU3MzcxNjk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>相比较于以前网络的直来直去结构，残差中有很多这样的跨层连接结构，这样的结构在反向传播中具有很大的好处，见下式：<br><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%20loss%7D%7B%5Cpartial%20%7B%7Bx%7D_%7Bl%7D%7D%7D=%5Cfrac%7B%5Cpartial%20loss%7D%7B%5Cpartial%20%7B%7Bx%7D_%7BL%7D%7D%7D%5Ccdot%20%5Cfrac%7B%5Cpartial%20%7B%7Bx%7D_%7BL%7D%7D%7D%7B%5Cpartial%20%7B%7Bx%7D_%7Bl%7D%7D%7D=%5Cfrac%7B%5Cpartial%20loss%7D%7B%5Cpartial%20%7B%7Bx%7D_%7BL%7D%7D%7D%5Ccdot%20%5Cleft%28%201%2B%5Cfrac%7B%5Cpartial%20%7D%7B%5Cpartial%20%7B%7Bx%7D_%7BL%7D%7D%7D%5Csum%5Climits_%7Bi=l%7D%5E%7BL-1%7D%7BF%28%7B%7Bx%7D_%7Bi%7D%7D,%7B%7BW%7D_%7Bi%7D%7D%29%7D%20%5Cright%29" alt="这里写图片描述"><br>式子的第一个因子  表示的损失函数到达 L 的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注：上面的推导并不是严格的证明。</span><br></pre></td></tr></table></figure><h3 id="2-6-解决方案6-LSTM"><a href="#2-6-解决方案6-LSTM" class="headerlink" title="2.6 解决方案6-LSTM"></a>2.6 解决方案6-LSTM</h3><p><strong>LSTM</strong>全称是长短期记忆网络（long-short term memory networks），是不那么容易发生梯度消失的，主要原因在于LSTM内部复杂的“门”(gates)，如下图，LSTM通过它内部的“门”可以接下来更新的时候“记住”前几次训练的”残留记忆“，因此，经常用于生成文本中。目前也有基于CNN的LSTM，感兴趣的可以尝试一下。</p><p><img src="http://upload-images.jianshu.io/upload_images/42741-b9a16a53d58ca2b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt="这里写图片描述"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料:"></a>参考资料:</h2><p>1.《Neural networks and deep learning》<br>2.<a href="https://www.baidu.com/s?wd=%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd" target="_blank" rel="noopener">《机器学习》</a>周志华 </p><ol><li><p><a href="https://www.cnblogs.com/willnote/p/6912798.html&gt;" target="_blank" rel="noopener">https://www.cnblogs.com/willnote/p/6912798.html&gt;</a> </p></li><li><p><a href="https://www.zhihu.com/question/38102762" target="_blank" rel="noopener">https://www.zhihu.com/question/38102762</a> </p><ol><li><a href="http://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="noopener">http://www.jianshu.com/p/9dc9f41f0b29</a></li></ol></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>python: list vs tuple</title>
      <link href="2019/03/20/python-list-vs-tuple/"/>
      <url>2019/03/20/python-list-vs-tuple/</url>
      
        <content type="html"><![CDATA[<ul><li><p><a href="https://shiyaya.github.io/2019/03/12/python%E5%9F%BA%E7%A1%80%EF%BC%9A-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-python-%E4%B8%AD%E7%9A%84%E8%B5%8B%E5%80%BC%E3%80%81%E5%BC%95%E7%94%A8%E3%80%81%E6%8B%B7%E8%B4%9D%E3%80%81%E4%BD%9C%E7%94%A8%E5%9F%9F/" target="_blank" rel="noopener">shiyaya.github.io-python基础： 深入理解 python 中的赋值、引用、拷贝、作用域</a></p></li><li><p><a href="https://data-flair.training/blogs/python-tuples-vs-lists/" target="_blank" rel="noopener">https://data-flair.training/blogs/python-tuples-vs-lists/</a>  </p></li></ul><div class="table-container"><table><thead><tr><th>list</th><th>tuple</th></tr></thead><tbody><tr><td>可变对象</td><td>不可变对象</td></tr><tr><td>参数传递是传递的是引用</td><td>参数传递是传递的是值</td></tr><tr><td></td><td></td></tr><tr><td>可以修改某个元素的值</td><td>不可以修改某个元素的值，即不可以按索引来修改元素值</td></tr><tr><td>a= [1,2,3]&lt;/br&gt;b=a&lt;/br&gt;b[0]=8&lt;/br&gt;print(a) #a=[8,2,3]</td><td>略</td></tr><tr><td>可以修改slice&lt;/br&gt;del a[0:2]</td><td>不可以修改slice&lt;/br&gt;del a[0:2]#会提示错误</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习知识点</title>
      <link href="2019/03/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
      <url>2019/03/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<ul><li><p>讲一下正则化，L1和L2正则化各自的特点和适用场景。<br>答：L1用来获得稀疏化特征；L2用来防止过拟合。L1让一部分特征的系数缩小到0，从而间接实现特征选择，用于特征间有关联的场合；L2让所有的特征系数都减小，但不会减为0，会使优化求解稳定快速。</p></li><li><p>防止过拟合的方法：<br>（1）早停，使用验证集，当验证集的损失下降，但是训练集的损失仍在上升时，则停止训练<br>（2）加入正则化项，L1、L2</p></li><li><p>分类问题有哪些评价指标？每种的适用场景<br>Precision  精确率，在所有预测为正样本的样本(TP+FP)中预测正确(TP)的比例，也就是：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1asfgshjuj307c01d3yb.jpg"><br>适用于：检索出的信息有多少是用户感兴趣的<br>Recall  召回率，在所有正样本(TP+FN)中，预测正确(TP)的比例，也就是：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1asfv7ikpj306d019t8i.jpg"><br>适用于：用户感兴趣的信息有多少被检索出来了<br>Accuracy  准确率，正确分类的样本占所有样本的比例，不适于数据极度不平衡的场景如广告点击率一般在千分之几。<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1asg2ggqgj30ab01eglg.jpg"><br>适用于多分类问题<br>F1-measure  F1分数，是综合考虑Precision和Recall得到的一个指标，一般在需要PR都要保证的场景使用，针对一个值的优化更加直观容易衡量<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1asgb0qo9j304701dwe9.jpg"></p></li><li><p>逻辑回归可以处理非线性问题吗<br>只用原始特征不能；对特征做非线性变换，比如kernel，当然可以。 但那就不是lr了 或者一个神经网络 最后一层看成是lr 前面看成是提特征<br>lr的应用场景主要是特征很多的情况下 比如特征是上亿维的一些场景</p></li><li><p>讲一下pooling的作用， 为什么max pooling要更常用？哪些情况下，average pooling比max pooling更合适？<br>（1）保证特征的位置与旋转不变性。对于图像处理这种特性是很好的，但是对于NLP来说特征出现的位置是很重要的。比如主语一般出现在句子头等等<br>maxpooling提供了一定position的invariance，当图像某些区域像素变化时，maxpooling得到的output并不会变<br>（2）减少模型参数数量，减少过拟合问题。2D或1D的数组转化为单一数值，对于后续的convolution层或者全连接隐层来说，减少了单个Filter参数或隐层神经元个数<br>（3）可以把变长的输入x整理成固定长度的输入。CNN往往最后连接全连接层，神经元个数需要固定好，但是cnn输入x长度不确定，通过pooling操作，每个filter固定取一个值。有多少个Filter，Pooling就有多少个神经元，这样就可以把全连接层神经元固定住<br>（4）yaya: pooling 一般是对缩小image size，从而可以减小后续步骤中的参数量<br>max-pooling还提供了非线性, 这是max-pooling效果更好的一个重要原因.<br><strong>average pooling比max pooling更合适：</strong>有的时候在模型接近分类器的末端使用全局平均池化还可以代替Flatten操作，使输入数据变成一位向量。</p></li><li><p>1x1的卷积核有什么作用？<br>1*1的卷积核在NIN、Googlenet中被广泛使用，作用：<br>（1）实现跨通道的交互和信息整合<br>（2）进行卷积核通道数的降维和升维<br>（3）对于单通道feature map 用单核卷积即为乘以一个参数，而一般情况都是多核卷积多通道，实现多个feature map的线性组合</p></li><li><p>梯度消失和梯度爆炸的原因是什么？ 有哪些解决方法？<br>转载：<a href="https://blog.csdn.net/raojunyang/article/details/79962665（解释的很清楚）" target="_blank" rel="noopener">https://blog.csdn.net/raojunyang/article/details/79962665（解释的很清楚）</a><br>梯度消失更容易发生，当网络较深或者使用了不合适的激活函数时，会发生梯度消失；当为深层网络且权值初始化值太大时，容易发生梯度爆炸<br>yaya: 由于深层网络，在底层的网络权重的更新，需要高层网络权重的连乘，因此，当高层网络权重较小时，使得发生梯度消失，相反，当权重较大时，则发生梯度爆炸。<br>那么什么时候高层网络权重小—-当使用sigmoid/tanh这样的激活函数时，因为sigmoid的导数最大为1/2；什么时候高层网络权重大—当初始化的权重较大时<br>如何解决梯度消失与爆炸：（1）使用正确的非线性激活函数（2）对于梯度爆炸问题使用梯度剪切（3）使用batch normalization（4）使用残差结构</p></li><li>CNN和RNN的梯度消失是一样的吗？  </li><li>有哪些防止过拟合的方法？<br>早停；添加正则化项：L1、L2；使用dropout</li><li>讲一下激活函数sigmoid，tanh，relu. Leaky ReLU各自的优点和适用场景？<br>sigmoid，tanh 有梯度消失的问题<br>relu 部分解决梯度消失问题（x&gt;0）<br>leaky relu </li><li>relu的负半轴导数都是0，这部分产生的梯度消失怎么办？  </li><li>batch size对收敛速度的影响。  </li><li>讲一下batch normalization<br>对输入的数据进行mini batch 的归一化</li><li>讲一下你怎么理解dropout，分别从bagging和正则化的角度<br><a href="https://blog.csdn.net/m0_37477175/article/details/77145459" target="_blank" rel="noopener">https://blog.csdn.net/m0_37477175/article/details/77145459</a><br>bagging 都是使用集成学习的思想，但是</li><li>data augmentation有哪些技巧？  </li><li>讲一下你了解的优化方法，sgd, momentum, rmsprop, adam的区别和联系  </li><li>如果训练的神经网络不收敛，可能有哪些原因？<br>可以参见此博文，具体来说，可以简述为以下几点：<br>（1）<strong>没有对数据进行归一化</strong>，即对数据减均值，并除以方差。而大部分神经网络的输入输出都是在0附近的分布。因此无法收敛。<br>（2）<strong>学习率不正确</strong><br>（3）<strong>在输出层使用错误的激活函数</strong>：在最后一层使用激活函数时，无法产生所需全部范围的值。假使你使用Relu这类限制范围的函数，神经网络便只会训练得到正值<br>（4）<strong>没有正确初始化权重</strong></li></ul><p>（1）代码题（leetcode类型），主要考察数据结构和基础算法，以及代码基本功<br>虽然这部分跟机器学习，深度学习关系不大，但也是面试的重中之重。基本每家公司的面试都问了大量的算法题和代码题，即使是商汤、face++这样的深度学习公司，考察这部分的时间也占到了我很多轮面试的60%甚至70%以上。我去face++面试的时候，面试官是residual net，shuffle net的作者；但他们的面试中，写代码题依旧是主要的部分。<br>大部分题目都不难，基本是leetcode medium的难度。但是要求在现场白板编程，思路要流畅，能做到一次性Bug-free. 并且，一般都是要给出时间复杂度和空间复杂度最优的做法。对于少数难度很大的题，也不要慌张。一般也不会一点思路也没有，尽力给面试官展现自己的思考过程。面试官也会引导你，给一点小提示，沿着提示把题目慢慢做出来也是可以通过面试的。<br>以下是我所遇到的一些需要当场写出完整代码的题目：  </p><p><1> 二分查找。分别实现C++中的lower_bound和upper_bound.  </1></p><p><2> 排序。 手写快速排序，归并排序，堆排序都被问到过。  </2></p><p><3> 给你一个数组，求这个数组的最大子段积<br>时间复杂度可以到O(n)  </3></p><p><4> 给你一个数组，在这个数组中找出不重合的两段，让这两段的字段和的差的绝对值最小。<br>时间复杂度可以到O(n)  </4></p><p><5> 给你一个数组，求一个k值，使得前k个数的方差 + 后面n-k个数的方差最小<br>时间复杂度可以到O(n)  </5></p><p><6> 给你一个只由0和1组成的字符串，找一个最长的子串，要求这个子串里面0和1的数目相等。<br>时间复杂度可以到O(n)  </6></p><p><7> 给你一个数组以及一个数K， 从这个数组里面选择三个数，使得三个数的和小于等于K， 问有多少种选择的方法？<br>时间复杂度可以到O(n^2)  </7></p><p><8> 给你一个只由0和1组成的矩阵，找出一个最大的子矩阵，要求这个子矩阵是方阵，并且这个子矩阵的所有元素为1<br>时间复杂度可以到O(n^2)  </8></p><p><9> 求一个字符串的最长回文子串<br>时间复杂度可以到O(n) (Manacher算法)  </9></p><p><10> 在一个数轴上移动，初始在0点，现在要到给定的某一个x点， 每一步有三种选择，坐标加1，坐标减1，坐标乘以2，请问最少需要多少步从0点到x点。  </10></p><p><11> 给你一个集合，输出这个集合的所有子集。  </11></p><p><12> 给你一个长度为n的数组，以及一个k值（k &lt; n) 求出这个数组中每k个相邻元素里面的最大值。其实也就是一个一维的max pooling<br>时间复杂度可以到O(n)  </12></p><p><13> 写一个程序，在单位球面上随机取点，也就是说保证随机取到的点是均匀的。  </13></p><p><14> 给你一个长度为n的字符串s，以及m个短串（每个短串的长度小于10）， 每个字符串都是基因序列，也就是说只含有A,T,C,G这四个字母。在字符串中找出所有可以和任何一个短串模糊匹配的子串。模糊匹配的定义，两个字符串长度相等，并且至多有两个字符不一样，那么我们就可以说这两个字符串是模糊匹配的。  </14></p><p><15> 其它一些描述很复杂的题这里就不列了。</15></p><p>（2）数学题或者”智力”题。<br>不会涉及特别高深的数学知识，一般就是工科数学（微积分，概率论，线性代数）和一些组合数学的问题。<br>下面是我在面试中被问到过的问题：  </p><p><1> 如果一个女生说她集齐了十二个星座的前男友，她前男友数量的期望是多少？<br>ps：这道题在知乎上有广泛的讨论，作为知乎重度用户我也看到过。如果一个女生说，她集齐了十二个星座的前男友，我们应该如何估计她前男友的数量？  </1></p><p><2> 两个人玩游戏。有n堆石头，每堆分别有a1, a2, a3…. an个石头，每次一个游戏者可以从任意一堆石头里拿走至少一个石头，也可以整堆拿走，但不能从多堆石头里面拿。无法拿石头的游戏者输，请问这个游戏是否有先手必胜或者后手必胜的策略？ 如果有，请说出这个策略，并证明这个策略能保证必胜。  </2></p><p><3> 一个一维数轴，起始点在原点。每次向左或者向右走一步，概率都是0.5. 请问回到原点的步数期望是多少？  </3></p><p><4> 一条长度为1的线段，随机剪两刀，求有一根大于0.5的概率。  </4></p><p><5> 讲一下你理解的矩阵的秩。低秩矩阵有什么特点？ 在图像处理领域，这些特点有什么应用？  </5></p><p><6> 讲一下你理解的特征值和特征向量。  </6></p><p><7> 为什么负梯度方向是使函数值下降最快的方向？简单数学推导一下</7></p><p>（3）机器学习基础<br>这部分建议参考周志华老师的《机器学习》。<br>下面是我在面试中被问到过的问题：  </p><p><1> 逻辑回归和线性回归对比有什么优点？  </1></p><p><2> 逻辑回归可以处理非线性问题吗？  </2></p><p><3> 分类问题有哪些评价指标？每种的适用场景。  </3></p><p><4> 讲一下正则化，L1和L2正则化各自的特点和适用场景。  </4></p><p><5> 讲一下常用的损失函数以及各自的适用场景。  </5></p><p><6> 讲一下决策树和随机森林  </6></p><p><7> 讲一下GBDT的细节，写出GBDT的目标函数。 GBDT和Adaboost的区别与联系  </7></p><p><8> 手推softmax loss公式  </8></p><p><9> 讲一下SVM, SVM与LR有什么联系。  </9></p><p><10>讲一下PCA的步骤。PCA和SVD的区别和联系  </10></p><p><11> 讲一下ensemble  </11></p><p><12> 偏差和方差的区别。ensemble的方法中哪些是降低偏差，哪些是降低方差？<br>…… 这部分问得太琐碎了，我能记起来的问题就这么多了。我的感觉，这部分问题大多数不是问得很深，所以不至于被问得哑口无言，总有得扯；但是要想给出一个特别深刻的回答，还是需要对机器学习的基础算法了解比较透彻。</12></p><p>（4）深度学习基础<br>这部分的准备，我推荐花书（Bengio的Deep learning）和 @魏秀参 学长的《解析卷积神经网络-深度学习实践手册》<br>下面是我在面试中被问到过的问题：  </p><p><1> 手推BP  </1></p><p><2> 手推RNN和LSTM结构  </2></p><p><3> LSTM中每个gate的作用是什么，为什么跟RNN比起来，LSTM可以防止梯度消失  </3></p><p><4> 讲一下pooling的作用， 为什么max pooling要更常用？哪些情况下，average pooling比max pooling更合适？  </4></p><p><5> 梯度消失和梯度爆炸的原因是什么？ 有哪些解决方法？  </5></p><p><6> CNN和RNN的梯度消失是一样的吗？  </6></p><p><6> 有哪些防止过拟合的方法？  </6></p><p><7> 讲一下激活函数sigmoid，tanh，relu. 各自的优点和适用场景？  </7></p><p><8> relu的负半轴导数都是0，这部分产生的梯度消失怎么办？  </8></p><p><9> batch size对收敛速度的影响。  </9></p><p><10> 讲一下batch normalization  </10></p><p><11> CNN做卷积运算的复杂度。如果一个CNN网络的输入channel数目和卷积核数目都减半，总的计算量变为原来的多少？  </11></p><p><12> 讲一下AlexNet的具体结构，每层的作用  </12></p><p><13> 讲一下你怎么理解dropout，分别从bagging和正则化的角度  </13></p><p><14> data augmentation有哪些技巧？  </14></p><p><15> 讲一下你了解的优化方法，sgd, momentum, rmsprop, adam的区别和联系  </15></p><p><16> 如果训练的神经网络不收敛，可能有哪些原因？  </16></p><p><17> 说一下你理解的卷积核， 1x1的卷积核有什么作用？<br>……..<br>同上，这部分的很多问题也是每个人都或多或少能回答一点，但要答得很好还是需要功底的。</17></p><p>（5）科研上的开放性问题<br>这部分的问题没有固定答案，也没法很好地针对性准备。功在平时，多读paper多思考，注意培养自己的insight和intuition<br>下面是我在面试中被问到过的问题：  </p><p><1> 选一个计算机视觉、深度学习、机器学习的子领域，讲一下这个领域的发展脉络，重点讲出各种新方法提出时的motivation，以及谈谈这个领域以后会怎么发展。  </1></p><p><2> 讲一下你最近看的印象比较深的paper  </2></p><p><3> 讲一下经典的几种网络结构， AlexNet， VGG，GoogleNet， Residual Net等等，它们各自最重要的contribution  </3></p><p><4> 你看过最近很火的XXX paper吗? 你对这个有什么看法？<br>……<br>（6） 编程语言、操作系统等方面的一些问题。<br>C++， Python， 操作系统，Linux命令等等。这部分问得比较少，但还是有的，不具体列了<br>（7）针对简历里项目/论文 / 实习的一些问题。<br>这部分因人而异，我个人的对大家也没参考价值，也不列了。</4></p><p>作者：wendy_要努力努力再努力<br>链接：<a href="https://www.jianshu.com/p/d40fc51874c8" target="_blank" rel="noopener">https://www.jianshu.com/p/d40fc51874c8</a><br>来源：简书<br>简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unexpected key(s) in state_dict: “**module**.features.conv1.0.weight”</title>
      <link href="2019/03/20/Unexpected-key-s-in-state-dict-%E2%80%9C-module-features-conv1-0-weight%E2%80%9D/"/>
      <url>2019/03/20/Unexpected-key-s-in-state-dict-%E2%80%9C-module-features-conv1-0-weight%E2%80%9D/</url>
      
        <content type="html"><![CDATA[<ul><li><p>参考此处<a href="https://discuss.pytorch.org/t/when-loading-a-model-unexpected-key-s-in-state-dict-module-features-conv1-0-weight/20505" target="_blank" rel="noopener">[link]</a></p></li><li><p>问题描述：在使用pytorch 加载预训练的模型时:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decoder.load_state_dict(checkpoint[<span class="string">'dec'</span>])</span><br></pre></td></tr></table></figure></li></ul><p>出现错误：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">Missing</span> <span class="selector-tag">key</span>(<span class="selector-tag">s</span>) <span class="selector-tag">in</span> <span class="selector-tag">state_dict</span>: “<span class="selector-tag">features</span><span class="selector-class">.conv1</span><span class="selector-class">.0</span><span class="selector-class">.weight</span>”,</span><br><span class="line"><span class="selector-tag">Unexpected</span> <span class="selector-tag">key</span>(<span class="selector-tag">s</span>) <span class="selector-tag">in</span> <span class="selector-tag">state_dict</span>: “**<span class="selector-tag">module</span>**<span class="selector-class">.features</span><span class="selector-class">.conv1</span><span class="selector-class">.0</span><span class="selector-class">.weight</span>”,</span><br></pre></td></tr></table></figure></p><ul><li>原因：<br>在训练阶段，使用的是多GPU，采用了nn.DataParallel，因此在测试阶段，对应的模型也需要是多GPU的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机视觉学术会议</title>
      <link href="2019/03/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/"/>
      <url>2019/03/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/</url>
      
        <content type="html"><![CDATA[<h3 id="A类"><a href="#A类" class="headerlink" title="A类"></a>A类</h3><ul><li>(AAAI)   Conference on Artificial Intelligence  </li></ul><p>截稿日期：2019-8-30</p><ul><li>CVPR 2019: IEEE Conference on Computer Vision and Pattern Recognition    </li></ul><p><a href="http://cvpr2019.thecvf.com/" target="_blank" rel="noopener">http://cvpr2019.thecvf.com/</a> </p><p>截稿日期：2018-11-16<br>通知日期：2019-03-02<br>会议日期：2019-06-15</p><ul><li>IJCAI 2019: International Joint Conference on Artificial Intelligence</li></ul><p><a href="http://www.ijcai19.org" target="_blank" rel="noopener">http://www.ijcai19.org</a></p><p>截稿日期：2019-02-05<br>会议日期：Aug 10 - Aug 16, 2019</p><ul><li>ICCV2019: International Conference on Computer Vision</li></ul><p><a href="http://iccv2019.thecvf.com" target="_blank" rel="noopener">http://iccv2019.thecvf.com</a></p><p>截稿日期：2019-05-01<br>会议日期：Oct 27 - Nov 3, 2019</p><ul><li>ECCV</li></ul><p>截稿时间：3 月 14 日<br>会议时间：9 月 8-14 日</p><ul><li>ACM International Conference on Multimedia (ACM MM) </li></ul><p><a href="https://www.acmmm.org/2019/" target="_blank" rel="noopener">https://www.acmmm.org/2019/</a></p><p>截稿日期：2019.4.1</p><h3 id="B类"><a href="#B类" class="headerlink" title="B类"></a>B类</h3><ul><li>ICME 2019: International Conference on Multimedia and Expo</li></ul><p><a href="http://www.icme2019.org" target="_blank" rel="noopener">http://www.icme2019.org</a> </p><p>截稿日期：2018-12-03<br>通知日期：2019-03-11<br>会议日期：2019-07-08</p><h3 id="C类"><a href="#C类" class="headerlink" title="C类"></a>C类</h3><ul><li>BMVC</li></ul><p><a href="http://bmvc2018.org" target="_blank" rel="noopener">http://bmvc2018.org</a></p><p>截稿时间：4 月 30 日</p><ul><li>ICIP</li></ul><p>截稿时间：3 月 2 日</p>]]></content>
      
      
      <categories>
          
          <category> 杂类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>非极大值抑制(NMS)</title>
      <link href="2019/03/20/%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6-NMS/"/>
      <url>2019/03/20/%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6-NMS/</url>
      
        <content type="html"><![CDATA[<p>非极大值抑制（Non-maximum suppression，NMS）是一种去除非极大值的算法，常用于计算机视觉中的边缘检测、物体识别等。</p><h2 id="算法流程："><a href="#算法流程：" class="headerlink" title="算法流程："></a>算法流程：</h2><p>给出一张图片和上面许多物体检测的候选框（即每个框可能都代表某种物体），但是这些框很可能有互相重叠的部分，我们要做的就是只保留最优的框。假设有N个框，每个框被分类器计算得到的分数为Si, 1&lt;=i&lt;=N。</p><p>0、建造一个存放待处理候选框的集合H，初始化为包含全部N个框；</p><p>建造一个存放最优框的集合M，初始化为空集。</p><p>1、将所有集合 H 中的框进行排序，选出分数最高的框 m，从集合 H 移到集合 M；</p><p>2、遍历集合 H 中的框，分别与框 m 计算交并比（Interection-over-union，IoU），如果高于某个阈值（一般为0~0.5），则认为此框与 m 重叠，将此框从集合 H 中去除。</p><p>3、回到第1步进行迭代，直到集合 H 为空。集合 M 中的框为我们所需。</p><h2 id="需要优化的参数："><a href="#需要优化的参数：" class="headerlink" title="需要优化的参数："></a>需要优化的参数：</h2><p>IoU 的阈值是一个可优化的参数，一般范围为0~0.5，可以使用交叉验证来选择最优的参数。<br>比如人脸识别的一个例子：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g19dbcocjfj30gw07kdmg.jpg"></p><p>已经识别出了 5 个候选框，但是我们只需要最后保留两个人脸。</p><p>首先选出分数最大的框（0.98），然后遍历剩余框，计算 IoU，会发现露丝脸上的两个绿框都和 0.98 的框重叠率很大，都要去除。</p><p>然后只剩下杰克脸上两个框，选出最大框（0.81），然后遍历剩余框（只剩下0.67这一个了），发现0.67这个框与 0.81 的 IoU 也很大，去除。</p><p>至此所有框处理完毕，算法结果：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g19dbr3kk4j30gw07kdmi.jpg"></p><p>（图片来自<a href="https://blog.csdn.net/shuzfan/article/details/52711706）" target="_blank" rel="noopener">https://blog.csdn.net/shuzfan/article/details/52711706）</a></p><h2 id="添加-by-yaya"><a href="#添加-by-yaya" class="headerlink" title="添加 by yaya:"></a>添加 by yaya:</h2><ul><li>在faster r-cnn中，得到了pred_boxes以及cls_boxes 之后，分别对每个类的objects进行NMS。（这里多说一句：使用了class_agnostic=false，即对每个bbox都有N个类别的得分）</li><li>首先得到得分最高的一个object bbox，之后，进行IOU分析，若IoU大于阈值，则剔除，否则保留。</li><li>对这一个得分最高的bbox分析完之后，再分析下一个次高得分的，并剔除所有与它IoU值大于阈值的object。一直这样分析，直到剩下的object之间的IoU值两两之间均小于阈值。</li><li>即可得到该类对应的bbox，且不交叠。</li><li>下一循环分析下一个类<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> xrange(<span class="number">1</span>, imdb.num_classes):</span><br><span class="line">    inds = torch.nonzero(scores[:,j]&gt;thresh).view(<span class="number">-1</span>) </span><br><span class="line">    <span class="comment"># thresh = 0   inds.shape = torch.Size([300])</span></span><br><span class="line">    <span class="comment"># if there is det</span></span><br><span class="line">    <span class="keyword">if</span> inds.numel() &gt; <span class="number">0</span>:</span><br><span class="line">      cls_scores = scores[:,j][inds] <span class="comment"># 某个类在300个object上的得分</span></span><br><span class="line">      _, order = torch.sort(cls_scores, <span class="number">0</span>, <span class="literal">True</span>) <span class="comment"># 某个类在这300个object上的得分的高低排序</span></span><br><span class="line">      <span class="keyword">if</span> args.class_agnostic:</span><br><span class="line">        cls_boxes = pred_boxes[inds, :]</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        cls_boxes = pred_boxes[inds][:, j * <span class="number">4</span>:(j + <span class="number">1</span>) * <span class="number">4</span>]  <span class="comment"># 某个类对应的predict bbox</span></span><br><span class="line">      </span><br><span class="line">      cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(<span class="number">1</span>)), <span class="number">1</span>)  <span class="comment"># torch.Size([300, 5])</span></span><br><span class="line">      <span class="comment"># cls_dets = torch.cat((cls_boxes, cls_scores), 1)</span></span><br><span class="line">      cls_dets = cls_dets[order]  <span class="comment"># torch.Size([300, 5]) 排了序之后的cat</span></span><br><span class="line">      keep = nms(cls_dets, cfg.TEST.NMS)  <span class="comment"># torch.Size([91, 1])</span></span><br><span class="line">      cls_dets = cls_dets[keep.view(<span class="number">-1</span>).long()]  <span class="comment"># torch.Size([91, 5])</span></span><br><span class="line">      <span class="keyword">if</span> vis:</span><br><span class="line">        im2show = vis_detections(im2show, imdb.classes[j], cls_dets.cpu().numpy(), <span class="number">0.3</span>)</span><br><span class="line">      all_boxes[j][i] = cls_dets.cpu().numpy()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      all_boxes[j][i] = empty_array</span><br></pre></td></tr></table></figure></li></ul><p>作者：HappyRocking<br>来源：CSDN<br>原文：<a href="https://blog.csdn.net/HappyRocking/article/details/79970627" target="_blank" rel="noopener">https://blog.csdn.net/HappyRocking/article/details/79970627</a><br>版权声明：本文为博主原创文章，转载请附上博文链接！</p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CV vs PIL</title>
      <link href="2019/03/20/CV-vs-PIL/"/>
      <url>2019/03/20/CV-vs-PIL/</url>
      
        <content type="html"><![CDATA[<h2 id="PIL"><a href="#PIL" class="headerlink" title="PIL"></a>PIL</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line">pil_image = Image.open(<span class="string">'test.jpg'</span>) <span class="comment"># 图片是360x480 宽x高  </span></span><br><span class="line">print(type(pil_image)) <span class="comment"># out: PIL.JpegImagePlugin.JpegImageFile  </span></span><br><span class="line">print(pil_image.size)  <span class="comment"># out: (360,480) # w,h  </span></span><br><span class="line">print(pil_image.mode) <span class="comment"># out: 'RGB'  </span></span><br><span class="line">  </span><br><span class="line">pil_image = np.array(pil_image,dtype=np.float32) <span class="comment"># image = np.array(image)默认是uint8  </span></span><br><span class="line">print(pil_image.shape) <span class="comment"># out: (480, 360, 3)  </span></span><br><span class="line"><span class="comment"># 神奇的事情发生了，w和h换了，变成(h,w,c)了  </span></span><br><span class="line"><span class="comment"># 注意ndarray中是 行row x 列col x 维度dim 所以行数是高，列数是宽</span></span><br></pre></td></tr></table></figure><blockquote><pre><code>输出结果：&lt;class &#39;PIL.JpegImagePlugin.JpegImageFile&#39;&gt;(360, 480)RGB(480, 360, 3)</code></pre></blockquote><p>这里截图在pycharm下调试的信息（未转化成numpy array之前）</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g193y86ko4j30lt0ixmy8.jpg">  </p><blockquote><p>可以看到PIL.Image读出的image格式为（w,h,c）且image.mode = ‘RGB’<br>并且由代码的注释可以看到，当PIL.Image转化成numpy.array格式之后，image.size将转为（h,w,c）,c 仍为“RGB”</p></blockquote><h2 id="CV"><a href="#CV" class="headerlink" title="CV"></a>CV</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line">cv_image = cv2.imread(<span class="string">'test.jpg'</span>)  </span><br><span class="line">print(type(cv_image)) <span class="comment"># out: numpy.ndarray  </span></span><br><span class="line">print(cv_image.dtype) <span class="comment"># out: dtype('uint8')  </span></span><br><span class="line">print(cv_image.shape) <span class="comment"># out: (360,480, 3) (h,w,c) 和skimage类似  </span></span><br><span class="line"><span class="comment"># print(image) # BGR</span></span><br></pre></td></tr></table></figure><h2 id="为了比较PIL-和-CV"><a href="#为了比较PIL-和-CV" class="headerlink" title="为了比较PIL 和 CV"></a>为了比较PIL 和 CV</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(cv_image == pil_image)</span><br></pre></td></tr></table></figure><p>可以看到 分别是 False True False<br><strong>原因是PIL提取的是“RGB”，而CV提取的是“BGR”</strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g194ap1lw4j306d07zmx0.jpg"></p><p>综上，可以凝练为以下几点：</p><ol><li>PIL提取的是PIL.image类型的数据（w,h,c），通道是“RGB”。</li><li>将该数据转为numpy.array之后，得到的是（h,w,c），通道仍是“RGB”。</li><li>CV提取的是numpy.array类型的数据（h,w,c），注意通道是“BGR”。</li></ol><ul><li>再</li><li>在pytorch中输入的image模式是“RGB”</li><li>在caffe中输入的是“BGR”</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pytorch_normalze</span><span class="params">(img)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    https://github.com/pytorch/vision/issues/223</span></span><br><span class="line"><span class="string">    return appr -1~1 RGB</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    normalize = tvtsf.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                                std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    img = normalize(t.from_numpy(img))</span><br><span class="line">    <span class="keyword">return</span> img.numpy()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">caffe_normalize</span><span class="params">(img)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    return appr -125-125 BGR</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    img = img[[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>], :, :]  <span class="comment"># RGB-BGR</span></span><br><span class="line">    img = img * <span class="number">255</span></span><br><span class="line">    mean = np.array([<span class="number">122.7717</span>, <span class="number">115.9465</span>, <span class="number">102.9801</span>]).reshape(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    img = (img - mean).astype(np.float32, copy=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python基础知识-try与except处理异常语句</title>
      <link href="2019/03/19/python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-try%E4%B8%8Eexcept%E5%A4%84%E7%90%86%E5%BC%82%E5%B8%B8%E8%AF%AD%E5%8F%A5/"/>
      <url>2019/03/19/python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-try%E4%B8%8Eexcept%E5%A4%84%E7%90%86%E5%BC%82%E5%B8%B8%E8%AF%AD%E5%8F%A5/</url>
      
        <content type="html"><![CDATA[<h2 id="try-except介绍"><a href="#try-except介绍" class="headerlink" title="try/except介绍"></a>try/except介绍</h2><blockquote><p>与其他语言相同，在python中，try/except语句主要是用于处理程序正常执行过程中出现的一些异常情况，如语法错（python作为脚本语言没有编译的环节，在执行过程中对语法进行检测，出错后发出异常消息）、数据除零错误、从未定义的变量上取值等；而try/finally语句则主要用于在无论是否发生异常情况，都需要执行一些清理工作的场合，如在通信过程中，无论通信是否发生错误，都需要在通信完成或者发生错误时关闭网络连接。尽管<strong>try/except和**</strong>try/finally**的作用不同，但是在编程实践中通常可以把它们组合在一起使用try/except/else/finally的形式来实现稳定性和灵活性更好的设计。</p><p>默认情况下，在程序段的执行过程中，如果没有提供try/except的处理，脚本文件执行过程中所产生的异常消息会自动发送给程序调用端，如python shell，而python shell对异常消息的默认处理则是终止程序的执行并打印具体的出错信息。这也是在python shell中执行程序错误后所出现的出错打印信息的由来。</p></blockquote><h2 id="try-except格式"><a href="#try-except格式" class="headerlink" title="try/except格式"></a>try/except格式</h2><blockquote><p>python中try/except/else/finally语句的完整格式如下所示：</p><p>try:</p><p>​     Normal execution block</p><p>except A:</p><p>​     Exception A handle</p><p>except B:</p><p>​     Exception B handle</p><p>except:</p><p>​     Other exception handle</p><p>else:</p><p>​     if no exception,get here</p><p>finally:</p><p>​     print(“finally”)   </p><p>说明：</p><p>正常执行的程序在try下面的Normal execution block执行块中执行，在执行过程中如果发生了异常，则<strong>中断当前在Normal execution block中的执行</strong>，跳转到对应的异常处理块中开始执行；</p><p>python<strong>从第一个except X处开始查找</strong>，如果找到了对应的exception类型则进入其提供的exception handle中进行处理，如果没有找到则直接进入except块处进行处理。except块是可选项，如果没有提供，该exception将会被提交给python进行默认处理，处理方式则是<strong>终止应用程序并打印提示信息</strong>；</p><p>如果在Normal execution block执行块中执行过程中没有发生任何异常，则在执行完Normal execution block后会进入else执行块中（如果存在的话）执行。</p><p>无论是否发生了异常，只要提供了finally语句，以上try/except/else/finally代码块执行的最后一步总是执行finally所对应的代码块。</p><p>需要注意的是：</p><p>1.在上面所示的完整语句中try/except/else/finally所出现的顺序必须是try—&gt;except X—&gt;except—&gt;else—&gt;finally，即所有的<strong>except必须在else和finally之前</strong>，<strong>else（如果有的话）必须在finally之前</strong>，而<strong>except X必须在except之前</strong>。否则会出现语法错误。</p><p>2.对于上面所展示的try/except完整格式而言，else和finally都是可选的，而不是必须的，但是如果存在的话e<strong>lse必须在finally之前</strong>，<strong>finally</strong>（如果存在的话）<strong>必须在整个语句的最后位置</strong>。</p><p>3.在上面的完整语句中，else语句的存在必须以except X或者except语句为前提，<strong>如果在没有except语句的try block中使用else语句会引发语法错误</strong>。也就是说<strong>else不能与try/finally配合使用</strong>。</p></blockquote><p>4.except的使用要非常小心，慎用。</p><p>class AError(Exception):<br>     “””AError—-exception”””<br>     print(‘AError’)</p><blockquote><p>try:</p><p>​     #raise AError</p><p>​     asdas(‘123’)</p><p>except AError:</p><p>​     print(“Get AError”)</p><p>except:</p><p>​     print(“exception”)     </p><p>else:</p><p>​     print(“else”)</p><p>finally:</p><p>​     print(“finally”)     </p><p>print(“hello wolrd”)</p><p>在上面的代码中，Normal execution block中出现了语法错误，但是由于使用了except语句，该语法错误就被掩盖掉了。因此在使用try/except是最好还是要非常清楚的知道Normal execution block中有可能出现的异常类型以进行针对性的处理。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(SGAE)Auto-Encoding Scene Graphs for Image Captioning</title>
      <link href="2019/03/16/SAGE-Auto-Encoding-Scene-Graphs-for-Image-Captioning/"/>
      <url>2019/03/16/SAGE-Auto-Encoding-Scene-Graphs-for-Image-Captioning/</url>
      
        <content type="html"><![CDATA[<p>本文是CVPR2019 的关于图像描述的文章，主要让我关注的原因是用到了scene graph 和 GCN，这也是本文最大的创新点。但是本文利用的是saptial GCN（悄悄说，构建的graph节点数量少，而且是异质的，不如说是多方面融合信息已达到丰富信息的目的 :-) ）</p><ul><li>后记<br>这里讲一下图卷积中spatial  gcn①②③ 与 spectral gcn ④⑤⑥<br>①Learning task-dependent distributed representations by backpropagation through structure.<br>②A new model for  learning in graph domains<br>③The graph neural network model<br>④Spectral networks and locally connected networks on graphs.<br>⑤Deep convolutional networks on graph-structured data.<br>⑥Semi-supervised classification with graph convolutional networks</li></ul><p>关于这篇论文采用的graph  convolutional network：采用的数</p><ul><li>为什么这样说？<br>本文提到了两类graph，一类是sentence scene graph，另一类是image scene graph，而在这两类下，又进行细分为relationship、attribute、object graph。但是，每个graph 中节点是异质的，比如在relationship graph中，obejct<sub>a</sub>， obejct<sub>b</sub>，relationship<sub>ab</sub>构成了一个graph，目的是来更新relationship embedding。从我的角度来看，只是融合相关信息来更新某一目标的特征，与graph无关。  </li><li>写在最前面，我个人的理解，在得到graph 节点的embedding 之后，就要输入gcn layer 来更新特征，这里，gcn layer 的表达公式可以这样理解： 该graph中所有的节点v<sub>i</sub>，经过concatenate之后，再经过一个全连接层。<br>gcn(.) = fc( concatenate(v1, v2, … , vn) )**</li></ul><p>下面说正文：</p><h1 id="General-encoder-decoder-network-for-image-captioning"><a href="#General-encoder-decoder-network-for-image-captioning" class="headerlink" title="General encoder-decoder network for image captioning"></a>General encoder-decoder network for image captioning</h1><ul><li><p>目前一般的encoder-decoder network for image captioning 是<strong>CNN提取image的特征，然后RNN来生成句子</strong>，例如下图所示。<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14r211sidj30kb0fy0v3.jpg" style="zoom:80%"></p></li><li><p>进一步有<strong>加入attention</strong> [1]，下图是提取14×14×512 feature map of the fourth  convolutional layer，然后 flatten to 196 × 512 before feed into decoder。在输入到decoder时对这196个feature vector进行attention的加权求和。<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14rdh7wg7j33ak1ep7wj.jpg" style="zoom:50%"></p></li><li><p>也有<strong>提取images 中的object，以此来提取显著信息，对object feature 进行attention的加权求和并送入decoder</strong>。具体地，使用RPN 的ROI pooling来提取objects feature，然后对LSTM的每一个step ,对这所有的object features进行attention操作，再作为输入送入LSTM[2]。如下图<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14u74yahwj30q30modsg.jpg" style="zoom:70%"></p></li></ul><h1 id="本文的encoder-decoder-baseline"><a href="#本文的encoder-decoder-baseline" class="headerlink" title="本文的encoder-decoder baseline"></a>本文的encoder-decoder baseline</h1><h2 id="1-Encoder"><a href="#1-Encoder" class="headerlink" title="1. Encoder"></a><strong>1.</strong> <strong>Encoder</strong></h2><p>本文有两个encoder ： image-encoder；sentence-encoder<br>本文sentence-encoder  是用来预训练Dictionary，并共享给 image-encoder。但是在baseline中不用GCN/MGCN 和 Dictionary，因此image-encoder与sentence-encoder 之间是没有交集的。（我猜测 baseline中没有用到sentence-encoder）</p><p><strong>（1）对于image encoder 得到object embedding，relationship embedding , attribute embedding。</strong></p><p>如何得到object embedding，relationship embedding , attribute embedding？</p><ul><li>object detector : 采用与[1]一致的方式来训练faster r-cnn， 0.7的阈值 for proposal NMS， 0.3的阈值for object NMS。Faster R-CNN在visual genome上预训练，预训练之后，对proposals采取0.7的IoU阈值进行NMS，对objects 采取0.3的IoU阈值进行NMS，对每个image，采取10-100个object。使用RoI pooling 来提取object features，该object features 将作为后边relationship classifier 和attribute classifier的输入。</li><li>relationship classifier：使用在[5] 中提到的LSTM结构来作为关系分类器，来为两个object 分配一个relationship label。</li><li>attribute classifier : 为某个object 分配属性标签，将该object feature输入fc-relu-fc-softmax网络，则得到属性标签。</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14vip74y0j311l09eq4y.jpg" style="zoom:60%">      </p><ul><li><p>以上三个输出的所有构成一个image scene graph  </p></li><li><p>需要说明以上三个是在<strong>Visual Genome数据集</strong>上预训练的，该数据集具有丰富的scene graph 标注，含有obejct’s categories，obejct’s attributes and pairwise relationships，因此可以用来训练目标检测器、属性分类器、关系分类器。但是由于这些标注含有很多噪声，因此采用一定的措施进行过滤：对于在数据集中出现超过2000次的objects，attributes，relationships保留下来，其余的去掉。经过这样的处理，则得到305个objects类，103个属性类，64个关系类  </p></li><li><p>经过分类器得到 label 之后，还需要得到相对应的embedding: <strong>u<sub>o</sub> , u<sub>r</sub> , u<sub>a</sub></strong> 。<br>其中 label 的维度472 = 305 + 103 + 64，即object/realation/attribute label的one hot vector 维度是三种节点的总类别数。<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14w80gi8tj30w70ik0xu.jpg" style="zoom:70%">  </p></li></ul><p><strong>（2）对于sentence encoder 使用[6] 来得到 parse scene graph，进而得到object embedding，relationship embedding , attribute embedding</strong><br>注意这里的sentences使用的是<strong>MS COCO</strong>中的caption，而不是Visual Genome中的caption。同样对其进行过滤，过滤掉在all parsed scene graph中出现的objects、attributes、relationships次数少于10的，则剩下5364个objects类，1308个realtionships类，3430个attributes类。<br>sentence encoder 使用[6] 来得到 parse scene graph，但实际上，[6]又是使用[7]中的方法，所以读者最好看[7]<br>这里介绍 一下[6] SPICE 是用来评价image caption的一个性能指标，这里为什么使用它，它是用来做为一个评价指标的吗？首先回答第二个问题，不是用来作为评价指标的，而是利用了它的原理：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1dwj56im1j30iq0nmk00.jpg"></p><p>parse scene graph 的过程：sentence-&gt; syntax dependency tree-&gt; scene graph<br>给定一个句子，首先分析句法依赖树，再根据给定句法依赖树的情况下，输出scene graph，而scene graph 的输出是对sentence中的每一个次元，输出其是object，还是 relation，还是attribute（即，对每个word 输出一个one hot vector （我的猜测））。如下图所示：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1gbldqsamj30q50kntkm.jpg"></p><p>经过parse scene graph得到的是object <strong>label</strong>、relationship <strong>label</strong>、attributes <strong>label</strong>的one hot vector（注意one hot 的长度是 5346+1308+3430 = 10102，即 将三种node合在了一起）。得到label之后，再经过word embedding层即可得到对应的word embedding: <strong>e<sub>o</sub> , e<sub>r</sub> , e<sub>a</sub></strong>。<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14w6j9aouj31670aqdiw.jpg" style="zoom:50%"></p><h2 id="2-Decoder"><a href="#2-Decoder" class="headerlink" title="2.  Decoder"></a><strong>2.</strong>  <strong>Decoder</strong></h2><p>由两层LSTM组成（该部分与论文[1][4] 的decoder部分是完全一致的，只是输入的encoder output不一样而已），下图给出了我自己画的decoder 结构[1]给出的结构[4]中给出的decoder结构</p><ul><li>在[4]中encoder output是GCN输出的object features（两个graph生成的encoder output分别送入decoder）。</li><li>但是在本文中encoder output是 <strong>u<sub>o</sub> , u<sub>r</sub> , u<sub>a</sub></strong> 组合成的d×M 矩阵，或者是 <strong>e<sub>o</sub> , e<sub>r</sub> , e<sub>a</sub></strong> 组合成的d×M 矩阵 。M = num_objects + num_relationships + num_attributes  注意：在sentence-encoder中的输出，这里的M是该句子的parse scene graph实际生成的object、relation、attribute的数量（有可能baseline 中不使用 sentence-encoder）；在image-encoder中的输出，这里也是实际的object detector、relationship classifier、attribute classifier 输出的实际数量总和</li></ul><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14xg3jwvdj30mo0h2wfi.jpg" style="zoom:60%"><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g19d1x7kiij30im0bvaat.jpg" style="zoom:60%"><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14v8kb8kmj30sr0gzwgz.jpg" style="zoom:60%"><br>具体地，这里也给出本文的表达方式如下表:<br>这里的10369是对MS COCO中的captions 进行预处理之后，得到的 len of vocabulary<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14wrwfzplj30ze0hcjwe.jpg" style="zoom:60%"></p><h1 id="在baseline-上加东西"><a href="#在baseline-上加东西" class="headerlink" title="在baseline 上加东西"></a>在baseline 上加东西</h1><ul><li>一般的encoder-decoder如下图中的top所示，本文提出加入MGCN for image和GCN for sentence，同时加入Dictionary（共享参数 betwen sentence and image）<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14yn2vbtej30k40jzac6.jpg" style="zoom:50%"><br>由以上可知，构建了image scene graph 和 sentence scene graph，下面将本文的主要创新点，加入GCN和Dictionary  </li></ul><h2 id="sentence-graph-gt-GCN-更新embedding"><a href="#sentence-graph-gt-GCN-更新embedding" class="headerlink" title="sentence graph -&gt; GCN (更新embedding )"></a>sentence graph -&gt; GCN (更新embedding )</h2><p>经上面的分析构建的sentence scene graph 在得到了object、relation、attribute label 对应的word embedding之后，将通过GCN来更新embedding。</p><ul><li>表中的（7）（8）（9）可以认为是三个relationship、attribute、object graph。<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14y1f5p02j31370e1jw1.jpg" style="zoom:60%"></li><li>解释这里的<strong>g<sub>r</sub> g<sub>a</sub> g<sub>o</sub> g<sub>s</sub></strong><br>以<strong>g<sub>r</sub></strong> 为例：g<sub>r</sub> (D<sub>in</sub>，D<sub>out</sub>）。输入维度为什么是3000？因为e<sub>oi</sub> , e<sub>rij</sub> , e<sub>oj</sub>的维度分别均是1000，论文中省略了将其concatenate的操作的说明，但是实际上是进行了concatenate操作，使得维度变为3000，并作为g<sub>r</sub>的输入。输出是1000维度。</li><li><strong>因此这里图卷积层，可以认为是 该graph中所有的节点v<sub>i</sub>，经过concatenate之后，再经过一个全连接层。<br>g(.) = fc( concatenate(v1, v2, … , vn) )</strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14y594pa8j30og0ksdi2.jpg" style="zoom:50%">  </li></ul><h2 id="image-graph-gt-Multi-modal-GCN-更新embedding"><a href="#image-graph-gt-Multi-modal-GCN-更新embedding" class="headerlink" title="image graph -&gt; Multi-modal GCN (更新embedding )"></a>image graph -&gt; Multi-modal GCN (更新embedding )</h2><p>与sentence graph 对应的GCN类似，这里的 multimodal 也没什么意思，就是特征融合时（9）（10）（11），既使用了label 对应的word embedding，又使用了feature。<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14yf14p1mj30sp0k0ten.jpg">  </p><ul><li>解释这里的<strong>f<sub>r</sub> f<sub>a</sub> f<sub>o</sub> f<sub>s</sub></strong><br>与sentence scene graph 对应的g一致，首先对输入进行了concatenate操作<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14yhc6gpzj314l0gktax.jpg"></li></ul><h2 id="Dictionary"><a href="#Dictionary" class="headerlink" title="Dictionary"></a>Dictionary</h2><ul><li><strong>该部分的作用</strong>是一个memory network。首先Dictionary在<strong>S-&gt;G-&gt;D-&gt;S</strong>上预训练，之后才被用于<strong>I-&gt;G-&gt;D-&gt;S</strong>  。即是参数共享的。而在<strong>S-&gt;G-&gt;D-&gt;S</strong>中，输入的sentence是由human generated。因此Dictionary中就preserve human’s inductive bias。进而与<strong>I-&gt;G-&gt;D-&gt;S</strong>  共享，使得由image 生成的predict caption也含有 human’s inductive bias</li><li>由上文分析可知，经过GCN/MGCN更新的embedding的维度均是1000，则Dictionary的输入维度均是1000，D是可学习参数矩阵1000*10000，输入的x经过Dictionary得到 x^<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14yrlt89sj310c07e40b.jpg">  </li></ul><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>经过encoder 结合GCN/MGCN之后，输出发生了改变，这里再讲一下decoder的输入。</p><ul><li>在<strong>S-&gt;G-&gt;D-&gt;S</strong> 这个序列过程中由D-&gt;S 即输入decoder LSTM的过程，输入的是D的输出。</li><li>而在<strong>I-&gt;G-&gt;D-&gt;S</strong> 这个序列过程中，输入decoder LSTM的过程，输入的是D的输出v’和G的输出v^，即concate[v’, v^]<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g1507wpmlij30zf0h8dku.jpg"></li></ul><h1 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h1><ul><li>首先使用交叉熵损失函数训练 <strong>S-&gt;G-&gt;S</strong>  20 epoch。注意D不参与训练</li><li>使用交叉熵损失函数训练 <strong>S-&gt;G-&gt;D-&gt;S</strong>  20 epoch。这里的D参与训练</li><li>使用交叉熵损失函数训练 <strong>I-&gt;G-&gt;D-&gt;S</strong>  20 epoch。这里的D使用在上一步骤中预训练的参数，并fine-tune</li><li>使用RL-based reward 训练 <strong>I-&gt;G-&gt;D-&gt;S</strong>  40 epoch。 D参与训练  </li></ul><h1 id="推理过程"><a href="#推理过程" class="headerlink" title="推理过程"></a>推理过程</h1><ul><li>文章中没有写，但是我认为是<strong>I-&gt;G-&gt;D-&gt;S</strong>  </li></ul><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><ul><li>由下表可知，实验结果并不突出，尤其是与GCN-LSTM[7]对比可知，GCN-LSTM的结构更加简单的情况下，两个模型的结果却相差不多。</li><li>可以看到表中有三个GCN-LSTM， 最上边那个是本文作者的复现，由于GCN-LSTM的作者batch_size 太大，本文作者觉得对比不公平，因此重新复现了代码（没公开代码）并减小了bs进行实验得到的结果。第二个GCN-LSTM（sem graph）是原作者论文中的实验数据。第三个GCN-LSTM是融合了semantic graph 和saptial graph。</li><li>对于SGAE的融合应该是输入decoder的不仅是<strong>I-&gt;G-&gt;D-&gt;S</strong> 中D的输出v’，而且也输入G的输出 v^，即concate[v’,  v^]<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g14zjng66yj30oc0f0djs.jpg">   </li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] Bottom-up and top-down attention for image captioning and visual question answering<br>[2] Show, Attend and Tell: Neural Image Caption  Generation with Visual Attention<br>[3] Image Captioning with Object Detection and Localization<br>[4] Exploring Visual Relationship  for Image Captioning<br><strong>[5] Neural motifs: Scene graph parsing with global context</strong><br><strong>[6] Spice:  Semantic propositional image caption evaluation</strong><br><strong>[7] Generating Semantically Precise Scene Graphs from Textual Descriptions for Improved Image Retrieval</strong></p>]]></content>
      
      
      <categories>
          
          <category> 图像描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像描述 </tag>
            
            <tag> 图卷积网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Video as Space-Time Region Graphs</title>
      <link href="2019/03/15/Video-as-Space-Time-Region-Graphs/"/>
      <url>2019/03/15/Video-as-Space-Time-Region-Graphs/</url>
      
        <content type="html"><![CDATA[<p>本篇文章主要是讲图卷积网络应用在行为识别任务中。<br>使用的两个数据集是：charades和something-something数据集</p><ul><li>从数据集中video丰富性方面：其中charades数据集含有丰富的室内场景，video中的object较为复杂，也不居中。而，something-something 数据集中的video只含有1~2个object，且位于画面中的中心位置。</li><li>从数据集中video时长：charades的一整个video近30s长，但是annotation是对clips of video进行的标注，clips的分割也不具备规律性，几秒到几十秒不等。something-something数据集的video 时长为3s-6s。均为较短的视频。  </li></ul><p>由以上对数据集的分析，与作者的实验结果，结合，由于something-something的video时长短，objects of video 也较小，因此gcn+i3d 相比于对i3d的提升不大，而相反，charades数据集的提升较大。</p><ul><li>这里给出自己的看法：由于在charades上的提升较为明显，因此使用该网络应用到其他的网络中时，最好可以使用charades数据集进行pre-train，而不要使用something-something数据集。</li></ul><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>在训练阶段，首先对video以6 fps的帧率进行截取帧，输入网络时，每个video选取30帧，这样相当于video中的5s。即在训练阶段，每个5s长的clips作为一个sample，赋给它相对应的label，进行训练。</p><h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>在测试阶段，charades对每个video 提取10个clips ，对每个clips的输出结果，以最大池化的方式进行聚合，对于something-something数据集，每个video提取2个clips，其他同理。</p><h2 id="Construct-Graph"><a href="#Construct-Graph" class="headerlink" title="Construct Graph"></a>Construct Graph</h2><p>对于charades dataset，每帧中提取50个object，对于something-something dataset ，每帧中提取10个object。</p><ul><li>Similarity Graph<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g13qsziabzj30fk0ahjrw.jpg" style="zoom:65%"><br>Similarity graph 含有可学习参数</li><li>Spatial Graph</li><li>无可学习参数</li><li>We denote the IoU between object i in frame t and object  j in frame t + 1 as σ<sub>ij</sub><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g13qv0z3bxj30fq0b30t3.jpg" style="zoom:65%"></li></ul>]]></content>
      
      
      <categories>
          
          <category> 行为识别 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 行为识别 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux-物理CPU和逻辑CPU</title>
      <link href="2019/03/13/linux-%E7%89%A9%E7%90%86CPU%E5%92%8C%E9%80%BB%E8%BE%91CPU/"/>
      <url>2019/03/13/linux-%E7%89%A9%E7%90%86CPU%E5%92%8C%E9%80%BB%E8%BE%91CPU/</url>
      
        <content type="html"><![CDATA[<p>通过cat /proc/cpuinfo 来查看CPU的信息</p><p><img src="https:////upload-images.jianshu.io/upload_images/5262207-4e29a8e7da45169c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/837/format/webp" alt="img"></p><p>cpu.png</p><p>physical id 表示物理CPU的编号<br> CPU cores 表示每个物理CPU上的内核数<br> core id 表示每个内核的编号<br> processor 表示每个逻辑CPU的编号</p><p>逻辑CPU的总数=物理CPU的数量 <em> 每个物理CPU上的核数 </em> 超线程数<br> 如果 逻辑CPU的总数=物理CPU的数量 * 每个物理CPU上的核数 则表示超线程没开，否则表示超线程以开</p><p>作者：君子亮剑</p><p>链接：<a href="https://www.jianshu.com/p/ff8e8be262ac" target="_blank" rel="noopener">https://www.jianshu.com/p/ff8e8be262ac</a></p><p>来源：简书</p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux-查看cpu状态</title>
      <link href="2019/03/13/linux-%E6%9F%A5%E7%9C%8Bcpu%E7%8A%B6%E6%80%81/"/>
      <url>2019/03/13/linux-%E6%9F%A5%E7%9C%8Bcpu%E7%8A%B6%E6%80%81/</url>
      
        <content type="html"><![CDATA[<ul><li>转载 “<a href="https://www.tianmaying.com/tutorial/cpu-top" target="_blank" rel="noopener">https://www.tianmaying.com/tutorial/cpu-top</a>“<br><code>top</code>命令是<code>Linux</code>下常用的性能分析工具，但本质上它提供了实时的对系统处理器的状态监视</li></ul><p>在命令行中输入<code>top</code>将输出一下信息：</p><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">top - 23:16:12 up  7:40, <span class="number"> 1 </span>user,  load average: 0.97, 0.98, 1.01</span><br><span class="line">Tasks:<span class="number"> 440 </span>total,  <span class="number"> 2 </span>running,<span class="number"> 438 </span>sleeping,  <span class="number"> 0 </span>stopped,  <span class="number"> 0 </span>zombie</span><br><span class="line">%Cpu(s):  1.3 us,  1.4 sy,  0.0 ni, 96.9 id,  0.0 wa,  0.0 hi,  0.4 si,  0.0 st</span><br><span class="line">KiB Mem : 13183891+total, 12378241+free, <span class="number"> 3884532 </span>used, <span class="number"> 4171956 </span>buff/cache</span><br><span class="line">KiB Swap:       <span class="number"> 0 </span>total,       <span class="number"> 0 </span>free,       <span class="number"> 0 </span>used. 12719112+avail Mem </span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                               </span><br><span class="line">11746 jenkins  <span class="number"> 20 </span> <span class="number"> 0 </span><span class="number"> 159972 </span> <span class="number"> 4760 </span> <span class="number"> 1600 </span>R  99.7  0.0 362:41.65 root/2                                                                                                                </span><br><span class="line">  <span class="number"> 42 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   4.3  0.0  14:46.50 rcu_sched                                                                                                             </span><br><span class="line">  <span class="number"> 68 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   2.6  0.0   0:55.10 rcuos/25                                                                                                              </span><br><span class="line">11414 jenkins  <span class="number"> 20 </span> <span class="number"> 0 </span>42.134g 1.652g <span class="number"> 24516 </span>S   0.7  1.3   2:37.54 java                                                                                                                  </span><br><span class="line">  <span class="number"> 49 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.3  0.0   0:32.35 rcuos/6                                                                                                               </span><br><span class="line"><span class="number"> 6818 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.3  0.0   0:34.33 kworker/0:1                                                                                                           </span><br><span class="line">14702 root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.3  0.0   0:00.26 kworker/2:0                                                                                                           </span><br><span class="line">15491 txq      <span class="number"> 20 </span> <span class="number"> 0 </span><span class="number"> 158044 </span> <span class="number"> 2616 </span> <span class="number"> 1552 </span>R   0.3  0.0   0:00.13 top                                                                                                                   </span><br><span class="line">   <span class="number"> 1 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span> <span class="number"> 45892 </span> <span class="number"> 8580 </span> <span class="number"> 3908 </span>S   0.0  0.0   0:13.06 systemd                                                                                                               </span><br><span class="line">   <span class="number"> 2 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.06 kthreadd</span><br></pre></td></tr></table></figure><p>前五行是当前整个系统资源的统计信息。</p><p>第一行是任务队列，包括当前时间，系统运行的总时间，系统用户登陆的数量，以及1分钟，5分钟，15分钟系统的负载情况。</p><p>第二行是<code>Tasks</code>信息，显示当前系统总共的进程数为440，运行状态的进程有两个，438个处于休眠状态，0个停止，0个僵尸进程。</p><p>第三行是<code>CPU</code>信息，很多人可能会忽略这些信息，我之前就是，所以详细说一下。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%<span class="selector-tag">Cpu</span>(<span class="selector-tag">s</span>):  1<span class="selector-class">.3</span> <span class="selector-tag">us</span>,  1<span class="selector-class">.4</span> <span class="selector-tag">sy</span>,  0<span class="selector-class">.0</span> <span class="selector-tag">ni</span>, 96<span class="selector-class">.9</span> <span class="selector-tag">id</span>,  0<span class="selector-class">.0</span> <span class="selector-tag">wa</span>,  0<span class="selector-class">.0</span> <span class="selector-tag">hi</span>,  0<span class="selector-class">.4</span> <span class="selector-tag">si</span>,  0<span class="selector-class">.0</span> <span class="selector-tag">st</span></span><br></pre></td></tr></table></figure><p><code>us</code> user CPU time ——用户空间占用<code>CPU</code>百分比</p><p><code>sy</code> system CPU time——内核空间占用<code>CPU</code>百分比</p><p><code>ni</code> nice CPU time——用户进程空间内改变过优先级的进程占用<code>CPU</code>百分比</p><p><code>id</code> idle——空闲<code>CPU</code>百分比</p><p><code>wa</code> iowait—— 等待输入输出的<code>CPU</code>时间百分比</p><p><code>hi</code> hardware irq——硬件中断</p><p><code>si</code> software irq——软件中断</p><p><code>st</code> steal time——实时</p><p>具体对应到第三行的详细信息，大家自己对照一下就行，或者在你命令行中试一下。</p><p>第四行<code>Memory</code>的状态信息，总共13183891+内存，空闲12378241+，使用3884532，缓存为4171956</p><p>第五行<code>Swap</code>交换分区信息，总共0，空闲0，使用0，缓存交换区总量12719112+</p><p>第六行是各个进程监视的项目列</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PID   <span class="built_in"> USER </span>     PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+   COMMAND</span><br><span class="line">PID` — 进程`id</span><br></pre></td></tr></table></figure><p><code>USER</code> — 进程所有者</p><p><code>PR</code> — 进程优先级</p><p><code>NI</code> — nice值。负值表示高优先级，正值表示低优先级</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">VIRT` — 进程使用的虚拟内存总量。`VIRT=SWAP+RES</span><br><span class="line">RES` — 进程使用的、未被换出的物理内存大小。`RES=CODE+DATA</span><br></pre></td></tr></table></figure><p><code>SHR</code> — 共享内存大小</p><p><code>S</code>— 进程状态。<code>D</code>=不可中断的睡眠状态 <code>R</code>=运行 <code>S</code>=睡眠 <code>T</code>=跟踪/停止 <code>Z</code>=僵尸进程</p><p><code>%CPU</code> — 上次更新到现在的<code>CPU</code>时间占用百分比</p><p><code>%MEM</code> — 进程使用的物理内存百分比</p><p><code>TIME+</code> — 进程使用的<code>CPU</code>时间总计</p><p><code>COMMAND</code> — 进程名称（命令名/命令行）</p><p>对应的每个进程的信息，大家可以自己看一下。</p><p>如果你在命令行下再输入<code>1</code>，输出如下：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">top - <span class="number">23</span>:<span class="number">16</span>:<span class="number">12</span> <span class="keyword">up</span>  <span class="number">7</span>:<span class="number">40</span>,  <span class="number">1</span> user,  load average: <span class="number">0.97</span>, <span class="number">0.98</span>, <span class="number">1.01</span></span><br><span class="line">Task<span class="variable">s:</span> <span class="number">440</span> total,   <span class="number">2</span> running, <span class="number">438</span> sleeping,   <span class="number">0</span> stopped,   <span class="number">0</span> zombie</span><br><span class="line">%Cpu0  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu1  :  <span class="number">0.0</span> us,  <span class="number">0.3</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni, <span class="number">99.7</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu2  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu3  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu4  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu5  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu6  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu7  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu8  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu9  :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu10 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu11 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu12 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu13 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu14 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu15 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu16 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu17 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu18 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu19 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu20 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu21 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu22 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu23 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu24 : <span class="number">44.2</span> us, <span class="number">43.9</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,  <span class="number">0.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>, <span class="number">12.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu25 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu26 :  <span class="number">0.3</span> us,  <span class="number">0.3</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni, <span class="number">99.3</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu27 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu28 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu29 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu30 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">%Cpu31 :  <span class="number">0.0</span> us,  <span class="number">0.0</span> <span class="keyword">sy</span>,  <span class="number">0.0</span> ni,<span class="number">100.0</span> id,  <span class="number">0.0</span> <span class="keyword">wa</span>,  <span class="number">0.0</span> <span class="keyword">hi</span>,  <span class="number">0.0</span> si,  <span class="number">0.0</span> <span class="keyword">st</span></span><br><span class="line">KiB Mem : <span class="number">13183891</span>+total, <span class="number">12377862</span>+free,  <span class="number">3887628</span> used,  <span class="number">4172660</span> buff/cache</span><br><span class="line">KiB Swap:        <span class="number">0</span> total,        <span class="number">0</span> free,        <span class="number">0</span> used. <span class="number">12718814</span>+avail Mem </span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                               </span><br><span class="line"><span class="number">11746</span> jenkins   <span class="number">20</span>   <span class="number">0</span>  <span class="number">159972</span>   <span class="number">4760</span>   <span class="number">1600</span> R <span class="number">100.0</span>  <span class="number">0.0</span> <span class="number">393</span>:<span class="number">16.94</span> root/<span class="number">2</span>                                                                                                                </span><br><span class="line">   <span class="number">42</span> root      <span class="number">20</span>   <span class="number">0</span>       <span class="number">0</span>      <span class="number">0</span>      <span class="number">0</span> S   <span class="number">2.7</span>  <span class="number">0.0</span>  <span class="number">15</span>:<span class="number">59.27</span> rcu_sched                                                                                                             </span><br><span class="line">   <span class="number">67</span> root      <span class="number">20</span>   <span class="number">0</span>       <span class="number">0</span>      <span class="number">0</span>      <span class="number">0</span> S   <span class="number">1.3</span>  <span class="number">0.0</span>   <span class="number">1</span>:<span class="number">03.60</span> rcuos/<span class="number">24</span></span><br></pre></td></tr></table></figure><p>输入<code>1</code>可以查看每个逻辑<code>CPU</code>的情况，如上总共有32个逻辑<code>CPU</code>；</p><p>其他命令：</p><p>输入<code>b</code>，显示高亮，<code>shift+&gt;</code>和<code>shift+&lt;</code>可以左右切换</p><p>输入<code>x</code>也是显示高亮，但没有<code>b</code>那么明显，同理<code>shift+&gt;</code>和<code>shift+&lt;</code>可以左右切换</p><p>直接输入<code>top -c</code>，会显示完整命令，输出如下：</p><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">top - 23:56:31 up  8:20, <span class="number"> 1 </span>user,  load average: 0.95, 0.97, 1.00</span><br><span class="line">Tasks:<span class="number"> 439 </span>total,  <span class="number"> 2 </span>running,<span class="number"> 437 </span>sleeping,  <span class="number"> 0 </span>stopped,  <span class="number"> 0 </span>zombie</span><br><span class="line">%Cpu(s):  1.4 us,  1.5 sy,  0.0 ni, 96.8 id,  0.0 wa,  0.0 hi,  0.4 si,  0.0 st</span><br><span class="line">KiB Mem : 13183891+total, 12377344+free, <span class="number"> 3892304 </span>used, <span class="number"> 4173168 </span>buff/cache</span><br><span class="line">KiB Swap:       <span class="number"> 0 </span>total,       <span class="number"> 0 </span>free,       <span class="number"> 0 </span>used. 12718340+avail Mem </span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                               </span><br><span class="line">11746 jenkins  <span class="number"> 20 </span> <span class="number"> 0 </span><span class="number"> 159972 </span> <span class="number"> 4760 </span> <span class="number"> 1600 </span>R 100.0  0.0 402:57.42 root/2                                                                                                                </span><br><span class="line">  <span class="number"> 42 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   3.3  0.0  16:21.24 [rcu_sched]                                                                                                           </span><br><span class="line">  <span class="number"> 57 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.7  0.0   1:03.01 [rcuos/14]                                                                                                            </span><br><span class="line">  <span class="number"> 63 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.3  0.0   0:29.22 [rcuos/20]                                                                                                            </span><br><span class="line"><span class="number"> 7933 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.3  0.0   0:05.50 [kworker/20:0]                                                                                                        </span><br><span class="line">11414 jenkins  <span class="number"> 20 </span> <span class="number"> 0 </span>42.134g 1.661g <span class="number"> 24516 </span>S   0.3  1.3   2:47.29 /etc/alternatives/java -Dcom.sun.akuma.Daemon=daemonized -Djava.awt.headless=true -DJENKINS_HOME=/var/lib/jenkins -j+ </span><br><span class="line">14702 root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.3  0.0   0:01.48 [kworker/2:0]                                                                                                         </span><br><span class="line">15098 root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.3  0.0   0:01.37 [kworker/6:2]                                                                                                         </span><br><span class="line">18465 txq      <span class="number"> 20 </span> <span class="number"> 0 </span><span class="number"> 158088 </span> <span class="number"> 2720 </span> <span class="number"> 1640 </span>R   0.3  0.0   0:00.18 top -c                                                                                                                </span><br><span class="line">   <span class="number"> 1 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span> <span class="number"> 45892 </span> <span class="number"> 8580 </span> <span class="number"> 3908 </span>S   0.0  0.0   0:13.78 /usr/lib/systemd/systemd --switched-root --system --deserialize<span class="number"> 21 </span>                                                   </span><br><span class="line">   <span class="number"> 2 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.06 [kthreadd]                                                                                                            </span><br><span class="line">   <span class="number"> 3 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.34 [ksoftirqd/0]                                                                                                         </span><br><span class="line">   <span class="number"> 5 </span>root      <span class="number"> 0 </span>-20      <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.00 [kworker/0:0H]                                                                                                        </span><br><span class="line">   <span class="number"> 8 </span>root      rt  <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.35 [migration/0]                                                                                                         </span><br><span class="line">   <span class="number"> 9 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.00 [rcu_bh]                                                                                                              </span><br><span class="line">  <span class="number"> 10 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.00 [rcuob/0]                                                                                                             </span><br><span class="line">  <span class="number"> 11 </span>root     <span class="number"> 20 </span> <span class="number"> 0 </span>     <span class="number"> 0 </span>    <span class="number"> 0 </span>    <span class="number"> 0 </span>S   0.0  0.0   0:00.00 [rcuob/1]</span><br></pre></td></tr></table></figure><p>输入<code>q</code>是退出，还有其他命令参数，用到的时候再说，今天先统计这几个。</p><p>版权声明</p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> cpu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NumPy 副本和视图</title>
      <link href="2019/03/13/NumPy-%E5%89%AF%E6%9C%AC%E5%92%8C%E8%A7%86%E5%9B%BE/"/>
      <url>2019/03/13/NumPy-%E5%89%AF%E6%9C%AC%E5%92%8C%E8%A7%86%E5%9B%BE/</url>
      
        <content type="html"><![CDATA[<h1 id="NumPy-副本和视图"><a href="#NumPy-副本和视图" class="headerlink" title="NumPy 副本和视图"></a>NumPy 副本和视图</h1><p><strong>副本（赋值）</strong>是一个数据的完整的拷贝，如果我们对副本进行修改，它不会影响到原始数据，物理内存不在同一位置。</p><p><strong>视图（引用）</strong>是数据的一个别称或引用，通过该别称或引用亦便可访问、操作原有数据，但原有数据不会产生拷贝。如果我们对视图进行修改，它会影响到原始数据，物理内存在同一位置。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><strong>视图（引用）一般发生在：</strong></p><ul><li>1、numpy 的切片操作返回原数据的视图。</li><li>2、调用 ndarray 的 view() 函数产生一个视图。</li></ul><p><strong>副本一般发生在：</strong></p><ul><li>Python 序列的切片操作，调用deepCopy()函数。</li><li>调用 ndarray 的 copy() 函数产生一个副本。</li></ul><h2 id="yaya-举例："><a href="#yaya-举例：" class="headerlink" title="yaya 举例："></a>yaya 举例：</h2><p><strong>视图（引用）一般发生在：</strong></p><ul><li>1、numpy 的切片操作返回原数据的视图。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">6</span>)  <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b = a[:]   <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b[<span class="number">0</span>] = <span class="number">10</span>  <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line">print(a)   <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line"><span class="comment"># 修改b数据值，将修改原数据a的值</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">6</span>)  <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b = a[:]   <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b.shape = <span class="number">2</span>,<span class="number">3</span>  <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line">print(b)  </span><br><span class="line"><span class="comment"># [[0 1]</span></span><br><span class="line"><span class="comment">#  [2 3]</span></span><br><span class="line"><span class="comment">#  [4 5]]</span></span><br><span class="line">print(a)   <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line"><span class="comment"># 修改b数据形状，不会修改原数据a的形状</span></span><br></pre></td></tr></table></figure><ul><li>2、调用 ndarray 的 view() 函数产生一个视图。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">6</span>)  <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b = a.view()   <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b[<span class="number">0</span>] = <span class="number">10</span>  <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line">print(a)   <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line"><span class="comment"># 修改b数据值，将修改原数据a的值</span></span><br></pre></td></tr></table></figure></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">6</span>)  <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b = a[:]   <span class="comment"># [0 1 2 3 4 5]</span></span><br><span class="line">b.shape = <span class="number">2</span>,<span class="number">3</span>  <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line">print(b)  </span><br><span class="line"><span class="comment"># [[0 1]</span></span><br><span class="line"><span class="comment">#  [2 3]</span></span><br><span class="line"><span class="comment">#  [4 5]]</span></span><br><span class="line">print(a)   <span class="comment"># [10 1 2 3 4 5]</span></span><br><span class="line"><span class="comment"># 修改b数据形状，不会修改原数据a的形状</span></span><br></pre></td></tr></table></figure><p><strong>副本一般发生在：</strong></p><ul><li>Python 序列的切片操作，调用deepCopy()函数。</li><li>调用 ndarray 的 copy() 函数产生一个副本。</li></ul><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><h3 id="无复制-指向同一地址"><a href="#无复制-指向同一地址" class="headerlink" title="无复制 (指向同一地址)"></a>无复制 (指向同一地址)</h3><p>简单的赋值不会创建数组对象的副本。 相反，它使用原始数组的相同id()来访问它。 id()返回 Python 对象的通用标识符，类似于 C 中的指针。</p><p>此外，一个数组的任何变化都反映在另一个数组上。 例如，一个数组的形状改变也会改变另一个数组的形状。</p><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np    </span><br><span class="line">a = np.arange(<span class="number">6</span>)   </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'我们的数组是：'</span>) </span><br><span class="line"><span class="keyword">print</span> (a) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'调用 id() 函数：'</span>) </span><br><span class="line"><span class="keyword">print</span> (id(a)) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'a 赋值给 b：'</span>) </span><br><span class="line">b = a  <span class="keyword">print</span> (b) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'b 拥有相同 id()：'</span>) </span><br><span class="line"><span class="keyword">print</span> (id(b)) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'修改 b 的形状：'</span>) </span><br><span class="line">b.shape =  <span class="number">3</span>,<span class="number">2</span>   </span><br><span class="line"><span class="keyword">print</span> (b) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'a 的形状也修改了：'</span>) </span><br><span class="line"><span class="keyword">print</span> (a)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">我们的数组是：</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span>]</span><br><span class="line">调用 id() 函数：</span><br><span class="line"><span class="number">4349302224</span></span><br><span class="line">a 赋值给 b：</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span>]</span><br><span class="line">b 拥有相同 id()：</span><br><span class="line"><span class="number">4349302224</span></span><br><span class="line">修改 b 的形状：</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span>]]</span><br><span class="line">a 的形状也修改了：</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span>]]</span><br></pre></td></tr></table></figure><h3 id="视图或浅拷贝"><a href="#视图或浅拷贝" class="headerlink" title="视图或浅拷贝"></a>视图或浅拷贝</h3><p>ndarray.view() 方会创建一个新的数组对象，该方法创建的新数组的维数更改不会更改原始数据的维数。但是修改新数组的数值将会更改原始数据的数值。</p><h2 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  numpy  <span class="keyword">as</span>  np  </span><br><span class="line">a = np.arange(<span class="number">6</span>)  </span><br><span class="line"><span class="keyword">print</span>  (<span class="string">'我们的数组是：'</span>)  </span><br><span class="line"><span class="keyword">print</span>  (a)  </span><br><span class="line"><span class="keyword">print</span>  (<span class="string">'调用 id() 函数：'</span>)  </span><br><span class="line"><span class="keyword">print</span>  (id(a))  </span><br><span class="line"><span class="keyword">print</span>  (<span class="string">'a 赋值给 b：'</span>)  </span><br><span class="line">b = a  <span class="keyword">print</span>  (b)  </span><br><span class="line"><span class="keyword">print</span>  (<span class="string">'b 拥有相同 id()：'</span>)  </span><br><span class="line"><span class="keyword">print</span>  (id(b))  </span><br><span class="line"><span class="keyword">print</span>  (<span class="string">'修改 b 的形状：'</span>)  </span><br><span class="line">b.shape = <span class="number">3</span>,<span class="number">2</span>  </span><br><span class="line"><span class="keyword">print</span>  (b)  </span><br><span class="line"><span class="keyword">print</span>  (<span class="string">'a 的形状也修改了：'</span>)  </span><br><span class="line"><span class="keyword">print</span>  (a)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">数组 a：</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span>]]</span><br><span class="line">创建 a 的视图：</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span>]]</span><br><span class="line">两个数组的 id() 不同：</span><br><span class="line">a 的 id()：</span><br><span class="line"><span class="number">4314786992</span></span><br><span class="line">b 的 id()：</span><br><span class="line"><span class="number">4315171296</span></span><br><span class="line">b 的形状：</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span> <span class="number">5</span>]]</span><br><span class="line">a 的形状：</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span>]]</span><br></pre></td></tr></table></figure><p>使用切片创建视图修改数据会影响到原始数组：</p><h2 id="实例-2"><a href="#实例-2" class="headerlink" title="实例"></a>实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np   </span><br><span class="line">arr = np.arange(<span class="number">12</span>) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'我们的数组：'</span>) </span><br><span class="line"><span class="keyword">print</span> (arr) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'创建切片：'</span>) </span><br><span class="line">a=arr[<span class="number">3</span>:] </span><br><span class="line">b=arr[<span class="number">3</span>:] </span><br><span class="line">a[<span class="number">1</span>]=<span class="number">123</span> </span><br><span class="line">b[<span class="number">2</span>]=<span class="number">234</span> </span><br><span class="line">print(arr) </span><br><span class="line">print(id(a),id(b),id(arr[<span class="number">3</span>:]))</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我们的数组：</span><br><span class="line">[ <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span>]</span><br><span class="line">创建切片：</span><br><span class="line">[  <span class="number">0</span>   <span class="number">1</span>   <span class="number">2</span>   <span class="number">3</span> <span class="number">123</span> <span class="number">234</span>   <span class="number">6</span>   <span class="number">7</span>   <span class="number">8</span>   <span class="number">9</span>  <span class="number">10</span>  <span class="number">11</span>]</span><br><span class="line"><span class="number">4545878416</span> <span class="number">4545878496</span> <span class="number">4545878576</span></span><br></pre></td></tr></table></figure><p>变量 a,b 都是 arr 的一部分视图，对视图的修改会直接反映到原数据中。但是我们观察 a,b 的 id，他们是不同的，也就是说，视图虽然指向原数据，但是他们和赋值引用还是有区别的。</p><h3 id="副本或深拷贝"><a href="#副本或深拷贝" class="headerlink" title="副本或深拷贝"></a>副本或深拷贝</h3><p>ndarray.copy() 函数创建一个副本。 对副本数据进行修改，不会影响到原始数据，它们物理内存不在同一位置。</p><h2 id="实例-3"><a href="#实例-3" class="headerlink" title="实例"></a>实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np    </span><br><span class="line">a = np.array([[<span class="number">10</span>,<span class="number">10</span>],  [<span class="number">2</span>,<span class="number">3</span>],  [<span class="number">4</span>,<span class="number">5</span>]])   </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'数组 a：'</span>) </span><br><span class="line"><span class="keyword">print</span> (a) <span class="keyword">print</span> (<span class="string">'创建 a 的深层副本：'</span>) </span><br><span class="line">b = a.copy()   </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'数组 b：'</span>) </span><br><span class="line"><span class="keyword">print</span> (b) <span class="comment"># b 与 a 不共享任何内容   </span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">'我们能够写入 b 来写入 a 吗？'</span>) </span><br><span class="line"><span class="keyword">print</span> (b <span class="keyword">is</span> a) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'修改 b 的内容：'</span>) </span><br><span class="line">b[<span class="number">0</span>,<span class="number">0</span>]  =  <span class="number">100</span>   </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'修改后的数组 b：'</span>) </span><br><span class="line"><span class="keyword">print</span> (b) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">'a 保持不变：'</span>) </span><br><span class="line"><span class="keyword">print</span> (a)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">数组 a：</span><br><span class="line">[[<span class="number">10</span> <span class="number">10</span>]</span><br><span class="line"> [ <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line"> [ <span class="number">4</span>  <span class="number">5</span>]]</span><br><span class="line">创建 a 的深层副本：</span><br><span class="line">数组 b：</span><br><span class="line">[[<span class="number">10</span> <span class="number">10</span>]</span><br><span class="line"> [ <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line"> [ <span class="number">4</span>  <span class="number">5</span>]]</span><br><span class="line">我们能够写入 b 来写入 a 吗？</span><br><span class="line"><span class="literal">False</span></span><br><span class="line">修改 b 的内容：</span><br><span class="line">修改后的数组 b：</span><br><span class="line">[[<span class="number">100</span>  <span class="number">10</span>]</span><br><span class="line"> [  <span class="number">2</span>   <span class="number">3</span>]</span><br><span class="line"> [  <span class="number">4</span>   <span class="number">5</span>]]</span><br><span class="line">a 保持不变：</span><br><span class="line">[[<span class="number">10</span> <span class="number">10</span>]</span><br><span class="line"> [ <span class="number">2</span>  <span class="number">3</span>]</span><br><span class="line"> [ <span class="number">4</span>  <span class="number">5</span>]]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
          <category> numpy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python  面试</title>
      <link href="2019/03/12/python-%E9%9D%A2%E8%AF%95/"/>
      <url>2019/03/12/python-%E9%9D%A2%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<p><a href="https://juejin.im/post/5b6bc1d16fb9a04f9c43edc3" target="_blank" rel="noopener">https://juejin.im/post/5b6bc1d16fb9a04f9c43edc3</a></p><p><a href="https://juejin.im/post/5b8505b6e51d4538884d22bf" target="_blank" rel="noopener">https://juejin.im/post/5b8505b6e51d4538884d22bf</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python基础： 深入理解 python 中的赋值、引用、拷贝、作用域</title>
      <link href="2019/03/12/python%E5%9F%BA%E7%A1%80%EF%BC%9A-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-python-%E4%B8%AD%E7%9A%84%E8%B5%8B%E5%80%BC%E3%80%81%E5%BC%95%E7%94%A8%E3%80%81%E6%8B%B7%E8%B4%9D%E3%80%81%E4%BD%9C%E7%94%A8%E5%9F%9F/"/>
      <url>2019/03/12/python%E5%9F%BA%E7%A1%80%EF%BC%9A-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-python-%E4%B8%AD%E7%9A%84%E8%B5%8B%E5%80%BC%E3%80%81%E5%BC%95%E7%94%A8%E3%80%81%E6%8B%B7%E8%B4%9D%E3%80%81%E4%BD%9C%E7%94%A8%E5%9F%9F/</url>
      
        <content type="html"><![CDATA[<h3 id="浅复制："><a href="#浅复制：" class="headerlink" title="浅复制："></a>浅复制：</h3><ul><li>仅复制对象的引用，而不开辟内存，即，改变复制后的对象时，其实是在改变原对象内存中的内容。</li><li>b = a[ : ]<h3 id="深复制"><a href="#深复制" class="headerlink" title="深复制"></a>深复制</h3>将开辟新的内存，把原对象内存中的内容复制到新的内存中来，如果改变复制后的对象，将改变原对象的内容。即，这两个对象在完成复制之后，已经是两个独立的对象了</li></ul><p><strong>- 转载： <a href="https://draapho.github.io/2016/11/21/1618-python-variable/" target="_blank" rel="noopener">https://draapho.github.io/2016/11/21/1618-python-variable/</a></strong></p><h3 id="可变对象：list-dict-set-（引用传递）"><a href="#可变对象：list-dict-set-（引用传递）" class="headerlink" title="可变对象：list dict set  （引用传递）"></a>可变对象：list dict set  （引用传递）</h3><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">   <span class="meta"># list</span></span><br><span class="line">a= [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">b = a</span><br><span class="line">b[<span class="number">0</span>] = <span class="number">9</span></span><br><span class="line"><span class="keyword">print</span>(b) <span class="meta"># [9, 2, 3]</span></span><br><span class="line"><span class="keyword">print</span>(a) <span class="meta"># [9, 2, 3]</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># dict</span></span><br><span class="line">a = &#123;&#125;</span><br><span class="line">a['key1'] = <span class="number">1</span></span><br><span class="line">b = a</span><br><span class="line">b['key1'] = <span class="number">9</span></span><br><span class="line"><span class="keyword">print</span>(b) <span class="meta"># &#123;'key1': 9&#125;</span></span><br><span class="line"><span class="keyword">print</span>(a) <span class="meta"># &#123;'key1': 9&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">values = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]  </span><br><span class="line">values[<span class="number">1</span>] = values  </span><br><span class="line">values  </span><br><span class="line">[<span class="number">0</span>, [...], <span class="number">2</span>] # 实际结果, 为何要赋值无限次?  </span><br><span class="line">[<span class="number">0</span>, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], <span class="number">2</span>] # 预想结果</span><br></pre></td></tr></table></figure><p>   Python 没有赋值，只有引用。你这样相当于创建了一个引用自身的结构，所以导致了无限循环</p><h3 id="不可变对象：tuple-string-int-float-bool-（值传递）"><a href="#不可变对象：tuple-string-int-float-bool-（值传递）" class="headerlink" title="不可变对象：tuple string int float bool （值传递）"></a>不可变对象：<strong><em>tuple</em></strong> string int float bool （值传递）</h3><p>对于可变对象，对象的操作不会重建对象，而对于不可变对象，每一次操作就重建新的对象。</p><pre><code>def func_int(a):    a += 4def func_list(a_list):    a_list[0] = 4t = 0func_int(t)print t# output: 0t_list = [1, 2, 3]func_list(t_list)print t_list# output: [4, 2, 3]</code></pre><h3 id="Dictionary-与-List-与-Tuple的区别"><a href="#Dictionary-与-List-与-Tuple的区别" class="headerlink" title="Dictionary 与 List 与 Tuple的区别"></a>Dictionary 与 List 与 Tuple的区别</h3><p>元组和列表在结构上没有什么区别，唯一的差异在于元组是只读的，不能修改。</p><p><strong>Dictionary</strong> </p><ol><li>Dictionary 是 Python 的内置数据类型之一, 它定义了键和值之间一对一的关系。</li><li>每一个元素都是一个 key-value 对, 整个元素集合用大括号括起来</li><li>您可以通过 key 来引用其值, 但是不能通过值获取 key</li><li>在一个 dictionary 中不能有重复的 key。给一个存在的 key 赋值会覆盖原有的值。 <a href="file:///home/echo/.chmsee/bookshelf/4791a2294b7b1d4cf68b0e1df606181d/native_data_types/index.html#odbchelper.dict.2.2" target="_blank" rel="noopener"><img src="file:///home/echo/.chmsee/bookshelf/4791a2294b7b1d4cf68b0e1df606181d/images/callouts/2.png" alt="2"></a> 在任何时候都可以加入新的 key-value 对。这种语法同修改存在的值是一样的。</li><li>当使用 dictionary 时, 您需要知道: dictionary 的 key 是大小写敏感的</li><li>Dictionary 不只是用于存储字符串。Dictionary 的值可以是任意数据类型, 包括字符串, 整数, 对象, 甚至其它的 dictionary。在单个 dictionary 里, dictionary 的值并不需要全都是同一数据类型, 可以根据需要混用和匹配。 Dictionary 的 key 要严格多了, 但是它们可以是字符串, 整数和几种其它的类型 (后面还会谈到这一点) 。也可以在一个 dictionary 中混用和配匹 key 的数据类型</li><li><code>del</code> 允许您使用 key 从一个 dictionary 中删除独立的元素。</li><li><code>clear</code> 从一个 dictionary 中清除所有元素。注意空的大括号集合表示一个没有元素的 dictionary。</li></ol><hr><p><strong>List</strong> </p><ol><li>list是一个使用方括号括起来的有序元素集合。</li><li>List 可以作为以 0 下标开始的数组。任何一个非空 list 的第一个元素总是 <code>li[0]</code></li><li><code>负数索引从 list 的尾部开始向前计数来存取元素。任何一个非空的 list 最后一个元素总是</code> li[-1]。 <a href="file:///home/echo/.chmsee/bookshelf/4791a2294b7b1d4cf68b0e1df606181d/native_data_types/lists.html#odbchelper.list.2.2" target="_blank" rel="noopener"><img src="file:///home/echo/.chmsee/bookshelf/4791a2294b7b1d4cf68b0e1df606181d/images/callouts/2.png" alt="2"></a>如果负数索引使您感到糊涂, 可以这样理解: <code>li[-n] == li[len(li) - n]</code>。 所以在这个 list 里, <code>li[-3] == li[5 - 3] == li[2]</code>。</li><li><code>您可以通过指定 2 个索引得到 list 的子集, 叫做一个 “slice” 。返回值是一个新的 list, 它包含了 list 中按顺序从第一个 slice 索引 (这里为</code> li[1]) 开始, 直到但是不包括第二个 slice 索引 (这里为<code>li[3]</code>) 的所有元素。</li><li><code>如果将两个分片索引全部省略, 这将包括 list 的所有元素。但是与原始的名为</code> li 的 list 不同, 它是一个新 list, 恰好拥有与 <code>li</code> 一样的全部元素。<code>li[:]</code> 是生成一个 list 完全拷贝的一个简写。</li><li><code>``append</code> 向 list 的末尾追加单个元素。</li><li><code>insert</code> 将单个元素插入到 list 中。数值参数是插入点的索引。请注意, list 中的元素不必唯一, 有有两个独立的元素具有 <code>&#39;new&#39;</code> 这个相同的值<code>。</code></li><li><code>extend</code> 用来连接 list。请注意不要使用多个参数来调用 <code>extend</code>, 要使用一个 list 参数进行调用。</li><li>Lists 的两个方法 <code>extend</code> 和 <code>append</code> 看起来类似, 但实际上完全不同。 <code>extend</code> 接受一个参数, 这个参数总是一个 list, 并且添加这个 list 中的每个元素到原 list 中</li><li>另一方面, <code>append</code> 接受一个参数, 这个参数可以是任何数据类型, 并且简单地追加到 list 的尾部。 在这里使用一个含有 3 个元素的 list 参数调用 <code>append</code> 方法。</li><li><code>index</code> 在 list 中查找一个值的首次出现并返回索引值。</li><li>要测试一个值是否在 list 内, 使用 <code>in</code>, 如果值存在, 它返回 <code>True</code>, 否则返为 <code>False</code> 。</li><li><code>remove</code> 从 list 中删除一个值的首次出现。</li><li><code>pop</code> 是一个有趣的东西。它会做两件事: 删除 list 的最后一个元素, 然后返回删除元素的值。请注意, 这与 <code>li[-1]</code> 不同, 后者返回一个值但不改变 list 本身。也不同于 <code>li.remove(*value*)</code>, 后者改变 list 但并不返回值。</li><li>Lists 也可以用 <code>+</code> 运算符连接起来。 <code>*list* = *list* + *otherlist*</code> 相当于 <code>*list*.extend(*otherlist*)</code>。 但 <code>+</code>运算符把一个新 (连接后) 的 list 作为值返回, 而 <code>extend</code> 只修改存在的 list。 也就是说, 对于大型 list 来说, <code>extend</code> 的执行速度要快一些。</li><li>Python 支持 <code>+=</code> 运算符。 <code>li += [&#39;two&#39;]</code> 等同于 <code>li.extend([&#39;two&#39;])</code>。 <code>+=</code> 运算符可用于 list, 字符串和整数, 并且它也可以被重载用于用户自定义的类中。</li><li><code>*</code> 运算符可以作为一个重复器作用于 list。 <code>li = [1, 2] * 3</code> 等同于 <code>li = [1, 2] + [1, 2] + [1, 2]</code>, 即将三个 list 连接成一个。</li></ol><hr><p><strong>Tuple</strong></p><ol><li>​    Tuple是不可变的list.一是创建了一个tuple就不能以任何方式改变它.</li><li>​    定义tuple与定义list的方式相同,除了整个元素集是用小括号包围的而不是方括号.</li><li>　 Tuple的元素与list一样按定义的次序进行排序.Tuples的索引与list一样从0开始,所以一个非空的tuple的第一个元素总是t[0].</li><li>​    负数索引与 list 一样从 tuple 的尾部开始计数。</li><li>​    与 list 一样分片 (slice) 也可以使用。注意当分割一个 list 时, 会得到一个新的 list ；当分割一个 tuple 时, 会得到一个新的 tuple。</li><li>​    Tuple 没有方法：没有 <code>append</code> 或 <code>extend</code> 方法、没有 <code>remove</code> 或 <code>pop</code> 方法、没有 <code>index</code> 方法、可以使用 <code>in</code> 来查看一个元素是否存在于 tuple 中。</li></ol><hr>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(LSTM_TSA)Video Captioning with Transferred Semantic Attributes</title>
      <link href="2019/03/03/LSTM-TSA-Video-Captioning-with-Transferred-Semantic-Attributes/"/>
      <url>2019/03/03/LSTM-TSA-Video-Captioning-with-Transferred-Semantic-Attributes/</url>
      
        <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>在这篇文章中，我们提出了一个 Long Short-Term Memory with Transferred Semantic Attributes （LSTM-TSA）model，这是一个新颖的结构，可以将从images 和 videos 中学习到的transferred semantic attributes  结合到 encoder - decoder 结构中去。</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch-Multi-Gpus</title>
      <link href="2019/03/02/pytorch-Multi-Gpus/"/>
      <url>2019/03/02/pytorch-Multi-Gpus/</url>
      
        <content type="html"><![CDATA[<ul><li>源为pytorch的官方文档</li><li><a href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html" target="_blank" rel="noopener">website</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters and DataLoaders</span></span><br><span class="line">input_size = <span class="number">5</span></span><br><span class="line">output_size = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">30</span></span><br><span class="line">data_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># Our model</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, output_size)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(input_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        output = self.fc(input)</span><br><span class="line">        print(<span class="string">"\tIn Model: input size"</span>, input.size(),</span><br><span class="line">              <span class="string">"output size"</span>, output.size())</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, length)</span>:</span></span><br><span class="line">        self.len = length</span><br><span class="line">        self.data = torch.randn(length, size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[index]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.len</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size), batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">model = Model(input_size, output_size)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">    print(<span class="string">"Let's use"</span>, torch.cuda.device_count(), <span class="string">"GPUs!"</span>)</span><br><span class="line">    <span class="comment"># dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs</span></span><br><span class="line">    model = nn.DataParallel(model)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> rand_loader:</span><br><span class="line">    input = data.to(device)</span><br><span class="line">    output = model(input)</span><br><span class="line">    print(<span class="string">"Outside: input size"</span>, input.size(),</span><br><span class="line">          <span class="string">"output_size"</span>, output.size())</span><br></pre></td></tr></table></figure><h2 id="转载：-PyTorch-论文pytorch复现中遇到的BUG"><a href="#转载：-PyTorch-论文pytorch复现中遇到的BUG" class="headerlink" title="转载：[PyTorch]论文pytorch复现中遇到的BUG]"></a>转载：<a href="https://www.cnblogs.com/kk17/p/10139884.html" target="_blank" rel="noopener">[PyTorch]论文pytorch复现中遇到的BUG]</a></h2><ul><li>我在Multi-GPUs时，也遇到了第一个问题</li></ul><p>目录</p><ul><li><a href="https://www.cnblogs.com/kk17/p/10139884.html#zip-argument-1-must-support-iteration" target="_blank" rel="noopener">1. zip argument #1 must support iteration</a></li><li><a href="https://www.cnblogs.com/kk17/p/10139884.html#torch.nn.dataparallel" target="_blank" rel="noopener">2. torch.nn.DataParallel</a></li><li><a href="https://www.cnblogs.com/kk17/p/10139884.html#model.state_dict" target="_blank" rel="noopener">3. model.state_dict()</a></li></ul><h1 id="1-zip-argument-1-must-support-iteration"><a href="#1-zip-argument-1-must-support-iteration" class="headerlink" title="1. zip argument #1 must support iteration"></a>1. zip argument #1 must support iteration</h1><p>在多gpu训练的时候，自动把你的batch_size分成n_gpu份，每个gpu跑一些数据， 最后再合起来。我之所以出现这个bug是因为返回的时候 返回了一个常量。。</p><h1 id="2-torch-nn-DataParallel"><a href="#2-torch-nn-DataParallel" class="headerlink" title="2. torch.nn.DataParallel"></a>2. torch.nn.DataParallel</h1><p>在使用torch.nn.DataParallel时候，要先把模型放在gpu上，再进行parallel。</p><h1 id="3-model-state-dict"><a href="#3-model-state-dict" class="headerlink" title="3. model.state_dict()"></a>3. model.state_dict()</h1><p>一般在现有的网络加载预训练模型通常是找到预训练模型在现有的model里面的参数，然后model进行更新，遇到一个bug， 发现加载预训练模型的时候， 效果很差，跟参数没有更新一样，找了一大顿bug，最后才发现，之前是单gpu进行的预训练，现在的模型使用的是多gpu， 打印现在模型的参数你会发现他所有的参数前面都加了一个module. 所以向以前一样更新，没有一个参数会被更新，因此写了一个万能模型参数加载函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">pretrained_dict = checkpoint[<span class="string">'state_dict'</span>]</span><br><span class="line">model_dict = self.model.state_dict()</span><br><span class="line"><span class="keyword">if</span> checkpoint[<span class="string">'config'</span>][<span class="string">'n_gpu'</span>] &gt; <span class="number">1</span> <span class="keyword">and</span> self.config[<span class="string">'n_gpu'</span>] == <span class="number">1</span>:</span><br><span class="line">    new_dict = OrderedDict()</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> pretrained_dict.items():</span><br><span class="line">        name = k[<span class="number">7</span>:]</span><br><span class="line">        new_dict[name] = v</span><br><span class="line">    pretrained_dict = new_dict</span><br><span class="line"><span class="keyword">elif</span> checkpoint[<span class="string">'config'</span>][<span class="string">'n_gpu'</span>] == <span class="number">1</span> <span class="keyword">and</span> self.config[<span class="string">'n_gpu'</span>] &gt; <span class="number">1</span>:</span><br><span class="line">    new_dict = OrderedDict()</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> pretrained_dict.items():</span><br><span class="line">        name = <span class="string">"module."</span>+k</span><br><span class="line">        new_dict[name] = v</span><br><span class="line">    pretrained_dict = new_dict</span><br><span class="line">print(<span class="string">"The pretrained model's para is following"</span>)</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> pretrained_dict.items():</span><br><span class="line">    print(k)</span><br><span class="line">pretrained_dict = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> pretrained_dict.items() <span class="keyword">if</span> k <span class="keyword">in</span> model_dict&#125;</span><br><span class="line">model_dict.update(pretrained_dict)</span><br><span class="line">self.model.load_state_dict(model_dict)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>coco-detection</title>
      <link href="2019/03/02/coco-detection/"/>
      <url>2019/03/02/coco-detection/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning</title>
      <link href="2019/03/01/Spatio-Temporal-Dynamics-and-Semantic-Attribute-Enriched-Visual-Encoding-for-Video-Captioning/"/>
      <url>2019/03/01/Spatio-Temporal-Dynamics-and-Semantic-Attribute-Enriched-Visual-Encoding-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>nltk-tokenize</title>
      <link href="2019/02/28/nltk-tokenize/"/>
      <url>2019/02/28/nltk-tokenize/</url>
      
        <content type="html"><![CDATA[<p>转载：<a href="https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/" target="_blank" rel="noopener">https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/</a></p><p>Next, you’re going to need NLTK 3. The easiest method to installing the NLTK module is going to be with pip.</p><p>For all users, that is done by opening up cmd.exe, bash, or whatever shell you use and typing:<br><code>pip install nltk</code></p><p>These are the words you will most commonly hear upon entering the Natural Language Processing (NLP) space, but there are many more that we will be covering in time. With that, let’s show an example of how one might actually tokenize something into tokens with the NLTK module.</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from nltk<span class="selector-class">.tokenize</span> import sent_tokenize, word_tokenize</span><br><span class="line"></span><br><span class="line">EXAMPLE_TEXT = <span class="string">"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard."</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(sent_tokenize(EXAMPLE_TEXT)</span></span>)</span><br></pre></td></tr></table></figure><p>At first, you may think tokenizing by things like words or sentences is a rather trivial enterprise. For many sentences it can be. The first step would be likely doing a simple .split(‘. ‘), or splitting by period followed by a space. Then maybe you would bring in some <a href="https://pythonprogramming.net/regular-expressions-regex-tutorial-python-3/" target="_blank" rel="noopener"><strong>regular expressions</strong></a> to split by period, space, and then a capital letter. The problem is that things like Mr. Smith would cause you trouble, and many other things. Splitting by word is also a challenge, especially when considering things like concatenations like we and are to we’re. NLTK is going to go ahead and just save you a ton of time with this seemingly simple, yet very complex, operation.</p><p>The above code will output the sentences, split up into a list of sentences, which you can do things like iterate through with a <a href="https://pythonprogramming.net/loop-python-3-basics-tutorial/" target="_blank" rel="noopener"><strong>for loop</strong></a>.<br><code>[&#39;Hello Mr. Smith, how are you doing today?&#39;, &#39;The weather is great, and Python is awesome.&#39;, &#39;The sky is pinkish-blue.&#39;, &quot;You shouldn&#39;t eat cardboard.&quot;]</code></p><p>So there, we have created tokens, which are sentences. Let’s tokenize by word instead this time:</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="name">word_tokenize</span>(<span class="name">EXAMPLE_TEXT</span>))</span><br></pre></td></tr></table></figure><p>Now our output is: <code>[&#39;Hello&#39;, &#39;Mr.&#39;, &#39;Smith&#39;, &#39;,&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;, &#39;doing&#39;, &#39;today&#39;, &#39;?&#39;, &#39;The&#39;, &#39;weather&#39;, &#39;is&#39;, &#39;great&#39;, &#39;,&#39;, &#39;and&#39;, &#39;Python&#39;, &#39;is&#39;, &#39;awesome&#39;, &#39;.&#39;, &#39;The&#39;, &#39;sky&#39;, &#39;is&#39;, &#39;pinkish-blue&#39;, &#39;.&#39;, &#39;You&#39;, &#39;should&#39;, &quot;n&#39;t&quot;, &#39;eat&#39;, &#39;cardboard&#39;, &#39;.&#39;]</code></p><p>There are a few things to note here. First, notice that punctuation is treated as a separate token. Also, notice the separation of the word “shouldn’t” into “should” and “n’t.” Finally, notice that “pinkish-blue” is indeed treated like the “one word” it was meant to be turned into. Pretty cool!</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Maxout Networks</title>
      <link href="2019/02/27/Maxout-Networks/"/>
      <url>2019/02/27/Maxout-Networks/</url>
      
        <content type="html"><![CDATA[<p>本文是蒙特利尔大学信息与信息技术学院的几位大牛2013年在ICML上发表的一篇论文，提出了一种叫maxout的新模型，到目前用的比较少，可能这个算法参数个数会成k倍增加(k是maxout的一个参数)。文中提到这样命名的原因：（1）它的输出是一组输入的最大值（2）它与dropout是天然的组合。</p><p>文章首先肯定了dropout的意义，从音频分类到超大规模物体识别都有很好的改进，同时提出不应该把dropout作为一个适用于任意模型的轻微性能增强，而是通过直接设计一个模型来提高dropout能力，作为模型平均技术，可以获得最好的性能。</p><p>Dropout：</p><ul><li>dropout可以训练集成模型，它们共享参数并近似的对这些模型的预测进行了平均。它可以被当作一种通用的方法用在任何一种MLP和CNN模型中，但是在论文中，由于dropout的模型平均过程没有被证明，因而一个模型最好的性能的获得，应该通过直接设计这个模型使之可以增强dropout的模型平均的能力。使用了dropout的训练过程和一般的SGD方法完全不同。dropout在更新时使用更大的步长最有效，因为这样可以在不同的训练子集上对不同的模型有明显的影响来使得目标函数有持续的波动性，理想情况下整个训练过程就类似于使用bagging来训练集成的模型（带有参数共享的约束）。而一般的SGD更新时会使用更小的步长，来使得目标函数平滑的下降。对于深度网络模型，dropout只能作为模型平均的一种近似，显式的设计模型来最小化这种近似误差也可以提高dropout的性能。</li><li>dropout训练的集成模型中，所有模型都只包括部分输入和部分隐层参数。对每一个训练样本，我们都会训练一个包括不同隐层参数的子模型。dropout与bagging的相同点是不同的模型使用不同数据子集，不同点是dropout的每个模型都只训练一次且所有模型共享参数。</li><li>对于预测时如何平均所有子模型的问题，bagging一般使用的是算数平均，而对dropout产生的指数多个子模型则并非显而易见。但是如果模型只有一层 <img src="https://www.zhihu.com/equation?tex=p%28y+%7C+v%3B%CE%B8%29%3Dsoftmax%28v%5E%7BT%7D%2Bb%29" alt="p(y | v;θ)=softmax(v^{T}+b)">作为输出（p(y | v;θ)的几何平均），则最终的预测分布就是简单的 <img src="https://www.zhihu.com/equation?tex=softmax%28v%5E%7BT%7DW%2F2%2Bb%29" alt="softmax(v^{T}W/2+b)">，即指数多个子模型的平均预测就是完整模型的预测仅仅将权重减半而已。这个结果只能用在单softmax层的模型中，如果是深层模型如MLP，那么权重减半的方法只是几何平均的一种近似。</li></ul><p>Maxout是深度学习网络中的一层网络，就像池化层、卷积层一样等，我们可以把maxout 看成是网络的激活函数层。我们假设网络某一层的输入特征向量为：X=（x1,x2,……xd），也就是我们输入是d个神经元。Maxout隐藏层每个神经元的计算公式如下：</p><p><img src="https://www.zhihu.com/equation?tex=h_%7Bi%7D%3D%5Cmax_%7Bj+%5Cin+%5B1%2Ck%5D%7D%7Bz_%7Bij%7D%7D" alt="h_{i}=\max_{j \in [1,k]}{z_{ij}}"></p><p>上面的公式就是maxout隐藏层神经元i的计算公式。其中，k就是maxout层所需要的参数了，由我们人为设定大小。就像dropout一样，也有自己的参数p(每个神经元dropout概率)，maxout的参数是k。公式中Z的计算公式为： <img src="https://www.zhihu.com/equation?tex=z_%7Bij%7D%3Dx%5E%7BT%7DW_%7B..ij%7D%2Bb_%7Bij%7D" alt="z_{ij}=x^{T}W_{..ij}+b_{ij}"> ，权重w是一个大小为(d,m,k)三维矩阵，b是一个大小为(m,k)的二维矩阵，这两个就是我们需要学习的参数。如果我们设定参数k=1，那么这个时候，网络就类似于以前我们所学普通的MLP网络。</p><p>我们可以这么理解，本来传统的MLP算法在第i层到第i+1层，参数只有一组，然而现在我们不怎么干了，我们在这一层同时训练n组参数，然后选择激活值最大的作为下一层神经元的激活值。下面还是用一个例子进行讲解，比较容易搞懂。</p><p>（1）以前MLP的方法。<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0ldm6b5ujj30ix08mdg0.jpg"></p><p>其中 f 就是我们所谓的激活函数，比如Sigmod、Relu、Tanh等。</p><p>(2)Maxout 的方法。如果我们设置maxout的参数k=5，maxout层就如下所示：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0ldmxgg2zj30n60e33zh.jpg"></p><p>相当于在每个输出神经元前面又多了一层。这一层有5个神经元。<br>所以这就是为什么采用maxout的时候，参数个数成k倍增加的原因。本来我们只需要一组参数就够了，采用maxout后，就需要有k组参数。</p><ul><li>对MLP而言，2个输入节点先构成5个“隐隐层”节点，然后在5个“隐隐层”节点中使用最大的构成了本层的一个节点，本层其他节点类似。<strong>实现技巧：</strong><code>maxout</code>和<code>relu</code>唯一的区别是，<code>relu</code>使用的max(x,0)是对隐层每一个单元执行的与0比较最大化操作，而<code>maxout</code>是对5个“隐隐层”单元的值执行最大化操作。如果将“隐隐层”单元在隐层展开，那么隐层就有20个“隐隐层”单元，<code>maxout</code>做的就是在这20个中每5个取一个最大值作为最后的隐层单元，最后的隐层单元仍然为4个。这里每5个取一个最大值也称为最大池化步长（max pooling stride）为5，最大池化步长默认和“隐隐层”个数相等，如果步长更小，则可以实现重叠最大池化。实现的时候，可以将隐层单元数设置为20个，权重维度（2，20）偏置维度（1，20），然后在20个中每5个取一个最大值得到4个隐层单元。</li><li>对于CNN而言，假设上一层有2个特征图，本层有4个特征图，那么就是将输入的2个特征图用5个滤波器卷积得到5张仿射特征图（affine feature maps），然后从这5张仿射特征图每个位置上选择最大值（跨通道池化，pool across channels）构成一张本层的特征图，本层其他特征图类似。<strong>实现技巧：</strong><code>relu</code>使用的max(x,0)是对每个通道的特征图的每一个单元执行的与0比较最大化操作，而<code>maxout</code>是对5个通道的特征图在通道的维度上执行最大化操作。而如果把5个特征图在本层展开，那么本层就有20个特征图，<code>maxout</code>做的就是在这20个中每5个取在通道维度上的最大值作为最后的特征图，最后本层特征图仍然为4个。同样最大池化步长默认为5。实现的时候，可以将本层特征图数设置为20个，权重维度（20，2，3，3）偏置维度（1，20，1，1），然后在20个中每5个取一个最大特征图得到4个特征图。<strong>注意：</strong> 对于CNN而言，在maxout输出后如果连接一个一般的降采样最大池化层，则可以将这个降采样最大池化融合进跨通道池化中，即在仿射特征图的每个池化窗口中选择最大值（相当于同时在通道间和空间取最大值）。这样就可以在maxout网络中省略显式的降采样最大池化层。</li></ul><h2 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h2><ol><li>MNIST</li></ol><p>排列不变限制的MNIST（MNIST with permutation invariant，即像素排列顺序可以改变，输入的数据是2维的），使用两个全连接maxout层再接上一个softmax层，结合dropout和权重衰减。验证集选取训练集中后10000个样本。在得到最小的验证集误差时记录下前50000个样本的训练集对数似然L，接着在整个60000样本的训练集上<strong>继续训练</strong>直到验证集的对数似然达到L。<em>0.94%</em></p><p>无排列不变限制的MNIST（MNIST without permutation invariant，即像素排列顺序不变，输入的数据是3维的），使用三个卷积maxout层，之后接上空间最大池化层，最后接上一个softmax层。还可以使用扩充数据集的方法进一步提高。<em>0.45%</em></p><p>\2. CIFAR-10</p><p>预处理：全局像素归一化和ZCA白化</p><p>过程与MNIST类似，只是将继续训练改为了<strong>重新训练</strong>，因为继续训练的学习率很低训练太久。</p><p>使用三个卷积maxout层，之后接上全连接maxout层，最后接上一个softmax层。<em>13.2%</em>（不使用验证集数据）<em>11.68%</em>（使用验证集数据）<em>9.35%</em>（使用平移、水平翻转的扩充数据集）</p><p>\3. CIFAR-100</p><p>超参数使用和CIFAR-10一样</p><p><em>41.48%</em>（不使用验证集数据）<em>38.57%</em>（使用验证集数据）</p><p>\4. SVHN</p><p>验证集为训练集每类选取400个样本和额外集每类选取200个样本，其他的为训练集。</p><p>预处理：局部像素归一化</p><p>使用三个卷积maxout层，之后接上全连接maxout层，最后接上一个softmax层（同CIFAR-10）。<em>2.47%</em></p><h2 id="maxout对比relu"><a href="#maxout对比relu" class="headerlink" title="maxout对比relu"></a>maxout对比relu</h2><ul><li>跨通道池化可以减少网络状态并减少模型所需要的参数。</li><li>对于maxout，性能与跨通道池化时滤波器数量有很大关系，但对relu，性能与输出单元的数量没有关系，也就是relu并不从跨通道池化中受益。</li><li>要让relu达到maxout的表现，需要使之具有和maxout相同数量的滤波器（即使用比原来k倍的滤波器，同样也要k倍的relu单元），但网络状态和所需要的参数也是原来的k倍，也是对应maxout的k倍。</li></ul><h2 id="模型平均"><a href="#模型平均" class="headerlink" title="模型平均"></a>模型平均</h2><ul><li>单层softmax有对模型进行平均的能力，但是通过观察，多层模型中使用dropout也存在这样的模型平均，只是有拟合精度的问题。</li><li>训练中使用dropout使得maxout单元有了更大的输入附近的线性区域，因为每个子模型都要预测输出，每个maxout单元就要学习输出相同的预测而不管哪些输入被丢弃。改变dropout mask将经常明显移动有效输入，从而决定了输入被映射到分段线性函数的哪一段。使用dropout训练的maxout具有一种特性，即当dropout mask改变时每个maxout单元的最大化滤波器相对很少变化。</li><li>maxout网络中的线性和最大化操作可以让dropout的拟合模型平均的精度很高。而一般的激活函数几乎处处都是弯曲的，因而dropout的拟合模型平均的精度不高。</li></ul><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><ul><li>训练中使用dropout时，maxout的优化性能比relu+max pooling好</li><li>dropout使用更大的步长最有效，使得目标函数有持续的波动性。而一般的SGD会使用更小的步长，来使得目标函数平滑的下降。dropout快速的探索着许多不同的方向然后拒绝那些损害性能的方向，而SGD缓慢而平稳的朝向最可能的方向移动。</li><li>实验中SGD使得relu饱和在0值的时间少于5%，而dropout则超过60%。由于relu激活函数中的0值是一个常数，这就会阻止梯度在这些单元上传播（无论正向还是反向），这也就使得这些单元很难再次激活，这会导致很多单元由激活转变为非激活。而maxout就不会存在这样的问题，梯度在maxout单元上总是能够传播，即使maxout出现了0值，但是这些0值是参数的函数可以被改变，从而maxout单元总是激活的。单元中较高比例的且不易改变的0值会损害优化性能。</li><li>dropout要求梯度随着dropout mask的改变而明显改变，而一旦梯度几乎不随着dropout mask的改变而改变时，dropout就简化成为了SGD。relu网络的低层部分会有梯度衰减的问题（梯度的方差在高层较大而反向传播到低层后较小）。maxout更好的将变化的信息反向传播到低层并帮助dropout以类似bagging的方式训练低层参数。relu则由于饱和使得梯度损失，导致dropout在低层的训练类似于一般的SGD。</li></ul><h2 id="总结文中的点"><a href="#总结文中的点" class="headerlink" title="总结文中的点"></a>总结文中的点</h2><ul><li>单个<code>maxout</code>激活函数可以理解成一种分段线性函数来近似任意凸函数（任意的凸函数都可由分段线性函数来拟合）。它在每处都是局部线性的（k个“隐隐层”节点都是线性的，取其最大值则为局部线性，分段的个数与k值有关），而一般的激活函数都有明显的曲率。</li><li>如同MLP一样，maxout网络也可以拟合任意连续函数。只要<code>maxout</code>单元含有任意多个“隐隐层”节点，那么只要两个隐层的maxout网络就可以实现任意连续函数的近似。</li><li>maxout网络不仅可以学习到隐层之间的关系，还可以学习到每个隐层单元的激活函数。</li><li>maxout放弃了传统激活函数的设计，它产生的表示不再是稀疏的，但是它的梯度是稀疏的，且dropout可以将它稀疏化。</li><li>maxout没有上下界，所以让它在某一端饱和是零概率事件。</li><li>如果训练时使用dropout，则dropout操作在矩阵相乘之前，而并不对<code>max</code>操作的输入执行dropout。</li><li>使用maxout会默认一个先验：样本集是凸集可分的。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python:理解 yield 关键字</title>
      <link href="2019/02/25/Python-%E7%90%86%E8%A7%A3-yield-%E5%85%B3%E9%94%AE%E5%AD%97/"/>
      <url>2019/02/25/Python-%E7%90%86%E8%A7%A3-yield-%E5%85%B3%E9%94%AE%E5%AD%97/</url>
      
        <content type="html"><![CDATA[<pre><code>转载：https://liam.page/2017/06/30/understanding-yield-in-python/</code></pre><h1 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h1><p>为了理解<a href="https://github.com/sususushi/reconstruction-network-for-video-captioning" target="_blank" rel="noopener">reconstruction-network</a> 代码中，如下代码是如何实现的，查看了此篇博客，并转载。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cycle</span><span class="params">(iterable)</span>:</span>  </span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:  </span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> iterable:  </span><br><span class="line">            <span class="keyword">yield</span> x</span><br><span class="line"></span><br><span class="line">train_data_loader = iter(cycle(MSVD.train_data_loader))</span><br><span class="line"><span class="keyword">for</span> iteration, batch <span class="keyword">in</span> enumerate(train_data_loader, <span class="number">1</span>):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> iteration == C.train_n_iteration:  </span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure></p><p>解释：<strong>iter()</strong> 是python的一个函数，用来生成迭代器。而cycle中一整个for循环是对整个数据集调用一遍，而外部又有一个while true，该判断是一直成立的，则，会一直调用数据。因此此处不适用n_epoch 来停止加载数据，而是使用train_n_iteration。</p><hr><p>Python 是非常灵活的语言，其中 <code>yield</code> 关键字是普遍容易困惑的概念。</p><p>此篇将介绍 <code>yield</code> 关键字，及其相关的概念。</p><h2 id="迭代、可迭代、迭代器"><a href="#迭代、可迭代、迭代器" class="headerlink" title="迭代、可迭代、迭代器"></a>迭代、可迭代、迭代器</h2><h3 id="迭代（iteration）与可迭代（iterable）"><a href="#迭代（iteration）与可迭代（iterable）" class="headerlink" title="迭代（iteration）与可迭代（iterable）"></a>迭代（iteration）与可迭代（iterable）</h3><blockquote><p>迭代是一种操作；可迭代是对象的一种特性。</p></blockquote><p>很多数据都是「容器」；它们包含了很多其他类型的元素。实际使用容器时，我们常常需要逐个获取其中的元素。<strong>逐个获取元素的过程，就是「迭代」</strong>。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iteration</span></span><br><span class="line">a_list = [1, 2, 3]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a_list:</span><br><span class="line">    <span class="builtin-name">print</span>(i)</span><br></pre></td></tr></table></figure><p>如果我们可以从一个对象中，逐个地获取元素，那么我们就说这个对象是「可迭代的」。</p><p>Python 中的顺序类型，都是可迭代的（<code>list</code>, <code>tuple</code>, <code>string</code>）。其余包括 <code>dict</code>, <code>set</code>, <code>file</code> 也是可迭代的。对于用户自己实现的类型，如果提供了 <code>__iter__()</code> 或者 <code>__getitem__()</code> 方法，那么该类的对象也是可迭代的。</p><h3 id="迭代器（iterator）"><a href="#迭代器（iterator）" class="headerlink" title="迭代器（iterator）"></a>迭代器（iterator）</h3><blockquote><p>迭代器是一种对象。</p></blockquote><p>迭代器抽象的是一个「数据流」，是只允许迭代一次的对象。对迭代器不断调用 <code>next()</code> 方法，则可以依次获取下一个元素；当迭代器中没有元素时，调用 <code>next()</code> 方法会抛出 <code>StopIteration</code> 异常。迭代器的 <code>__iter__()</code> 方法返回迭代器自身；因此迭代器也是可迭代的。</p><h3 id="迭代器协议（iterator-protocol）"><a href="#迭代器协议（iterator-protocol）" class="headerlink" title="迭代器协议（iterator protocol）"></a>迭代器协议（iterator protocol）</h3><blockquote><p>迭代器协议指的是容器类需要包含一个特殊方法。</p></blockquote><p>如果一个容器类提供了 <code>__iter__()</code> 方法，并且该方法能返回一个能够逐个访问容器内所有元素的迭代器，则我们说该容器类实现了迭代器协议。</p><p>Python 中的迭代器协议和 Python 中的 <code>for</code> 循环是紧密相连的。</p><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">iterator</span> protocol <span class="keyword">and</span> <span class="keyword">for</span> <span class="keyword">loop</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> something:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><p>Python 处理 <code>for</code> 循环时，首先会调用内建函数 <code>iter(something)</code>，它实际上会调用 <code>something.__iter__()</code>，返回 <code>something</code> 对应的迭代器。而后，<code>for</code> 循环会调用内建函数 <code>next()</code>，作用在迭代器上，获取迭代器的下一个元素，并赋值给 <code>x</code>。此后，Python 才开始执行循环体。</p><h2 id="生成器、yield-表达式"><a href="#生成器、yield-表达式" class="headerlink" title="生成器、yield 表达式"></a>生成器、<code>yield</code> 表达式</h2><h3 id="生成器函数（generator-function）和生成器（generator）"><a href="#生成器函数（generator-function）和生成器（generator）" class="headerlink" title="生成器函数（generator function）和生成器（generator）"></a>生成器函数（generator function）和生成器（generator）</h3><blockquote><p>生成器函数是一种特殊的函数；生成器则是特殊的迭代器。</p></blockquote><p>如果一个函数包含 <code>yield</code> 表达式，那么它是一个生成器函数；调用它会返回一个特殊的迭代器，称为生成器。</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def <span class="built_in">func</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">def <span class="built_in">gen</span>():</span><br><span class="line">    yield <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span>(<span class="built_in">type</span>(func))   <span class="meta"># &lt;class 'function'&gt;</span></span><br><span class="line"><span class="keyword">print</span>(<span class="built_in">type</span>(gen))    <span class="meta"># &lt;class 'function'&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span>(<span class="built_in">type</span>(<span class="built_in">func</span>())) <span class="meta"># &lt;class 'int'&gt;</span></span><br><span class="line"><span class="keyword">print</span>(<span class="built_in">type</span>(<span class="built_in">gen</span>()))  <span class="meta"># &lt;class 'generator'&gt;</span></span><br></pre></td></tr></table></figure><p>如上，生成器 <code>gen</code> 看起来和普通的函数没有太大区别。仅只是将 <code>return</code> 换成了 <code>yield</code>。用 <code>type()</code> 函数打印二者的类型也能发现，<code>func</code> 和 <code>gen</code> 都是函数。然而，二者的返回值的类型就不同了。<code>func()</code> 是一个 <code>int</code> 类型的对象；而 <code>gen()</code> 则是一个迭代器对象。</p><h3 id="yield-表达式"><a href="#yield-表达式" class="headerlink" title="yield 表达式"></a><code>yield</code> 表达式</h3><p>如前所述，如果一个函数定义中包含 <code>yield</code> 表达式，那么该函数是一个生成器函数（而非普通函数）。实际上，<code>yield</code> 仅能用于定义生成器函数。</p><p>与普通函数不同，生成器函数被调用后，其函数体内的代码并不会立即执行，而是返回一个生成器（generator-iterator）。当返回的生成器调用成员方法时，相应的生成器函数中的代码才会执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        <span class="keyword">yield</span> x ** <span class="number">2</span></span><br><span class="line">square_gen = square()</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> square_gen:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><p>前面说到，<code>for</code> 循环会调用 <code>iter()</code> 函数，获取一个生成器；而后调用 <code>next()</code> 函数，将生成器中的下一个值赋值给 <code>x</code>；再执行循环体。因此，上述 <code>for</code> 循环基本等价于：</p><figure class="highlight monkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">genitor = square_gen.__iter__()</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    x = geniter.<span class="keyword">next</span>()<span class="meta"> # Python 3 是 __next__()</span></span><br><span class="line">    <span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p>注意到，<code>square</code> 是一个生成器函数；作为它的返回值，<code>square_gen</code> 已经是一个迭代器；迭代器的 <code>__iter__()</code> 返回它自己。因此 <code>geniter</code> 对应的生成器函数，即是 <code>square</code>。</p><p>每次执行到 <code>x = geniter.next()</code> 时，<code>square</code> 函数会从上一次暂停的位置开始，一直执行到下一个 <code>yield</code> 表达式，将 <code>yield</code> 关键字后的表达式列表返回给调用者，并再次暂停。注意，<strong>每次从暂停恢复时，生成器函数的内部变量、指令指针、内部求值栈等内容和暂停时完全一致</strong>。</p><h3 id="生成器的方法"><a href="#生成器的方法" class="headerlink" title="生成器的方法"></a>生成器的方法</h3><p>生成器有一些方法。调用这些方法可以控制对应的生成器函数；不过，若是生成器函数已在执行过程中，调用这些方法则会抛出 <code>ValueError</code> 异常。</p><ul><li><code>generator.next()</code>：从上一次在 <code>yield</code> 表达式暂停的状态恢复，继续执行到下一次遇见 <code>yield</code> 表达式。当该方法被调用时，当前 <code>yield</code> 表达式的值为 <code>None</code>，下一个 <code>yield</code> 表达式中的表达式列表会被返回给该方法的调用者。若没有遇到 <code>yield</code> 表达式，生成器函数就已经退出，那么该方法会抛出 <code>StopIterator</code> 异常。</li><li><code>generator.send(value)</code>：和 <code>generator.next()</code> 类似，差别仅在与它会将当前 <code>yield</code> 表达式的值设置为 <code>value</code>。</li><li><code>generator.throw(type[, value[, traceback]])</code>：向生成器函数抛出一个类型为 <code>type</code> 值为 <code>value</code> 调用栈为 <code>traceback</code> 的异常，而后让生成器函数继续执行到下一个 <code>yield</code> 表达式。其余行为与 <code>generator.next()</code> 类似。</li><li><code>generator.close()</code>：告诉生成器函数，当前生成器作废不再使用。</li></ul><h3 id="举例和说明"><a href="#举例和说明" class="headerlink" title="举例和说明"></a>举例和说明</h3><h4 id="如果你看不懂生成器函数"><a href="#如果你看不懂生成器函数" class="headerlink" title="如果你看不懂生成器函数"></a>如果你看不懂生成器函数</h4><p>如果你还是不太能理解生成器函数，那么大致上你可以这样去理解。</p><ul><li>在函数开始处，加入 <code>result = list()</code>；</li><li>将每个 <code>yield</code> 表达式 <code>yield expr</code> 替换为 <code>result.append(expr)</code>；</li><li>在函数末尾处，加入 <code>return result</code>。</li></ul><h4 id="关于「下一个」yield-表达式"><a href="#关于「下一个」yield-表达式" class="headerlink" title="关于「下一个」yield 表达式"></a>关于「下一个」<code>yield</code> 表达式</h4><p>介绍「生成器的方法」时，我们说当调用 <code>generator.next()</code> 时，生成器函数会从当前位置开始执行到下一个 <code>yield</code> 表达式。这里的「下一个」指的是执行逻辑的下一个。因此</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f123</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="number">2</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> f123(): <span class="comment"># 1, 2, and 3, will be printed</span></span><br><span class="line">    print(item)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f13</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">False</span>:</span><br><span class="line">        <span class="keyword">yield</span> <span class="number">2</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> f13(): <span class="comment"># 1 and 3, will be printed</span></span><br><span class="line">    print(item)</span><br></pre></td></tr></table></figure><h4 id="使用-send-方法与生成器函数通信"><a href="#使用-send-方法与生成器函数通信" class="headerlink" title="使用 send() 方法与生成器函数通信"></a>使用 <code>send()</code> 方法与生成器函数通信</h4><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def <span class="function"><span class="keyword">func</span><span class="params">()</span>:</span></span><br><span class="line">    x = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        y = (yield x)</span><br><span class="line">        x += y</span><br><span class="line"></span><br><span class="line">geniter = <span class="function"><span class="keyword">func</span><span class="params">()</span></span></span><br><span class="line">geniter.<span class="keyword">next</span>()  <span class="meta"># 1</span></span><br><span class="line">geniter.<span class="built_in">send</span>(<span class="number">3</span>) <span class="meta"># 4</span></span><br><span class="line">geniter.<span class="built_in">send</span>(<span class="number">10</span>)<span class="meta"># 14</span></span><br></pre></td></tr></table></figure><p>此处，生成器函数 <code>func</code> 用 <code>yield</code> 表达式，将处理好的 <code>x</code> 发送给生成器的调用者；与此同时，生成器的调用者通过 <code>send</code> 函数，将外部信息作为生成器函数内部的 <code>yield</code> 表达式的值，保存在 <code>y</code> 当中，并参与后续的处理。</p><p>这一特性是使用 <code>yield</code> 在 Python 中使用协程的基础。</p><h2 id="yield-的好处"><a href="#yield-的好处" class="headerlink" title="yield 的好处"></a><code>yield</code> 的好处</h2><p>Python 的老用户应该会熟悉 Python 2 中的一个特性：内建函数 <code>range</code> 和 <code>xrange</code>。其中，<code>range</code> 函数返回的是一个列表，而 <code>xrange</code> 返回的是一个迭代器。</p><blockquote><p>在 Python 3 中，<code>range</code> 相当于 Python 2 中的 <code>xrange</code>；而 Python 2 中的 <code>range</code> 可以用 <code>list(range())</code> 来实现。</p></blockquote><p>Python 之所以要提供这样的解决方案，是因为在很多时候，我们只是需要逐个顺序访问容器内的元素。大多数时候，我们不需要「一口气获取容器内所有的元素」。比方说，顺序访问容器内的前 5 个元素，可以有两种做法：</p><ul><li>获取容器内的所有元素，然后取出前 5 个；</li><li>从头开始，逐个迭代容器内的元素，迭代 5 个元素之后停止。</li></ul><p>显而易见，如果容器内的元素数量非常多（比如有 <code>10 ** 8</code> 个），或者容器内的元素体积非常大，那么后一种方案能节省巨大的时间、空间开销。</p><p>现在假设，我们有一个函数，其产出（返回值）是一个列表。而若我们知道，调用者对该函数的返回值，只有逐个迭代这一种方式。那么，如果函数生产列表中的每一个元素都需要耗费非常多的时间，或者生成所有元素需要等待很长时间，则使用 <code>yield</code> 把函数变成一个生成器函数，每次只产生一个元素，就能节省很多开销了。</p><p>俗话说，投资效率是最好的投资。 如果您感觉我的文章质量不错，读后收获很大，预计能为您提高 10% 的工作效率，不妨小额捐助我一下，让我有动力继续写出更多好文章。</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Video captioning summary</title>
      <link href="2019/02/23/Video-captioning-summary/"/>
      <url>2019/02/23/Video-captioning-summary/</url>
      
        <content type="html"><![CDATA[<ul><li>总结一下各论文使用的encoder的model分别是什么，是否采用了C3D，若采用了C3D,每16帧输出一个特征向量，这样的话，n_frames/16 个特征向量，那么论文中又是如何聚合特征来得到video 特征的？<h2 id><a href="#" class="headerlink" title=" "></a> </h2></li></ul><h2 id="训练和测试的一般过程"><a href="#训练和测试的一般过程" class="headerlink" title="训练和测试的一般过程"></a>训练和测试的一般过程</h2><p>The training process predicts the next word given the previous words from groundtruth, while the generation process conditions the prediction on the ones previously generated by itself.  </p><h2 id="训练损失"><a href="#训练损失" class="headerlink" title="训练损失"></a>训练损失</h2><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0iyn2xsz9j30r00higpg.jpg" style="zoom:70%"></p><h2 id="Code-总结"><a href="#Code-总结" class="headerlink" title="Code 总结"></a>Code 总结</h2><div class="table-container"><table><thead><tr><th>model</th><th>batch_size</th><th>epoch</th><th>learning_rate</th><th>MSVD&lt;/br&gt;train-dataset</th><th>MSR-train-dataset</th></tr></thead><tbody><tr><td>video-caption.pytorch</td><td>128</td><td>6001（MSR）</td><td>0.0004 (每200epoch下降0.8)</td><td>✘</td><td>6513 pairs&lt;/br&gt;(每一次随机的从captions中选择一个作为label)</td></tr><tr><td>SA-tensorflow</td><td>100</td><td>200</td><td>0.0001（不变）</td><td>1200×41个pairs</td><td>✘</td></tr><tr><td>reconstruction-network</td><td>100</td><td>iter=100000  （epoch=100000×100/(1200*41）=203</td><td>0.00001（不变）</td><td>1200×41个pairs</td><td>✘</td></tr><tr><td>saliency-based</td><td>100</td><td>100</td><td>0.0003（不变）</td><td>略</td><td>✘</td></tr><tr><td>HRNE</td><td>200</td><td>128</td><td>0.0002</td><td>略</td><td>✘</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>model</th><th>construct vocab use which dataset</th></tr></thead><tbody><tr><td>video-caption.pytorch</td><td>MSR: all</td></tr><tr><td>SA-tensorflow</td><td>MSVD: train</td></tr><tr><td>reconstruction-network</td><td>MSVD: all</td></tr><tr><td>saliency-based</td><td></td></tr><tr><td>HRNE</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>model</th><th>loss function</th><th>input of decoder</th></tr></thead><tbody><tr><td>video-caption.pytorch</td><td>output of decoder（bs, hidden size）,经过一个全连接层得到one-hot形式(bs, n_vocab），在经过F.log_softmax。则损失函数 nn.NLLLoss（）</td><td>rnn1的输入是video feature;&lt;/br&gt;rnn2的输入是rnn1的输出cancatenate 上一步ground truth的word embedding&lt;/br&gt; output1, state1 = self.rnn1(vid_feats, state1)&lt;/br&gt; input2 = torch.cat((output1, padding_words), dim=2)&lt;/br&gt;          output2, state2 = self.rnn2(input2, state2)</td></tr><tr><td>SA-tensorflow</td><td><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0izmumz8sj30ry01l74i.jpg">LSTM的output/hidden state、经过attention加权求和得到的videofeature、上一步step的groundtruth word embedding进行concatenate，在经过全连接层、非线性层、全连接层、输入交叉熵损失函数：tf.nn.softmax_cross_entropy_with_logits</td><td>LSTM的输入是concatenate(video_feature, 上一步ground truth的word embdedding）</td></tr><tr><td>reconstruction-network</td><td>output of decoder（bs, hidden size）,经过一个全连接层得到one-hot形式(bs, n_vocab），在经过dropout。  则损失函数 nn.CrossEntropyLoss()</td><td>LSTM的输入是concatenate(video_feature, 上一步ground truth的word embdedding）</td></tr></tbody></table></div><hr><h2 id="Paper-总结"><a href="#Paper-总结" class="headerlink" title="Paper 总结"></a>Paper 总结</h2><div class="table-container"><table><thead><tr><th>model</th><th>dataset</th><th>n_frames</th></tr></thead><tbody><tr><td>S2VT</td><td>MSVD</td><td>每10帧取1帧</td></tr><tr><td>SA</td><td>MSVD</td><td>前240帧等间隔取26帧</td></tr><tr><td>h-RNN</td><td>MSVD</td><td>没讲( ˇˍˇ )</td></tr><tr><td>HRNE</td><td>MSVD</td><td>fixed 160帧</td></tr><tr><td>LSTM-TSA</td><td>MSVD</td><td>等间隔采取25帧</td></tr><tr><td>LSTM-E</td><td>MSVD</td><td>all frames</td></tr><tr><td>Reconstruction</td><td>MSVD  MSR-VTT</td><td>等间隔28帧</td></tr><tr><td>M3</td><td>MSVD  MSR-VTT</td><td>28帧for MSVD; 40帧for MSR-VTT</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>model</th><th>词频</th><th>MSVD  vocabulary</th><th>MSR-VTT  vocabulary</th></tr></thead><tbody><tr><td>Hierarchical Boundary-Aware Neural Encoder for Video Captioning</td><td>大于等于5</td><td>4215</td><td></td></tr><tr><td>Multimodal Memory Modelling for Video Captioning</td><td></td><td>13,000</td><td>29,000</td></tr><tr><td>Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning</td><td></td><td>9450</td><td>23500</td></tr><tr><td>Describing Videos by Exploiting Temporal Structure</td><td></td><td>16,000</td><td></td></tr><tr><td>Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks</td><td></td><td>12, 766（1, 297 and 670 videos ）</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>model</th><th>feature</th><th>METER</th></tr></thead><tbody><tr><td>Mean Pool + LSTM</td><td>在COCO上预训练的Alex net</td><td>29.1</td></tr><tr><td>S2VT</td><td>RGB frames on VGG Net&lt;/br&gt;optical flows on AlexNet</td><td>29.8</td></tr><tr><td>SA</td><td>GoogLeNet and 3D-CNN</td><td>29.6</td></tr><tr><td>LSTM-E</td><td>VGGNet and C3D</td><td>31.0</td></tr><tr><td>h-RNN</td><td>VGGNet and C3D</td><td>32.6</td></tr><tr><td>HRNE</td><td>GooLeNet</td><td>33.1</td></tr><tr><td>Reconstruction</td><td>Inception-V4&lt;/br&gt; last pooling layer</td><td>34.1</td></tr></tbody></table></div><h2 id="提取frames-features-之后，获取video-feature的几种方法："><a href="#提取frames-features-之后，获取video-feature的几种方法：" class="headerlink" title="提取frames features 之后，获取video feature的几种方法："></a>提取frames features 之后，获取video feature的几种方法：</h2><p><strong>1. Mean pooling</strong></p><ul><li>Translating videos to natural language using deep recurrent neural networks. NACACL, 2015</li><li>Jointly modeling embedding and translation to bridge video and language. CoRR,  2015  </li></ul><p><strong>2. Weighted mean Pooling with an attention model</strong>    </p><ul><li>Describing videos by exploiting temporal structure. ICCV, 2015  </li><li>Exploring Visual Relationship for Image Captioning</li><li>2层LSTM，第一层LSTM的输入是对object/frames features进行平均池化，第二层LSTM的输入是给定第一层的hidden state 来得到attention 系数，从而对object/frames features进行加权求和。 即第一层用平均池化的特征来表征 global feture，第二层用加权求和的特征来表征 global feature</li></ul><p><strong>3. Taking the last output from an RNN encoder which summarizes the feature sequence</strong>    </p><ul><li>Long-term recurrent convolutional networks for visual recognition and description. CVPR, 2015</li><li>Sequence to sequence - video to tex. ICCV, 2015</li><li>A multi-scale multiple instance video description network. CoRR, 2015  </li></ul><h2 id="video-captioning-的模型中，含有extract-object-proposal的论文"><a href="#video-captioning-的模型中，含有extract-object-proposal的论文" class="headerlink" title="video captioning 的模型中，含有extract object proposal的论文"></a>video captioning 的模型中，含有extract object proposal的论文</h2><ul><li>Video paragraph captioning using hierarchical recurrent neural networks.  CVPR, 2016.  </li><li>object-aware aggregation with bidirectional temporal graph for video capioning. CVPR, 2019</li></ul><h2 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h2><ul><li>一般的情况下是对decoder的部分计算loss, 并反向传播，encoder采用预训练好的model，并不在进行finetune。</li></ul><h2 id="使用objects-feature-的文章"><a href="#使用objects-feature-的文章" class="headerlink" title="使用objects feature 的文章"></a>使用objects feature 的文章</h2><ul><li></li><li>video as graph : charades 每帧提取50个objects(当objects 的数量将为25的时候，score只降了0.2), something2 :每帧提取10个objects</li><li>我的msr-vtt: 提取5个效果比较好，不会包含太多的噪声</li><li>==尽量让一个video中的objects 不同，去聚类帧之间的objects==  </li></ul><div class="table-container"><table><thead><tr><th>论文</th><th>charades( 30s)</th><th></th><th>something-something( 3-6s )</th><th>activity</th><th>MSVD（10-25s）</th></tr></thead><tbody><tr><td>video as graph</td><td>16帧 *50</td><td></td><td>16帧* 10</td><td>10帧*100</td><td></td></tr><tr><td>HTM （video captioning)</td><td>80帧 *30</td><td></td><td></td><td></td><td>28帧*30</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>论文</th><th>object detector</th><th>return</th></tr></thead><tbody><tr><td>(ACM 2019)Hierarchical Global-Local Temporal Modeling for VideoCaptioning</td><td>Faster rcnn 去掉rcnn的分类层，</td><td>提取_head_to_tail之后的特征 2048维</td></tr><tr><td>(CVPR 2019)Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning</td><td>YoLo</td><td>没说</td></tr><tr><td>(CVPR 2019)Grounded Video Description</td><td>a Faster RCNN model [24] with a ResNeXt-101 FPN backbone (在VG上预训练，类别会比coco 多，同时训练目标检测和属性分类)</td><td>返回的是fc6,我认为是_head_to_tail</td></tr><tr><td>( ECCV 2018)Videos as Space-Time Region Graph</td><td>the RPN with ResNet-50 backbone and FPN ==(需要注意，这里具体的：先由I3D得到THWd的特征，然后对32帧，每2帧取1帧，去得到这16帧的bbox，得到了bbox不是直接去得到pooled_feats，而是通过I3D的空间特征，bbox, Roi Align来得到bbox 的 region feat) ==</td><td>返回的是roi_pooling的7*7的，然后再进行平均池化</td></tr><tr><td>(CVPR 2019)Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning</td><td>MASK RCNN，在COCO上预训练，</td><td>将得到的regions 裁剪成图像，再送入ResNet-200中，得到res-layer5c的局部特征</td></tr><tr><td>（CVPR 2019）Auto-Encoding Scene Graphs for Image Captioning</td><td>faster rcnn , 使用 r-cnn 输出的 rois， 然后作用到 base feat上，使用 roi pooling 的到 pooled feats</td><td>返回 7*7的pooled feats</td></tr></tbody></table></div><ul><li>根据faster r-cnn 的网络结构，rpn部分输出bbox的预测，rcnn部分也输出bbox的预测，在目标检测任务中，采用rcnn的输出作为最后的结果。</li><li>但是在视频帧提取 object 的任务中，一般采用的是rpn部分输出的bbox,  why？ 这是因为，想要得到的不是bbox的坐标，而是bbox feats， 因此，直接取pooled_feats更加简洁方便。</li><li>所以在利用mmdetection时，设置 在rpn部分的max_region_per</li><li></li></ul>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(h-RNN)Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks</title>
      <link href="2019/02/23/h-RNN-Video-Paragraph-Captioning-Using-Hierarchical-Recurrent-Neural-Networks/"/>
      <url>2019/02/23/h-RNN-Video-Paragraph-Captioning-Using-Hierarchical-Recurrent-Neural-Networks/</url>
      
        <content type="html"><![CDATA[<h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li>这篇文章主要针对于一个video 划分为多个interval，并分别对其进行caption这样的数据集。其中段落生成器的作用：可以捕捉句子之间的相互依赖关系，同时段落生成器的输出作为句子生成器的输入，可以使得<strong>下一个句子的生成是建立在当前句子的语境下生成的</strong>。</li><li>另外对于MSVD这种一个video直接由一个sentence来描述的数据集，段落生成器不起作用，只是在<strong>decoder的结构相较于其他的model有不同之处</strong>：video feature 不输入decoder 的 RNN，而是与RNN的hidden state 级联后输入Multimodal层，Multimodal( concatenate( hidden state，video feature ) )。</li></ul><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>提出了一个方法：<strong>利用分层RNN开解决视频描述</strong>。我们的分层框架包含一个句子生成器和一个段落生成器。<strong><em>句子生成器</em></strong>产生一个简短的句子，这个句子可以描述一个特定的短视频间隔。它利用时间和空间的注意力机制，有选择地将注意力集中在视觉元素上。<strong><em>段落生成器</em></strong>通过将句子生成器产生的句子嵌入与段落历史结合起来作为输入来捕获句子间的依赖关系，并段落生成器的parahraph state 将作为输出语句生成器的新初始状态，然后句子生成器再生成下一个句子，~ 循环</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li><strong>对当前方法的总结：</strong><br>当给出了从视频帧中提取的深度卷积特征序列(例如vggnet 和c3d)，则视频的特征可以用以下几种方法获取：<br>（1）平均池化[1, 2]<br>（2）加权平均池化（attention 机制）[3]<br>（3）RNN encoder 的最后一个输出[4, 5, 6]<br>当前基于序列学习的视频描述方法，只专注于为一个简短的视频剪辑生成一个句子。到目前为止，深层次学习方法还没有尝试为长视频生成多个句子或段落的问题。使用平均池化得到 video feature 的方法，只适用于 short video clips where there is only one major event，随后有了 recurrent encoder 和 attention model。<br>我们的方法也采用了attention 机制。但是我们的框架和他们的框架之间存在两个不同之处，1. 解释<strong>空间注意力</strong>：即对每个frames 提取object proposals 然后基于注意力机制对proposal features of one frames 进行加权求和来得到frames features。这对于数据集中 object 非常小且难定位的情况有很大的帮助。另外，也解释一下<strong>时域注意力</strong>：是指对features of frames 进行加权求和，从而得到 video feature。 <strong>本文的注意力机制</strong>：提取M帧，每帧K个object, 则对这M*K个 object 进行基于attention 系数的加权求和。2. 在加权视频特征和注意权重之后，我们不会在加权特征的基础上限制递归层的隐藏状态。 </li><li><strong>Motivation</strong><br>大多数视频描述的不仅仅是一个事件。只用一个简短的句子来描述一个语义丰富的视频通常会产生信息不多甚至无聊的结果。例如，一个video 应该描述成<strong>那个人把土豆切成片，把洋葱切成块，把洋葱和土豆放进锅里</strong>，但是只产生one sentence的方法可能会说<strong>这个人在做饭</strong>。</li><li><strong>Idea</strong><br>我们想要利用句子之间的时域依赖性，这样，在生成段落时，句子就不会独立地生成。相反，一个句子的生成可能会受到前几句所提供的语义上下文的影响。<br>我们的分层RNN结构包括两个生成器，一个句子生成器和一个段落生成器，这两个生成器都使用RNN layers<br>据我们所知，这是分层RNN在视频字幕任务中的首次应用。  </li></ul><h2 id="Hierarchical-RNN-for-Video-Captioning"><a href="#Hierarchical-RNN-for-Video-Captioning" class="headerlink" title="Hierarchical RNN for Video Captioning"></a>Hierarchical RNN for Video Captioning</h2><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0gmi1zayjj314o0h2n1f.jpg"><br><strong><em>designed by yaya:</em></strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0k7r1p5ygj316f0jyq47.jpg"><br>我们的方法：在句子生成器之上堆叠了一个段落生成器<br><strong>句子生成器</strong>：1) RNN 用来语言建模 2) 多模态层对多源信息进行聚合 3) 注意力模型</p><ul><li>RNN1：word embedding 作为RNN的输入，并更新 hidden state </li><li>Attention layer: RNN 的hidden state 作为attention layer 的输入，来计算weight:<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0gohii8woj30xx02saab.jpg" style="zoom:30%"><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0gom9zybsj30kg04mwes.jpg" style="zoom:45%"><br>假设视频中有M帧，每帧有K个objects，则features：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0go2kemkqj30bq01pt8m.jpg" style="zoom:50%"><br>若计算出了一组权重:<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0go382u2qj309m0250so.jpg" style="zoom:50%"><br>则 video feature：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0go5n8g7dj30if036mx8.jpg" style="zoom:30%"><br>得到的video feature是一个特征通道，完整的模型是两个特征通道，一个由 object appearance 生成，另一个由action 生成</li><li>Multimodal<br>输入：RNN 的hidden state <strong>concate</strong> 2个Attention 的输出（两个特征通道Ua  C3D，action feature；Uo aggregate object appearance）。既有语言，又有视觉，因此成为多模态。<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0gpaasqgmj30sb022dg0.jpg" style="zoom:50%"></li><li>Hidden layer<br>-输出维度512 与 word embdeding 的维度一致</li><li>Softmax layer<br>输出维度与vocabulary size 一致</li><li>Maxid layer<br>Maxid layer 在softmax layer 的输出中挑选了最大值所在的索引，该索引将会被作为predicted word 的id(对应到vocabulary 的 索引)</li><li>预测的单词将会作为句子生成器的下一个输入（test）；下一个输入单词总是由带注释的句子（ground truth/ reference）提供。</li></ul><p><strong>段落生成器</strong> : 另外一个RNN，来建模句子之间的相互依赖。输入：1.句子生成器的输出， 2. paragraph history  输出：该输出作为句子生成器的初始状态<br>使用的RNN为GRU</p><ul><li>Word Embedding<br>1) 对sentences中的所有单词的embedding 取平均，得到一个压缩embedding vector<br>2) 同时也接受RNN1 的最后一个hidden state 作为 压缩表达<br>将上面两个压缩表达concatenated </li><li>Sentences Embedding<br>将上面concatenated 的特征输入该层，得到512维度的输出</li><li>RNN2</li><li>Paragraph State layer<br>输入：结合RNN2的hidden state 和 sentence embedding<br>输出：作为RNN1下一个句子的初始状态，为句子生成器提供了段落历史是有必要的，以便在上下文语境中中生成下一句。<br>它实质上为句子生成器提供了段落历史，以便在上下文中生成下一句。</li></ul><h2 id="Training-and-Generation"><a href="#Training-and-Generation" class="headerlink" title="Training and Generation"></a>Training and Generation</h2><p><strong>整个网络的循环过程</strong></p><ul><li>当RNN1在每一时间步骤中不断更新其hidden state，RNN2只在处理完整句子时才更新其hidden state。</li><li>RNN1 由beam search 得到 J 个sequence cost 最低的句子，挑选出1个最低的，然后送入RNN2。RNN2又输出隐层状态，最为RNN1下一个句子的初始隐层状态。如此循环，直至， when the sentence received by the paragraph generator is the EOP (end-of-paragraph) which consists of only the BOS and the EOS。</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>数据集</strong><br>two benchmark datasets: YouTubeClips and TACoS-MultiLevel<br>YouTubeClips： 虽然每个video 有多个sentences进行描述，但是sentences是对这个video的整体描述，而不是分别对video进行分段描述。因此这个数据集作为我们方法的特例，paragraph length N=1.<br><strong>Encoder</strong></p><ul><li>由于YouTubeClips数据集中的object 十分显著，因此不进行提取object的操作，只对frame 提取特征，这样attention 只包括temporal attention ，而不包括 spatial attention。</li><li>对于TACoS-MultiLevel 数据集，首先使用光流大体的提取boundinig box，然后沿着bounding box 的边，提取220*220的image patches，保证相邻两个box 重合度为50%。使用VGG模型对每个patch提取特征，并使用attention的权重，对这些patches进行加权求和。此时，attention同时包括temporal 和 spatial。</li><li>C3D 提取 action/motion feature of video<br>C3D 模型：输入frames of video ，每16帧输出一个固定长度的特征向量。然后采用attention机制对C3D特征进行polling（加权求和）<br><strong>实验结果对比分析</strong></li><li>YouTubeClips<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0hdv8rde9j30i7099jtg.jpg"><br>相比于<strong>LSTM-E[2]</strong>（同样同时利用了VGG和C3D特征），我们的模型效果更好。<br>相比于<strong>SA[3]</strong>（同样利用了temporal attention）我们的方法更好，原因：RNN的输入不包括视频特征，换句话说， hidden state的更新不建立在video feature的基础上。video feature直接的输入到multimodal layer。</li><li>TACoS-MultiLevel<br>这里不做分析（可以自行参考论文）</li></ul><h2 id="Discussions-and-Limitations"><a href="#Discussions-and-Limitations" class="headerlink" title="Discussions and Limitations"></a>Discussions and Limitations</h2><ol><li>目前我们使用的目标检测方法很难处理small object，造成在生成句子时，极容易混淆，比如应该是orange ，却生成了mango</li><li>句子信息通过段落循环层单向流动，从段落开始到结尾，但也不是以相反的方式。如果第一个句子中含有错误信息，则会导致错误信息依次传递，目前使用双向RNN来生成句子，仍然是一个开放性的问题（yaya: sorry , i don’t kow what’s mean，可能是目前还不知道使用BiRNN来生成句子的效果是否好于单向RNN）。</li><li>与其他大多数图像/视频字幕方法一样，我们的方法存在一个已知的问题，即训练所使用的目标函数与生成方法所使用的目标函数之间存在差异。训练过程给定来自groundtruth的先前单词来预测下一个单词，而生成过程则对先前由其自身生成的单词进行预测。这个问题在我们的分层框架中更加放大，因为在训练时，段落生成器输入的是groundtruth，但是在测试阶段，输入的是句子生成器生成的句子。潜在的解决办法：</li></ol><ul><li>Scheduled Sampling<br>在训练过程中增加Scheduled Sampling，即随机的选择words of groundtruth或者由model生成的单词。</li><li>在训练的过程中直接优化metric(BLEU， CIDER, etc)</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <strong><em>Translating videos to natural language using deep recurrent neural networks</em></strong>. NACACL, 2015<br>[2] <strong><em>Jointly modeling embedding and translation to bridge video and language</em></strong>. CoRR,  2015<br>[3] <strong><em>Describing videos by exploiting temporal structure</em></strong>. ICCV, 2015<br>[4] <strong><em>Long-term recurrent convolutional networks for visual recognition and description</em></strong>. CVPR, 2015<br>[5] <strong><em>Sequence to sequence - video to text</em></strong>. ICCV, 2015<br>[6]  <strong><em>A multi-scale multiple instance video description network</em></strong>. CoRR, 2015</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hierarchical Boundary-Aware Neural Encoder for Video Captioning</title>
      <link href="2019/02/22/Hierarchical-Boundary-Aware-Neural-Encoder-for-Video-Captioning/"/>
      <url>2019/02/22/Hierarchical-Boundary-Aware-Neural-Encoder-for-Video-Captioning/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0g5c5cwmxj30l00mh45o.jpg"><br>In this paper, we focus on the video encoding stage. we propose a recurrent network which can learn to adapt its temporal structure to input data.Our network is the first proposal which exploits temporal segments in<br>video captioning。<br>在这篇文章中，给出了一个循环视频编码方案，该方案可以发现和利用视频的分层结构。不同于经典的编码解码方法（视频由一个循环层来连续的编码），我们提出了一个新颖的LSTM单元， 其可以识别帧/段之间非连续的点，相应地修改编码层的时间连接。</p><h2 id="Encoder-Model"><a href="#Encoder-Model" class="headerlink" title="Encoder Model"></a>Encoder Model</h2><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0g57reyepj316c0m7k74.jpg">  </p><ul><li><strong>traditional lstm network</strong><br>使用LSTM来获得video feature，将每个frames 按每time step 依次送入LSTM，最后一个LSTM cell 的hidden state 用来得到video feature。  <ul><li><strong>Time Boundary-aware LSTM network</strong> <strong><em>(ours)</em></strong><br>figure1 与 figure2 结合来看，存在BD（boundary detection ）来检测该帧是否为一个边界（an appearance or action change），若BD检测到存在一个边界，则<strong>保存当前LSTM的输出</strong>，并开始一个新的LSTM（即，hidden state and the cell memory 被重新初始化）。这就确保了在边界之后的输入数据，不受边界之前数据的影响。<br>经过对all frames of video 进行这样的操作，于是得到可变长度的输出 (s1; s2; …; sm), m是检测到segments的数量。<br>这组输出又经过另外一个LSTM层（称为第二LSTM层），第二LSTM层的hidden state 作为整个视频的特征（参考figure1）。<br>Decoder model</li></ul></li></ul><hr><p>A Gated Recurrent Unit (GRU) layer</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>video-caption-dataset</title>
      <link href="2019/02/22/video-caption-dataset/"/>
      <url>2019/02/22/video-caption-dataset/</url>
      
        <content type="html"><![CDATA[<h2 id="Youtube2Text-（MSVD）-1"><a href="#Youtube2Text-（MSVD）-1" class="headerlink" title="Youtube2Text （MSVD）[1]"></a>Youtube2Text （MSVD）[1]</h2><ul><li>这个数据集包含 1967个短视频，10-25s，平均时长为9s，视频包含不同的人，动物，动作，场景等。</li><li>每个视频由不同的人标注了多个句子，大约41 annotated sentences per clip，共有 80839 个sentences，平均每个句子有8个words，这些所有的句子中共包含近16000个 unique words。</li><li>caption中包括多国的语言进行描述，部分论文中采取只选用laguage = english 的caption 进行训练和测试[3][4]</li><li>采用的split根据 [2] ： 1,200 videos for training, 100 for validation and 670 for testing.<br><a href="https://github.com/ShiYaya/video_captioning/tree/master/MSVD" target="_blank" rel="noopener">我的整理</a></li></ul><ul><li>数据的下载：</li><li><a href="https://www.microsoft.com/en-us/download/details.aspx?spm=a2c4e.11153940.blogcont209612.6.42ba7e9eAA1K2o&amp;id=52422&amp;from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F38cf15fd-b8df-477e-a4e4-a4680caa75af%2Fdefault.aspx" target="_blank" rel="noopener">[website]</a></li><li>原数据：只给出了video_id,以及strart and end time , 若需要video数据，则需要自己通过url下载</li><li>某篇对于视频分析的总结，给出了<a href="https://github.com/sinyeratlantis/sinyeratlantis.github.io/blob/master/content/dl/%E8%A7%86%E9%A2%91%E5%88%86%E6%9E%90%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94.md" target="_blank" rel="noopener">MSVD的下载链接</a>，可用，推荐(下载速度快，且video命名相对较好)☀☀&lt;/br&gt;<br><a href="http://www.cs.utexas.edu/users/ml/clamp/videoDescription/YouTubeClips.tar" target="_blank" rel="noopener">直接的下载链接</a><strong>[推荐]</strong></li><li>某篇github 含有MSVD(Youtube2Text)的<a href="https://github.com/yaoli/arctic-capgen-vid" target="_blank" rel="noopener">[preproceed dataset]</a>&lt;/br&gt;<br><a href="http://lisaweb.iro.umontreal.ca/transfert/lisa/users/yaoli/youtube2text_iccv15.zip" target="_blank" rel="noopener">直接的下载链接</a>(下载速度慢)</li><li>该篇github上含有<a href="https://github.com/ShiYaya/Video-Description-with-Spatial-Temporal-Attention#video-datas-and-pre-extracted-features-on-msvd-dataset" target="_blank" rel="noopener">MSVD数据集下载链接</a>&lt;/br&gt;<br><a href="https://www.multcloud.com/share/050e69cd-cab9-4ba3-a671-ed459341ab41" target="_blank" rel="noopener">直接的下载链接</a></li><li>对caption 常用的预处理: 1) verting all text to lower case, 2) tokenizing the sentences, 3) removing punctuation.</li></ul><h2 id="MSR-VTT-dataset"><a href="#MSR-VTT-dataset" class="headerlink" title="MSR-VTT dataset"></a>MSR-VTT dataset</h2><p>下载链接：<a href="https://www.mediafire.com/folder/h14iarbs62e7p/shared" target="_blank" rel="noopener">https://www.mediafire.com/folder/h14iarbs62e7p/shared</a></p><p>以下的链接可能不能用了 </p><p>共10000个video, 每个video有20个sentences, 共20万 video/sentence pair，10-30s居多</p><ul><li>split:  train:6513, val:497, test：2990</li><li>MSR-VTT dataset v2 , just video url: <a href="http://ms-multimedia-challenge.com/2017/dataset" target="_blank" rel="noopener">http://ms-multimedia-challenge.com/2017/dataset</a></li><li>author split train test val by himself and provied video data :<a href="https://github.com/xiadingZ/video-caption.pytorch" target="_blank" rel="noopener">https://github.com/xiadingZ/video-caption.pytorch</a>  &lt;/br&gt;<br><strong>下载这个数据集即可使用，但是还需要再找split!</strong>&lt;/br&gt;</li><li>MSR VTT 采用的split 是2016年提供的，<strong>目前科研广泛使用的都是2016年的</strong>。</li><li><a href="https://github.com/adi-dhal/In_Depth_Video_Analysis/tree/master/msr-vtt/2016" target="_blank" rel="noopener">[split]</a><br>MSR-VTT. Test video doesn’t have captions, so I spilit train-viedo to train/val/test. Extract and put them in <code>./data/</code> directory</li></ul><p>train-video: <a href="https://drive.google.com/file/d/1Qi6Gn_l93SzrvmKQQu-drI90L-x8B0ly/view?usp=sharing" target="_blank" rel="noopener">download link</a> &lt;/br&gt;<br>test-video: <a href="https://drive.google.com/file/d/10fPbEhD-ENVQihrRvKFvxcMzkDlhvf4Q/view?usp=sharing" target="_blank" rel="noopener">download link</a> &lt;/br&gt;<br>json info of train-video: <a href="https://drive.google.com/file/d/1LcTtsAvfnHhUfHMiI4YkDgN7lF1-_-m7/view?usp=sharing" target="_blank" rel="noopener">download link</a> &lt;/br&gt;<br>json info of test-video: <a href="https://drive.google.com/file/d/1Kgra0uMKDQssclNZXRLfbj9UQgBv-1YE/view?usp=sharing" target="_blank" rel="noopener">download link</a> &lt;/br&gt;</p><ul><li><p>download.py 可以下载MSR-VTT数据集(step by video)：<a href="https://github.com/OSUPCVLab/VideoToTextDNN" target="_blank" rel="noopener">[链接]</a></p></li><li><p>msr-vtt 2017 vs 2016<br>In the 2nd MSR Video to Language Challenge, we have combined the training set, validation set, and testing data in the 1st MSR Video to Language Challenge as the new training data. An additional test set of around 3K video clips will be released on June 1st as the final evaluation set. As such, we have 10K video clips for training and 3K video clips for testing this year. Each video is annotated with 20 natural sentences.&lt;/br&gt;<br>总结：就仅仅是将2016的train val and test 综合到一起，组成了2017： 一个大的含10000个video的train 数据集，并另外提供了2000个test video。&lt;/br&gt;<br>科研上普遍使用2016的分割方案，</p></li></ul><ul><li>In MSR-VTT dataset, we provide the category information for each video clip and the video clip contains audio information as well.</li></ul><h2 id="VATEX数据集"><a href="#VATEX数据集" class="headerlink" title="VATEX数据集"></a>VATEX数据集</h2><ul><li>一个新的数据集，41269个video， 时长大约10s, 每个video有10个中文，10个英文，同时这10个之中，中英文之间有5个是两两配对的</li><li>提出了两个新的任务：（1）一个encoder-decoder模型，在两种语言之间共享参数，即希望一个模型，可以得到两种语言的描述。（2）提出了一种新的机器翻译任务，即当进行中英文的机器翻译任务时，可以添加视频的视觉特征作为辅助信息</li><li><p>数据集的来源：来自于kinetics的validation dataset, 然后它们找人进行了caption的标注。它们将这41269个video 分成了4部分，train, validation, public test, secret test(不公开，用于比赛)</p></li><li><p>具体我的介绍见这篇博文</p></li></ul><h2 id="三个数据集的caption-length-的长度的统计情况"><a href="#三个数据集的caption-length-的长度的统计情况" class="headerlink" title="三个数据集的caption length 的长度的统计情况"></a>三个数据集的caption length 的长度的统计情况</h2><ul><li>eg, 句长为10 的captions 在当前这个数据集中所占比例   </li><li>msvd : 主要是len=6 为中心的居多<br><img src="https://i.loli.net/2019/09/07/jJ7ztsQb9MUR15X.png" alt="msvd_cap_length_.png"></li><li>msr-vtt：以len=9 为中心的居多<br><img src="https://i.loli.net/2019/09/07/SNYoIqHxPLWmU9D.png" alt="msr-vtt_cap_length_.png"></li><li>vatex：以len=15为中心的居多<br><img src="https://i.loli.net/2019/09/07/he7KYqMt8xj5pUs.png" alt="vatex_cap_length_.png"></li></ul><p>[1] Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition. In ICCV 2013</p><p>[2] Translating videos to natural language using deep recurrent neural networks. NAACL, 2015.<br>[3] (ICCV 2015)Sequence to Sequence – Video to Text<br>[4] Jointly Modeling Embedding and Translation to Bridge Video and Language</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(LSTM-E)Jointly Modeling Embedding and Translation to Bridge Video and Language</title>
      <link href="2019/02/22/LSTM-E-Jointly-Modeling-Embedding-and-Translation-to-Bridge-Video-and-Language/"/>
      <url>2019/02/22/LSTM-E-Jointly-Modeling-Embedding-and-Translation-to-Bridge-Video-and-Language/</url>
      
        <content type="html"><![CDATA[<h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><p>本文的主要贡献：</p><ol><li>同时使用了action feature of C3D and frames features。但是由于对C3D的特征也采用了mean pooling的方法，有缺陷，对action 特征的一种破坏。</li><li>提出了relevance loss ， 来加强整个句子的语义与视觉特征之间的关系。  </li></ol><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>目前存在的方法，依据给定的先前的单词以及视觉信息，来生成words，但是并没有利用句子语义与视觉内容之间的关系，导致生成的句子可能上下文是正确的，但是语义是错误的。<br>如 figure1，LSTM model 生成的句子是a man is riding a horse，逻辑上是没有错误的，但是语义却错了，图中出现的是woman 而不是man。<br>　　<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0f0qr6d2ij30lq0biq8s.jpg" width="500" hegiht="313" align="center"></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>  简单介绍LSTM-E：LSTM-E可以同时利用LSTM学习和视觉-语义embedding。LSTM 学习是为了在给定先前的单词以及视觉特征的基础上，最大化生成下一个单词的概率，后者是为了生成视觉-语义embedding，来加强整个句子的语义与视觉特征之间的关系。<br>  <img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0f87gorc9j316d0kr11m.jpg" title="overview">  </p><ul><li><strong>Overview</strong><br>2D/3D 卷积神经网络被用来提取视频frames/clips的特征，平均池化来得到视频的特征。<br>基于视频特征<strong><em>v</em></strong>和句子语义<strong><em>s</em></strong>，生成sentences的<strong>LSTM model</strong> 和<strong>视觉-语义embeddeing model</strong> 联合学习。</li><li><strong>The sprit of LSTM-E</strong><br>在coherence和relevance之间的相互增强下来生成sentences。<strong><em>coherence:</em></strong>表达了生成words与视频内容之间的相关关系，由LSTM优化完成。<strong><em>relevance:</em></strong>整个句子的语义与视频内容之间的关系，由视觉-语义embeddeing model来度量。通过联合学习coherence和relevance，期望生成的句子在语境和语义上是正确的。</li><li><strong>说人话</strong><br>由两个model组成，一个是sequence learning 都有的coherence loss ，来最大化生成next word的似然概率；另一个是本文添加的relevance loss，通过优化视频特征与生成句子之间的差距，使得生成的句子语义上能对应video的内容。即同时考虑了句子单词之间的上下文关系，也考虑了句子语义与视频内容之间的关系。</li><li><strong>contribution</strong><br>提出了relevance loss !  </li></ul><h2 id="Video-Description-with-Relevance-and-Coherence"><a href="#Video-Description-with-Relevance-and-Coherence" class="headerlink" title="Video Description with Relevance and Coherence"></a>Video Description with Relevance and Coherence</h2><ul><li><strong>Visual-Semantic Embedding: Relevance</strong><br><strong><em>v</em></strong> 和 <strong><em>s</em></strong> 分别是视频的特征和sentences的特征（即，都是已知的），Ts和Tv用来降维到相同的维度，为了度量视频内容与句子语义之间的相关性，一个自然地方法是计算embedding之间的距离，因此定义relevance loss:<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0f8zoni8tj30bc01pmx2.jpg" 　style="zoom:45%"></li><li><strong>Translation by Sequence Learning: Coherence</strong><br>coherence loss：即为在给定视频特征的条件下，生成sentences的最大似然概率。<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0f9l8bbvoj30ph02pglq.jpg" style="zoom:50%"><br>由overview的图可知，在实际的情况下LSTM的输入是：第一个LSTM输入是视频特征，其余的是前一个time step 生成的单词（在train时：是caption中给定的第t个单词，在test时：是前一个time step 生成的单词）。因此似然函数可以具体的表示为：  　　<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0f9lgy6nfj317806fgm5.jpg" style="zoom:40%"></li></ul><h2 id="Joint-Modeling-Embedding-and-Translation"><a href="#Joint-Modeling-Embedding-and-Translation" class="headerlink" title="Joint Modeling Embedding and Translation"></a>Joint Modeling Embedding and Translation</h2><ul><li><strong>simultaneously minimizing the relevance loss and coherence loss.</strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0fa1y3nkgj30nu05pq3p.jpg" style="zoom:70%"></li><li><p><strong>LSTM结构 </strong><br>这里有多种方式来结合visual content 和 word of last time step。法一：each time step 都输入视频特征；法二：只在第一步输入视频特征。但是在[ 1 ]中指出，由于网络可以显式地利用噪声和更容易覆盖，所以每次输入图像都会产生劣质的效果。 因此，采用第二种方法，在给定视频特征v 和相对应的 sentence W ≡ [w0, w1, …, wNs]，LSTM的更新步骤如下：<br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0fcc8tiqej30z708l0t3.jpg"><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0faic2fuzj30n508njs2.jpg" style="width: 50%; height: 50%"><br>在初始的第一步，视频特征作为LSTM的输入；在第二步，&lt;#start#&gt;开始的标志作为LSTM的输入，同时接受上一步的hidden state, cell state，以后每一步，都将上一步生成的word 作为输入，直至生成&lt;#end#&gt;。<br>从第二步开始，使用LSTM cell 的hidden state 来预测 word( 对于LSTM output 与 hidden[0] 是一样的，参考：<a href="https://mp.csdn.net/postedit/87516958" target="_blank" rel="noopener">https://mp.csdn.net/postedit/87516958</a>)</p><blockquote><p><code>output, hidden = self.rnn(input, hidden)</code></p></blockquote></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul><li><strong>dataset</strong><br>MSVD:<br>Microsoft Research Video Description Corpus (YouTube2Text) , which contains 1,970 YouTube snippets. There are roughly 40 available English descriptions per video. In our experiments, we follow the setting used in prior works, taking 1,200 videos for training, 100 for validation and 670 for testing.</li><li><strong>result</strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0fcs7kt8cj31ap0bhgow.jpg"></li><li><strong>The effect of hidden layer size</strong><br><img src="http://ww1.sinaimg.cn/large/006uWRWVly1g0fcvqbzvsj30qz09x760.jpg" style="width: 70%; height: 70%">  </li></ul><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>a visual-semantic embedding space is additionally incorporated into LSTM learning. In this way, <strong>a global relationship between the video content and sentence semantics</strong> is simultaneously measured in addition to <strong>the local contextual relationship between the word at each step and the previous ones</strong> in LSTM learning. On the popular YouTube2Text dataset, the results of our experiments demonstrate the success of our approach, outperforming the current state-ofthe-art models </p><h2 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h2><p>视频作为一个时域序列，未来将会探究使用RNN来获得更好的特征；另外，如果有更大的数据集，更多的video sentences pairs ，那么可以使用更深的RNN，来得到更好的视频描述</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR, 2015.</p>]]></content>
      
      
      <categories>
          
          <category> 视频描述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频描述 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GitHub+Hexo 搭建个人网站详细教程</title>
      <link href="2019/02/21/GitHub-Hexo-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B/"/>
      <url>2019/02/21/GitHub-Hexo-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26625249</a></p><p><a href="https://blog.csdn.net/xuezhisdc/article/details/53130328" target="_blank" rel="noopener">https://blog.csdn.net/xuezhisdc/article/details/53130328</a></p><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>node.js 使用 12 版本</p><h3 id="部署在本地"><a href="#部署在本地" class="headerlink" title="部署在本地"></a>部署在本地</h3><ul><li><p>部署</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo clean</span></span><br><span class="line"><span class="attribute">hexo g -s</span></span><br></pre></td></tr></table></figure></li><li><p>使用端口 5000</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s -<span class="selector-tag">p</span> <span class="number">5000</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="增加本地搜索功能"><a href="#增加本地搜索功能" class="headerlink" title="增加本地搜索功能"></a>增加本地搜索功能</h3><h4 id="自定义站点内容搜索"><a href="#自定义站点内容搜索" class="headerlink" title="自定义站点内容搜索"></a>自定义站点内容搜索</h4><ol><li><p>安装 hexo-generator-searchdb，在站点的根目录下执行以下命令：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm <span class="keyword">install</span> hexo-generator-searchdb <span class="comment">--save</span></span><br></pre></td></tr></table></figure></li><li><p>编辑博客配置文件，新增以下内容到任意位置：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">search:</span></span><br><span class="line"><span class="symbol">      path:</span> search.xml</span><br><span class="line"><span class="symbol">      field:</span> post</span><br><span class="line"><span class="symbol">      format:</span> html</span><br><span class="line"><span class="symbol">      limit:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure></li><li><p>编辑主题配置文件，启用本地搜索功能：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Local search</span></span><br><span class="line"><span class="attr">local_search:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="加密"><a href="#加密" class="headerlink" title="加密"></a>加密</h3><p><a href="https://www.jianshu.com/p/44e211829447" target="_blank" rel="noopener">https://www.jianshu.com/p/44e211829447</a></p><p><a href="http://npm.taobao.org/package/hexo-blog-encrypt" target="_blank" rel="noopener">http://npm.taobao.org/package/hexo-blog-encrypt</a></p><p><a href="https://github.com/MikeCoder/hexo-blog-encrypt/" target="_blank" rel="noopener">https://github.com/MikeCoder/hexo-blog-encrypt/</a></p>]]></content>
      
      
      <categories>
          
          <category> 杂类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
