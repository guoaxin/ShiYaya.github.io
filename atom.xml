<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShiYaya</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-10-27T09:59:04.045Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ShiYaya</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>N-Gram模型</title>
    <link href="http://yoursite.com/2020/10/27/N-Gram%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/10/27/N-Gram模型/</id>
    <published>2020-10-27T09:22:01.000Z</published>
    <updated>2020-10-27T09:59:04.045Z</updated>
    
    <content type="html"><![CDATA[<p>转载：<a href="https://zhuanlan.zhihu.com/p/32829048" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32829048</a></p><h2 id="一、什么是n-gram模型"><a href="#一、什么是n-gram模型" class="headerlink" title="一、什么是n-gram模型"></a><strong>一、什么是n-gram模型</strong></h2><p>N-Gram是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。</p><p>每一个字节片段称为gram，对所有gram的出现频度进行统计，并且按照事先设定好的阈值进行过滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量维度。</p><p>该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。</p><p>说完了n-gram模型的概念之后，下面讲解n-gram的一般应用。</p><h2 id="二、n-gram模型用于评估语句是否合理"><a href="#二、n-gram模型用于评估语句是否合理" class="headerlink" title="二、n-gram模型用于评估语句是否合理"></a><strong>二、n-gram模型用于评估语句是否合理</strong></h2><p>如果我们有一个由 m 个词组成的序列（或者说一个句子），我们希望算得概率 $P(w_1, w_2,…,w_m)$，根据链式规则，可得</p><p><img src="https://i.loli.net/2020/10/27/ye3X8vh6LcKTzjr.png" alt="image-20201027172437915"></p><p>这个概率显然并不好算，不妨利用马尔科夫链的假设，即当前这个词仅仅跟前面几个有限的词相关，<strong><em>因此也就不必追溯到最开始的那个词，这样便可以大幅缩减上述算式的长度\</em></strong>。即</p><p><img src="https://i.loli.net/2020/10/27/XjDPInlQY9GN7ZU.png" alt="image-20201027172458291" style="zoom: 67%;"></p><p><strong><em>这个马尔科夫链的假设为什么好用？我想可能是在现实情况中，大家通过真实情况将n=1，2，3，….这些值都试过之后，得到的真实\</em></strong>的效果和时间空间的开销权衡之后，发现能够使<strong><em>用。\</em></strong></p><p>下面给出一元模型，二元模型，三元模型的定义：</p><p>当 n=1, 一个一元模型（unigram model)即为 ：</p><p><img src="https://i.loli.net/2020/10/27/UMNPucLtdw3zxgo.png" alt="img"></p><p>当 n=2, 一个二元模型（bigram model)即为 ：</p><p><img src="https://i.loli.net/2020/10/27/UmZkyzIK4fws7Nj.png" alt="img"></p><p>当 n=3, 一个三元模型（trigram model)即为</p><p><img src="https://i.loli.net/2020/10/27/Vr1LZIG7snCvokK.png" alt="img"></p><p>然后下面的思路就很简单了，在给定的训练语料中，利用贝叶斯定理，将上述的条件概率值（<strong>因为一个句子出现的概率都转变为右边条件概率值相乘了</strong>）都统计计算出来即可。下面会给出具体例子讲解。这里先给出公式：</p><p><img src="https://i.loli.net/2020/10/27/locAQnW9bGpXa3K.png" alt="img"></p><p>对第一个进行解释，后面同理,如下：</p><p><img src="https://i.loli.net/2020/10/27/iGbljDacPTSWu92.png" alt="image-20201027172711422"></p><p>下面给出具体的例子。</p><h2 id="三、二元语言模型判断句子是否合理"><a href="#三、二元语言模型判断句子是否合理" class="headerlink" title="三、二元语言模型判断句子是否合理\"></a><strong><em>三、二元语言模型判断句子是否合理\</em></strong></h2><p><strong><em>下面例子来自于：\</em></strong><a href="https://link.zhihu.com/?target=http%3A//blog.csdn.net/baimafujinji/article/details/51281816" target="_blank" rel="noopener">自然语言处理中的N-Gram模型详解 - 白马负金羁 - CSDN博客</a>和《北京大学 常宝宝 以及 The University of Melbourne “Web Search and Text Analysis” 课程的幻灯片素材》</p><p>假设现在有一个语料库，我们统计了下面的一些词出现的数量</p><p><img src="https://i.loli.net/2020/10/27/ufWOseUyrAm9qV3.png" alt="img"></p><p>下面的这些概率值作为已知条件：</p><p><img src="https://i.loli.net/2020/10/27/G2Y6zKZNxBCitVo.png" alt="img"></p><p><img src="https://pic1.zhimg.com/80/v2-a7c0d77143e0c997abd45e1535eaeb8c_1440w.jpg" alt="img"></p><p>$p(want|<s>) = 0.25$</s></p><p>下面这个表给出的是基于Bigram模型进行计数之结果</p><p><img src="https://i.loli.net/2020/10/27/G2Y6zKZNxBCitVo.png" alt="img"></p><p><img src="https://i.loli.net/2020/10/27/az7ewSWFYZGUbIu.png" alt="img"></p><p>例如，其中第一行，第二列 表示给定前一个词是 “i” 时，当前词为“want”的情况一共出现了827次。据此，我们便可以算得相应的频率分布表如下。</p><p><img src="https://i.loli.net/2020/10/27/9WhUqaZtcC3xDn7.jpg" alt="img"></p><p>比如说，我们就以表中的$p(eat|i)=0.0036$这个概率值讲解，从表一得出“i”一共出现了2533次，而其后出现eat的次数一共有9次，$p(eat|i)=p(eat,i)/p(i)=count(i,eat)/count(i)=9/2533 = 0.0036$</p><p>下面我们通过基于这个语料库来判断$s1=“<s> i want english food</s>” $ 与 $s2 = “<s> want i english food</s>“$哪个句子更合理：通过例子来讲解是最人性化的，我在网上找了这么久发现这个例子最好：</p><p><strong>首先来判断$p(s1)$</strong></p><p>$P(s1)=P(i|<s>)P(want|i)P(english|want)P(food|english)P(</s>|food)$</p><p>=$0.25×0.33×0.0011×0.5×0.68=0.000031$</p><p><strong>再来求$p(s2)$</strong></p><p>$P(s2)=P(want|<s>)P(i|want)P(english|want)P(food|english)P(</s>|food)$</p><p>=$0.25×0.0022×0.0011×0.5×0.68 = 0.00000002057$</p><p><strong>通过比较我们可以明显发现0.00000002057&lt;0.000031,也就是说s1= “i want english food&lt;/s&gt;”更像人话。</strong></p><p><strong>再深层次的分析，我们可以看到这两个句子的概率的不同，主要是由于顺序i want还是want i的问题，根据我们的直觉和常用搭配语法，i want要比want i出现的几率要大很多。所以两者的差异，第一个概率大，第二个概率小，也就能说的通了。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32829048&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/32829048&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;一
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>PPL-语句通顺</title>
    <link href="http://yoursite.com/2020/10/27/PPL-%E8%AF%AD%E5%8F%A5%E9%80%9A%E9%A1%BA/"/>
    <id>http://yoursite.com/2020/10/27/PPL-语句通顺/</id>
    <published>2020-10-27T03:37:20.000Z</published>
    <updated>2020-10-27T08:29:59.560Z</updated>
    
    <content type="html"><![CDATA[<h3 id="语句通顺-一些调研"><a href="#语句通顺-一些调研" class="headerlink" title="语句通顺 - 一些调研"></a>语句通顺 - 一些调研</h3><ul><li><p>BERT模型通过在大量语料的训练可以判断一句话是否通顺</p></li><li><p>理解 NNLM <a href="https://zhuanlan.zhihu.com/p/65446874" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/65446874</a></p><p>以上推理就是</p><ol><li>用词汇的联合概率表达一个语句是否通顺；</li><li>将计算联合概率转换为计算条件概率；</li><li>将条件概率由不定长度的且一般较大的t维降到一般较小的n-1维;</li></ol></li></ul><h3 id="PPL-评价指标"><a href="#PPL-评价指标" class="headerlink" title="PPL-评价指标"></a>PPL-评价指标</h3><p>在得到不同的语言模型（一元语言模型、二元语言模型….）的时候，我们如何判断一个语言模型是否好还是坏，一般有两种方法：</p><p>1、一种方法将其应用到具体的问题当中，比如机器翻译、speech recognition、spelling corrector等。然后看这个语言模型在这些任务中的表现（extrinsic evaluation，or in-vivo evaluation）。但是，这种方法一方面难以操作，另一方面可能非常耗时，可能跑一个evaluation需要大量时间，费时难操作。</p><p>2、针对第一种方法的缺点，大家想是否可以根据与语言模型自身的一些特性，来设计一种简单易行，而又行之有效的评测指标。于是，人们就发明了perplexity这个指标。</p><p><img src="https://i.loli.net/2020/10/27/UcxVWKjtlCi5Tw7.png" alt="image-20201027113452950" style="zoom: 25%;"></p><p>困惑度（perplexity）的基本思想是：<strong>给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，</strong>公式如下：</p><p>由公式可知，<strong>句子概率越大，语言模型越好，迷惑度越小。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;语句通顺-一些调研&quot;&gt;&lt;a href=&quot;#语句通顺-一些调研&quot; class=&quot;headerlink&quot; title=&quot;语句通顺 - 一些调研&quot;&gt;&lt;/a&gt;语句通顺 - 一些调研&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;BERT模型通过在大量语料的训练可以判断一句话是否通顺&lt;/
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>PPL-语句通顺</title>
    <link href="http://yoursite.com/2020/10/27/PPPL-%E8%AF%AD%E5%8F%A5%E9%80%9A%E9%A1%BA/"/>
    <id>http://yoursite.com/2020/10/27/PPPL-语句通顺/</id>
    <published>2020-10-27T03:37:20.000Z</published>
    <updated>2020-10-27T08:40:33.988Z</updated>
    
    <content type="html"><![CDATA[<h3 id="语句通顺-一些调研"><a href="#语句通顺-一些调研" class="headerlink" title="语句通顺 - 一些调研"></a>语句通顺 - 一些调研</h3><ul><li><p>BERT模型通过在大量语料的训练可以判断一句话是否通顺</p></li><li><p>理解 NNLM <a href="https://zhuanlan.zhihu.com/p/65446874" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/65446874</a></p><p>以上推理就是</p><ol><li>用词汇的联合概率表达一个语句是否通顺；</li><li>将计算联合概率转换为计算条件概率；</li><li>将条件概率由不定长度的且一般较大的t维降到一般较小的n-1维;</li></ol></li></ul><h3 id="PPL-评价指标"><a href="#PPL-评价指标" class="headerlink" title="PPL-评价指标"></a>PPL-评价指标</h3><p>在得到不同的语言模型（一元语言模型、二元语言模型….）的时候，我们如何判断一个语言模型是否好还是坏，一般有两种方法：</p><p>1、一种方法将其应用到具体的问题当中，比如机器翻译、speech recognition、spelling corrector等。然后看这个语言模型在这些任务中的表现（extrinsic evaluation，or in-vivo evaluation）。但是，这种方法一方面难以操作，另一方面可能非常耗时，可能跑一个evaluation需要大量时间，费时难操作。</p><p>2、针对第一种方法的缺点，大家想是否可以根据与语言模型自身的一些特性，来设计一种简单易行，而又行之有效的评测指标。于是，人们就发明了perplexity这个指标。</p><p><img src="https://i.loli.net/2020/10/27/UcxVWKjtlCi5Tw7.png" alt="image-20201027113452950" style="zoom: 25%;"></p><p>困惑度（perplexity）的基本思想是：<strong>给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，</strong>公式如下：</p><p>由公式可知，<strong>句子概率越大，语言模型越好，迷惑度越小。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;语句通顺-一些调研&quot;&gt;&lt;a href=&quot;#语句通顺-一些调研&quot; class=&quot;headerlink&quot; title=&quot;语句通顺 - 一些调研&quot;&gt;&lt;/a&gt;语句通顺 - 一些调研&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;BERT模型通过在大量语料的训练可以判断一句话是否通顺&lt;/
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Visual Grounding in Video for Unsupervised Word Translation</title>
    <link href="http://yoursite.com/2020/10/18/Visual-Grounding-in-Video-for-Unsupervised-Word-Translation/"/>
    <id>http://yoursite.com/2020/10/18/Visual-Grounding-in-Video-for-Unsupervised-Word-Translation/</id>
    <published>2020-10-18T10:53:46.000Z</published>
    <updated>2020-10-18T10:54:21.489Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>我们的目标是使用视觉基础来改进语言之间的非监督词映射。其核心思想是通过学习母语教学视频中未配对的嵌入语，在两种语言之间建立一种共同的视觉表达。</p><p>本文的工作就是<strong>向机器提供不同的教学视频</strong>，这些视频的内容是人们用本国语言的教学视频。比如说，说中文和英文教别人榨橙汁的教学视频。这类视频有两个特点：视频网站上<strong>大量存在</strong>和<strong>内容相似度高</strong>，非常适合用于训练。但是这些视频也有一些弊端，会有很多无关废话（如“观众老爷们记得素质三连哦~”）。</p><p>即使如此，这种基于视觉的翻译提高了翻译的精度。</p><h3 id="Unsupervised-Multilingual-Learning"><a href="#Unsupervised-Multilingual-Learning" class="headerlink" title="Unsupervised Multilingual Learning"></a>Unsupervised Multilingual Learning</h3><p><img src="https://i.loli.net/2020/10/18/qzv2QN3bd8oInSx.png" alt="image-20201018172741937"></p><p>一个无监督的系统，该系统通过将语言嵌入视频中翻译单词。其中，不需要任何配对数据来学习翻译。</p><p><strong>Our method</strong> is unsupervised in that it learns the correspondences between two languages $X$ and $Y$ (e.g. English and French) without any parallel (paired) corpora.</p><p>given two distinct collections of instructional videos, i.e. n videos narrated with language $X$and another m different videos with language $Y$.</p><p><strong>Our goal</strong> is to learn to map languages $X$ and $Y$ by leveraging the shared visual modality $Z$ – the videos.</p><p><strong>Loss function</strong></p><p><img src="https://i.loli.net/2020/10/18/uYrymIKnJwvL2G6.png" alt="image-20201018173014409" style="zoom: 25%;"></p><h4 id="Multilingual-Visual-Embedding-Architecture"><a href="#Multilingual-Visual-Embedding-Architecture" class="headerlink" title="Multilingual Visual Embedding: Architecture"></a>Multilingual Visual Embedding: Architecture</h4><p><img src="https://i.loli.net/2020/10/18/XU2slICkuhyLBRT.png" alt="image-20201018172904212" style="zoom:33%;"></p><p><strong>yaya:</strong>  通过 视觉将两种语言做一种映射是存在困难的。文中列出了三点：<br>（1）learning video-text embeddings from instructional videos is difficult as the speech in these videos is only loosely related to the scene.</p><p>（2）in multilingual setting, such errors compound since both languages have this low video-text relevance;<br>（3）visually similar videos may not be semantically similar.</p><p>因此本文不同video 作为桥梁直接学习两种语言的映射，而是采取了间接的方式：we learn a joint (monolingual) video-text embedding space from instructional videos.</p><p>对于一种语言X, 学习视频及其字幕的映射，对于另一种语言，也学习一种映射，同时，在这种语言上加一个Adaptlayer, 使得 X和Y 能够映射到一个共同的空间。</p><p><strong>模型细节：</strong></p><p>其中X编码器 = WordEmbed + （Liner + ReLU MaxPool) + Linear</p><p>（WordEmbed层，度向量的转换；Linear层，建立与 Joint Embedding Space的映射）</p><p>而Y编码器则多了一个调整层（AdaptLayer），进行的是跨语言共享模型的权重分配，尽量让Y语言的词和X语言的词有相似的嵌入。</p><h4 id="MUVE-Improving-Unsupervised-Translation"><a href="#MUVE-Improving-Unsupervised-Translation" class="headerlink" title="MUVE: Improving Unsupervised Translation"></a>MUVE: Improving Unsupervised Translation</h4><p>略</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;p&gt;我们的目标是使用视觉基础来改进语言之间的非监督词映射。其核心思想是通过学习母语教学视频中未配
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[VIVO] Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training</title>
    <link href="http://yoursite.com/2020/10/18/VIVO-Surpassing-Human-Performance-in-Novel-Object-Captioning-with-Visual-Vocabulary-Pre-Training/"/>
    <id>http://yoursite.com/2020/10/18/VIVO-Surpassing-Human-Performance-in-Novel-Object-Captioning-with-Visual-Vocabulary-Pre-Training/</id>
    <published>2020-10-18T04:11:10.000Z</published>
    <updated>2020-10-27T08:41:32.401Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>VIVO可以在没有文本标签的数据上进行文本和图像的多模态预训练，摆脱了对配对图文数据的依赖，可以直接利用ImageNet等数据集的类别标签。借助VIVO，模型可以学习到物体的视觉外表和语义之间的关系，建立视觉词表。</p><p>这个视觉词表是啥呢？其实就是一个图像和文本的联合特征空间，在这个特征空间中，语义相近的词会聚类到一起，如金毛和牧羊犬，手风琴和乐器等。</p><p>预训练建好词表后，模型只需在有少量共同物体的配对图文的数据上进行微调，模型就能自动生成通用的模板语句，使用时，即使出现没见过的词，也能从容应对，相当于把图片和描述的各部分解耦了。</p><p>所以VIVO既能利用预训练强大的物体识别能力，也能够利用模板的通用性，从而应对新出现的物体。</p><p><img src="https://i.loli.net/2020/10/17/gakiQWDp3YAITCd.png" alt="image-20201017185401941" style="zoom:50%;"></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文要针对describe novel objects which are unseen in caption-labeled training data。This paper presents VIsual VOcabulary pretraining (VIVO) that performs pre-training in the absence of caption annotations。</p><p>By breaking the dependency of paired image-caption training data in VLP, VIVO can leverage large amounts of <strong>paired image-tag data</strong> to learn <strong>a visual vocabulary</strong>.<br>This is done by pre-training a <strong>multi-layer Transformer model</strong> that learns to align image-level tags with their corresponding image region features. Given that tags are not ordered, we employ <strong>the Hungarian matching loss</strong> for tag prediction optimization. </p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="https://i.loli.net/2020/10/17/JuOMYRtzW7LEgfo.png" alt="image-20201017193144259"></p><h4 id="VIVO-Pre-training"><a href="#VIVO-Pre-training" class="headerlink" title="VIVO Pre-training"></a>VIVO Pre-training</h4><p>We pre-train the Transformer model on a large-scale dataset with abundant tags, e.g., the Open Images training set with <strong>6.4K classes of image-level tags.</strong></p><p><strong>The training objective</strong> is to predict the missing (masked) tags given a bag of image-level tags and image regions. </p><p>We denote the training set: N images $I_i$ and their corresponding tags $G_i$. 一个image有多个tags.</p><p>use <strong>bi-directional attention mask</strong> in VIVO pre-training.</p><h4 id="Fine-tuning-and-Inference"><a href="#Fine-tuning-and-Inference" class="headerlink" title="Fine-tuning and Inference"></a>Fine-tuning and Inference</h4><p>After pre-training, the Transformer model is fine-tuned on a dataset where both captions and tags are available, e.g., the COCO set annotated with tags from 80 object classes and captions.</p><p>the input to the model during <strong>fine-tuning is a triplet of image region features $V$, a set of tags $T$ and a  caption $C$</strong>, where $V$ and $T$ are constructedin the same way as described in pre-training, and $C$ is a sequence of tokens. During fine-tuning, we <strong>randomly mask outsome of the tokens in a caption sentence</strong> for  prediction, and optimize the model parameters using the cross-entropy loss.</p><p>during fine-tuning we apply <strong>the uni-directional attention mask</strong> on a caption sequence to prevent the positions from attending to subsequent positions.</p><p>During inference, we first extract image region features and detect tags from a given image. Then the model is applied to <strong>generate a sequence, one token at a time,</strong> until it outputs the end of sentence token or reaches the maximum length.</p><p><strong>detect tags</strong> ：We use an object detector trained on the Open Images dataset （500 classes bboxes）to detect object tags for all datasets.</p><p><strong style="color:red;"><strong>yaya：</strong> tags detector的限制，仅能输出 500个类别tags, 因此，novel objects 的生成也是受到限制的</strong></p><p>以下这个表就可以说明问题，当不预训练时，是第一行的数据；当仅使用tags detector 的500个类时，是第二行的数据；当使用open-image 所有的 6.4k 个类时，是第三行的数据。因此，在inference阶段，使用 tag detector 来提供tags 是存在问题的。至少限制了模型的性能。</p><p><strong>改进</strong>：本文的model，pre-training, fine-tune，都是在一个fix model上。但是pre-training 的目的，仅仅是为了构建 image-tag vocabulary, 可以先构建，然后再离线使用！！！</p><p><img src="https://i.loli.net/2020/10/18/eyKJkcQPibhX3nS.png" alt="image-20201018115538016" style="zoom:33%;"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;VIVO可以在没有文本标签的数据上进行文本和图像的多模态预训练，摆脱了对配对图文数据的依赖，可以直接利用ImageNet等数据集的类别标签。
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
      <category term="region-word-embedding" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/region-word-embedding/"/>
    
    
      <category term="region-word-embedding" scheme="http://yoursite.com/tags/region-word-embedding/"/>
    
  </entry>
  
  <entry>
    <title>Vokenization Improving Language Understanding with Contextualized, Visual-Grounded Supervision</title>
    <link href="http://yoursite.com/2020/10/18/Vokenization-Improving-Language-Understanding-with-Contextualized-Visual-Grounded-Supervision/"/>
    <id>http://yoursite.com/2020/10/18/Vokenization-Improving-Language-Understanding-with-Contextualized-Visual-Grounded-Supervision/</id>
    <published>2020-10-18T04:10:43.000Z</published>
    <updated>2020-10-20T12:17:39.619Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><h4 id="现存问题"><a href="#现存问题" class="headerlink" title="现存问题"></a>现存问题</h4><p>人类学习语言都是结合多模态信息，但是当前的 language pre-training frameworks 仅通过自监督的方式，学习语言这一种模态。</p><p>虽然这种自监督的方式取得了很大的成功，但是它们没有利用grounding information from external visual word.</p><blockquote><p>Emily M Bender and Alexander Koller. 2020. <strong>Climbing towards nlu: On meaning, form, and understanding in the age of data.</strong> In ACL.</p><p>Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata,<br>Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. 2020. <strong>Experience grounds language.</strong> In EMNLP</p></blockquote><h4 id="本文的解决"><a href="#本文的解决" class="headerlink" title="本文的解决"></a>本文的解决</h4><p><img src="https://i.loli.net/2020/10/17/1cz3MHUdNXA9o5J.png" alt="image-20201017095623630" style="zoom:33%;"></p><p>本文：介绍了一个 <strong>视觉</strong>监督语言模型，如图1，该模型使用 language tokens 作为输入，使用token-related images 作为视觉监督。本文将这些images称作 vokens，which act as visualizations of the corresponding tokens.</p><p>假若a large aligned token-voken dataset 存在，那么模型可以通过voken-prediction task 从这些vokens中进行学习。但是不幸的是，不存在这种大型数据集，主要是有两个挑战：(1) 视觉性单词与 其他非视觉性单词之间，数量上存在很大的差异。如，在visually-grounded language datasets中仅有120M tokens, 但是在BERT的训练数据中有3300M tokens。grounded language 一般会更短，偏向于instructive descriptions, 因此在句子长度和有效词的数量上与其他语言类型的分布不同。(2) 自然语言中的大部分单词是 not visually grounded，因此对是否建立一个 visual supervision的数据集提出了质疑。粗略估计，英语维基百科中 grounded tokens 的比例仅为大约28％。 这种 low grounded ratio 导致以前方法中的视觉监控覆盖率低。</p><p><img src="https://i.loli.net/2020/10/17/27sWCBNOiqp13Vz.png" alt="image-20201017111702738"></p><p>为解决以上的两个挑战，本文提出了一个 <strong>vokenization method, that contextually maps the tokens to the visualized tokens (i.e., vokens) by retrieval.</strong>  而不是直接使用具有visually grounded的语言数据集来监督语言模型。</p><p>解决第一个挑战：(1) relative small datasets to train the <strong>vokenization processor</strong> (2) generate vokens for large language corpora.<br>our visually-supervised language model will take the input supervision from these large datasets, thus <strong>bridging the gap between different data sources,</strong> which solves the first challenge.</p><p>解决第一个挑战：low grounded ratio 的第二个挑战似乎是语言的固有特征。 但是，我们发现，考虑到它的上下文，可以将一些非可视化的tokens 有效地映射到相关图像。by our contextual token-image matching model (defined in Sec. 3.2) inside our vokenization processor, where we map tokens to images by viewing the sentence as the context.</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>Using our proposed vokenizer with a <strong>contextualized</strong> token-image matching model, we generate vokens for English Wikipedia. </p><p>Supervised by these generated vokens, we show consistent improvements upon a BERT model on several diverse NLP tasks such as GLUE (Wang et al., 2019), SQuAD (Rajpurkar et al., 2016), and SWAG (Zellers et al., 2018).  We also show the transferability of our vokens to other frameworks (i.e., RoBERTa).</p><h3 id="Vokenization"><a href="#Vokenization" class="headerlink" title="Vokenization"></a>Vokenization</h3><p><img src="https://i.loli.net/2020/10/17/fUWtQpIO8kZjAcY.png" alt="image-20201017120304025" style="zoom:50%;"></p><p>we <strong>retrieve an image for a token</strong> from a set of images $X$ = {$x_1; x_2; … ; x_n$} regarding a token-image-relevance scoring function $r_\theta(w_i; x; s)$. This scoring function $r_\theta(w_i; x; s)$, parameterized by $\theta$</p><h4 id="Contextual-Token-Image-Matching-Model"><a href="#Contextual-Token-Image-Matching-Model" class="headerlink" title="Contextual Token-Image Matching Model"></a>Contextual Token-Image Matching Model</h4><p>输入：The model takes a sentence $s$ and an image $x$ as input.</p><p>输出：The output $r_\theta(w_i; x; s)$ is the relevance score between the token $w_i \in s$ and the image $x$ while considering the whole sentence $s$ as a context.</p><p>Model: an inner product of the language feature representation $f_\theta(w_i; s)$ and the visual feature representation $g_\theta(x)$: $r_\theta(w_i; x; s)$ = $f_\theta(w_i; s)^T$ $g_\theta(x)$</p><p>token-image paris: 使用MS-COCO image caption pairs， 将caption中的所有tokens的vokens 都指定为该 image.</p><p>Training: 训练模型，maximizing the relevance score of these aligned token-image pairs over unaligned pairs. 使用 hinge loss.</p><h3 id="Visually-Supervised-Language-Models"><a href="#Visually-Supervised-Language-Models" class="headerlink" title="Visually-Supervised Language Models"></a>Visually-Supervised Language Models</h3><p>Based on these vokens, we propose a new pre-training task for language: voken classification.</p><h4 id="The-Voken-Classification-Task"><a href="#The-Voken-Classification-Task" class="headerlink" title="The Voken-Classification Task"></a>The Voken-Classification Task</h4><p><img src="C:\Users\shiyaya\AppData\Roaming\Typora\typora-user-images\image-20201017105855799.png" alt="image-20201017105854742"></p><p>BERT 的结果，会在每个token $w_i$的位置输出一个localized feature representation ${h_i}$，因此这将会很容易增加一个 token-level classification task, 而不需要修改模型的结构。Suppose the vokens come<br>from a finite set $X$, we convert the hidden output to ${h_i}$ a probability distribution ${p_i}$ with a linear layer and a softmax layer. </p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>受到这篇文章对的影响，是否可以结合视频，设计一个这种模型，比如有一些动词，仅能在视频中体现出来。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;h4 id=&quot;现存问题&quot;&gt;&lt;a href=&quot;#现存问题&quot; cla
      
    
    </summary>
    
    
      <category term="region-word-embedding" scheme="http://yoursite.com/tags/region-word-embedding/"/>
    
  </entry>
  
  <entry>
    <title>Re-evaluating Evaluation in Text Summarization</title>
    <link href="http://yoursite.com/2020/10/16/Re-evaluating-Evaluation-in-Text-Summarization/"/>
    <id>http://yoursite.com/2020/10/16/Re-evaluating-Evaluation-in-Text-Summarization/</id>
    <published>2020-10-16T08:34:45.000Z</published>
    <updated>2020-10-16T08:36:14.592Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>ROUGE 在 text summarization 任务中被广泛使用，但是，关于ROUGE是否可能偏离human judgement 以及这种偏离可能改变有关baseline 和 proposed methods的相对优势得出的结论的讨论很少。</p></li><li><p>为了表征<strong>评估指标</strong>的相对优势，有必要执行meta-evaluation。</p><p>where a dataset annotated with human judgments is used to test the degree to which automatic metrics correlate there with.</p></li><li><p>现在存在关键问题：现有的人类判断数据集很少，尚不清楚<strong>现有指标</strong>在<em>当前得分最高的摘要系统</em>上的表现。</p></li><li><p>在本文中提出一个问题：摘要模型中模型开发的快速发展是否需要我们<strong>重新评估</strong>用于文本摘要的<strong>评估过程</strong>。因此，在本文中，收集了一个large benchmark 来用于 meta-evaluating summarization metrics。</p><ul><li><p>数据来源于：25 top-scoring extractive and abstractive summarization systems on the CNN/DailyMail dataset.</p></li><li><p>Automatic: traditional metrics (e.g. ROUGE) and modern semantic matching metrics (e.g.  BERTScore, MoverScore).</p></li><li><p>Manual evaluations: 使用轻量级金字塔方法（Shapira等，2019），我们将其用作summarization systems 和 automated metrics的黄金标准。（yaya: 收集的human judgements 既可以作为评判systems好坏的标准，也可以作为评判metrics好坏的标准）</p><blockquote><p>Ori Shapira, <strong>Crowdsourcing lightweight pyramids for manual summary evaluation.</strong>  NAACL 2019</p></blockquote></li></ul></li></ul><h3 id="标注过程"><a href="#标注过程" class="headerlink" title="标注过程"></a>标注过程</h3><ul><li>对于一个document，仅存在一个reference，对该reference, 提取SCUs, 如下表展示出来的所示。该步骤由作者本人完成。</li><li>对于该document 的 candidate summary, 查看 SCUs 是否出现在 candidate summary 中，并标注为 “present” 或者是 “not present”。该步骤的操作由4个workers共同完成。</li><li>对于 each documents，查验是否存在 noisy worker, 即对于一个SCU，大多数认为其”present”，但是他却认为”not present”，在该document的大多数SCUs中他的annotations都与众数不同，则定义为 noisy workers，并将其标注的结果去除掉。</li></ul><p><img src="https://i.loli.net/2020/10/16/m7Cxl6hnvOwPeSc.png" alt="image-20201016121420397"></p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>本文想要研究的核心问题：“does the rapid progress of model development in summarization models require us to re-evaluate the evaluation process used for text summarization?”</p><p>因此本文从四个方面 to meta-evaluate current metrics。(1) evaluate all systems; (2) evaluate top-k strongest systems; (3) compare two systems; (4) evaluate individual summaries.</p><h4 id="evaluate-all-systems"><a href="#evaluate-all-systems" class="headerlink" title="evaluate all systems"></a>evaluate all systems</h4><p><img src="https://i.loli.net/2020/10/16/Cy7RSaKrP6XOUnY.png" alt="image-20201016160058531" style="zoom:33%;"></p><p>通过对比不同的metrics 在 system level 的相关性发现，在不同的数据集上，metrics的相关性，性能并不一致。</p><blockquote><p>that metrics run the risk of overfitting to some datasets</p></blockquote><p>本文建议，在不同的数据集上，使用不同的metric来评估该数据集上不同的systems 的性能好坏</p><h4 id="evaluate-top-k-strongest-systems"><a href="#evaluate-top-k-strongest-systems" class="headerlink" title="evaluate top-k strongest systems"></a>evaluate top-k strongest systems</h4><p><img src="https://i.loli.net/2020/10/16/Zfn4Hh6g8uG5eXM.png" alt="image-20201016160240565"></p><p>结论：当 top-systems 数量较少时，或者说数量不稳定时，不同的metrics在同一个数据集上的效果也不稳定。</p><h4 id="compare-two-systems"><a href="#compare-two-systems" class="headerlink" title="compare two systems"></a>compare two systems</h4><p>we only have 100 annotated summaries to compare any two systems, sys1 and sys2, we use paired bootstrap resampling,</p><p>对于人类在sys1/2 上对所有的summaries 都有一个得分。</p><p>现，要比较两个system, 若有95%以上的confidence认为sys1 better than sys2, 则 ytrue=1, 否则 ytrue=2, 如果confidence&lt;95%, ytrue=0.</p><p>同理，对于所有的metrics, 通过同样的方式，也可以有此比较得分 ypred。</p><p>现有 J 个systems, 则可以得到 J<em>(J-1)/2 个compaired paris. 即，得到 长度为 J</em>(J-1)/2的mask.</p><p>计算 ytrue_mask 与 ypred_mask 的F1score.即可评估metrics在 compare two systems上的性能与human 的一致性。</p><p><img src="https://i.loli.net/2020/10/16/ys241wqHUpGmltO.png" alt="image-20201016161340716" style="zoom:33%;"></p><p>结论：Different metrics are better suited for different datasets. For example, on the CNNDM datasets, we recommend using R-2 while, on the TAC datasets, we recommend using JS-2.</p><h4 id="evaluate-individual-summaries"><a href="#evaluate-individual-summaries" class="headerlink" title="evaluate individual summaries"></a>evaluate individual summaries</h4><p><img src="https://i.loli.net/2020/10/16/iWLrRBSITJGpMoN.png" alt="image-20201016162009349" style="zoom: 50%;"></p><ul><li>以上三个实验都是system-level , 此实验是 summary-level</li></ul><p>分析：在不同的数据集上，同一metrics的性能不一致。如，R_1在TAC上与human相关性较低，但是在CNNDM上相关性较高。</p><p>另，前有文章表明，automatic metrics趋向于在system level 与 human的相关性较好，但是在 instance-level (summary-level) 相关性较差。我们进行实验发现，这种现象仅在TAC-2009 上表现明显。</p><p>结论：autometrics 在 summary-level 与 system-level 上的性能是一样的。</p><blockquote><p>Even though some metrics might be good at comparing summaries, they may point in the wrong direction when comparing systems</p></blockquote><p>另外，<strong>some metric在不同的数据集上的性能是不同的，因此是有必要在不同的数据集上测试各个评价指标的有效性</strong></p><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>（1）metric的选择不仅取决于不同的任务（例如，摘要，翻译），而且还取决于不同的数据集（例如，TAC，CNNDM）和应用程序场景（例如，系统级别，摘要级别）。未来的meta-evaluating 工作应调查效果这些设置对指标性能的影响。</li><li>（2）指标很容易在有限的数据集上过拟合。多数据集meta-evaluate 可以帮助我们更好地了解每个指标的特殊性，从而在各种情况下获得更好的指标选择。</li><li>（3）我们收集的人工判断可以用作监督，以实例化最近提出的预训练-然后-微调框架（最初用于机器翻译）（Sellam等，2020），学习一个强大的文本摘要指标。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ROUGE 在 text summariz
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="评价指标" scheme="http://yoursite.com/categories/NLP/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>shiyaya-Instruction-Final-bilibili</title>
    <link href="http://yoursite.com/2020/09/17/shiyaya-Instruction-Final-bilibili/"/>
    <id>http://yoursite.com/2020/09/17/shiyaya-Instruction-Final-bilibili/</id>
    <published>2020-09-17T02:18:37.000Z</published>
    <updated>2020-09-17T13:01:01.178Z</updated>
    
    <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning</title>
    <link href="http://yoursite.com/2020/09/15/Learning-to-Compose-Topic-Aware-Mixture-of-Experts-for-Zero-Shot-Video-Captioning/"/>
    <id>http://yoursite.com/2020/09/15/Learning-to-Compose-Topic-Aware-Mixture-of-Experts-for-Zero-Shot-Video-Captioning/</id>
    <published>2020-09-15T02:19:42.000Z</published>
    <updated>2020-09-15T02:21:37.215Z</updated>
    
    <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
      <category term="视频描述" scheme="http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="视频描述" scheme="http://yoursite.com/tags/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>A Survey of Evaluation Metrics Used for NLG Systems</title>
    <link href="http://yoursite.com/2020/08/28/A-Survey-of-Evaluation-Metrics-Used-for-NLG-Systems/"/>
    <id>http://yoursite.com/2020/08/28/A-Survey-of-Evaluation-Metrics-Used-for-NLG-Systems/</id>
    <published>2020-08-28T06:20:13.000Z</published>
    <updated>2020-10-16T08:36:38.860Z</updated>
    
    <content type="html"><![CDATA[<h3 id><a href="#" class="headerlink" title=" "></a> </h3><h3 id="Recommendations-Possible-future-research-directions"><a href="#Recommendations-Possible-future-research-directions" class="headerlink" title="Recommendations (Possible future research directions)"></a>Recommendations (Possible future research directions)</h3><ul><li><p><strong style="color:blue;">为所有的评估指标构建一个通用的工具包</strong></p></li><li><p><strong style="color:blue;">构建一个包含 human judgments 的数据集。</strong></p><p>（1）human 会从不同的角度进行评估</p><p>（2）根据收集的数据，可以训练出来一个评价指标</p></li><li><p><strong style="color:blue;">提出 task-specific and context-dependent metrics</strong></p><p>类似于 dialog 这个任务，reference response 与 right prediction response 之间的word overlap 很小，因此，only reference dependent 的评价指标是有缺陷的，还需要结合 context 来设计评价指标</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh954nipuej319z0k4q9z.jpg"></p></li><li><p><strong style="color:blue;">提出具有可解释性的评价指标</strong></p><p>当前大部分的评价指标， 对预测仅仅给出 a single score, 没有任何具体的指向。但是对于 human evaluation, 会从具体的层面，eg: fluency, adequency. coherence 来进行评价。因此 a single score 不够具有可解释性。</p><p>应该设计不同的评价指标，每一个评价指标，从特定的层面进行评价。</p></li><li><p><strong style="color:red;">Creating robust benchmarks for evaluating evaluation metrics </strong></p><p>early metrics, 例如，BLEU，METEOR 等，已经在各种各样的任务上进行了验证。</p><p>但是，最近新提出的评价指标还没有被 examined critically，为了实施这一研究，需要收集一个 <strong>对抗 evaluation benchmarks</strong>， <strong>这个 benchmarks可以测试这些metrics 的鲁棒性。</strong></p><p>举个例子：对于dialog，给定一个context， 可以收集一些 adversarially crafted responses（与 passage 有较高的 word overlap, 但实际上是不相关的，或者是不正确的）  。查看evaluation metrics 是否会对这种手工创造的对抗例子给<strong>低分</strong>，已验证其鲁棒性。</p><p><strong>除了这种 adversarial evaluations， 还需要研究 提出的evaluation metrics 是否有specific biases.</strong> 比如，GAN based evaluators 会在一些systems上进行训练，则其更容易对这些 systems 给高分。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot; &quot;&gt;&lt;/a&gt; &lt;/h3&gt;&lt;h3 id=&quot;Recommendations-Possible-future-research-directions&quot;&gt;&lt;a href=&quot;#Recommendat
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="评价指标" scheme="http://yoursite.com/categories/NLP/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>correlation coefficient</title>
    <link href="http://yoursite.com/2020/08/26/correlation-coefficient/"/>
    <id>http://yoursite.com/2020/08/26/correlation-coefficient/</id>
    <published>2020-08-26T06:41:03.000Z</published>
    <updated>2020-08-27T02:58:28.571Z</updated>
    
    <content type="html"><![CDATA[<h1 id="correlation-coefficient"><a href="#correlation-coefficient" class="headerlink" title="correlation coefficient"></a>correlation coefficient</h1><ul><li>spearman 和 kendall 计算的都是对排序 之间的计算</li><li>pearson 计算的是直接的数值，协方差，标准差之间的计算</li></ul><h3 id="pearson"><a href="#pearson" class="headerlink" title="pearson"></a>pearson</h3><ul><li><p>代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下两种方式都可以</span></span><br><span class="line">scipy.stats.pearsonr(array_1, array_2)</span><br><span class="line">np.corrcoef(array_1, array_2)</span><br></pre></td></tr></table></figure></li><li><p>计算公式</p><p><img src="https://i.loli.net/2020/08/26/lXA4uMz9UkG2vFx.png" alt="image-20200826144001717" style="zoom: 33%;"></p></li><li><p>适用范围</p><p>当两个变量的标准差都不为零时，相关系数才有定义，皮尔逊相关系数适用于：</p><p>(1)、两个变量之间是线性关系，都是连续数据。</p><p>(2)、两个变量的总体是正态分布，或接近正态的单峰分布。</p><p>(3)、两个变量的观测值是成对的，每对观测值之间相互独立。</p></li><li><p>注意</p><p>公式的分母是变量的标准差，这就意味着计算pearson时，变量的标准差不能为0（分母不能为0），也就是说你的两个变量中任何一个的值不能都是相同的。如果没有变化，用pearson是没办法算出这个变量与另一个变量之间是不是有相关性的。</p><p>就好比我们想研究人跑步的速度与心脏跳动的相关性，如果你无论跑多快，心跳都不变（即心跳这个变量的标准差为0），或者你心跳忽快忽慢的，却一直保持一个速度在跑（即跑步速度这个变量的标准差为0），那我们都无法通过pearson的计算来判断心跳与跑步速度到底相不相关。</p></li><li><p>使用Pearson线性相关系数有2个局限：</p><ol><li>必须假设数据是成对地从正态分布中取得的。</li><li>数据至少在逻辑范围内是等距的。</li></ol></li></ul><h3 id="spearman"><a href="#spearman" class="headerlink" title="spearman"></a>spearman</h3><p><a href="https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php" target="_blank" rel="noopener">https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php</a></p><ul><li><p>代码实现</p><ul><li><p>对于一般情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result, _ = scipy.stats.spearmanr(array_1, array_2)</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>对于离散整数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spearmanr</span><span class="params">(set_1, set_2)</span>:</span></span><br><span class="line"></span><br><span class="line">    ar = np.apply_along_axis(scipy.stats.rankdata, <span class="number">0</span>, set_1)</span><br><span class="line">    br = np.apply_along_axis(scipy.stats.rankdata, <span class="number">0</span>, set_2)</span><br><span class="line">    d = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(ar)):</span><br><span class="line">        d.append(ar[i] - br[i])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    d_sq = [i ** <span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> d]</span><br><span class="line">    sum_d_sq = sum(d_sq)</span><br><span class="line">    n_cu_min_n = len(set_1) ** <span class="number">3</span> - len(set_1)</span><br><span class="line">    r = <span class="number">1</span> - ((<span class="number">6.0</span> * sum_d_sq) / n_cu_min_n)</span><br><span class="line">    <span class="keyword">return</span> r</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>计算公式</p><ul><li><p>存在 并列排序时：</p><p>先排序，对排序值 pair 计算 pearson 系数</p></li><li><p>不存在并列排序时，</p><p>先排序，计算d<sub>i</sub> 再按照下面第一个公式进行计算</p></li></ul></li></ul><p><img src="https://i.loli.net/2020/08/26/ON5iLc2kl6EAM1p.png" alt="微信截图_20200826151113"></p><ul><li><p>另外一种说法</p><ul><li><p>一般情况：</p><p>先排序，对排序值 pair 计算 pearson 系数</p></li><li><p>对于数值为离散的整数时，</p><p>先排序，计算d<sub>i</sub> 再按照吐下的公式进行计算</p></li></ul><p><img src="https://i.loli.net/2020/08/26/z3iJWqUxRcBSegV.png" alt="微信截图_20200826151324"></p></li><li><p>适用范围</p><p>spearman 对数据条件的要求没有皮尔逊相关系数严格，只要两个变量的观测值是成对的等级评定资料，或者是由连续变量观测资料转化得到的等级资料，不论两个变量的总体分布形态、样本容量的大小如何，都可以用spearman 来进行研究</p></li></ul><h3 id="kendall"><a href="#kendall" class="headerlink" title="kendall"></a>kendall</h3><ul><li><p>代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scipy.stats.kendalltau(array_1, array_2)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Kendallta</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    Lens = len(a)</span><br><span class="line"></span><br><span class="line">    ties_onlyin_x = <span class="number">0</span></span><br><span class="line">    ties_onlyin_y = <span class="number">0</span></span><br><span class="line">    con_pair = <span class="number">0</span></span><br><span class="line">    dis_pair = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Lens - <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, Lens):</span><br><span class="line">            test_tying_x = np.sign(a[i] - a[j])</span><br><span class="line">            test_tying_y = np.sign(b[i] - b[j])</span><br><span class="line">            panduan = test_tying_x * test_tying_y</span><br><span class="line">            <span class="keyword">if</span> panduan == <span class="number">1</span>:</span><br><span class="line">                con_pair += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> panduan == <span class="number">-1</span>:</span><br><span class="line">                dis_pair += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> test_tying_y == <span class="number">0</span> <span class="keyword">and</span> test_tying_x != <span class="number">0</span>:</span><br><span class="line">                ties_onlyin_y += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> test_tying_x == <span class="number">0</span> <span class="keyword">and</span> test_tying_y != <span class="number">0</span>:</span><br><span class="line">                ties_onlyin_x += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    result = (con_pair - dis_pair) / np.sqrt(</span><br><span class="line">        (con_pair + dis_pair + ties_onlyin_x) * (dis_pair + con_pair + ties_onlyin_y))</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure></li><li><p>计算公式</p><p><a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient" target="_blank" rel="noopener">Kendall_rank_correlation_coefficient</a></p><p>有好几个计算公式</p></li><li><p>适用范围</p><p>kendall与spearman 对数据条件的要求相同，可参见<a href="http://blog.csdn.net/wsywl/archive/2010/09/02/5859751.aspx" target="_blank" rel="noopener">统计相关系数(2)—Spearman Rank(斯皮尔曼等级)相关系数及MATLAB实现</a>中介绍的spearman 对数据条件的要求。</p></li></ul><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><ul><li>这三种 相关系数，计算 array_1 与 array_2 之间的相关性，若array_1 或者 array_2 中的元素都相同（eg: array_1 = np.array([5,5,5,5,5])） 则会使得输出为NaN.</li></ul><p>三种方法的适用场合</p><h4 id="主要参数methods介绍"><a href="#主要参数methods介绍" class="headerlink" title="主要参数methods介绍:"></a>主要参数methods介绍:</h4><ol><li>pearson correlation coefficient（皮尔逊相关性系数）。<br> 常用的相关系数求法，采用协方差cov(X,Y)/标准差的乘积(σX, σY)。<br> 数据要求： 线性数据、连续且符合正态分布；数据间差异不能太大；变量准差不能为0，即两变量中任何一个值不能都是相同。</li><li>spearman correlation coefficient（斯皮尔曼秩相关性系数）。<br> 根据原始数据的排序位置进行计算。<br> 数据要求：用于解决称名数据和顺序数据相关的问题，适用于两列变量，而且具有等级变量性质具有线性关系的数据，能够很好处理序列中相同值和异常值。</li><li>kendall correlation coefficient（肯德尔相关性系数）。<br> 等级相关系数，适用于两个变量均为有序分类的情况<br> 数据要求：肯德尔相关性系数，它也是一种秩相关系数，不过它所计算的对象是分类变量。</li></ol><p>所以针对【连续、正态分布、线性】数据，采用pearson相关系数；针对【非线性的、非正态】数据，采用spearman相关系数；针对【分类变量、无序】数据，采用Kendall相关系数。一般来讲，线性数据采用pearson，否则选择spearman，如果是分类的则用kendall。</p><p>作者：王叽叽的小心情<br>链接：<a href="https://www.jianshu.com/p/f9304da68d98" target="_blank" rel="noopener">https://www.jianshu.com/p/f9304da68d98</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><h3 id="相关系数和P-value-值"><a href="#相关系数和P-value-值" class="headerlink" title="相关系数和P-value 值"></a>相关系数和P-value 值</h3><p>看两者是否算相关要看两方面</p><p>显著水平以及相关系数</p><p>（1）显著水平,就是P值,这是首要的,因为如果不显著,相关系数再高也没用,可能只是因为偶然因素引起的,那么多少才算显著,一般p值小于0.05就是显著了；如果小于0.01就更显著；例如p值=0.001,就是很高的显著水平了,只要显著,就可以下结论说：拒绝原假设无关,两组数据显著相关也说两者间确实有明显关系.通常需要p值小于0.1,最好小于0.05甚至0.01,才可得出结论：两组数据有明显关系,如果p=0.5,远大于0.1,只能说明相关程度不明显甚至不相关.起码不是线性相关.</p><p>（2）相关系数,也就是pearson spearman等,通常也称为R值,在确认上面指标显著情况下,再来看这个指标,一般相关系数越高表明两者间关系越密切.R&gt;0 代表连个变量正相关,即一个变大另一个随之变大</p><h3 id="需要的数据量"><a href="#需要的数据量" class="headerlink" title="需要的数据量"></a>需要的数据量</h3><p><a href="https://bbs.pinggu.org/thread-3240378-1-1.html" target="_blank" rel="noopener">https://bbs.pinggu.org/thread-3240378-1-1.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;correlation-coefficient&quot;&gt;&lt;a href=&quot;#correlation-coefficient&quot; class=&quot;headerlink&quot; title=&quot;correlation coefficient&quot;&gt;&lt;/a&gt;correlation coeff
      
    
    </summary>
    
    
      <category term="评价指标" scheme="http://yoursite.com/tags/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
  </entry>
  
  <entry>
    <title>评价者之间的一致性-Kappas</title>
    <link href="http://yoursite.com/2020/08/06/%E8%AF%84%E4%BB%B7%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7-Kappas/"/>
    <id>http://yoursite.com/2020/08/06/评价者之间的一致性-Kappas/</id>
    <published>2020-08-06T09:06:38.000Z</published>
    <updated>2020-08-06T09:14:45.311Z</updated>
    
    <content type="html"><![CDATA[<h3 id="评价者之间的一致性—Kappas-Inter-rater-agreement-Kappas"><a href="#评价者之间的一致性—Kappas-Inter-rater-agreement-Kappas" class="headerlink" title="评价者之间的一致性—Kappas Inter-rater agreement Kappas"></a>评价者之间的一致性—Kappas Inter-rater agreement Kappas</h3><p>inter-rater reliability == inter-rater agreement == concordance</p><p>评价者之间的一致性的Kappa分数代表着在打分判断中，他们有多少共识，有多一致。</p><p>Kappa分数处于0-1之间，具体地：</p><div class="table-container"><table><thead><tr><th style="text-align:center">K</th><th style="text-align:center">Interpretation</th></tr></thead><tbody><tr><td style="text-align:center">&lt;0</td><td style="text-align:center">Poor agreement 不一致</td></tr><tr><td style="text-align:center">0.0-0.20</td><td style="text-align:center">Slight agreement</td></tr><tr><td style="text-align:center">0.21-0.40</td><td style="text-align:center">Fair agreement</td></tr><tr><td style="text-align:center">0.41-0.60</td><td style="text-align:center">Moderate agreement</td></tr><tr><td style="text-align:center">0.61-0.80</td><td style="text-align:center">Substantial agreement</td></tr><tr><td style="text-align:center">0.81-1.0</td><td style="text-align:center">Almost perfect agreement</td></tr></tbody></table></div><h3 id="Cohen’s-Kappa"><a href="#Cohen’s-Kappa" class="headerlink" title="Cohen’s Kappa"></a>Cohen’s Kappa</h3><p>Cohen’s Kappa 计算了评分者之间的一致性。当评分者对同一项任务给出了相同的判断或分数，那么他们的一致性得到了体现。</p><p>Cohen’s Kappa 只能在以下的条件下使用：</p><ul><li>两个评价者分别对每个样本进行评分</li><li>一个评价者对每个样本进行两次评分</li></ul><p><strong>Cohen’s Kappa 计算</strong></p><p>要注意的是，一般情况下，Cohen’s Kappa 的计算背景是：有<strong>两个</strong>评分者对每个样本进行<strong>二分类</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">postive (rater A)</th><th style="text-align:center">negative (rater A)</th><th style="text-align:center">Total</th></tr></thead><tbody><tr><td style="text-align:center"><strong>postive (rater B)</strong></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B11%7D" alt="n_{11}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B12%7D" alt="n_{12}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B1.%7D" alt="n_{1.}"></td></tr><tr><td style="text-align:center"><strong>negative (rater B)</strong></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B21%7D" alt="n_{21}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B22%7D" alt="n_{22}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B2.%7D" alt="n_{2.}"></td></tr><tr><td style="text-align:center"><strong>Total</strong></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B.1%7D" alt="n_{.1}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B.2%7D" alt="n_{.2}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D" alt="n_{11}+n_{12}+n_{21}+n_{22}"></td></tr></tbody></table></div><p>计算公式为：<br> <img src="https://math.jianshu.com/math?formula=k%20%3D%20%5Cfrac%7Bp_o-p_e%7D%7B1-p_e%7D%20%3D%201-%5Cfrac%7B1-p_o%7D%7B1-p_e%7D" alt="k = \frac{p_o-p_e}{1-p_e} = 1-\frac{1-p_o}{1-p_e}"><br> 其中，<img src="https://math.jianshu.com/math?formula=p_o" alt="p_o"> 代表评价者之间的相对观察一致性（the relative <strong>observed agreement</strong> among raters）<br> <img src="https://math.jianshu.com/math?formula=p_o%3D%5Cfrac%7Bn_%7B11%7D%2Bn_%7B22%7D%7D%7Bn_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D%7D" alt="p_o=\frac{n_{11}+n_{22}}{n_{11}+n_{12}+n_{21}+n_{22}}"><br> <img src="https://math.jianshu.com/math?formula=p_e" alt="p_e"> 代表偶然一致性的假设概率（the hypothetical probability of <strong>chance agreemnet</strong>）<br> <img src="https://math.jianshu.com/math?formula=p_e%3D%5Cfrac%7Bn_%7B.1%7D*n_%7B1.%7D%7D%7B(n_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D" alt="p_e=\frac{n_{.1}*n_{1.}}{(n_{11}+n_{12}+n_{21}+n_{22})^2}+\frac{n_{.2}*n_{2.}}{(n_{11}+n_{12}+n_{21}+n_{22})^2}=\frac{n_{.1}*n_{1.}+n_{.2}*n_{2.}}{(n_{11}+n_{12}+n_{21}+n_{22})^2}">%5E2%7D%2B%5Cfrac%7Bn_%7B.2%7D<em>n_%7B2.%7D%7D%7B(n_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D)%5E2%7D%3D%5Cfrac%7Bn_%7B.1%7D</em>n_%7B1.%7D%2Bn_%7B.2%7D<em>n_%7B2.%7D%7D%7B(n_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D)%5E2%7D)<br> <em>*例子</em></em></p><p>rater A和rater B对50张图片进行分类，正类和负类。结果为：</p><ul><li>20张图片两个评价者都认为是正类</li><li>15张图片两个评价者都认为是负类</li><li>rater A认为25张图片是正类，25张图片是负类</li><li>rater B 认为30张图片是正类，20张图片是负类</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">postive (rater A)</th><th style="text-align:center">negative (rater A)</th><th style="text-align:center">Total</th></tr></thead><tbody><tr><td style="text-align:center"><strong>postive (rater B)</strong></td><td style="text-align:center">20</td><td style="text-align:center">10</td><td style="text-align:center">30</td></tr><tr><td style="text-align:center"><strong>negative (rater B)</strong></td><td style="text-align:center">5</td><td style="text-align:center">15</td><td style="text-align:center">20</td></tr><tr><td style="text-align:center"><strong>Total</strong></td><td style="text-align:center">25</td><td style="text-align:center">25</td><td style="text-align:center">50</td></tr></tbody></table></div><p><strong>Step1</strong> ：计算<img src="https://math.jianshu.com/math?formula=p_o" alt="p_o"><br> <img src="https://math.jianshu.com/math?formula=p_o%3Dnumber%5C%20in%5C%20agreement%2F%5C%20total%3D(20%2B15" alt="p_o=number\ in\ agreement/\ total=(20+15)/50=0.70">%2F50%3D0.70)</p><p><strong>Step2</strong> ：计算<img src="https://math.jianshu.com/math?formula=p_e" alt="p_e"><br> <img src="https://math.jianshu.com/math?formula=p_e%3DThe%5C%20total%5C%20probability%5C%20the%5C%20raters%5C%20both%5C%20saying%5C%20postive%20%5C%5Cand%5C%20negative%20%5C%20randomly%20%3D(25%2F50" alt="p_e=The\ total\ probability\ the\ raters\ both\ saying\ postive \\and\ negative \ randomly =(25/50)*(30/50)+(25/50)*(20/50)=0.50"><em>(30%2F50)%2B(25%2F50)</em>(20%2F50)%3D0.50)<br> <strong>Step3</strong> ：计算<img src="https://math.jianshu.com/math?formula=k" alt="k"><br> <img src="https://math.jianshu.com/math?formula=k%3D%5Cfrac%7Bp_o-p_e%7D%7B1-p_e%7D%3D%5Cfrac%7B0.70-0.50%7D%7B1-0.50%7D%3D0.40" alt="k=\frac{p_o-p_e}{1-p_e}=\frac{0.70-0.50}{1-0.50}=0.40"><br> <img src="https://math.jianshu.com/math?formula=k%3D0.40" alt="k=0.40"> 代表<strong>fair agreement</strong></p><h3 id="Fleiss’s-Kappa"><a href="#Fleiss’s-Kappa" class="headerlink" title="Fleiss’s Kappa"></a>Fleiss’s Kappa</h3><p>Fleiss’s Kappa 是对 Cohen‘s Kappa 的扩展：</p><ul><li>衡量<strong>三个或更多</strong>评分者的一致性</li><li>不同的评价者可以对不同的项目进行评分，而不用像Cohen’s 两个评价者需要对相同的项目进行评分</li><li>Cohen’s Kappa 的评价者是精心选择和固定的，而Fleiss’s Kappa 的评价者是从较大的人群中随机选择的</li></ul><p>举一个例子对 Fleiss’s Kappa 的计算进行说明：14个评价者对10个项目进行1-5的评分，<img src="https://math.jianshu.com/math?formula=N%3D10%2Cn%3D14%2Ck%3D5" alt="N=10,n=14,k=5"></p><blockquote><p>关于这  “10个项目” 的理解：比如在 NLI 数据标注中，需要为很多 promise-hypotheses pair 进行打分，这每一个pair就是一个pair.</p></blockquote><div class="table-container"><table><thead><tr><th style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7Bij%7D" alt="n_{ij}"></th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th><th style="text-align:center">5</th><th style="text-align:center"><img src="https://math.jianshu.com/math?formula=P_i" alt="P_i"></th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">14</td><td style="text-align:center">1.000</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">6</td><td style="text-align:center">4</td><td style="text-align:center">2</td><td style="text-align:center">0.253</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">3</td><td style="text-align:center">5</td><td style="text-align:center">6</td><td style="text-align:center">0.308</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">0</td><td style="text-align:center">3</td><td style="text-align:center">9</td><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">0.440</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">8</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0.330</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">7</td><td style="text-align:center">7</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0.462</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">3</td><td style="text-align:center">2</td><td style="text-align:center">6</td><td style="text-align:center">3</td><td style="text-align:center">0</td><td style="text-align:center">0.242</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">2</td><td style="text-align:center">5</td><td style="text-align:center">3</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">0.176</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">6</td><td style="text-align:center">5</td><td style="text-align:center">2</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0.286</td></tr><tr><td style="text-align:center">10</td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">3</td><td style="text-align:center">7</td><td style="text-align:center">0.286</td></tr><tr><td style="text-align:center">Total</td><td style="text-align:center">20</td><td style="text-align:center">28</td><td style="text-align:center">39</td><td style="text-align:center">21</td><td style="text-align:center">32</td><td style="text-align:center">140</td></tr><tr><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=p_j" alt="p_j"></td><td style="text-align:center">0.143</td><td style="text-align:center">0.200</td><td style="text-align:center">0.279</td><td style="text-align:center">0.150</td><td style="text-align:center">0.229</td></tr></tbody></table></div><p><strong>Step1</strong> ：计算<img src="https://math.jianshu.com/math?formula=p_j" alt="p_j"> ，以<img src="https://math.jianshu.com/math?formula=p_1" alt="p_1">为例，评价者随机打1分的概率<br> <img src="https://math.jianshu.com/math?formula=p_1%3Dthe%5C%20total%5C%20number%5C%20of%5C%20the%5C%20column%2F%5C%20the%5C%20total%5C%20number%5C%20of%20%5C%20tasks%20%3D%2020%2F14*10%3D0.143" alt="p_1=the\ total\ number\ of\ the\ column/\ the\ total\ number\ of \ tasks = 20/14*10=0.143"><br> <strong>Step2</strong> ：计算<img src="https://math.jianshu.com/math?formula=P_i" alt="P_i"> ，以<img src="https://math.jianshu.com/math?formula=P_2" alt="P_2">为例,14个评价者对第2个任务达成共识的程度<br> <img src="https://math.jianshu.com/math?formula=P_2%3D%5Cfrac%7Bthe%5C%20sum%5C%20of%5C%20suqare%20%5C%20of%5C%20the%5C%20row%7D%7Bn*(n-1" alt="P_2=\frac{the\ sum\ of\ suqare \ of\ the\ row}{n*(n-1)}=\frac{0^2+2^2+6^2+4^2-14}{14*(14-1)}=0.253">%7D%3D%5Cfrac%7B0%5E2%2B2%5E2%2B6%5E2%2B4%5E2-14%7D%7B14<em>(14-1)%7D%3D0.253)<br> <strong>Step3</strong> ：计算<img src="https://math.jianshu.com/math?formula=P_e%2CP_o" alt="P_e,P_o"><br> ![P_o=\frac{1}{N}\sum_{i=1}^{N}P_i=\frac{1}{10}</em>3.78=0.378](<a href="https://math.jianshu.com/math?formula=P_o%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DP_i%3D%5Cfrac%7B1%7D%7B10%7D*3.78%3D0.378" target="_blank" rel="noopener">https://math.jianshu.com/math?formula=P_o%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DP_i%3D%5Cfrac%7B1%7D%7B10%7D*3.78%3D0.378</a>)</p><p><img src="https://math.jianshu.com/math?formula=P_e%3D%5Csum_%7Bj%3D1%7D%5E%7Bk%7Dp_j%5E2%3D0.143%5E2%2B0.200%5E2%2B0.279%5E2%2B0.150%5E2%2B0.229%5E2%3D0.213" alt="P_e=\sum_{j=1}^{k}p_j^2=0.143^2+0.200^2+0.279^2+0.150^2+0.229^2=0.213"></p><p><img src="https://math.jianshu.com/math?formula=k%3D%5Cfrac%7BP_o-P_e%7D%7B1-P_e%7D%3D%5Cfrac%7B0.378-0.213%7D%7B1-0.213%7D%3D0.210" alt="k=\frac{P_o-P_e}{1-P_e}=\frac{0.378-0.213}{1-0.213}=0.210"></p><p><img src="https://math.jianshu.com/math?formula=k%3D0.210" alt="k=0.210"> 代表<strong>fair agreement</strong></p><blockquote><p>[1] Landis JR, Koch GG. The measurement of observer agreement for categorical data. Biometrics.     1977;33(1):159–74</p><p>[2] <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.pmean.com%2Fdefinitions%2Fkappa.htm" target="_blank" rel="noopener">http://www.pmean.com/definitions/kappa.htm</a></p><p>[3] <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.statisticshowto.datasciencecentral.com%2Fcohens-kappa-statistic%2F" target="_blank" rel="noopener">https://www.statisticshowto.datasciencecentral.com/cohens-kappa-statistic/</a></p><p>[4] <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.statisticshowto.datasciencecentral.com%2Ffleiss-kappa%2F" target="_blank" rel="noopener">https://www.statisticshowto.datasciencecentral.com/fleiss-kappa/</a></p><p>[5]  <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Famirziai%2Flearning%2Fblob%2Fmaster%2Fstatistics%2FInter-rater%20agreement%20kappas.ipynb%5D(https%3A%2F%2Fgithub.com%2Famirziai%2Flearning%2Fblob%2Fmaster%2Fstatistics%2FInter-rater" target="_blank" rel="noopener">[https://github.com/amirziai/learning/blob/master/statistics/Inter-rater%20agreement%20kappas.ipynb](https://github.com/amirziai/learning/blob/master/statistics/Inter-rater</a> agreement kappas.ipynb)</p><p>[6] <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fqq_31113079%2Farticle%2Fdetails%2F76216611" target="_blank" rel="noopener">https://blog.csdn.net/qq_31113079/article/details/76216611</a></p></blockquote><p>作者：Luuuuuua<br>链接：<a href="https://www.jianshu.com/p/f9c383b39859" target="_blank" rel="noopener">https://www.jianshu.com/p/f9c383b39859</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;评价者之间的一致性—Kappas-Inter-rater-agreement-Kappas&quot;&gt;&lt;a href=&quot;#评价者之间的一致性—Kappas-Inter-rater-agreement-Kappas&quot; class=&quot;headerlink&quot; title=&quot;评价
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Bridging by Word Image Grounded Vocabulary Construction for Visual Captioning</title>
    <link href="http://yoursite.com/2020/08/01/Bridging-by-Word-Image-Grounded-Vocabulary-Construction-for-Visual-Captioning/"/>
    <id>http://yoursite.com/2020/08/01/Bridging-by-Word-Image-Grounded-Vocabulary-Construction-for-Visual-Captioning/</id>
    <published>2020-08-01T08:34:04.000Z</published>
    <updated>2020-08-01T08:35:13.103Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>当前 普遍使用的 CNN-RNN 策略，基于整个 training dataset构建 vocabulary，但是，这会<strong>导致生成的句子中的 N-grams 也是在训练集中常见的</strong>，但是语义上却与given image 无关。</p><p>为了解决这个问题，本文提出了构建一个 image-grounded vocabulary。具体地，提出了一个 two-step approach，通过结合 visual information 和 relationships among words来构建 新的vocabulary。</p><p>并提出了两个策略在 text generation过程中<strong>利用</strong>构建的vocabulary。（1）generator 从image-grounded vocabulary中挑选words （2）soft-attention聚合 vocabulary information 到RNN cell 中来生成下一个单词。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>当前生成单词的方式，是从整个vocab 中select，但是当描述一个 particular image时，the possible words 应该是从一小部分单词集中挑选出来。因此，可以想一个方案，在image caption generation 过程中，有效的约束 word selection space。这将会解决 生成的句子中常常是 irrelavant n-gram problem. </p><p>本文，提出构建一个 image-grounded vocabulary，</p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>整个结构包括两个阶段：(1) image-grounded vocabulary construction, (2) text generation with vocabulary constraints.</p><p><strong>The image-grounded vocabulary</strong> constructor builds a  vocabulary related to a given image.</p><p><strong>The text generator</strong> with vocabulary constraints generates captions using the constructed vocabulary in two different ways. (1) words generated are strictly limited to those in the image-grounded vocabulary. (2) words in the image-grounded vocabulary are re-weighted within the RNN cell such that they are more likely to be generated.</p><h4 id="Image-Grounded-Vocabulary-Construction"><a href="#Image-Grounded-Vocabulary-Construction" class="headerlink" title="Image-Grounded Vocabulary Construction"></a>Image-Grounded Vocabulary Construction</h4><p>caption 中的单词，一般可以分类两类，一类是直接与image content 相关的单词（entities or objects depicted in the image），另一类是function words or words which 没有和image content 有直接的对应关系。</p><p>本文假设， directly-related words 可以由视觉信息来决定，而第二类单词，可以由第一类单词之间的relationship 来决定。因此提出了两步策略来构建 image-grounded vocabulary。</p><ul><li><p>第一步</p><p>使用 <code>From captions to visual concepts and back</code> 中提到的方法，获取 textual concept as H.</p><p>H中words 与 image 的相关性分布：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbctjwumaj30cz02ga9z.jpg" style="zoom:33%;"></p></li><li><p>第二步</p><p>计算  full vocabulary <code>V</code> 中单词的 相关性分数，</p><p>The probability distribution of words in <code>V</code>:</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbd1t40doj30if03b3yt.jpg" style="zoom:33%;"></p></li></ul><p>本文挑选 top k words 来构成 <strong style="color:red;">the image-grounded vocabulary ($W_i$) </strong>for given image.</p><h4 id="Text-Generation-with-Vocabulary-Constraints"><a href="#Text-Generation-with-Vocabulary-Constraints" class="headerlink" title="Text Generation with Vocabulary Constraints"></a>Text Generation with Vocabulary Constraints</h4><p>提出了两个不同的策略来利用 the image-grounded vocab-ulary $W_i$ 和 word relevance distribution  $S_i^{(V)}$ : 一种，使用$W_i$ 作为 hard constraint; 另一种，聚合每个单词的相关性到 RNN cell 来生成 caption.</p><p><strong>Generator with Hard Constraint</strong></p><p>正常的方法，是生成 full vocab 的 概率分布，然后取 argmax。</p><p>但是在 hard constraint 下，生成 $W_i$ 的概率分布，再取 argmax。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbdj6x0ruj30m402haa0.jpg" style="zoom: 50%;"></p><p>对于在 $W_i$ 中没有出现的单词，打掩码：a mask operation $m_i$ is introduced  to replace the $j_{th}$ value in the vector with 1 if $w_j$ is not found in $W_i$ 。</p><p><strong>Generator with Soft Constraint </strong></p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbdt8khggj31cg0mw4i2.jpg" style="zoom:50%;"></p><p>图中展示的 image-grounded vocabulary 其实是 $S_i^{V}$</p><p>结合到RNN cell: </p><p>这个新的 RNN cell 结合了image-grounded vocabulary，因此，会更加容易生成该vocab中的单词。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbe0h10qcj30pt0avmy9.jpg" style="zoom:33%;"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;p&gt;当前 普遍使用的 CNN-RNN 策略，基于整个 training dataset构建 vo
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations</title>
    <link href="http://yoursite.com/2020/07/31/Aligning-Visual-Regions-and-Textual-Concepts-for-Semantic-Grounded-Image-Representations/"/>
    <id>http://yoursite.com/2020/07/31/Aligning-Visual-Regions-and-Textual-Concepts-for-Semantic-Grounded-Image-Representations/</id>
    <published>2020-07-31T09:08:23.000Z</published>
    <updated>2020-07-31T09:09:05.184Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Understanding the image, which necessitates the acquisition of grounded image representations. </p><p>以下，提供了几种方式来 表达image content.</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha8v7kk80j316b0ciqmw.jpg"></p><p><strong style="color:blue;">[1]</strong> 基于 R-CNN 的方法可以获得 regions，但是却没有与 actual words关联起来，这将会造成两个域之间的语义不一致，并且需要由 downstream systems 自己学习 alignments。</p><p><strong style="color:blue;">[2]</strong> 此外，这些representations 仅包含局部特征，缺少全局结构信息。 这些问题 使system 难以有效地理解图像。</p><p>因此，本文提出一个 Mutual Iterative Attention (MIA) 模块，在编码阶段，从<strong style="color:red;">视觉域和语言域</strong>（解决[1]） 构建<strong style="color:red;">聚合的 image representations</strong>(解决[2])。</p><p>we perform mutual attention <strong style="color:blue;">iteratively </strong> between the two domains to realize the procedure <strong>without annotated alignment data.</strong> </p><p>The visual receptive fields gradually concentrate on salient visual regions, and the original word-level concepts are gradually merged to recapitulate corresponding visual regions. </p><p>In addition, the aligned visual features and textual concepts provide a more clear definition of the image aspects they represent. </p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha9crz1pbj30de0s4jxg.jpg" style="zoom:50%;"></p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha9gjk5vbj31h50j3jvs.jpg"></p><h3 id="textual-concepts"><a href="#textual-concepts" class="headerlink" title="textual concepts"></a>textual concepts</h3><p>从下面这篇论文中提取 text concepts</p><blockquote><p> <strong>From captions to visual concepts and back.</strong> In CVPR, 2015</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h3&gt;&lt;p&gt;Understanding the image, which necessi
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Improving Image Captioning with Conditional Generative Adversarial Nets</title>
    <link href="http://yoursite.com/2020/07/31/Improving-Image-Captioning-with-Conditional-Generative-Adversarial-Nets/"/>
    <id>http://yoursite.com/2020/07/31/Improving-Image-Captioning-with-Conditional-Generative-Adversarial-Nets/</id>
    <published>2020-07-31T07:56:54.000Z</published>
    <updated>2020-07-31T07:58:07.993Z</updated>
    
    <content type="html"><![CDATA[<p>From:  <a href="https://zhuanlan.zhihu.com/p/39890390" target="_blank" rel="noopener">GAN in Image Captioning</a></p><h3 id="Improving-Image-Captioning-with-Conditional-Generative-Adversarial-Nets"><a href="#Improving-Image-Captioning-with-Conditional-Generative-Adversarial-Nets" class="headerlink" title="Improving Image Captioning with Conditional Generative Adversarial Nets"></a><strong>Improving Image Captioning with Conditional Generative Adversarial Nets</strong></h3><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>本文提出了一种新的基于条件生成对抗网络的图像字幕框架，作为传统的基于增强学习(RL)的编解码结构的扩展。为了应对不同的目标语言的指标之间不一致的评价问题，，论文设计了两种鉴别器网络来自动地、逐步地确定生成的描述是人工描述的还是机器生成的。</p><p>生成器是采用传统图像描述的模型，<strong>在强化学习自我批判算法（SCST）下进行优化</strong>。</p><p>由于基于CNN和RNN的结构各有其优点，因此引入了两种鉴别器结构。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha02tza7dj31im0hvtk0.jpg"></p><h4 id="CNN-discriminator："><a href="#CNN-discriminator：" class="headerlink" title="CNN discriminator："></a><strong>CNN discriminator：</strong></h4><p>（1）首先创建了一个feature map,编码了图像与句子特征。</p><p>（2）接着采用了m组有不同窗大小，核数目的卷积核来获取不同的特征，</p><p>（3）然后把所有特征作max pooling操作再联结在一起，并用一个highway架构提升性能。</p><p>（4）最后激活特征通过全连接层与sigmoid 转换来获得决策器的输出。输出在[0,1]之间。</p><h4 id="RNN-discriminator："><a href="#RNN-discriminator：" class="headerlink" title="RNN discriminator："></a><strong>RNN discriminator：</strong></h4><p>基于RNN的决策器采用了一种标准的LSTM架构，把图像特征输入到第一个LSTM，接下来输入的LSTM是输入是单词编码信息。最后通过全连接层与softmax层获得RNN决策器的输出。</p><h4 id="固定G，更新D："><a href="#固定G，更新D：" class="headerlink" title="固定G，更新D："></a>固定G，更新D：</h4><p>Discriminator的目标函数为：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha04mt3u7j31m4070tal.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha02tyrdqj31j50kxdv1.jpg"></p><h4 id="固定D，更新G"><a href="#固定D，更新G" class="headerlink" title="固定D，更新G"></a>固定D，更新G</h4><p>设计reward，以强化学习来更新generator。</p><p>在强化学习的设定下，本文采用GAN与RL结合的reward来权衡<strong style="color:red;">图像描述的保真度</strong>（在评价标准下获得高得分）与<strong style="color:red;">自然性</strong>（生成描述符合人类的风格）。</p><blockquote><p><strong style="color:blue;">这也是本文主要的创新点，结合 保真度 的评价</strong></p></blockquote><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha06jgm6tj31e409cmz3.jpg" style="zoom: 50%;"></p><h4 id="整个算法的伪代码如下："><a href="#整个算法的伪代码如下：" class="headerlink" title="整个算法的伪代码如下："></a>整个算法的伪代码如下：</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha09ceqgkj30ns0wg13f.jpg" style="zoom: 50%;"></p><h3 id="GAN-image-captioning-task"><a href="#GAN-image-captioning-task" class="headerlink" title="GAN + image captioning task"></a>GAN + image captioning task</h3><p><strong>[1703.06029] Towards Diverse and Natural Image Descriptions via a Conditional GAN</strong></p><p><strong>[1703.10476] Speaking the Same Language Matching Machine to Human Captions by Adversarial Training</strong></p><p><strong>[1705.00930] Show, Adapt and Tell Adversarial Training of Cross-domain Image Captioner</strong></p><p><strong>[1805.00063] Improved Image Captioning with Adversarial Semantic Alignment</strong></p><p><strong>[1804.00861] Generating Diverse and Accurate Visual Captions by Comparative Adversarial Learning</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;From:  &lt;a href=&quot;https://zhuanlan.zhihu.com/p/39890390&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GAN in Image Captioning&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Improving-I
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
      <category term="GAN" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/GAN/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>GAN简介</title>
    <link href="http://yoursite.com/2020/07/30/GAN%E7%AE%80%E4%BB%8B/"/>
    <id>http://yoursite.com/2020/07/30/GAN简介/</id>
    <published>2020-07-30T13:47:25.000Z</published>
    <updated>2020-07-31T03:33:37.708Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>From: <a href="https://blog.csdn.net/shanlepu6038/article/details/84335117?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param" target="_blank" rel="noopener">GAN（一）:基本框架</a></p></blockquote><p>一个GAN包含两部分，一个generator,一个discriminator（互相对抗）<br>generator和discriminator就像是猎食者和猎物之间的关系，一个产生图片，一个辨别图片的真假，互相促进，使得最终产生的图片接近realistic</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ol><li><p>随机初始化 generator 和 discriminator</p></li><li><p>In each training iteration：</p><ul><li><p>固定generator，更新discriminator</p><p>Discriminator learns to assign high scores to real objects and low scores to generated objects.</p></li><li><p>固定discriminator， 更新generator</p></li></ul></li></ol><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh9bgltng3j30hu0djage.jpg"></p><h3 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h3><p><strong>G和D是互相促进的：</strong>G的目的是产生的图像让D感到模糊不知道该分成realistic（看起来像是现实的）还是fake（看起来是假的），D的目的是将realistic和fake的图像准确分辨。所以G产生的图像会越来越真，D的辨别能力会越来越强，最终达到一个平衡。</p><p>P<sub>data</sub> 表示真实数据的分布，P<sub>g</sub> 表示generator产生的分布，最终的目的就是让P<sub>g</sub> 的分布尽可能的和P<sub>data</sub> 相同。<br>我们用D(x)表示真实图像经过discriminator后的分数，G(z)表示随机变量z经过generator后产生的图像，那么有：D(G(z)) 表示generator产生的图像经过discriminator后的分数</p><p>第一阶段，固定 generator，更新discriminator，最大化下面对的这个式子：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh9bxggo1rj30gv01lwf1.jpg"></p><p>第二阶段，固定discriminator，更新generator，最大化下面的这个式子：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh9bxgfqjnj30ad01s3yt.jpg"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;From: &lt;a href=&quot;https://blog.csdn.net/shanlepu6038/article/details/84335117?utm_medium=distribute.pc_relevant.none-task-blog-
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment</title>
    <link href="http://yoursite.com/2020/07/30/Align2Ground-Weakly-Supervised-Phrase-Grounding-Guided-by-Image-Caption-Alignment/"/>
    <id>http://yoursite.com/2020/07/30/Align2Ground-Weakly-Supervised-Phrase-Grounding-Guided-by-Image-Caption-Alignment/</id>
    <published>2020-07-30T08:44:12.000Z</published>
    <updated>2020-07-30T08:57:17.807Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文提出使用caption-to-image retrieval 作为下游任务，来引导 phrase localization。</p><p>第一步，学习 RoIs 与 phrases 之间的隐式对应，并利用这些匹配的RoIs来生成具有判别性的image representation。</p><p>第二步，learnedd representaion 与caption 对齐。</p><p>本文的贡献是，构建了“caption-conditioned” image encodinng，这件所有的任务都耦合在一起，并使得弱监督可以有效的引导 visual grounding。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li>监督学习的方法：依赖 region-phrase correspondence 数据。</li><li>弱监督：grounding free-form textual phrase，从image-caption pairs 这种weak correspondence中进行学习。</li></ul><p><strong>weakly supervised paradigm 的一个关键是，紧密耦合监督学习任务（image-caption matching）和无法获得显示标签的任务（region-phrase matching）。联合推理确保前者的监督损失可以有效的引导后者的学习。</strong></p><blockquote><p>[1] Andrej Karpathy and Li Fei-Fei. <strong>Deep visual-semantic alignments for generating image descriptions.</strong> In CVPR, 2015</p><p>[2] AndrejKarpathy,ArmandJoulin,andLiFFei-Fei. <strong>Deep fragment embeddings for bidirectional image sentence mapping.</strong> In NIPS,  2014</p></blockquote><p>[ 1 ] [ 2 ] 采用了 such paradigm，一般地，这种模型存在两个阶段：（1）local matching mudule: 得到 region-phrase 的隐式对应，进而生成local matching information.（2）gobal matching module: 使用（1）中得到的information来得到 image-caption matching.</p><p>需要注意的是，这种方案的设计，primary objective 是 image-caption matching 而不是 phrase matching。这种训练方式，将会放大selective regions 和 phrases 之间的相关性。举例说明：如果 第一阶段中，a small subset of phrases 存在很强的 match, 那么将会传递到第二阶段，<strong>via average pooling of the RoI–phrase matching scores</strong>, 使得image 和 caption 之间存在 high matching score。</p><p>这将会<strong>使得模型不去学习 准确的ground <strong style="color:red;">所有的</strong> phrases</strong>。分析可得，将visual grounding 作为 primary aim 不是一个有效的解决办法。<strong>这种“作弊”倾向，使模型学会了在下游任务上做得很好而不必在中间任务上做得更好。</strong></p><p>本文将这种现象称之为：“selective amplification” behavior</p><p>本文解决这个问题：我们通过提出一种novel mechanism 来解决这一问题，该机制以使两个阶段之间更紧密耦合的方式 to relay this information about the latent, inferred correspondences</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh8vyf8slej30wt0q914q.jpg" style="zoom: 50%;"></p><p>Our novelty lies in designing this effective transfer of  information between the supervised and unsupervised parts  of the model such that the quality of image representations for the supervised matching task is a direct consequence of  <strong>the correct localization of all phrases.</strong> </p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>our proposed model uses a novel technique that builds a discriminative image representation from the matched RoIs and uses this representation for the image-caption matching. </p><p>Specifically, the image representation that is used to match an image with a caption is conditioned only on <strong style="color:red;">the subset of image regions</strong> that align semantically with <strong style="color:red;">all the phrases</strong> in that caption. </p><p>本文认为，与标准的 pooling-based method 相比，这种结构的设计使图像字幕对的监督，成为 visual grounding 的更强学习信号。</p><h4 id="The-Local-Matching-module"><a href="#The-Local-Matching-module" class="headerlink" title="The Local Matching module"></a>The Local Matching module</h4><p>将 region 和 phrase 映射到相同的空间，然后计算 cosine similarity。</p><p>infer the matched RoI for a phrase最直接的方法是 选择top scoring box，但是，这种方案容易<strong>过拟合</strong>，因为模型经常持续选择 相同的错误region。</p><p>改进：使用attened region vector 作为matched RoI，虽然这种方法在其他的多模态任务中是有效的，但在这个任务中不是一个有效的方法。这是因为在训练过程中，多个匹配的RoI的加权平均似乎会损害匹配的RoI的辨别力（discriminativeness）。</p><p>再次改进：选择 top-k (k=3) scoring RoI candidates，然后随机的选择其中的一个作为 query phrase 的 匹配 RoI。这种策略通过在巡林过程中探索多样性的选择，进而可以增加鲁棒性。</p><h4 id="The-Local-Aggregator-module"><a href="#The-Local-Aggregator-module" class="headerlink" title="The Local Aggregator module"></a>The Local Aggregator module</h4><p>这个模块的设计比较玄学。说是为了generate a caption-conditioned representation of the image.</p><p>设计的模块：a two-layer Multilayer Perceptron (MLP) with a mean operation. </p><p>这个模块的输入，是从上一步中得到的 matched RoIs for correspondance phrases.</p><h4 id="The-Global-Matching-module"><a href="#The-Global-Matching-module" class="headerlink" title="The Global Matching module"></a>The Global Matching module</h4><p>将 caption 映射到与 上一步中得到  representation of the image 相同的空间，然后利用 cosine similarity</p><h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh939pfhcgj30tq040wen.jpg" style="zoom:33%;"></p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><p>motivation 挺好的 ，但是local aggregator 的设计比较朴素。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;p&gt;本文提出使用caption-to-image retrieval 作为下游任务，来引导 ph
      
    
    </summary>
    
      <category term="Visual Grounding" scheme="http://yoursite.com/categories/Visual-Grounding/"/>
    
    
      <category term="Visual Grounding" scheme="http://yoursite.com/tags/Visual-Grounding/"/>
    
  </entry>
  
  <entry>
    <title>Improving Image Captioning Evaluation by Considering Inter References Variance</title>
    <link href="http://yoursite.com/2020/07/26/Improving-Image-Captioning-Evaluation-by-Considering-Inter-References-Variance/"/>
    <id>http://yoursite.com/2020/07/26/Improving-Image-Captioning-Evaluation-by-Considering-Inter-References-Variance/</id>
    <published>2020-07-26T09:41:05.000Z</published>
    <updated>2020-07-26T10:01:20.734Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li><p>one-to-one metrics的方法存在缺陷：without considering the intrinsic variance between ground truth captions.  </p><p>bertscore 是最新的one-to-one metric， 可以实现与human很好的相关性，但是如果一些问题可以解决的话，可以能够进一步的提升性能。</p><p>本文则基于 bertscore，提出了一个新方法。</p></li></ul><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p><strong>当前的评价指标</strong>—over penalize：对于M个reference，metric 通过one-to-one 的方式得到M个得分，通过 pooling 操作，得到最终得分。<br>但是不同的reference caption关注到image的不同方面，因此会存在 variance。所以基于 pooling 的操作，太过简单。</p><p>如果度量标准仅查看一个参考字幕，那么为这种过度惩罚而寻找补救措施是一项挑战。</p></li><li><p><strong>bertscore</strong>—under penalize</p><p>由于在计算 bertscore 时，采用了贪婪搜索的方式，而且对于每个reference word, pick 一个最大值，这里有可能 没有candidate word 与 reference word相匹配，却给了一个高分。因此，导致 under-penalize</p></li><li><p><strong>分析</strong>： In one-to-one evaluation，尽管很难直接考虑所有reference，但可以使用来自预训练语言模型的上下文嵌入将references 合并为单个reference。</p></li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><ul><li><p>3.1  Preliminary concept of references<br>combination  </p></li><li><p>3.2 Mismatch detection with overlap and<br>cosine similarity  </p></li><li>3.3 The combination of references  </li><li>3.4 Importance of different words</li><li>3.5 Summary and metric formula   </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;one-to-one metrics的方法存在缺陷：without con
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
      <category term="评价指标" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
    
      <category term="评价指标" scheme="http://yoursite.com/tags/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
  </entry>
  
  <entry>
    <title>automatic metric 总结</title>
    <link href="http://yoursite.com/2020/07/23/automatic-metric-%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2020/07/23/automatic-metric-总结/</id>
    <published>2020-07-23T09:39:04.000Z</published>
    <updated>2020-07-23T09:40:03.239Z</updated>
    
    <content type="html"><![CDATA[<h1 id="automatic-metric-总结"><a href="#automatic-metric-总结" class="headerlink" title="automatic metric 总结"></a>automatic metric 总结</h1><p>转载：<code>肝了1W字！文本生成评价指标的进化与推翻</code></p><h2 id="基于词重叠率的方法"><a href="#基于词重叠率的方法" class="headerlink" title="基于词重叠率的方法"></a>基于词重叠率的方法</h2><h2 id="机器翻译-amp-摘要-常用指标"><a href="#机器翻译-amp-摘要-常用指标" class="headerlink" title="机器翻译 &amp; 摘要 常用指标"></a><strong>机器翻译 &amp; 摘要 常用指标</strong></h2><p>基于词重叠率的方法是指基于词汇的级别计算模型的生成文本和人工的参考文本之间的相似性，比较经典的代表有BLEU、METEOR和ROUGE，其中BLEU和METEOR常用于机器翻译任务，ROUGE常用于自动文本摘要。</p><h3 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a><strong>BLEU</strong></h3><p>BLEU （Bilingual Evaluation Understudy，双语评估辅助工具）可以说是所有评价指标的鼻祖，它的核心思想是比较候选译文和参考译文里的 n-gram 的重合程度，重合程度越高就认为译文质量越高。unigram用于衡量单词翻译的准确性，高阶n-gram用于衡量句子翻译的流畅性。实践中，通常是取N=1~4，然后对进行加权平均。<img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAg4GOn0EZAicLNZY3lw97TdibGCPyA09s0Ms9KZT1CpypiaZ0EwLuG5ZGrA/640?wx_fmt=jpeg" alt="img" style="zoom: 50%;"></p><ul><li>BLEU 需要计算译文 1-gram，2-gram，…，N-gram 的精确率，一般 N 设置为 4 即可，公式中的 <em>Pn 指 n-gram 的精确率</em>。</li><li>Wn 指 n-gram 的权重，一般设为均匀权重，即对于任意 n 都有 Wn = 1/N。</li><li>BP 是惩罚因子，如果译文的长度小于最短的参考译文，则 BP 小于 1。</li><li>BLEU 的 1-gram 精确率表示译文忠于原文的程度，而其他 n-gram 表示翻译的流畅程度。</li></ul><p>不过BLEU对词重复和短句有着非常不好的表现，所以改进的BLEU分别使用 <strong>改进的多元精度（n-gram precision）</strong> 和<strong>短句惩罚因子</strong>进行了优化。</p><h4 id="1-改进的多元精度（n-gram-precision）"><a href="#1-改进的多元精度（n-gram-precision）" class="headerlink" title="1. 改进的多元精度（n-gram precision）"></a>1. 改进的多元精度（n-gram precision）</h4><p>假设机器翻译的译文C和一个参考翻译S1如下：</p><blockquote><p>C: a cat is on the table<br>S1: there is a cat on the table</p></blockquote><p>则可以计算出 1-gram，2-gram，… 的精确率（参考文献里写的是准确率(accuracy),我理解是写错了，此处应该是精确率(precision)）</p><p>p1 计算 a cat is on the table 分别都在参考翻译S1中 所以 p1 = 1</p><p>p2  (a, cat)在, (cat is) 没在, (is on) 没在, (on the) 在, (the table)在 所以p2 = 3/5</p><p>p3  (a cat is)不在, (cat is on)不在, (is on the)不在, (on the table)在 所以 p3 = 1/4</p><p>依次类推(上面的在或者不在, 说的都是当前词组有没有在参考翻译中)。直接这样算, 会存在很大的问题. 例如:</p><blockquote><p>C: there there there there there S1: there is a cat on the table</p></blockquote><p>这时候机器翻译的结果明显是不正确的，但是其 1-gram 的 Precision 为1，因此 BLEU 一般会使用修正的方法。给定参考译文S1,S2, …,S<em>m</em>，可以计算C里面 n 元组的 Precision，计算公式如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgXmshfK57WrgarjibbBcy4ZdtxHS9Y3EtyDfTzjTNNl2GMxwJAIPmfbA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>针对上面的例子  p1 = 1/5  (因为there在C和S1中都出现了 我们按最少的次数来)</p><p><strong style="color:red;">yaya: 从公式中，可以看到，对于m个reference，取max 的方式，进行聚合</strong></p><h4 id="2-惩罚因子"><a href="#2-惩罚因子" class="headerlink" title="2. 惩罚因子"></a>2. 惩罚因子</h4><p>上面介绍了 BLEU 计算 n-gram 精确率的方法， 但是仍然存在一些问题，当机器翻译的长度比较短时，BLEU 得分也会比较高，但是这个翻译是会损失很多信息的，例如：</p><blockquote><p>C: a cat<br>S1: there is a cat on the table</p></blockquote><p>因此需要在 BLEU 分数乘上惩罚因子</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgaghJVhQZHwspVre1F1yyaAkZj4UnUwUDIurNHI8aPb8vlNP3GWg0Lw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><h4 id="3-优点"><a href="#3-优点" class="headerlink" title="3. 优点"></a>3. 优点</h4><ul><li>它的易于计算且速度快，特别是与人工翻译模型的输出对比；</li><li>它应用范围广泛，这可以让你很轻松将模型与相同任务的基准作对比。</li></ul><h4 id="4-缺点"><a href="#4-缺点" class="headerlink" title="4. 缺点"></a>4. 缺点</h4><ul><li>它不考虑语义，句子结构</li><li>不能很好地处理形态丰富的语句（BLEU原文建议大家配备4条翻译参考译文）</li><li>BLEU 指标偏向于较短的翻译结果（brevity penalty 没有想象中那么强）</li></ul><h3 id="ROUGE"><a href="#ROUGE" class="headerlink" title="ROUGE"></a><strong>ROUGE</strong></h3><p>英文全称Recall-Oriented Understudy for Gisting Evaluation，可以看做是BLEU 的改进版，专注于<strong>召回率而非精度</strong>。换句话说，它会查看有多少个参考译句中的 n 元词组出现在了输出之中。</p><p>ROUGE大致分为四种（常用的是前两种）：</p><ul><li>ROUGE-N （将BLEU的精确率优化为召回率）</li><li>ROUGE-L （将BLEU的n-gram优化为公共子序列）</li><li>ROUGE-W （将ROUGE-L的连续匹配给予更高的奖励）</li><li>ROUGE-S  （允许n-gram出现跳词(skip)）</li></ul><p>ROUGE 用作机器翻译评价指标的初衷是这样的：在 SMT（统计机器翻译）时代，机器翻译效果稀烂，需要同时评价翻译的准确度和流畅度；等到 NMT （神经网络机器翻译）出来以后，神经网络脑补能力极强，翻译出的结果都是通顺的，但是有时候容易瞎翻译。</p><p>ROUGE的出现很大程度上是为了解决NMT的漏翻问题（低召回率）。所以 ROUGE 只适合评价 NMT，而不适用于 SMT，因为它不管候选译文流不流畅</p><p>这里只介绍 ROUGE_L</p><h4 id="ROUGE-L"><a href="#ROUGE-L" class="headerlink" title="ROUGE-L"></a>ROUGE-L</h4><p>ROUGE-L 中的 L 指最长公共子序列 (longest common subsequence, LCS)，ROUGE-L 计算的时候使用了机器译文C和参考译文S的最长公共子序列，计算公式如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgfXBibHaKdNJDXNPgWhZ0L9FKG1b8LuabowzXZiaMhGXB3WQSepe0gYiaw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img" style="zoom:33%;"></p><p>公式中的 RLCS 表示召回率，而 PLCS 表示精确率，FLCS 就是 ROUGE-L。一般 beta 会设置为很大的数，因此 FLCS 几乎只考虑了 RLCS (即召回率)。注意这里 beta 大，则 F 会更加关注 R，而不是 P，可以看下面的公式。如果 beta 很大，则 PLCS 那一项可以忽略不计。</p><p><strong style="color:red;">yaya: 对于含有多个reference的情况，先分别计算 R<sub>LCS</sub> 和 P<sub>LCS</sub>， 再分别取max，得到 max_R, max_P之后，再带入 F<sub>LCS</sub> 中。</strong></p><h3 id><a href="#" class="headerlink" title=" "></a> </h3><h3 id="METEOR"><a href="#METEOR" class="headerlink" title="METEOR"></a><strong>METEOR</strong></h3><p>和BLEU不同，METEOR同时考虑了基于整个语料库上的准确率和召回率，而最终得出测度。</p><p>METEOR也包括其他指标没有发现一些其他功能，如<strong>同义词匹配</strong>等。METEOR用 WordNet 等知识源扩充了一下同义词集，同时考虑了单词的词形（词干相同的词也认为是部分匹配的，也应该给予一定的奖励，比如说把 likes 翻译成了 like 总比翻译成别的乱七八糟的词要好吧？）</p><p><strong>在评价句子流畅性的时候，用了 chunk 的概念</strong>（候选译文和参考译文能够对齐的、空间排列上连续的单词形成一个 chunk，这个对齐算法是一个有点复杂的启发式 beam serach），chunk 的数目越少意味着每个 chunk 的平均长度越长，也就是说候选译文和参考译文的语序越一致。</p><p>最后，METEOR计算为对应最佳候选译文和参考译文之间的准确率和召回率的调和平均：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggzxernqjrj30pe0hut9u.jpg" style="zoom:33%;"></p><h4 id="1-理解"><a href="#1-理解" class="headerlink" title="1. 理解"></a>1. 理解</h4><p>看公式总是挺抽象的，下面我们还是看看来自维基百科的例子吧。计算的最基本单元是句子。算法首先从待评价字符串和参考字符串之间创建一个平面图如下：<img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgruCbgRHd3tQITE2N2mQsfJUficbIQav7TBGZA7wvUpnyiaoMlwfYYxrg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>所谓<strong>平面图</strong>，就是1元组之间的映射集。平面图有如下的一些限制：在待评价翻译中的每个1元组必须映射到参考翻译中的1个或0个一元组，然后根据这个定义创建平面图。<strong>如果有两个平面图的映射数量相同，那么选择映射交叉数目较少的那个。</strong> 也就是说，上面左侧平面图会被选择。状态会持续运行，在每个状态下只会向平面图加入那些在前一个状态中尚未匹配的1元组。<em>一旦最终的平面图计算完毕，就开始计算METEOR得分</em>：</p><p>1元组精度：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggzxernte4j307d04fmx0.jpg" style="zoom:33%;"></p><p>其中m是<em>在参考句子中同样存在的，**待评价句子中的一元组的数量</em>。wt是<em>待评价翻译中一元组的数量</em>。</p><p>1元组召回率：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggzxersvbuj307s04h745.jpg" style="zoom:33%;"></p><p>m同上，是参考翻译中一元组的数量。</p><p>然后使用调和平均来计算F-mean，且召回的权重是精度的9（上面说的超参数α）倍。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggzxerunotj30eo054jre.jpg" style="zoom:33%;"></p><p>到目前为止，这个方法只对单个单词的一致性进行了衡量，还没有用到为了评价流畅性的 <strong>chunk</strong> 。chunk 块的定义是在待评价语句和参考语句中毗邻的一元组集合。</p><p>在参考和待评价句子中的没有毗连的映射越多，惩罚就越高。为了计算惩罚，1元组被分组成最少可能的块（chunks）。<em>在待评价语句和参考语句之间的毗邻映射越长，块的数量就越少</em>。一个待评价翻译如果和参考翻译相同，那么就只有一个块。惩罚p的计算如下：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggzxeruhhij30dj05pwei.jpg" style="zoom:33%;"></p><p>（假设参数都已经设置好了）其中c就是块的数量，Um是被映射的一元组的数量。p可以减少F-mean的值。最后：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ggzxertdasj30fn02xjrb.jpg" style="zoom:33%;"></p><h4 id="2-优点"><a href="#2-优点" class="headerlink" title="2. 优点"></a>2. 优点</h4><ul><li>该方法基于一元组的精度和召回的调和平均，召回的权重比精度要高一点 ， 与人类判断相关性高</li><li><em>引入了外部知识，评价更加友好了。</em></li></ul><h4 id="3-缺点"><a href="#3-缺点" class="headerlink" title="3. 缺点"></a>3. 缺点</h4><ul><li>实现非常复杂，目前只有java版本</li><li>α、γ和θ 均为用于评价的默认参数。这些都是对着某个数据集调出来的（让算法的结果和人的主观评价尽可能一致，方法我记得是 grid search）。参数一多听起来就不靠谱（给个眼神体会一下）</li><li>需要有外部知识。如果很多词不在wordnet，那其实就没什么意义了</li></ul><h2 id="image-caption-常用指标"><a href="#image-caption-常用指标" class="headerlink" title="image caption 常用指标"></a><strong>image caption 常用指标</strong></h2><h3 id="CIDEr"><a href="#CIDEr" class="headerlink" title="CIDEr"></a><strong>CIDEr</strong></h3><p>CIDEr 是专门设计出来用于图像标注问题的。这个指标将每个句子都看作“文档”，将其表示成 Term Frequency Inverse Document Frequency（tf-idf）向量的形式，通过对每个n元组进行(TF-IDF) 权重计算，计算参考 caption 与模型生成的 caption 的余弦相似度，来衡量图像标注的一致性的。</p><ul><li>公式<br><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgicMuoIiabcIRSiaXj1tLEmgWU5ysVK6ZO4FlTJmfc5S3j3vS7tyzuibkEg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></li><li>举例<img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAg6qxl9VK30SIG0T4LBfVOkbrqRlx4DyBcDK9tRk9vMrwCKfvZYC1ZBw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></li></ul><h4 id="1-好处"><a href="#1-好处" class="headerlink" title="1. 好处"></a>1. 好处</h4><p>是一种加权的评价指标，他更关注你是否说到了重点，而常见的词权重则没有那么高。在 Kaustav_slides image caption的综述里，也提到这个评价指标和人类的评价相关性更高一些</p><h3 id="SPICE"><a href="#SPICE" class="headerlink" title="SPICE"></a><strong>SPICE</strong></h3><p>SPICE 也是专门设计出来用于 image caption 问题的。全称是 Semantic Propositional Image Caption Evaluation。</p><p>我们考虑如下图片：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgl3KCxN5scYCpo7RnLCVOWpTmaG8scssC1iaibPwzaNCQNATuOsU7Dq3g/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>你很快会发现尽管生成的句子与参考句极为相似（只有basketball一词不一样），但我们仍认为这是一个糟糕的生成。原因在于考虑了语义的情况下，模型把网球场错误的识别成了篮球场。这个时候BLEU或者其他指标就不能很好的评价生成效果了。</p><p>SPICE 使用基于图的语义表示来编码 caption 中的 objects, attributes 和 relationships。它先将待评价 caption 和参考 captions 用 Probabilistic Context-Free Grammar (PCFG) dependency parser parse 成 syntactic dependencies trees，然后用基于规则的方法把 dependency tree 映射成 scene graphs。最后计算待评价的 caption 中 objects, attributes 和 relationships 的 F-score 值。</p><p>还是已上图为例，a young girl standing on top of a tennis court (参考句) 可以被SPICE做如下处理：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgRYbeW3NVtIAQDicktJATMl9o5KkyImngjsiaIdnX8SdvwFEibsxA59UyQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">得到了若干个三元组之后，我们通过下面的公式来计算候选句c和参考句（或集合）S的得分：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh01gh6umpj313d0fx408.jpg" style="zoom:33%;"></p><p>这里有一个例子：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qFEWM9OIZ9BADhs7bP0lrAgwIdbmLATA8Zk5uKHZDAAGkyJlf0J0ZDHPicGUibNgAlUCdrPlT82javA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><h4 id="1-好处-1"><a href="#1-好处-1" class="headerlink" title="1. 好处"></a>1. 好处</h4><ul><li>对目标，属性，关系有更多的考虑；</li><li>和基于n-gram的评价模式相比，有更高的和人类评价的相关性</li></ul><h4 id="2-缺点"><a href="#2-缺点" class="headerlink" title="2. 缺点"></a>2. 缺点</h4><ul><li>不考虑语法问题</li><li>依赖于semantic parsers ， 但是他不总是对的</li><li>每个目标，属性，关系的权重都是一样的（一幅画的物体显然有主次之分）</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;automatic-metric-总结&quot;&gt;&lt;a href=&quot;#automatic-metric-总结&quot; class=&quot;headerlink&quot; title=&quot;automatic metric 总结&quot;&gt;&lt;/a&gt;automatic metric 总结&lt;/h1&gt;&lt;p&gt;转载
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
      <category term="评价指标" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
    
      <category term="评价指标" scheme="http://yoursite.com/tags/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
  </entry>
  
  <entry>
    <title>Describing like Humans on Diversity in Image Captioning</title>
    <link href="http://yoursite.com/2020/07/17/Describing-like-Humans-on-Diversity-in-Image-Captioning/"/>
    <id>http://yoursite.com/2020/07/17/Describing-like-Humans-on-Diversity-in-Image-Captioning/</id>
    <published>2020-07-17T09:35:29.000Z</published>
    <updated>2020-07-23T09:40:45.186Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>当前的automatic metric 仅仅评估 generated caption 与 human annotations 之间的相似性。</p><p>但是，一张图片中包含很多内容和细节，不同的人对 image content 会有不同的兴趣点，则 human captions 也会不同。</p><p>基于此，仅仅去评估accuracy 是不足以评估captioning models的性能的。生成的captions 的多样性也应该考虑进来。</p><p>本文，针对 多样性-diversity 提出了一个评价指标。</p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>即，该文要评价的系统 是可以生成多个caption的系统。是来评估，生成的这几个captions 之间的多样性是怎样的。</li><li>本文提到一句话：The drawback of using retrieval model is that the fluency of the captions could be poor [20], and using a very large weight for the retrieval reward will cause the model to repeat the distinctive words.</li></ul><h3 id="以下转自知乎"><a href="#以下转自知乎" class="headerlink" title="以下转自知乎"></a>以下转自知乎</h3><p><a href="https://zhuanlan.zhihu.com/p/67904095" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/67904095</a></p><p>这周读了CVPR 2019一篇有关测试Image Captioning多样性的文章，<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1903.12020" target="_blank" rel="noopener">Describing like humans: on diversity in image captioning</a>，其主要的亮点是用Latent semantic analysis (LSA)方法来定量地衡量多样性。</p><h3 id="Latent-semantic-analysis-LSA"><a href="#Latent-semantic-analysis-LSA" class="headerlink" title="Latent semantic analysis (LSA)"></a>Latent semantic analysis (LSA)</h3><p>1990年的论文<a href="https://link.zhihu.com/?target=http%3A//lsa.colorado.edu/papers/JASIS.lsi.90.pdf" target="_blank" rel="noopener">Indexing by latent semantic analysis</a>最早在信息检索领域提出了LSA，其原理比较简单，核心思想是利用SVD来提取文本的潜在语义信息。</p><p>在Image Captioning这个特定的应用场景中，对于一张图片，假设有 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]"> 条与之相关的caption，且词汇表的总数为 <img src="https://www.zhihu.com/equation?tex=d" alt="[公式]"> ，则这组caption可以用一个 <img src="https://www.zhihu.com/equation?tex=d+%5Ctimes+m" alt="[公式]"> 的矩阵 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 表示，其中的每个列向量是一条caption的bag-of-words表示。 对 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> 应用SVD进行分解，得到： <img src="https://www.zhihu.com/equation?tex=M+%3D+USV%5E%5Ctop" alt="[公式]"> 。 参考Quora上<a href="https://link.zhihu.com/?target=https%3A//www.quora.com/What-is-an-intuitive-explanation-of-singular-value-decomposition-SVD%23" target="_blank" rel="noopener">对SVD的讨论</a>，我们可以认为，SVD的最大作用是找到数据的“pattern”：</p><ul><li><img src="https://www.zhihu.com/equation?tex=U" alt="[公式]"> 的每一个列向量代表一种pattern，在这里可以引申为一个topic；</li><li>singular values <img src="https://www.zhihu.com/equation?tex=S%3Ddiag%28%5Csigma_1%2C...%2C%5Csigma_m%29" alt="[公式]"> （ <img src="https://www.zhihu.com/equation?tex=%5Csigma_1%3E%5Csigma_2%3E...%3E0" alt="[公式]"> ）则代表这些pattern对原数据的影响程度。当 <img src="https://www.zhihu.com/equation?tex=%5Csigma_1" alt="[公式]"> 明显大于其他所有 <img src="https://www.zhihu.com/equation?tex=%5Csigma_i" alt="[公式]"> 时，表示原数据仅仅受一种pattern“支配”，体现了较差的多样性；而当所有 <img src="https://www.zhihu.com/equation?tex=%5Csigma_i" alt="[公式]"> 都趋向于一致时，表示原数据由 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]"> 种pattern共同组成，说明原来的 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]"> 个caption彼此之间差异较大，体现了丰富的多样性。</li></ul><p>因此，文章认为，一组caption的多样性可以用 <img src="https://www.zhihu.com/equation?tex=r+%3D+%5Cfrac+%7B%5Csigma_1%7D+%7B%5Csum%5Em_%7Bi%3D1%7D%5Csigma_i%7D" alt="[公式]"> 度量。结合上面的分析， <img src="https://www.zhihu.com/equation?tex=r" alt="[公式]"> 的值域是 <img src="https://www.zhihu.com/equation?tex=%5B1%2Fm%2C+1%5D" alt="[公式]"> ，为了把多样性映射到 <img src="https://www.zhihu.com/equation?tex=%5B0%2C+1%5D" alt="[公式]"> ，文章对 <img src="https://www.zhihu.com/equation?tex=r" alt="[公式]"> 进行了对数变换，得到最终的多样性分数 <img src="https://www.zhihu.com/equation?tex=div+%3D+-%5Clog_m%28r%29" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=div" alt="[公式]"> 越大表示多样性越强。</p><h3 id="结合CIDEr的核方法"><a href="#结合CIDEr的核方法" class="headerlink" title="结合CIDEr的核方法"></a>结合CIDEr的核方法</h3><p>根据SVD的定义，得到的singular value等于矩阵 <img src="https://www.zhihu.com/equation?tex=K+%3D+M%5E%5Ctop+M" alt="[公式]"> 的eigenvalue的平方根。可以把 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 看作一个核矩阵，其中的元素 <img src="https://www.zhihu.com/equation?tex=K_%7Bij%7D" alt="[公式]"> 表示caption <img src="https://www.zhihu.com/equation?tex=i" alt="[公式]"> 和caption <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 的相似度，因此，完全可以应用核方法（Kernelized Method），直接构造出一个 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> ，然后通过 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 的eigenvalue计算出 <img src="https://www.zhihu.com/equation?tex=div" alt="[公式]"> 。 CIDEr是一个度量两个句子相似度的指标，主要从n-gram和TF-IDF的角度来进行度量，比起上述基于bag-of-words的方法，它显得更加合理。所以文章把CIDEr作为核函数，即 <img src="https://www.zhihu.com/equation?tex=K_%7Bij%7D+%3D+CIDEr%28c_i%2C+c_j%29" alt="[公式]"> ，从而得到另一种多样性分数，称为Self-CIDEr。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>文章对多个Image Captioning模型进行了Accuracy和Diversity的测试，分别用常规的CIDEr和上述的Self-CIDEr度量。实验结果如下，其中的红色五角星代表人类标记员的表现，可以看到，人类的标记结果很好地兼顾了Accuracy和Diversity，与实际的情况相符。</p><p><img src="https://pic2.zhimg.com/80/v2-27a4cd22fa5acf55682d4b77d742311e_1440w.jpg" alt="img"></p><p>但这还不足以充分证明文章这种度量方法的合理性，于是文章计算了自动化多样性指标与人工标注的多样性分数之间的相关度，结果如下：</p><p><img src="https://pic4.zhimg.com/80/v2-8546e7cad02e0de2e3a5448e109049f0_1440w.jpg" alt="img"></p><p>其中的mBLEU-mix计算了一组caption内部的平均相似度，被一些研究者用来简单地度量多样性，但从这个实验结果可以看到，mBLEU-mix与人工标注的相关度远不如该文章提出的基于LSA的方法，证明了后者的合理性。同时，应用了核方法的Self-CIDEr又优于朴素的LSA。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;p&gt;当前的automatic metric 仅仅评估 generated caption 与 h
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
      <category term="评价指标" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
    
      <category term="评价指标" scheme="http://yoursite.com/tags/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
  </entry>
  
</feed>
