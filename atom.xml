<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShiYaya</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-03-13T11:25:05.880Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ShiYaya</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Object Relational Graph with Teacher-Recommended Learning for Video Captioning</title>
    <link href="http://yoursite.com/2021/03/13/Object-Relational-Graph-with-Teacher-Recommended-Learning-for-Video-Captioning/"/>
    <id>http://yoursite.com/2021/03/13/Object-Relational-Graph-with-Teacher-Recommended-Learning-for-Video-Captioning/</id>
    <published>2021-03-13T11:13:46.000Z</published>
    <updated>2021-03-13T11:25:05.880Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">U2FsdGVkX19N2yU2yo4iPhE6AjErtiZwFp5vlImkv51HvmnREmPunc4JQPtaRXtRA0WiM405Ck2XQ+mt1Gf3g9x4iayLChDWRwXPltFNq2YY6KNIZVLsYhE8gwqo82aC0RrypyzcseZqhp1VTYyP5MGkd2/9vyD/QAZUCSIz0wJvdwfgQTGlriddHc5uZnEooFgRygFrF6gzd1lyZFnL2Gmx+OLMBayRwN4gtvv82xF446SXIcITONX2FgQJxR5Jyz3npIGtY4ZyYWIEzSDhlpg21c2a6kCNwODeZgFKIGE=</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="video captoning" scheme="http://yoursite.com/categories/cross-modal/video-captoning/"/>
    
    
      <category term="cross-modal,video captoning" scheme="http://yoursite.com/tags/cross-modal-video-captoning/"/>
    
  </entry>
  
  <entry>
    <title>[VATEX Captioning Challenge 2019] Multi-modal Information Fusion and Multi-stage Training Strategy for Video Captioning</title>
    <link href="http://yoursite.com/2021/03/13/VATEX-Captioning-Challenge-2019-Multi-modal-Information-Fusion-and-Multi-stage-Training-Strategy-for-Video-Captioning/"/>
    <id>http://yoursite.com/2021/03/13/VATEX-Captioning-Challenge-2019-Multi-modal-Information-Fusion-and-Multi-stage-Training-Strategy-for-Video-Captioning/</id>
    <published>2021-03-13T07:39:13.000Z</published>
    <updated>2021-03-14T06:59:29.254Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="video captioning" scheme="http://yoursite.com/categories/cross-modal/video-captioning/"/>
    
    
      <category term="cross-modal,video captioning" scheme="http://yoursite.com/tags/cross-modal-video-captioning/"/>
    
  </entry>
  
  <entry>
    <title>多模态人工智能</title>
    <link href="http://yoursite.com/2021/03/12/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    <id>http://yoursite.com/2021/03/12/多模态交互人工智能/</id>
    <published>2021-03-12T10:11:52.000Z</published>
    <updated>2021-03-12T12:22:43.491Z</updated>
    
    <content type="html"><![CDATA[<p>多模态：视觉，语音，自然语言</p><p><img src="https://i.loli.net/2021/03/12/8ObjKzRVd4UXgS3.png" alt="image-20210312202150477"></p><p><img src="https://i.loli.net/2021/03/12/IqFHGuL6ZYOKwXb.png" alt="image-20210312202209438"></p><p><img src="https://i.loli.net/2021/03/12/CsF8TR3m9UlS54N.png" alt="image-20210312202229192"></p><p><img src="https://i.loli.net/2021/03/12/HbPD8yaB3GU7IAk.png" alt="image-20210312202108780"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;多模态：视觉，语音，自然语言&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/12/8ObjKzRVd4UXgS3.png&quot; alt=&quot;image-20210312202150477&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https:
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering</title>
    <link href="http://yoursite.com/2021/03/12/Learning-to-Contrast-the-Counterfactual-Samples-for-Robust-Visual-Question-Answering/"/>
    <id>http://yoursite.com/2021/03/12/Learning-to-Contrast-the-Counterfactual-Samples-for-Robust-Visual-Question-Answering/</id>
    <published>2021-03-12T09:43:10.000Z</published>
    <updated>2021-03-12T10:05:32.371Z</updated>
    
    <content type="html"><![CDATA[<p>转自：<a href="https://blog.csdn.net/weixin_45347379/article/details/112182143" target="_blank" rel="noopener">https://blog.csdn.net/weixin_45347379/article/details/112182143</a></p><p>学习对比反事实样本，以实现稳健的视觉问答<br>Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering<br>在阅读本文之前，一定要阅读论文：Counterfactual Samples Synthesizing for Robust Visual Question Answering（简称CSS）</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="https://i.loli.net/2021/03/12/gODk8QFCNWaUsVP.png" alt="image-20210312175656070"></p><p>文章的方法主要包括三个部分：（1）一个基本的VQA模型。（2）一个事实和反事实样本合成（CSS）模块。（3）一个对比学习（CL）目标。</p><h4 id="第一部分和第二部分"><a href="#第一部分和第二部分" class="headerlink" title="第一部分和第二部分"></a><strong>第一部分和第二部分</strong></h4><p>属于CSS已经实现的，主要作用在于：</p><p>（1）并通过多分类的方法预测答案，并产生图中右上方基本VQAloss。</p><p><img src="https://i.loli.net/2021/03/12/z3Q52bActwqMhov.png" alt="在这里插入图片描述"></p><p>（2）得到（I, I+, I-）和（Q, Q+, Q-），</p><p><img src="https://i.loli.net/2021/03/12/din9DctNLV1EORo.png" alt="在这里插入图片描述"></p><h4 id="第三部分"><a href="#第三部分" class="headerlink" title="第三部分"></a><strong>第三部分</strong></h4><p>以（I, I+, I-）为例，将（I, I+, I-）和Q喂给VQA模型，分别产生原始样本的嵌入mm（V, Q）作为anchor（a），事实样本的嵌入mm(V+, Q)作为positive（p），反事实样本嵌入mm(V-, Q)作为negati（n）<br>利用余弦相似度作为评分函数，对正样本输出高值，对负样本输出低值，公式如下：</p><p><img src="https://i.loli.net/2021/03/12/XIjKWu7szqi3A4w.png" alt="在这里插入图片描述"></p><p>同样的方法得到anchor和negative之间的评分s(a, n), 这就相当于图中展示的，拉近原始图像与事实区域图像的关系，推远原始图像与反事实区域的距离。<br>对比损失定义为：（这就是图片下方得到的Contrastive loss）</p><p><img src="https://i.loli.net/2021/03/12/CY4OJ7PZmD9Eaud.png" alt="在这里插入图片描述"></p><p>最后，这种对比损失与基础分类损失的加权总和弥补了整体损失：</p><p><img src="https://i.loli.net/2021/03/12/8GL3BxlSEpAg951.png" alt="在这里插入图片描述"></p><p>虽然文章说，<strong style="color:red;">这种方法能够使模型学习他们之间的关系，并从更有因果关系的方面预测正确答案。</strong>但是，个人感觉如果仅仅使以上方法，并不能从理论上提高模型的能力。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p><img src="https://i.loli.net/2021/03/12/wAmbOSxRU97dN12.png" alt="在这里插入图片描述"></p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>看了本文博客之后，没有看原文，个人任务这种方法有限，</li><li>可能模型的设计上，是有新意的，使用对比学习来增强VQA模型的性能，但是往往自己做的时候会收效甚微</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转自：&lt;a href=&quot;https://blog.csdn.net/weixin_45347379/article/details/112182143&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/weixin_
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="VQA" scheme="http://yoursite.com/categories/cross-modal/VQA/"/>
    
    
      <category term="cross-modal,VQA" scheme="http://yoursite.com/tags/cross-modal-VQA/"/>
    
  </entry>
  
  <entry>
    <title>Semantic Equivalent Adversarial Data Augmentation for Visual Question Answering</title>
    <link href="http://yoursite.com/2021/03/12/Semantic-Equivalent-Adversarial-Data-Augmentation-for-Visual-Question-Answering/"/>
    <id>http://yoursite.com/2021/03/12/Semantic-Equivalent-Adversarial-Data-Augmentation-for-Visual-Question-Answering/</id>
    <published>2021-03-12T03:27:29.000Z</published>
    <updated>2021-03-12T09:31:48.943Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>受到深度学习的快速发展，VQA 近年来取得了非常成功的进展。数据增强是深度学习中的一个有用的技巧，但是，目前很少有工作关注于VQA任务的数据增强。</p><p>对于image side: 一些简单的数据增强操作不能直接应用到VQA这一场景下，比如，rotation and flipping 等操作，都可能导致<image, question, answer> 这一结构的正确性遭到破坏。</image,></p><p>对于text side (eg: questions) , it is challenging to come up with <strong>generalized rules for language transformation.</strong> 另外，有一类任务是Visual Question Generation，根据image和 answer来生成问题，但是生成的问题常常是有语法错误的，而且，他们在同一个目标数据集上进行学习，生成的数据与原始数据的分布是一致的，因此，<strong>若使用这种方案来做数据扩充，难以解决过拟合问题</strong>(通常训练数据和测试数据不是同一个分布)。</p><p>在本文中，不直接对image或者是question进行操作，而是对images 和 questions生成对抗样本作为数据增强。增强的样本不会改变image的原始语义，也不会改变questions中的semantic meaning。对抗性示例是经过<strong>策略</strong>修改的样本，可以成功地欺骗深层模型以做出不正确的预测。这种修改是难以察觉的，<strong>它在使对抗性示例的基础分布远离原始数据的同时保持了数据的语义。</strong>本文是第一个同时对image 和 text进行数据扩充的方法（已有的方法只是单独对一方面进行数据扩充）。</p><p>进而，使用本文方法产生的<strong>数据增强样本</strong>和<strong>对抗训练</strong>来训练经典的VQA model (BUTD) 。</p><p>实验结果证明，不仅可以提高 VQAv2的整体性能，而且相比于baseline还可以有效抵抗对抗攻击。</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="https://i.loli.net/2021/03/12/rmyqUFXQewT2PBK.png" alt="image-20210312165449352" style="zoom:50%;"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="何时加入对抗样本"><a href="#何时加入对抗样本" class="headerlink" title="何时加入对抗样本"></a>何时加入对抗样本</h4><ul><li><p>本文发现，将干净样本和对抗样本进行混合，然后<strong>从头到尾</strong>的训练，这种方案不会在干净样本上收敛。因此本文只在特定的训练时期对样本进行混合，最后使用干净样本进行微调。</p><p>本文实验中max-epoch=25.</p><p><img src="https://i.loli.net/2021/03/12/YOblV6WT4rtMkSf.png" alt="image-20210312171853940" style="zoom: 50%;"></p><p>本文的解释：与干净样本相比，对抗样本与其有不同的分布。如果把提升模型在VQA任务上的性能作为我们的主要目标，那么模型在干净样本上的拟合能力需要<strong>在结束</strong>的时候to be retrieved。而<strong>在开始</strong>时，模型需要warm up，此时不适合加入对抗样本。因此在中间阶段加入融合对抗样本的训练。</p></li><li><p>实验证明本文提出的方法不仅可以提高在干净样本上的VQA任务的性能，还能提高<strong>在对抗样本上的鲁棒性</strong>。</p></li></ul><h4 id="相比于baseline还可以有效抵抗对抗攻击"><a href="#相比于baseline还可以有效抵抗对抗攻击" class="headerlink" title="相比于baseline还可以有效抵抗对抗攻击"></a>相比于baseline还可以有效抵抗对抗攻击</h4><p><img src="https://i.loli.net/2021/03/12/np4eUgXwkz2rK5M.png" alt="image-20210312172242341" style="zoom:50%;"></p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li><strong>何时加入对抗样本</strong> 这个实验告诉我们：一般情况，我们提出一种数据增强方案，通常会从头到尾的使用，但是未必是好的。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h3&gt;&lt;p&gt;受到深度学习的快速发展，VQA 近年来取得了非常成功的进展。数据增强是深度学
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="VQA" scheme="http://yoursite.com/categories/cross-modal/VQA/"/>
    
    
      <category term="cross-modal,VQA" scheme="http://yoursite.com/tags/cross-modal-VQA/"/>
    
  </entry>
  
  <entry>
    <title>[Behind the Scene] Revealing the Secrets of Pre-trained Vision-and-Language Models</title>
    <link href="http://yoursite.com/2021/03/05/Behind-the-Scene-Revealing-the-Secrets-of-Pre-trained-Vision-and-Language-Models/"/>
    <id>http://yoursite.com/2021/03/05/Behind-the-Scene-Revealing-the-Secrets-of-Pre-trained-Vision-and-Language-Models/</id>
    <published>2021-03-05T09:34:24.000Z</published>
    <updated>2021-03-05T10:01:56.309Z</updated>
    
    <content type="html"><![CDATA[<h3 id="本文研究的任务"><a href="#本文研究的任务" class="headerlink" title="本文研究的任务"></a>本文研究的任务</h3><p>最近Transformer-based 大规模预训练模型推动了 多模态任务的发展，比如，ViL-BERT，LXMERT and UNITER。然而，对于使它们取得成功的<strong>内部机制</strong>知之甚少。为了揭示内部机制，本文提出了VALUE（Vision-And-Language Understanding Evaluation），一组精心设计的<strong>探测任务</strong>（probing task, eg: Visual Coreference Resolution, Visual Relation Detection），可推广到标准的预训练V + L模型， 破译多模式预训练的内部运作方式（例如，在各个attention heads 中获得的隐性知识，通过上下文化多模式嵌入学习的 inherent cross-modal alignment）。</p><h3 id="本文的实验发现"><a href="#本文的实验发现" class="headerlink" title="本文的实验发现"></a>本文的实验发现</h3><p>经由这些探测任务，通过对每个原型模型体系结构的广泛分析，我们的主要观察结果是：（i）预训练的模型在推理过程中表现出对文本而不是图像的关注。 （ii）存在专门为捕获cross-modal interactions 而设计的 a subset of attetion heads（iii）在多个预训练模型中学习的注意力矩阵显示出图像区域和文本单词之间的<strong>潜在对齐</strong>，表现出<strong>一致的模式</strong>。 （iv）绘制的注意力模式(attention patern)揭示了图像区域之间的视觉可解释的关系。 （v）纯粹的语言知识也被有效地编码在注意力集中。 这些宝贵的见解可指导未来的工作，以设计更好的模型架构和多模式预训练的目标。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;本文研究的任务&quot;&gt;&lt;a href=&quot;#本文研究的任务&quot; class=&quot;headerlink&quot; title=&quot;本文研究的任务&quot;&gt;&lt;/a&gt;本文研究的任务&lt;/h3&gt;&lt;p&gt;最近Transformer-based 大规模预训练模型推动了 多模态任务的发展，比如，ViL-BE
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[COCO-LM] Correcting and Contrasting Text Sequences for Language Model Pretraining</title>
    <link href="http://yoursite.com/2021/03/04/COCO-LM-Correcting-and-Contrasting-Text-Sequences-for-Language-Model-Pretraining/"/>
    <id>http://yoursite.com/2021/03/04/COCO-LM-Correcting-and-Contrasting-Text-Sequences-for-Language-Model-Pretraining/</id>
    <published>2021-03-04T02:59:16.000Z</published>
    <updated>2021-03-04T04:08:45.357Z</updated>
    
    <content type="html"><![CDATA[<p>转自：<a href="https://zhuanlan.zhihu.com/p/353624306" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/353624306</a></p><p>该篇文章2021年2月16日上传，提出了一种新的预训练模型的框架，个人认为<strong>COCO-LM结合了许多当下比较新进的思想，在后bert时代，一定程度上突破了对BERT模型传统的预训练方法</strong>。</p><p>We present COCO-LM, a new self-supervised learning framework that pretrains Language Models by COrrecting challenging errors and COntrasting text sequences. COCO-LM employs an auxiliary language model to mask-and-predict tokens in original text sequences. It creates more challenging pretraining inputs, where noises are sampled based on their likelihood in the auxiliary language model. COCO-LM then pretrains with two tasks: <strong style="color:blue;">The first task, corrective language modeling</strong>, learns to correct the auxiliary model’s corruptions by recovering the original tokens. <strong style="color:blue;">The second task, sequence contrastive learning</strong>, ensures that the language model generates sequence representations that are invariant to noises and transformations. In our experiments on the GLUE and SQuAD benchmarks, COCO-LM outperforms recent pretraining approaches in various pretraining settings and few-shot evaluations, with higher pretraining efficiency. Our analyses reveal that COCO-LM’s advantages come from its challenging training signals, more contextualized token representations, and regularized sequence representations.</p><p><img src="https://i.loli.net/2021/03/04/xOA857rdWJktsEY.png" alt="FireShot Capture 018 -  - arxiv.org"></p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>在标准语言模型预训练框架内，可以观察到PLM在下游任务上的the empirical performance 仅随着参数大小和预训练成本的指数增长而线性提高，<strong>这是不可持续的</strong>，因为PLM已达到数万亿个参数。</p><p>最近的研究揭示了现有预训练框架的某些固有局限性，这些局限性可能导致这种亚线性效率（sublinear efficiency）。【1】一个挑战是，<strong>使用随机更改的文本</strong>（<em>例如</em>，randomly masked tokens）进行预训练会<strong>产生许多非信息性信号</strong>，经过一定程度的预训练后它们不再有用。【2】另外一个挑战是，在 token level 进行预训练不会在 sequence level 上显式学习语言语义，并且在预训练过程中，<strong>Transformers可能无法有效地推广到 higher level 的语义 。</strong></p><p>在本文中，我们旨在通过一个<strong>新的自我监督学习框架</strong>COCO-LM来克服这些限制。该框架通过使用 more challenging noises 来 COrrecting and COntrasting text sequences，进而预训练语言模型。</p><p>【1】leverages an auxiliary language model，to corrupt text sequences by <strong>sampling more contextually plausible noises</strong> from its masked language modeling (MLM) probability. COCO-LM revives a language modeling task, corrective language modeling (CLM), which pretrains the Transformer to <strong>not only detect the challenging noises in the corrupted texts, but also correct them via a multi-task setting.</strong></p><p>【2】To improve the learning of sequence level semantics, COCOLM introduces a sequence level pretraining task, sequence contrastive learning (SCL), that uses contrastive learning to enforce the pretraining model to <strong>align the corrupted text sequence and its cropped original sequence close</strong> in the representation space, while away from other random sequences.</p><h3 id="COCO-LM框架延续了ELECTRA预训练模型的思想"><a href="#COCO-LM框架延续了ELECTRA预训练模型的思想" class="headerlink" title="COCO-LM框架延续了ELECTRA预训练模型的思想"></a>COCO-LM框架延续了ELECTRA预训练模型的思想</h3><p>ELECTRA预训练模型主要应用了GAN对抗神经网络的思想，不了解的小伙伴们可以参考一些其他资料，这里我简单说一下我的理解。</p><p>GAN对抗神经网络在CV领域上应用比较成熟，在CV的应用上GAN主要包括两个神经网络模型：一个是生成式模型G，一个是判别式模型D。生成式模型的作用是通过随机噪声生成和原始样本相似的数据（注意这里是通过随机噪声），判别式模型的作用是判断给定的实例是真实实例还是人为伪造的（也就是生成式模型所生成的）。那么这里就包含了对抗的思想，即生成式模型的目的是能够生成欺骗判别式模型的实例，判别式模型的目的是判别给定的实例是否是人为伪造的。</p><p>ELECTRA当中引用了这样的“对抗”思想，将判别式模型引入到了模型的预训练之中。像BERT、ROBERTA、XLNET等等预训练模型都属于生成式模型，在输入上用 [MASK] 遮蔽掉部分 tokens，再训练一个模型以重建出原始的 tokens。而ELECTRA预训练模型使用了判别式模型，其效果也出乎意料的好。</p><blockquote><p>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</p></blockquote><p>ELECTRA模型的主要思想也是包括了两个神经网络模型：一个生成式模型G，一个生成式模型D。生成式模型G是MLM（Masked Language Model）模型，给定一个真实样例（GAN的生成式模型给定的是随机噪声），用 [MASK] 遮蔽掉部分 tokens，生成替换的tokens；判别式模型D判断输入中每个 token 是否是由生成器生成。其过程如图所示：</p><p><img src="https://i.loli.net/2021/03/04/V7kIJGSo3L1m5ua.png" alt="image-20210304111201636" style="zoom:50%;"></p><p>通过实验表明这种新的预训练任务比 MLM 更高效，该任务定义于全部的输入 tokens，而非仅仅被遮蔽掉的那一部分小小的输入子集。</p><p>在COCO-LM模型中<strong>Corrective Language Modeling (CLM)</strong>也延续了这样的思想。</p><h3 id="COCO-LM模型引入了对比学习的思想"><a href="#COCO-LM模型引入了对比学习的思想" class="headerlink" title="COCO-LM模型引入了对比学习的思想"></a>COCO-LM模型引入了对比学习的思想</h3><p>我认为是非常非常棒的创新点。最近刚好再看对比学习的相关paper，更多的是在CV领域中使用了对比学习，而COCO-LM刚好将对比学习带入到了NLP领域中。</p><p>什么是对比学习呢？</p><p>对比学习是一种自监督的学习方法。其主要思想我的理解是，把正样本距离拉近，正样本与负样本距离拉远。对比学习的例子如下：</p><ul><li>给每个例子绘制两个独立的增强函数</li><li>使用两种增强机制，为每个示例生成两个互相关联的视图</li><li>让相关视图互相吸引，同时排斥其他示例</li></ul><p><img src="https://i.loli.net/2021/03/04/A3wEY1gPnTxIOaQ.jpg" alt="SimCLR论文解读- 知乎" style="zoom:33%;"></p><p>如上图，（Z1，Z2），（Z3，Z4）…（Z2n-1，Z2n）这些可以看作正例对，而Z1可以与除Z1、Z2的任何实例组成负例对，如（Z1，Z3）（Z1，Z4）等等。那么这样一个实例X，在一个大小为N的batch里便可以产生一个正例，以及N-1个负例，那么这个 loss 就可以看做是一个 N 分类问题，实际上就是一个交叉熵，由此可以进行网络模型的训练。</p><p>以上是我认为COCO-LM框架比较出色的地方，框架的一些细节还需要进一步的理解，之后会进一步的更新，欢迎知乎各位巨佬一起讨论~</p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li><p>在预训练任务中，引入了对抗扰动，且通过对比学习的思想来训练。</p></li><li><p>这种扰动+对比学习的思想，在vision-text pretraining model 中是否有使用？</p></li><li>本篇的idea 是如何来的，motivation 是什么？</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转自：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/353624306&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/353624306&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;该篇文章
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Neural Machine Translation with universal Visual Representation</title>
    <link href="http://yoursite.com/2021/03/03/Neural-Machine-Translation-with-universal-Visual-Representation/"/>
    <id>http://yoursite.com/2021/03/03/Neural-Machine-Translation-with-universal-Visual-Representation/</id>
    <published>2021-03-03T12:14:27.000Z</published>
    <updated>2021-03-04T02:52:08.225Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>为了降低Multi-modal NMT对有图像标注的翻译数据集的依赖，本文提出通过建立Topic-image Lookup Table的方式更高效地利用已有图像文本数据，并且在训练和测试NMT的时候通过Image Retrieval的方式获得图像信息，从而在更大规模的数据上训练Multi-modal NMT。</p><p>通过Retrieval的方式来扩充数据的工作其实有很多，比如这篇：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1904.02331" target="_blank" rel="noopener">Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation</a>。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>长期以来，机器翻译都只涉及到文本之间的转换，但实际上，人的感知功能可以是“多模态”的。</p><p><strong>本文提出一种通用的视觉表征，将图片信息融合到机器翻译模型中。</strong></p><p>使用这种视觉知识融合方法，<strong>不需要额外的<strong style="color:red;">双语-图片</strong>标注数据，模型就能够在多个数据集上取得显著的效果提升。</strong></p><h3 id="多模态与机器翻译"><a href="#多模态与机器翻译" class="headerlink" title="多模态与机器翻译"></a><strong>多模态与机器翻译</strong></h3><p>机器翻译是两种语言间的转换，比如“A dog is playing in the snow”翻译为中文就是“小狗在雪地里玩耍”。</p><p>但人类理解世界不只是用文字，还有视觉、听觉等感知能力；并且<strong style="color:blue;">翻译的过程需要保持“语义”不变</strong>。比如下面的图：</p><p><img src="https://i.loli.net/2021/03/03/31QcdLfykOoiCEX.jpg" alt="img" style="zoom:50%;"></p><p>讲中文的人会说“小狗在雪地里玩耍”，而讲英文的人会说“A dog is playing in the snow”。也就是说，人们对客观世界的本质认知是相同的，只是“方法”不同，体现在语言上，就是语法上的差异。</p><p>为此，我们可以假设<strong style="color:blue;">在机器翻译模型中，融入这种“客观的世界知识”，比如把图片信息加入，以此期望增强翻译能力。</strong>同时考虑文本和图片，这就是一种多模态。</p><p>然而，过去的翻译-图片研究大都需要大量的双语-图片标注数据，这在数据上成为一个研究的瓶颈。本文针对这种情况，<strong>提出“通用的视觉表示”，<strong style="color:red;">仅用单语-图片标注数据</strong>，就能显著提高机器翻译的效果。</strong></p><p>本文的方法<strong>在数据集EN-RO，EN-DE，EN-FR上均有约一个BLEU值的提高</strong>，这说明了本方法的有效性。</p><h3 id="本文贡献"><a href="#本文贡献" class="headerlink" title="本文贡献"></a>本文贡献</h3><ul><li>提出一种通用的视觉表示方法，无需双语-图片标注语料；</li><li>该方法<strong>可以在只有文本的数据集上使用</strong>；</li><li>实验证明了该方法效果提升的一致性。</li></ul><h3 id="通用视觉表示"><a href="#通用视觉表示" class="headerlink" title="通用视觉表示"></a><strong>通用视觉表示</strong></h3><p>首先我们有一个单语-图片数据集 $\mathcal{S}=\{X, E\}$，也就是，其中的每条数据都是一张图片 $e$ 和对图片的描述 $X_{e}=\left\{x_{1}, \cdots, x_{I}\right\},$ 把其中的停用词去掉后得到了 $X_{e}^{\prime}=\left\{x_{1}^{\prime}, \cdots, x_{J}^{\prime}\right\}$ 。</p><p>然后, 对 $X_{e}^{\prime}$ 中的每个词 $x_{j},$ 计算它在整个数据集 $\mathcal{S}$ 中的TF-IDF值， 然后取 $X_{e}^{\prime}$ 中TF-IDF值最大的前 $w$个词作为这个图片 $e$ 的主题词 $T_{e}$, 也就是和图片最相关的 $w$ 个词。</p><p>这样一来，<strong>每个图片e都有它主题词</strong> $T_{e},$ 同时，<strong>每个词都有可能同时是多个图片的主题词</strong>。我们可以把这看成一个 “主题词-图片” 查询表，输入一个词 $t$ ，就可以在表中查询以 $t$ 为主题的所有图片 $E_{t}=\left\{e_{1}, \cdots, e_{n}\right\}$。</p><p>那么，现在输入一个句子，我们就可以按照同样的步骤：</p><p>1.去除停用词；</p><p>2.计算每个词的TF-IDF；</p><p>3.取前$w$个TF-IDF最高的词；</p><p>4.在查询表中找到所有对应的图片；</p><p>5.按照出现次数的多少排序，取出前$m$个出现次数最多的图片（因为多个词可能对应同一个图片），得到集合 $G$</p><p>现在，这个图片集合 $G$ 就可以认为是和输入句子对应的视觉信息，可以用它去增强翻译效果了。下图是流程示意图：</p><p><img src="https://i.loli.net/2021/03/03/wMeWCU7jfD4NqIO.png" alt="image-20210303203731211" style="zoom: 50%;"></p><h3 id="在机器翻译中融合图片信息"><a href="#在机器翻译中融合图片信息" class="headerlink" title="在机器翻译中融合图片信息"></a><strong>在机器翻译中融合图片信息</strong></h3><p>为了把图片融合进去，我们首先用一个预训练的ResNet提取图片集$G$ 的表示，然后计算 $\bar{H}=$ Self-Attention $\left(H^{L}, K_{G}, V_{G}\right)$ 与 $H=H^{L}+\lambda \bar{H}$<br>这里, $H^{L}$ 是Transformer Encoder的最后一层, $K_{G}, V_{G}$ 是用ResNet得到的图片集的表示, $\lambda$ 使用<br>sigmoid 计算。<br>在Decoder端，直接把 $H$ 送入即可。融合步骤如下所示：</p><p><img src="https://i.loli.net/2021/03/03/SWYB4lDXt7ysVrO.png" alt="image-20210303204216912" style="zoom:50%;"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a><strong>实验</strong></h3><p>我们在三个数据集上进行实验：WMT16 En-RO, WMT14 EN-DE和WMT14 EN-FR。这三个数据集大小从小到大增加，从而在不同大小的数据集上都能验证该方法。</p><p>下表是在这三个数据集上的结果，++表示显著更优。</p><p>可以看到，和基线模型(Trans.(base/big))相比，本文的方法(+VR)在三个数据集上都能得到显著的提升，平均提升约一个BLEU值。同时，只引入了很少的参数量，这就不会使训练时间几乎不会增加。</p><p><img src="https://i.loli.net/2021/03/03/F8z9SReVPkuNdov.png" alt="image-20210303205316435" style="zoom:33%;"></p><p>下表是在数据集Multi30K上的结果，这是一个多模态数据集。可以看到，即使在多模态设置下，本文方法依旧能够取得显著结果。</p><p><img src="https://i.loli.net/2021/03/03/taQRuKN6ElJYbLg.png" alt="image-20210303205337060" style="zoom:33%;"></p><p>最后，我们来看看每个句子对应的图片集 $G$ 的大小 $m$, 和手动控制参数 $\lambda$ 的影响。下图分别是两个因素的影响结果。从图片数量来看，并不是越多的图片数量越好, 也不是越少越好,而是在 $m=5 \sim 15$ 的区间较好。这是因为， <strong>过少的图片信息不充分, 过多的图片噪声太多。</strong></p><p>参数$\lambda$控制的是图片信息融合的程度，可以看到，无论融合多少，效果都比不融合图片信息要好，这说明多模态是有效果的。</p><p>而且，手动控制它都没有模型自动学习好，这也说明模型对不同的输入句子，需要的视觉信息也是不同的。</p><p><img src="https://i.loli.net/2021/03/03/De8nVYPabyIrBwE.png" alt="image-20210303205616909" style="zoom:33%;"></p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a><strong>小结</strong></h3><p>本文提出了一种简单、有效的多模态视觉知识融合方法——首先构建从主题词到图片的查询表，然后对输入句子找到相关的图片，然后使用ResNet提取图片信息融入到机器翻译模型中。</p><p>使用这种方法，可以避免对大规模双语-图片数据的依赖。实验结果也表明，这种方法可以一致地提高翻译效果。</p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><ul><li><p>如果要翻译单语-图片数据集中没有的语言，可以怎么做？</p><p>比如$S$没有日语，我们可以用一个日语的image caption模型去自动标注每个图片的描述。</p><p>或者可以用X-日语的机器翻译得到图片翻译后的描述；或者直接用一个现有的词典，把图片的主题词直接翻译成日语。其他方法亦可。</p></li><li><p>在融合步骤，是否可以有其他的方法进行融合？</p><p>另外一个简单的方法是，把ResNet得到的图片表示和句子一起，送入Encoder，再像往常一样解码。</p></li><li><p><strong>你认为本文这种方法从逻辑上是否真的有效？为什么？</strong></p><p>见仁见智，笔者倾向于有效，但是作用不大，因为只从模型的角度难以验证图片和文本之间语义的相关性，至于效果的提升，有可能是ResNet和Aggregate的共同结果。</p><p>笔者认为，可以考虑加一个图片预测描述的任务，和翻译一起学习；再将ResNet替换为普通的CNN进行实验。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;为了降低Multi-modal NMT对有图像标注的翻译数据集的依赖，本文提出通过建立Topic-image Lookup Table的方式
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="image-guided MT" scheme="http://yoursite.com/categories/cross-modal/image-guided-MT/"/>
    
    
      <category term="cross-modal,image-guided MT" scheme="http://yoursite.com/tags/cross-modal-image-guided-MT/"/>
    
  </entry>
  
  <entry>
    <title>[MultiSubs] A Large-scale Multimodal and Multilingual Dataset</title>
    <link href="http://yoursite.com/2021/03/03/MultiSubs-A-Large-scale-Multimodal-and-Multilingual-Dataset/"/>
    <id>http://yoursite.com/2021/03/03/MultiSubs-A-Large-scale-Multimodal-and-Multilingual-Dataset/</id>
    <published>2021-03-03T07:44:10.000Z</published>
    <updated>2021-03-04T02:52:19.884Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="目前存在的问题"><a href="#目前存在的问题" class="headerlink" title="目前存在的问题"></a>目前存在的问题</h3><p>使用视觉信息对 language grounding 的计算模型的研究导致了许多有趣的应用，例如图像字幕，视觉问答和视觉对话。各种各样的多模态数据集（由图片和文本组成）被构建，并且用于不同的应用。大部分数据集，图像被标注了文本标签，但是没有提供应用文本或图像的上下文。</p><p>在 image captioning dataset 中，为每张图片标注了 sentence-level text。虽然这些句子为图片提供了strong concept，但是存在一个基本的缺点：每个 sentence-level text 将 image 看做一个整体，但是实际上，text 中的内容仅仅包含了image中的部分元素。这将使得很难学到视觉和文本中的元素的对应（correspondences between elements）。图像和文本之间的连接是多样的。比如，很难用单个句子描述整个图像或用单个图像说明整个句子。因此，为了了解单词和图像之间更好的基础（grounding），需要在图像和文本段之间建立<strong>更紧密的局部对应关系</strong>。</p><p>此外，文本仅限于非常特定的领域（图像描述），而图像也仅限于极少数和非常特定的对象类别或人类活动；这使得很难概括可能的现实世界场景的多样性。</p><h3 id="本文的解决办法"><a href="#本文的解决办法" class="headerlink" title="本文的解决办法"></a>本文的解决办法</h3><p>在本文中，提出了一个新的大规模 多模态和多语言的数据集 (MultiSubs)，可以促进 <strong style="color:red;"><strong>grounding words to images</strong> </strong>in the context of their corresponding sentences。如下图1。</p><p><img src="https://i.loli.net/2021/03/03/PCQZBX2DtnaFzAu.png" alt="image-20210303162030516" style="zoom: 33%;"></p><p>与以前的数据集相比，我们的基础单词不仅针对图像，而且还针对其在语言中的上下文用法，从而有可能对现实世界中的人类语言学习产生更深刻的见解。具体来说：（1）MultiSubs中的文本片段和图像具有更紧密的局部对应，便于学习文本片段及其对应的视觉表示之间的关联；（2）与图像字幕数据集相比，图像更通用，范围更广，并且不受特定域的限制；（3）每个给定的文本片段和句子都可以有多个图像；（4）文字包含类似于自由形式的真实世界文字的grammar ot syntax；（5）文本是多语言的，而不仅仅是单语言或双语的。</p><p>从电影字幕的平行语料库开始，我们提出了一种<strong>跨语言多模态消歧方法</strong>，通过利用并行多语言文本来消歧文本中单词的含义，来说明文本片段。如图2所示。</p><p><img src="https://i.loli.net/2021/03/03/wBsAvKgbT2LSk16.png" alt="image-20210303163357136"></p><p>据我们所知，目前尚未对在文本插图的上下文中对此进行探讨。我们还通过人工判断来评估数据集和 illustrated text fragments 的质量。</p><p>使用本文提出的MultiSubs 数据集，本文提出了两个不同的多模态任务：（1）A fill-inn-the-blank task：to guess a missing word from a sentence, with or without image(s) of the word as clues。（2）Lexical translation：在给定 source sentence 和与该source word 相关联的零个或多个图像的情况下，我们将带有句子上下文的 source word 翻译为外语中的target word。</p><h2 id="语料库和文本片段选择"><a href="#语料库和文本片段选择" class="headerlink" title="语料库和文本片段选择"></a>语料库和文本片段选择</h2><p><em>MultiSubs</em>基于OpenSubtitles 2016（OPUS）语料库，该语料库是从 OpenSubtitles 中获得的，涵盖了65种语言的movie subtitles。</p><blockquote><p>OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles.</p><p>OpenSubtitles: Subtitles - download movie and TV Series subtitles. <a href="http://www.opensubtitles.org/" target="_blank" rel="noopener">http://www.opensubtitles.org/</a></p></blockquote><p>本文挑选了5类电影：冒险，动画，喜剧，纪录片和家庭。大多数 subtitles 是对话性的（对话）或叙事性的（故事叙事或纪录片）。进一步将字幕过滤为仅在OPUS中与来自语料库的前30种非英语语言中的至少一种字幕对接的英语字幕的子集。这样一来，总共有45,482个电影实例<em>≈</em>38M个英语句子。对于前30种语言，电影的数量从2,354到31,168不等。</p><p>我们的目标是选择可能“在视觉上可描绘”的文本片段，并因此可以用图像进行说明。我们首先将英语字幕通过spacy  POS （en_core_web_md）来提取名词，动词，复合名词和简单形容词名词短语。这些 text frgments 的图像可成像性评分是通过MRC心理语言学数据库PaetzoldSpecia：2016通过引导获得的 ; 对于多词短语，我们将每个单词的图像可比性得分平均，为每个未见单词分配零得分。我们 retain text fragments 的可成像性得分至少为500，这是通过人工检查单词的子集来确定的。删除掉仅出现一次的 text fragments 后，输出为一组144,168个唯一候选片段（超过1600万个实例）<em>≈</em>1100万个句子。</p><h2 id="Illustration-of-text-fragments"><a href="#Illustration-of-text-fragments" class="headerlink" title="Illustration of text fragments"></a>Illustration of text fragments</h2><h3 id="Cross-lingual-sense-disambiguation"><a href="#Cross-lingual-sense-disambiguation" class="headerlink" title="Cross-lingual sense disambiguation"></a>Cross-lingual sense disambiguation</h3><p>本文提出的text illustration approach 的关键直觉是：一个带有歧义的英语句子，在另外一种语言的parallel sentence 中 可能没有歧义。</p><p>【1】<strong>Cross-lingual word alignment</strong> </p><p>在选择正确的图像 to illustrate our candidate text fragments (nouns) 时，我们尝试了多达四种<em>目标</em>语言：<strong>西班牙语</strong>（<strong>ES</strong>）和<strong>巴西葡萄牙语</strong>（<strong>PT</strong>），以及<strong>法文</strong>（<strong>FR</strong>）和<strong>德文</strong>（<strong>DE</strong>）。对于每种语言，选择字幕，以使（i）每个字幕都与英语字幕对齐；（ii）每个都至少包含一个感兴趣的名词。对于英语和每个目标语言，我们在全组平行句（不管句子中是否含有候选片段）中训练 <strong><em>fast_align</em></strong> DyerEtAl：2013，以获得在两种语言中词与词之间的对齐 。<strong>这将生成一个字典，该字典将英语名词映射到目标语言中的单词。</strong>我们对此字典进行过滤，以删除不常见的目标短语（语料库的1％以下）对。我们还将目标语言中具有相同lemmas 的单词归为一组。</p><p>【2】<strong>Sense disambiguation</strong></p><p>source -&gt; target </p><p>将名词翻译成不同的词(in the target language) 并不一定意味着它是模棱两可的。target phrases 可以简单地是指代相同概念的同义词。因此，我们进一步尝试在target side 对同义词进行分组，同时还通过查看跨多语言语料库的对齐短语来确定正确的词义。</p><p>对于 word senses，我们使用<strong>BabelNet</strong> NavigliPonzetto：2012，这是一个大型语义网络和涵盖多种语言的多语言百科全书，并统一了其他语义网络。</p><p>为了帮助我们确定给定上下文中英语名词的正确含义，我们使用目标语言中平行句子中的对齐词来消除歧义。我们计算两个查询返回的BabelNet同义词集ID之间的交集。比如 bank(english) -&gt; banco(spanish) ，如果使用英语bank 来查询将得到 financial-bank 和river-bank，如果用西班牙语banco查询将得到 financial-bank。取这两个查询结果的交集。</p><p>如果仅针对一种语言对执行上述操作，则该目标语言可能不足以消除英语术语的歧义，因为该术语在两种语言中可能是歧义的。对于紧密相关的语言（例如葡萄牙语和西班牙语）尤其如此。因此，<strong>我们建议利用<em>多种</em>目标语言，以进一步提高我们消除英语单词歧义的信心</strong>。我们的假设是，更多的语言最终将允许识别单词的正确上下文。</p><p>更具体地说，我们研究了包含<strong>多达四种目标语言</strong>的并行句子的字幕。对于每个英语短语，我们保留所有实例之间的同义词集ID之间<strong>至少有一个交集的实例</strong></p><p>【3】<strong>Image Selection</strong></p><p>构造<em>MultiSubs</em>的最后一步是为每个歧义的英语术语分配至少一个图像，and by design the term in the aligned target language(s)。由于BabelNet通常为给定的同义词集ID提供多个图像，因此我们用与该同义词集关联的所有Creative Commons images 说明该term。</p><h2 id="Human-evaluation"><a href="#Human-evaluation" class="headerlink" title="Human evaluation"></a>Human evaluation</h2><p>为了定量评估我们的automated cross-lingual sense disambiguation cleaning procedure，我们收集了人类注释，<strong>以确定<em>MultiSubs</em>中的图像是否确实对预测填空任务中的遗漏单词有用</strong>。<strong>注释还可以作为任务的人工上限</strong></p><p>我们将注释任务设置为<em>The Gap Filling Game</em>（图 <a href="https://www.arxiv-vanity.com/papers/2103.01910/#S5.F5" target="_blank" rel="noopener">5</a>）。在此游戏中，用户尝试进行<strong>三种尝试来猜测从<em>MultiSubs</em>的句子中删除的确切单词</strong>。在<strong>第一次尝试</strong>中，游戏仅显示句子（以及遗漏单词的空白）。在<strong>第二次尝试</strong>中，游戏还会为丢失的单词提供一个图像作为线索。在第三次也是<strong>最后一次尝试</strong>中，系统将显示与缺失单词关联的所有图像。在每次尝试中，如果用户输入的单词与原始单词完全匹配，则用户将获得1.0分；否则，将按预先训练的CBOW word2vec MikolovEtAl：2013之间的余弦相似度计算得出的部分分值（介于0.0和1.0之间） 预测词和原始词的嵌入。当用户输入完全匹配的内容时，或者在用尽所有三个尝试之后（以先发生者为准），每个“转”（一个句子）都会结束。第二次和第三次尝试的得分乘以<em>惩罚因子</em>（分别为0.90和0.80），以鼓励用户尽早正确猜出该单词。用户单回合的得分是所有三个尝试中的最高分，每个用户的最终累积得分是所有带注释的句子中分的总和。该最终分数确定了游戏结束时（在预定的截止日期之后）的获胜者和亚军，他们两人都分别获得了亚马逊代金券。在游戏过程中，不会为用户提供确切的“当前最高得分”表，而是会为他们提供比其当前得分更低的所有得分的所有用户所占的百分比</p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><p>visual grounding of words</p><p>language grounding</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;h3 id=&quot;目前存在的问题&quot;&gt;&lt;a href=&quot;#目前存在的问
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>[The GEM Benchmark] Natural Language Generation, its Evaluation and Metrics</title>
    <link href="http://yoursite.com/2021/03/01/The-GEM-Benchmark-Natural-Language-Generation-its-Evaluation-and-Metrics/"/>
    <id>http://yoursite.com/2021/03/01/The-GEM-Benchmark-Natural-Language-Generation-its-Evaluation-and-Metrics/</id>
    <published>2021-03-01T11:10:46.000Z</published>
    <updated>2021-03-01T11:22:29.397Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Multimodal Transformer for Multimodal Machine Translation</title>
    <link href="http://yoursite.com/2021/02/26/Multimodal-Transformer-for-Multimodal-Machine-Translation/"/>
    <id>http://yoursite.com/2021/02/26/Multimodal-Transformer-for-Multimodal-Machine-Translation/</id>
    <published>2021-02-26T09:24:51.000Z</published>
    <updated>2021-02-26T09:25:47.986Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>多模态机器翻译任务，是从其他模态中引入信息（一般是静态的图像）来提高翻译质量。先前的方法没有考虑多个模态的相对重要性，它们常常平等对待文本和图像信息，并分别编码，但是这种方式，将会导致从图像中<strong>引入许多无关的信息</strong>。</p><p>在本文中，提出了一个multi-modal self-attention in Transformer 来解决上述的问题。本文提出的方法能够based on text to encode vision, 从而避免了编码图像中与文本无关的信息。</p><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><ul><li>The focus of our work is to build a powerful encoder to incorporate the information from other modality.</li></ul><p><img src="https://i.loli.net/2021/02/26/4AnNwV85DpBkqZm.png" alt="image-20210226172540508" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h1&gt;&lt;p&gt;多模态机器翻译任务，是从其他模态中引入信息（一般是静态的图像）来提高翻译质量
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="image-guided MT" scheme="http://yoursite.com/categories/cross-modal/image-guided-MT/"/>
    
    
      <category term="cross-modal,image-guided MT" scheme="http://yoursite.com/tags/cross-modal-image-guided-MT/"/>
    
  </entry>
  
  <entry>
    <title>A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</title>
    <link href="http://yoursite.com/2021/02/26/A-Novel-Graph-based-Multi-modal-Fusion-Encoder-for-Neural-Machine-Translation/"/>
    <id>http://yoursite.com/2021/02/26/A-Novel-Graph-based-Multi-modal-Fusion-Encoder-for-Neural-Machine-Translation/</id>
    <published>2021-02-26T09:21:24.000Z</published>
    <updated>2021-02-26T09:24:02.504Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>当前主流的multi-modal NMT models 不能充分利用不同模态语义单元之间的<strong>细粒度的语义对应。</strong></li><li>在本文中，提出了一个新颖的graph-based  cross-modal fusion encoder 来处理NMT task。具体地，（1）首先使用一个 unified multi-modal graph来编码input sentence and image。这种方式可以捕获到多模态语义单元（words and visual objects）之间各种语义关系。（2）使用多个 graph-based multi-modal fusion layers 来迭代的执行语义交互，以学习node representations。（3）以上获得的contextual representations 送入decoder中。</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li><p>该任务的重要性，有很多现实的应用：包括翻译多媒体新闻，Web产品信息和电影字幕。</p><blockquote><p>A visual attention grounding neural model for multimodal machine translation.</p></blockquote></li><li><p>该任务对于提高机器翻译的准确性有作用：视觉环境有助于解决歧义的多义词。</p><blockquote><p>Distilling translations with visual awareness.</p></blockquote></li></ul><p>很显然，再在 multi-modal NMT 中，如何有效的利用视觉信息是一个核心的问题。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://i.loli.net/2021/02/26/7IOXHm89QM5DNq4.png" alt="Untitled"></p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul><li><p>Multi-modal Graph</p><p><img src="https://i.loli.net/2021/02/26/w1vmXMebCR3Kt5y.png" alt="Untitled3"></p><ul><li><p>所有的单词都作为 textual nodes。使用Stanford parser来找到文本中的所有名词，然后使用 visual grounding tookit来检测 bbox，并作为visual nodes。</p><blockquote><p>visual grounding tookit: <strong>A fast and accurate one-stage approach to visual grounding</strong></p></blockquote></li><li><p>在 multi-modal graph中使用了两种类型的edges。<strong>intra-modal edge(fully-connected)</strong> and <strong>inter-modal edge(partly-connected)</strong></p></li></ul></li><li><p>Embedding Layer</p><ul><li>Before inputting the multi-modal graph into the stacked fusion layers，首先获得其初始特征。</li><li>对于textual modes， 使用word embedding 和 position embedding 的求和。</li><li>对于visual nodes，使用Faster-RCNN提取 roi pooling layer 的特征，然后使用MLP + RELU 将视觉特征映射到与文本特征相同的空间。</li></ul></li><li><p>Graph-based Multi-modal Fusion Layers</p><ul><li>On the top of embedding layer, stack multiple graph-based multimodal fusion layers to encode the above-mentioned multi-modal graph.</li><li>在每个融合层，序列地实施模态内和模态间的融合，来更新所有的节点状态。这种方式，可以使得最终的节点状态能够同时编码到相同模态和跨模态的语义信息。</li><li>由于视觉节点和文本节点是包含了不同模态信息的两种语义单元。因此，使用相同的操作，但是不同的参数（不共享）来更新它们的节点状态。</li></ul></li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul><li><p>Our decoder is similar to the conventional Transformer decoder。堆叠多个相同的层来生成 target-side hidden states，每一层由三个子层组成。</p><p>前两个子层是一个masked self-attention 和 一个encoder-decoder attention 来分别聚合target-side and source-side contexts。</p><p>最后由a position-wise fully-connected forward neural network 和 线性变化来生成next-step predict word。</p></li></ul><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li>与本文类似的模型结构有以下两篇</li></ul><blockquote><p>Multi-Modality Cross Attention Network for Image and Sentence Matching</p><p>(LXMERT) LXMERT Learning Cross-Modality Encoder Representations from Transformers</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;当前主流的multi-modal NMT models 不能充分利用不同模态语义
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="image-guided MT" scheme="http://yoursite.com/categories/cross-modal/image-guided-MT/"/>
    
    
      <category term="cross-modal,image-guided MT" scheme="http://yoursite.com/tags/cross-modal-image-guided-MT/"/>
    
  </entry>
  
  <entry>
    <title>The Style-Content Duality of Attractiveness: Learning to Write Eye-Catching Headlines via Disentanglement</title>
    <link href="http://yoursite.com/2021/02/26/The-Style-Content-Duality-of-Attractiveness-Learning-to-Write-Eye-Catching-Headlines-via-Disentanglement/"/>
    <id>http://yoursite.com/2021/02/26/The-Style-Content-Duality-of-Attractiveness-Learning-to-Write-Eye-Catching-Headlines-via-Disentanglement/</id>
    <published>2021-02-26T01:46:15.000Z</published>
    <updated>2021-03-01T11:19:42.763Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/02/26/9xToFaqiXjyKLOU.png" alt="image-20210226094912200" style="zoom: 50%;"></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>抢眼的头条新闻是触发更多点击的第一个设备，在制作人和观众之间产生了相互影响。生产者可以获得更多的流量和利润，而读者可以访问优秀的文章。生成吸引人的头条新闻时，不仅要捕捉吸引人的<strong>内容</strong>，而且要遵循醒目的书面<strong>风格</strong>。</p><p>本文中，提出了一个a Disentanglement-based Attractive Headline Generator (DAHG)。该标题生成器根据有吸引力的样式来捕获有吸引力的内容的标题。具体而言，【1】我们首先设计一个解纠缠模块，将引人注目的原型标题的样式和内容划分为潜在空间，并带有两个辅助约束以确保两个空间确实被纠缠。【2】然后，潜在内容信息将用于进一步polish the document representation 并帮助捕获重要部分。【3】最后，生成器将 polished document 作为输入，以在引人注目的样式的指导下生成标题。</p><p>本文在Kuaibao dataset 上实现了最好的性能。人工评估还表明，与现有模型相比，DAHG触发的点击次数增加了22％。</p><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><h4 id="Headline-Generation"><a href="#Headline-Generation" class="headerlink" title="Headline Generation"></a>Headline Generation</h4><p>头条生成目前是NLP中的一个研究热点，目前大部分存在的头条生成工作仅仅关注于 summarizing the document。目前在Attractive headline generation上的研究还相对较少，目前有以下几篇。据我们所知，目前没有工作considers the style-content duality of attractiveness（考虑  吸引力的 内容-风格 二重性）。</p><blockquote><p>【1】Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning</p><p>【2】Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator. EMNLP 2019</p><p>【3】Structure Learning for Headline Generation.</p><p>【4-（not）】Hooks in the Headline: Learning to Generate Headlines with Controlled Styles</p></blockquote><h4 id="Disentanglement"><a href="#Disentanglement" class="headerlink" title="Disentanglement."></a>Disentanglement.</h4><p>现有作品集中于学习learning the disentangled representation，并且我们进一步采取了这种方法来利用这种representation来生成attractive headlines。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/02/26/9xToFaqiXjyKLOU.png&quot; alt=&quot;image-20210226094912200&quot; style=&quot;zoom: 50%;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Abstract&quot;&gt;&lt;a h
      
    
    </summary>
    
      <category term="title" scheme="http://yoursite.com/categories/title/"/>
    
      <category term="style" scheme="http://yoursite.com/categories/title/style/"/>
    
    
      <category term="title,style" scheme="http://yoursite.com/tags/title-style/"/>
    
  </entry>
  
  <entry>
    <title>[DeepFuse] HKU’s Multimodal Machine Translation System for VMT’20</title>
    <link href="http://yoursite.com/2021/02/26/DeepFuse-HKU%E2%80%99s-Multimodal-Machine-Translation-System-for-VMT%E2%80%9920/"/>
    <id>http://yoursite.com/2021/02/26/DeepFuse-HKU’s-Multimodal-Machine-Translation-System-for-VMT’20/</id>
    <published>2021-02-26T01:15:34.000Z</published>
    <updated>2021-02-26T09:16:26.077Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/02/26/bTNP7oqc3nxXfMV.png" alt="image-20210226154022062" style="zoom:50%;"></p><p>VATEX： video-guided machine translation(EN-&gt;CH) Challenge</p><p>ACL 2020 workshop <a href="https://alvr-workshop.github.io/2020/index.html" target="_blank" rel="noopener">https://alvr-workshop.github.io/2020/index.html</a></p><h3 id="以前方法存在的问题"><a href="#以前方法存在的问题" class="headerlink" title="以前方法存在的问题"></a>以前方法存在的问题</h3><p>以前的image-guied Machine Translations, 在encode 阶段，往往单独对 视觉信息和语言信息进行编码。然后，在decode 阶段使用attention将视觉信息结合进来。模态之间的信息仅仅进行了浅融合。</p><h3 id="本文提出的方案"><a href="#本文提出的方案" class="headerlink" title="本文提出的方案"></a>本文提出的方案</h3><p>本文中，提出了一个 video-augmented encoder，以此，获得一个multi-modal representation 来作为decoder的输入。使用attention 机器在多个层融合了多模态的表征。</p><p>实验证明，这种深融合方法相比于之前的浅融合方法要更加的有效。</p><h3 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h3><p><img src="https://i.loli.net/2021/02/26/7qgti5pwnDkcWNZ.png" alt="image-20210226160753355" style="zoom:50%;"></p><p>本文提出的visual-augmented encoder 如图1所示，encoder 包括L=6层相同的层。</p><p>将sentence表征为一个embedding的输入序列： $\mathbf{X}=x_{1}, x_{2}, \ldots, x_{n}$</p><p>将 video 使用I3D提取clips的特征，表征为segment-level feature 的序列：$\mathbf{E}=e_{1}, e_{2}, \ldots, e_{m}$</p><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>The encoder process the input X and E as follows:</p><p>【1】input X 输入<strong>Multi-Head attention and Feed Forward</strong> 这两个sub layers， 得到 $H^L$</p><p>【2】在video-encoder attention module, 使用$H^L$ 作为 query 来选择与query 相关的 video representation。 </p><p>$\overline{\mathcal{H}}=$ Attention $\left(\mathbf{H}^{L}, \mathbf{K}_{E}, \mathbf{V}_{E}\right)$</p><p>【3】使用权重求和来得到多模态特征：</p><p>$\mathcal{H}=\mathbf{H}^{L}+\lambda \overline{\mathcal{H}}$， where，$\lambda=\operatorname{sigmoid}\left(\mathbf{W}_{\lambda} \overline{\mathcal{H}}+\mathbf{U}_{\lambda} \mathbf{H}^{L}\right)$</p><p>【4】最后的输出是：$\operatorname{LayerNorm}\left(\mathbf{H}^{L}+\mathcal{H}\right)$</p><p>【yaya】最后一步，似乎是有问题，因为，这样多模态特征，似乎就是加了两遍 $H^L$</p><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>遵循transformer 中 decoder的设计，存在L=6个相同的层。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h4><p>学习率的变化曲线：lrate $=d_{\text {model }}^{-0.5} \cdot \min \left(K^{-0.5}, K \cdot N^{-1.5}\right)$，where K is the current number of step and N=4000 is the number of warm-up steps.</p><h4 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h4><p>（1）<strong>lstm</strong>: Text-only LSTM-based encoderdecoder NMT </p><p>（2）<strong>vatex</strong>: The LSTM-based videoguided machine translation system proposed in VATEX<br>（3）<strong>Transformer</strong>: standard text-only transformer architecture proposed by “Attention is all you need.”</p><p><img src="https://i.loli.net/2021/02/26/ZD7SxmKUBMA3tIE.png" alt="image-20210226170500065" style="zoom:33%;"></p><ul><li>相比于text-only transformer 有一个显著的提高。证明了，使用深层融合来结合视觉信息的有效性。</li></ul><h3 id="收获与总结"><a href="#收获与总结" class="headerlink" title="收获与总结"></a>收获与总结</h3><ul><li><p>本文的关键是提出了使用transformer 的结构来融合文本和视觉信息。</p></li><li><p>在image-guided machine translation 任务中，也存在两篇使用co-attention 来融合两个模态信息的。如下：</p><blockquote><p>[1] (ACL 2020) Multimodal Transformer for Multimodal Machine Translation</p><p>[2] (ACL 2020) A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation 【Graph Fusion】</p></blockquote></li></ul><p>【Graph Fusion】与本文相比，encoder 的设计方式不同，但是decoder的设计是相同的。关于不同：【Graph Fusion】首先对两个模态各自self-attention 而后再根据graph edge 进行co-attention。而本文对视觉信息没有进行self-attention，仅对语言信息进行了self-attention, 在co-attention步骤中也没有显示的graph edge,而是采用了隐式的全连接。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/02/26/bTNP7oqc3nxXfMV.png&quot; alt=&quot;image-20210226154022062&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
&lt;p&gt;VATEX： video-guided 
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="video-guided MT" scheme="http://yoursite.com/categories/cross-modal/video-guided-MT/"/>
    
    
      <category term="cross-modal,video-guided MT" scheme="http://yoursite.com/tags/cross-modal-video-guided-MT/"/>
    
  </entry>
  
  <entry>
    <title>A Closer Look at the Robustness of Vision-and-Language Pre-trained Models</title>
    <link href="http://yoursite.com/2021/02/24/A-Closer-Look-at-the-Robustness-of-Vision-and-Language-Pre-trained-Models/"/>
    <id>http://yoursite.com/2021/02/24/A-Closer-Look-at-the-Robustness-of-Vision-and-Language-Pre-trained-Models/</id>
    <published>2021-02-24T02:12:45.000Z</published>
    <updated>2021-02-26T07:01:23.540Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>大规模的预训练多模态transformer将最新的视觉-语言任务推进到了一个新的高度。虽然在标准任务上实现了令人印象深刻的性能，但是，迄今为止，任然不清楚这些预训练模型的鲁棒性。</p><p>为了进行调查，我们针对现有的预训练模型对4种不同类型的V + L特定模型的鲁棒性进行了全面的评估：(i) Linguistic Variation; (ii) Logical Reasoning; (iii) Visual Content Manipulation; and (iv) Answer Distribution Shift. 有趣的是，by standard model finetuning，预训练的V+L模型相比于task-specific 模型展示出更好的鲁棒性。</p><p>为了<strong>进一步增强模型的鲁棒性</strong>，本文提出了<strong>MANGO</strong>，一个具有泛化性且鲁棒的方法，可以在embedding space 学习a Multimodal Adversarial Noise GeneratOr 以愚弄pre-trained V+L models。与以往针对一种特定类型的鲁棒性的研究不同，MANGO具有任务不可知性，并且可以针对各种任务（旨在评估鲁棒性的广泛方面）对预训练模型进行通用性能提升。</p><p> 全面的实验表明，MANGO在9个鲁棒性基准中有7个达到了最新水平，大大超过了现有方法。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>当前的 V+L pre-tranining model取得了很大的进展在各种 V+L tasks，但是这些benchmark 在测试集和数据集上的分布常常是相似的，textual query 几乎没有 linguistic variation, 使用干净的自然图像，而没有任何visual content manipulation。 因此，尽管这些标准基准对于通用模型评估有效，但仍<strong>缺乏明确评估模型鲁棒性的能力</strong>。（在本文中，我们不关注 adversarial robustness，<strong style="color:red;">因为目前没有可用的adversarial benchmark</strong>。因此，我们在已有的robustness benchmark上进行观测，这些benchmark 设有挑战性的设置，并且经过了人类的验证）</p><p>(i) VQA-Rephrasings[56] for <strong>linguistic variation</strong>;</p><p>(ii) VQA-LOL (Compose and Supplement) [18], VQA-Introspect [54] and GQA [25] for <strong>logical reasoning</strong>; </p><p>(iii) IV-VQA and CVVQA [2] for <strong>visual content manipulation</strong>;  </p><p>(iv) VQA-CP v2 [3] and GQA-OOD [31] for <strong>answer distribution shift</strong>.</p><h3 id="当前方法存在的问题"><a href="#当前方法存在的问题" class="headerlink" title="当前方法存在的问题"></a>当前方法存在的问题</h3><p>VILLA，在multimodal embedding 加入对抗扰动，<strong>projected gradient descent（PGD） attack training（AT）</strong> 可以在 linguistic variation and visual content manipulation 增强鲁棒性，但是在训练集和测试集之间有显著的数据分布差异时，会有收效甚微的影响甚至drop model performence。</p><h3 id="本文方法简介"><a href="#本文方法简介" class="headerlink" title="本文方法简介"></a>本文方法简介</h3><p>为了在所有方面都实现鲁棒性，本文提出了 MANGO，通过在multi-modal embedding space 引入adversarial noise来增强鲁棒性。</p><p><img src="https://i.loli.net/2021/02/24/yDSgc5JwWFHCU8h.png" alt="image-20210224120151245"></p><p>如图 figure 1a所示，不使用PGD来生成对抗扰动，而是使用一个基于可训练神经网络来学习一个adversarial noise generator。与 VILLA相同，在embedding space 加入扰动，因为本文的目标 是对抗训练的最终结果，而不是制造对抗样本。</p><p>【1】本文要学习的是一个 universial noise generator，但是在VILLA中使用的PGD方法是针对每个特定样本来生成的，本文提出的noise generator 是通用的，对输入训练样本是不加区别的。【2】而PGD的方法是<strong>耗时</strong>的，而本文提出的方法是轻量级的，不需要梯度计算中的重复迭代。同时，为了使能多样性的对抗embedding，本文进一步提出随机对image regions 和 textual tokens掩码。</p><h3 id="本文的贡献"><a href="#本文的贡献" class="headerlink" title="本文的贡献"></a>本文的贡献</h3><ul><li>第一个系统性的分析pre-trained V+L 模型的鲁棒性</li><li>提出了 MANGO，一个generic and efficient 对抗训练方法来增强 V+L model 鲁棒性</li><li>实验结果证明了本文提出方法的鲁棒性。</li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h4><p>Perturbing clean images with Gaussian noise. we use Gaussian noise augmentation as a simple baseline to investigate model robustness under V+L setting. Instead of adding noise to raw image pixels as in [52], we add perturbations directly to the embeddings.</p><h4 id="Adversarial-Noise-Generator-our"><a href="#Adversarial-Noise-Generator-our" class="headerlink" title="Adversarial Noise Generator (our)"></a><strong>Adversarial Noise Generator</strong> (our)</h4><p>Adding Gaussian noise to clean image-text pairs 可以补充训练样本。但是，随着训练的持续，模型可以逐渐的适应这种扰动，因为扰动都是从同一个 Gaussian noise distribution 中采样来的。</p><p>为了得到 harder perturbations，本文提出了一个可学习的 adversarial noise generator。对抗性噪声发生器将高斯噪声样本作为输入，通过可学习神经网络产生对抗性噪声样本。</p><p>Intuitively, to maximally fool the backbone network, 【1】we want to <strong>maximize prediction errors on these adversarially perturbed samples.</strong> 【2】In the meantime, we want the model to possess <strong>less confidence in its predictions on perturbed samples</strong> than clean samples, to promote harder adversarial examples。因此，<strong>adversarial noise generator 的目标是</strong>最大化这两个损失的求和：【1】task-speficic loss 【2】KL loss, which measures the distance between the predicted answer distribution of perturbed samples and that of clean samples.</p><p>另一方面，the trained model 旨在通过将对抗性生成的嵌入作为数据增强来最大程度地减少这两种损失。</p><p>综合上述两种方面，提出了如下的min-max game:</p><script type="math/tex; mode=display">\min _{\boldsymbol{\theta}} \max _{\boldsymbol{\phi}_{v}(\boldsymbol{v}, \boldsymbol{w}, \boldsymbol{y}) \sim \mathcal{D} \boldsymbol{\alpha} \in \mathcal{N}(\mathbf{0}, \mathbf{1})} \mathbb{E}\left[\mathcal{L}_{s t d}\left(\boldsymbol{\theta}, \boldsymbol{\phi}_{v}\right)+\beta \mathcal{R}_{k l}\left(\boldsymbol{\theta}, \boldsymbol{\phi}_{v}\right)\right]</script><p>where $\beta$ is a hyper-parameter, and</p><script type="math/tex; mode=display">\begin{array}{l}\mathcal{L}_{s t d}\left(\boldsymbol{\theta}, \boldsymbol{\phi}_{v}\right)=\mathcal{L}_{\mathrm{BCE}}\left(f_{\boldsymbol{\theta}}\left(\boldsymbol{v}+g_{\boldsymbol{\phi}_{v}}(\boldsymbol{\alpha}), \boldsymbol{w}\right), \boldsymbol{y}\right) \\\mathcal{R}_{k l}\left(\boldsymbol{\theta}, \boldsymbol{\phi}_{v}\right)=\mathcal{L}_{k l}\left(f_{\boldsymbol{\theta}}\left(\boldsymbol{v}+g_{\phi_{v}}(\boldsymbol{\alpha}), \boldsymbol{w}\right), f_{\boldsymbol{\theta}}(\boldsymbol{v}, \boldsymbol{w})\right)\end{array}</script><p>在训练时，迭代跟新 an outer loop of the backbone network and an inner loop of noise generator.</p><p>本文提出的adversarial noise generator 是轻量级的，仅仅存在 a few linear layers。相比于一个深层模型，这种轻量模型更容易陷入局部最优。因此，定期地，we replace the learned noise generator with a new one trained from scratch。每次，new  generator is trained against the latese learned parameters of the backbone.</p><h4 id="Random-Masking"><a href="#Random-Masking" class="headerlink" title="Random Masking"></a>Random Masking</h4><p>虽然 adversarial noise generator 可以产生具有挑战性，更加多样化的噪声扰动，但是不会改变训练样本的内在统计（例如，问题长度和image regions的分布）。然而，实际上，在robustness benchmark 的训练和测试集中存在这种 significant mismatch。比如，the average length of questions in VQA-LOL 测试集是VQA V2 训练集的 2-3倍。</p><p>为了补偿这种统计上的不匹配，我们建议在向图像和单词嵌入中添加对抗性噪声时，<strong>随机掩盖图像区域</strong> and <strong>随机插入[MASK]令牌</strong>。</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>进行实验分析了 pre-trained V+L model 的鲁棒性和 本文提出的MANGO framework 的有效性。本文使用UNITER作为 backbone，并将 MANGO与 UNITER和 VILLA 在9个 robustness datasets + VQA v2 dataset上进行了比较。本文在这10个 benchmark上进行研究，<strong style="color:red;"><strong>因为目前在其他任务上没有这种 robustness dataset。</strong></strong></p><p>VILLA 在预训练阶段和微调阶段都采用了 对抗训练，而本文只是在微调阶段（即针对特定任务）</p><p><img src="https://i.loli.net/2021/02/25/HwjDn2voZfWKFOk.png" alt="image-20210225095811994"></p><h3 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h3><p>investigate the adversarial robustness of pre-trained V+L models.</p><p>（在本文中，我们不关注 adversarial robustness，<strong style="color:red;">因为目前没有可用的adversarial benchmark</strong>。因此，我们在已有的robustness benchmark上进行观测，这些benchmark 设有挑战性的设置，并且经过了人类的验证）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;p&gt;大规模的预训练多模态transformer将最新的视觉-语言任务推进到了一个新的高度。虽然在
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="对抗" scheme="http://yoursite.com/categories/cross-modal/%E5%AF%B9%E6%8A%97/"/>
    
    
      <category term="cross-modal,对抗" scheme="http://yoursite.com/tags/cross-modal-%E5%AF%B9%E6%8A%97/"/>
    
  </entry>
  
  <entry>
    <title>UNIMO Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning</title>
    <link href="http://yoursite.com/2021/02/23/UNIMO-Towards-Unified-Modal-Understanding-and-Generation-via-Cross-Modal-Contrastive-Learning/"/>
    <id>http://yoursite.com/2021/02/23/UNIMO-Towards-Unified-Modal-Understanding-and-Generation-via-Cross-Modal-Contrastive-Learning/</id>
    <published>2021-02-23T03:27:15.000Z</published>
    <updated>2021-02-23T12:00:49.165Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>VX2TEXT End-to-End Learning of Video-Based Text Generation From Multimodal Inputs</title>
    <link href="http://yoursite.com/2021/02/23/VX2TEXT-End-to-End-Learning-of-Video-Based-Text-Generation-From-Multimodal-Inputs/"/>
    <id>http://yoursite.com/2021/02/23/VX2TEXT-End-to-End-Learning-of-Video-Based-Text-Generation-From-Multimodal-Inputs/</id>
    <published>2021-02-23T02:49:31.000Z</published>
    <updated>2021-02-23T07:28:59.717Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li>本文提出了一个框架 for text generation from multimodal inputs consisting of video plus text, speech, or audio。</li><li><p>为了利用 transformer networks，每个模态通过一个 learnable tokenzier 首先转换为 a set of language embeddinngs。这将使得我们的方法可以在语言空间执行多模态融合，从而消除了对ad-hoc cross-modal fusion modules 的需要。</p></li><li><p>为了解决在连续输入（例如视频或音频）上tokenization 的不可微性，我们利用了一种放松方案，该方案可进行端到端训练。</p></li><li><p>进一步地，不像先前的 encoder-only models。本文提出的网络包括一个 autoregressive decoder来生成 open-ended text。同时在语言空间执行多模态融合，这使我们的方法完全具有生成性，并使其<strong>直接适用于不同的“video + $x $ to text” 问题，而无需为每个任务设计专门的网络.</strong></p></li><li><p>本文提出的框架不仅概念简单，而且效果显着。实验结果证明，our approach based on a single architecture 在三个video basedd text-generation task （captioning, question answering and audio-visual scene-aware dialog）上实现了最好的性能，而且本文提出的方法不需要任何的预训练任务。</p></li></ul><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>While this and a few other recent works [55] have leveraged decoders for text-generation from multimodal inputs, we believe <strong>we are the first</strong> to empirically demonstrate via systematic ablation the performance improvements achieved with generative learning with decoding, compared to discriminative learning applied to the same encoder model.</p><p>当前的multimodal transformer-based models inspired by the success of pretext tasks in the language domain（预训练任务）。这些工作，依赖消耗大的预训练任务。但是本文提出的VX2TEXT 可以在 unified language space 执行 跨模态融合，这不需要multimodal pretext pretraining.</p><blockquote><p>Hero: Hierarchical encoder for video language omni-representation pre-training</p><p>Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.</p><p>Videobert: A joint model for video and language representation learning, 2019.</p><p>Lxmert: Learning crossmodality encoder representations from transformers.</p><p>Unified vision-language pre-training for image captioning and vqa</p></blockquote><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>本文提出的方法可以概括为三步: (1) 利用一个 预训练的 modality-specific classifiers来为每个模态获得最可能的类别预测。(2) 将预测类别的textual names 经由本文提出的可微分tokenization scheme 嵌入到一个语义语言空间，这将使得整个系统可以端到端的训练（including the modality-specific classifiers)。（3）最终，使用一个generative encoder-decoder language model 将 多个模态的，embedding vector 映射到 free-form text，这将使得 不同形式的 ”video+$x$ to text” 问题变形为一个 sequence-to-sequence task。</p><h4 id="Differentiable-Tokenization"><a href="#Differentiable-Tokenization" class="headerlink" title="Differentiable Tokenization"></a>Differentiable Tokenization</h4><ul><li>We first leverage modality-specific classifiers trained  to predict a large set of categories over <strong>predefined language vocabularies</strong>.</li><li>虽然概念上是简单的，但是这个方法有一些缺点。第一，预训练的 modality-specific classifiers 可能不能泛化到目标数据。第二，每个分类器中选择top categories，这一操作是不可微分的，这阻止我们针对 target task 来微调modality-specific classifiers。</li><li>为了解决这些限制，本文提出了一个 differentiable tokenization scheme，这个方案可以在整个系统（modality specific classifer + sequence-to-sequence model）上进行端到端的训练。</li><li><strong>将预测类别的textual names 嵌入到一个语义语言空间</strong>：（1）对于每个模态的类别概率输出，采样top $K_m$个类别。（2）将采样的类别名称嵌入到语言空间：$\mathbf{e}_{m}^{k}=\mathbf{W}_{m}^{T} \mathbf{c}_{m}^{k}$，the embedding transformation  $\mathbf{W}_{m}$ can be initialized using a pretrained language embedding space </li></ul><h3 id="Generative-Encoder-Decoder"><a href="#Generative-Encoder-Decoder" class="headerlink" title="Generative Encoder-Decoder"></a>Generative Encoder-Decoder</h3><p>上一阶段，将不同的模态嵌入到了一个相同的语言空间，因此，现在可以使用一个<strong>text encoder</strong>来融合多模态信息。将多个模态得到的embedding vectors 组成一个长为L的序列，并结合<strong>task token</strong> 输入到 <strong>text encoder</strong>，并生成一个长为L的序列，该序列从多个模态中捕捉到了task  specific information。</p><p>将得到的新序列送入 decoder 中来做text generation。本文提出的decoder使用auto-regressive的方式。</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ul><li>使用 teacher-forcing 和 cross-entropy 来训练模型</li></ul><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul><li>大部分先前的 multimodal transformer 依赖 task-specific heads 来处理不同的任务。具体而言，为生成式任务设计的heads 通常与 判别式任务是不同的。但是，本文提出的VX2TEXT 可以同时处理这两种任务，而不需要改变结构</li><li>对于生成式任务，captioning and video dialog，使用 beam search and greedy decoding 来生成句子。</li><li>对于判别式任务，QA on TVQA，模型需要从候选答案中挑选出一个最可能的答案。在这种情况下，本文include the entire set of candidate answers as additional input to the model (using separator tokens to mark them)。然后评估每个候选答案，根据autoregressive decoder对它们输出的概率分布。</li></ul><h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><p>We use T5-base [39] as our text transformer including the text token embedding layer, the encoder and the decoder. We use pretrained weights provided in HuggingFace [50]</p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>简要总结本文有效的点</li><li>（1）提出将不同的模态，通过一个modality-specific classifier 映射到语言空间。（2）提出了一个端到端训练的模式，同时可以将 classifier 一起训练，这样解决了 迁移，泛化性不好的问题。（3）为了可以进行端到端的训练，采取了一些技术方案。we leverage the Gumbel-Softmax trick [18] and a differentiable approximation of tokenization [8].</li></ul><blockquote><p>Eric Jang, Shixiang Gu, and Ben Poole. <strong>Categorical reparameterization with gumbel-softmax.</strong>  arXiv preprint arXiv:1611.01144, 2016.</p><p>Yoshua Bengio, Nicholas L´eonard, and Aaron Courville. <strong>Estimating or propagating gradients through stochastic neurons for conditional computation.</strong> arXiv preprint arXiv:1308.3432, 2013.</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;本文提出了一个框架 for text generation from multi
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>Adaptive Offline Quintuplet Loss for Image-Text Matching</title>
    <link href="http://yoursite.com/2021/02/21/Adaptive-Offline-Quintuplet-Loss-for-Image-Text-Matching/"/>
    <id>http://yoursite.com/2021/02/21/Adaptive-Offline-Quintuplet-Loss-for-Image-Text-Matching/</id>
    <published>2021-02-21T08:00:22.000Z</published>
    <updated>2021-02-21T08:01:38.586Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>现有的image-text matching 的方法一般使用<strong>在线</strong>负样本和三元组损失来训练模型。对于mini-batch中的 image 或 text anchor, 模型被训练以希望区分与anchor 相对应的 positive sample 和 most confusing negative sample。这种策略能够提高模型区分image 和 text 之间细粒度的对应或者是不对应。</p><p>但是，这种方法存在几个缺陷。（1）负样本的选择策略，给模型提供了较少的机会：从<strong>很难区分的样本</strong>中学习。（2）训练的模型从训练集到测试集的泛化性较差。（3）The penalty lacks hierarchy and adaptiveness for hard negatives with different “hardness” degrees。</p><p>在本文中，（1）我们提出了一个从整个训练集中采样 <strong>negative offline samples</strong> 的解决方法。这种方法，提供了 “harder” offline negatives than online hard negatives 让模型来区分。（2）基于 <strong>the offline hard negatives,</strong> 一个五元组损失被提出来，以提高模型的泛化性。（3）另外，提出了一个新颖的损失函数来结合 <strong>the knowledge of</strong> positives, online hard negatives and online hard negatives.</p><p>由于本文提出的方法，不是创建了一个新颖的模型，而是在采集样本与损失函数上做的改进，因此，本文在三个最好的模型上添加了本文提出的模块，并报告了实验结果，证明了本文提出方法的有效性。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li>本文使用了<strong>两轮训练</strong>来增加<strong>offline “harder” negatives</strong>。在第一轮，本文使用原始的 online triple loss来训练matching model。 然后，使用训练好的模型，对于训练集中的 image or text anchor，模型预测它们与训练中的负样本的相似性分数，并对这些负样本进行排序。在第二轮，对于mini-batch 中的每个anchor，本文从 top negative list中采取采样offline negatives. 在这个过程中，多种offline hard negative pairs被构建，这些负样本与anchor之间共享或者不共享common elements。</li><li>进一步地，本文修改了损失函数，将offline hard negative pairs的信息融合到 online triplet loss中。The complete training loss实现了对(positive pairs， offline hard negatives、online hard negatives)  <strong>分等级</strong>和<strong>自适应</strong>的惩罚，</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h3&gt;&lt;p&gt;现有的image-text matching 的方法一般使用&lt;strong&gt;在线&lt;/strong&gt;负样本和三元组损失来训练模型。对于mini
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="Image-Text Matching" scheme="http://yoursite.com/categories/cross-modal/Image-Text-Matching/"/>
    
    
      <category term="cross-modal,Image-Text Matching" scheme="http://yoursite.com/tags/cross-modal-Image-Text-Matching/"/>
    
  </entry>
  
  <entry>
    <title>VMSMO Learning to Generate Multimodal Summary for Video-based News Articles</title>
    <link href="http://yoursite.com/2021/02/21/VMSMO-Learning-to-Generate-Multimodal-Summary-for-Video-based-News-Articles/"/>
    <id>http://yoursite.com/2021/02/21/VMSMO-Learning-to-Generate-Multimodal-Summary-for-Video-based-News-Articles/</id>
    <published>2021-02-21T07:52:40.000Z</published>
    <updated>2021-02-23T07:29:14.533Z</updated>
    
    <content type="html"><![CDATA[<h2 id="VMSMO-Learning-to-Generate-Multimodal-Summary-for-Video-based-News-Articles"><a href="#VMSMO-Learning-to-Generate-Multimodal-Summary-for-Video-based-News-Articles" class="headerlink" title="VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles"></a>VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles</h2><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>当前受欢迎的多媒体新闻格式是提供<strong>一个视频和相对应的文章</strong>。 这种格式广泛应用于 new media (CNN and BBC)，social media (Twitter and Weibo)。<br>在这种情况下，自动选择<strong>合适的视频封面</strong>并生成相应的<strong>文章摘要</strong>可帮助编辑人员节省时间，并使读者更有效地做出决定。</li><li>因此，在本文中，we propose the task of <strong>Video-based Multimodal Summarization with Multimodal Output</strong> (VMSMO) to tackle such a problem. </li><li>此任务中的主要挑战是使用文章的语义共同对视频的时间依赖性进行建模。</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>视频的封面应该是整个视频最显著的要素，而文本摘要应该是从原文章中提取出来的重要信息。因为视频和文章都关注于同一个报告内容中的相同事件，因此这两种信息形式在 summarizing 过程中应该是<strong>互为补充的</strong>。但是由于视频和文章是不同的模态（空间），如何充分的探索视频中的时域依赖与文章中的语义内容<strong>之间的关系</strong>仍然是一个问题。</p><p>因此，在本文中，我们提出了一个模型（DIMS）。该模型通过在过程中实施一个 dual interaction strategy 来同时summarize video and article。（1）使用 RNN 来编码 text and video. （2）设计了一个对偶交互模块（a dual interaction module）来让视频和文章相互交互。具体地，包括一个conditional self-attention mechanism 该模块可以在文章的指导下学习 local video representation。还包括一个global-attention mechanism 来学习 high level representation of video-aware article and article-aware video。（3）最后，based on fusion repersentation multimodal generator generates textual summary and cover image。</p><p>为了证明本文提出模型的有效性，本文从社会媒体网站上收集了 a large-scale news article-summary dataset associated with video-cover。实验证明，在当前广泛使用的评价标准上，DIMS可以显著的超过当前最好的baseline methods.</p><h2 id="本文的贡献"><a href="#本文的贡献" class="headerlink" title="本文的贡献"></a>本文的贡献</h2><ul><li>提出了一个任务，需要同时生成一个视频封面和一个文本摘要。</li><li>针对该任务，提出了一个模型，该模型可以同时建模视频中对的时域依赖和文章中的语义信息。</li><li>本文提出了一个大规模的数据集。在该数据集上，本文提出的方法在 automatic and human evaluation上都显著好于 baseline methods。</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><ul><li><p>Text Summarization: (1) Extractive models：从原文中提取一句话来表征整个文章的内容。（2）abstractive models：generate a summary from scratch</p></li><li><p>Multimodal Summarization：（1）结合多模态的输入，生成更高的texual summaries。（2）Multimodal<br>summarization with multimodal output 这一方向，研究的相对较少。[zhu 2018] 提出同时输出一个 textual summary 和 从6个候选中挑选出来的most relevent image。[zhu 2020] 增加了一个多模态的目标函数。</p><p>但是，在实际应用中，我们通常需要为包含数百帧的视频选择封面图。 因此，视频中帧之间的时间相关性不能通过几种静态编码方法简单地建模。</p><blockquote><p>[zhu 2018] Msmo: multimodal summarization with multimodal output. EMNLP/IJCNLP. </p><p>[zhu 2020] Multimodal summarization with guidance of multimodal reference.</p></blockquote></li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://i.loli.net/2021/02/21/KqO2uQEkZj1fyPB.png" alt="image-20210221114344434"></p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>本文从 <strong>微博</strong> 上收集了做 VMSMO task 的数据集。视频的平均时长是1分钟，帧率是25fps。对于文本，文章的平均长度是96个字，文章summary的长度是12个字。整体上，有184k 个样本被收集，180k作为训练集，2.4k作为验证集，2.4k作为测试集。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ul><li><p>compare baseline：本文提出的方法与summarization baseline 和 VQA baseline 进行比较。</p></li><li><p>evaluation metrics：<br>（1）评估生成的summary：standard full-length <strong>Rouge F1</strong>. R-1, R-2, and R-L refer to unigram, bigrams, and the longest common2 subsequence respectively.</p><p>（2）评估chosen cover frame: mean average precision （MAP）and recall at precision（$R_n@k$）。$R_n@k$用来评估是否positive sample 被排在n candidatas的前k个位置。</p></li><li><p>实验结果证明，（1）本文提出的方法相比于 baseline methods 都要好。（2）本文提出了一个联合损失，同时训练两个任务，可以看做是一个 Multi-task。在本文的实验中，探索了，如果分别训练这两个任务，是怎样的结果。从倒数第二列中可以发现，我们采用的多任务方式训练，效果会好。（3）在本文实验中探索了conditional self-attention 和 global-attention对效果的影响。从最后一列的实验结果可以发现，self-attention 模块对挑选 封面图有很大的贡献，global attention对生成文本摘要有很大的贡献。</p></li></ul><p><img src="https://i.loli.net/2021/02/21/PcFyBkTZNQlHY8m.png" alt="image-20210221154509463"></p><h2 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h2><ul><li>本文最后说到，可以结合video  script (subtitles) 来做该任务。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;VMSMO-Learning-to-Generate-Multimodal-Summary-for-Video-based-News-Articles&quot;&gt;&lt;a href=&quot;#VMSMO-Learning-to-Generate-Multimodal-Summary
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>BART</title>
    <link href="http://yoursite.com/2021/02/19/BART/"/>
    <id>http://yoursite.com/2021/02/19/BART/</id>
    <published>2021-02-19T09:59:22.000Z</published>
    <updated>2021-02-21T07:54:06.268Z</updated>
    
    <content type="html"><![CDATA[<h1 id="BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and-Comprehension"><a href="#BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and-Comprehension" class="headerlink" title="BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"></a>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>本文提出了一个<strong>denoising autoencoder</strong> (BART)来预训练一个 sequence-to-sequence model。It is implemented as a sequence-to-sequence model<br>with a bidirectional encoder over corrupted text and a<br>left-to-right autoregressive decoder</li><li>BART 通过两点来训练：（1）使用一个<strong>arbitrary noising function</strong> 来对原始文本添加噪声。（2）让模型去学习重构原始文本。</li><li>BART使用<strong>一个标准的 transformer-based neural machine translation architecture</strong>。由于采用了 <strong style="color:blue;">bidirectional encoder</strong> 可以看做是对BERT的推广，由于其采用了 <strong style="color:blue;">the left-to-right decoder</strong> 也可以看做是对GPT2的推广，同时也采用了许多现在广泛使用的预训练方案。</li><li>本文对许多 noising approaches 进行评估，发现最好的方案是对原始的句子进行打乱顺序。本文采用了一个 novel in-filling scheme，<strong>使用spans of text 来代替 a single mask token</strong>。</li><li>实验结果：（1）当对文本生成进行微调之后，BART 对生成式任务的性能尤其的好。BART对理解型任务也表现很好。（2）BART与RoBERTa的性能相匹配，在 GLUE 和 SQuAD 上有可比较的training resources，在一系列  abstractive dialogue, question answering, and summarization tasks 取得了最新成果，并在ROUGE上获得了多达6个点的提升。（3）BART在机器翻译任务上，仅使用 target language 来预训练。在BLEU 指标上，获得了比 back translation system 1.1个点的提升。</li><li>消融实验：使用BART framework，采用不同的预训练方案，来确定影响了最终任务性能的关键因素是什么。</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>现存方法的一个问题：However, these methods typically focus on particular types of end tasks (e.g. span prediction, generation, etc.), limiting their applicability.<br>yaya: 现存的方法可以只限定对某一类任务有效，如只对 comprehensive tasks有效，而对 generation tasks无法使用。</li><li><p>在本文中，提出了一个可以结合 <strong style="color:red;">bidirectional</strong> and <strong style="color:blue;">auto-regressive</strong> Transformers. BART是使用<strong style="color:blue;">序列到序列</strong>模型构建的<strong style="color:red;">去噪自动编码器</strong>，BART可以适用于非常广泛的最终任务。</p></li><li><p>本文提出框架的优点：噪声的灵活性，可以将任意转换应用于原始文本，包括更改其长度。</p></li><li><p>本文对许多 noising approaches 进行评估，发现最好的方案是 shuffling the order of the<br>original sentences。本文采用了一个 novel in-filling scheme，使用spans of text 来代替 a single mask token。<br>本文提出的方法通过强制模型对更多的总体句子长度进行推理，并对输入进行更远距离的转换，从而泛化了BERT中的两个预训练任务（word masking and next sentence prediction）</p></li><li><p>BART还开辟了关于微调的新思路。本文提出了一种新的机器翻译方案，其中BART模型堆叠在其他几个transformer layers 之上。这些层经过培训，从而将外语从本质上翻译为噪声英语，可以通过BART进行传播，从而将BART用作预训练的目标方语言模型。</p></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p><strong>BART</strong> is a denoising autoencoder that maps a corrupted document to the original document it was derived from. </p><p>It is implemented as <strong>a sequence-to-sequence model</strong> with <strong>a bidirectional encoder</strong> over corrupted text and <strong>a left-to-right autoregressive decoder</strong>. For pre-training, we optimize the negative log likelihood of the original document.</p><p><img src="https://i.loli.net/2021/02/19/BCSUOQkGX5KcJTR.png" alt="image-20210219151202969" style="zoom:50%;"></p><h2 id="Pre-training-BART"><a href="#Pre-training-BART" class="headerlink" title="Pre-training BART"></a>Pre-training BART</h2><p>Unlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to apply any type of document corruption.</p><p>解释：BERT 对句子中的token进行随机掩码，采用的特定类型的 noising schemes，而 BART 可以应用任意类型的corruption。<strong>极端情况下，当原句中的所有信息都丢失时，BART可以看做是一个 language model。</strong></p><p>本文采用了 几个先前提出的变换，和几个自己新提出的变换。</p><p><img src="https://i.loli.net/2021/02/19/LxrawUIdKp8TyFX.png" alt="image-20210219152433064"></p><p>（1）Token Masking.</p><p>（2）Token Deletion. 模型必须去决定哪个在位置丢失了。</p><p>（3）Text Infilling. 采样了多个 text spans, spans lengths 从一个泊松分布（$\lambda$ = 3）中随机采样。每个span被替换成一个[MASK]。<strong>Text infilling teaches the model to predict how many tokens are missing from a span.</strong> yaya:仅仅是预测[MASK 位置处有几个tokens，而不需要预测具体点的tokens是什么]</p><p>（4）Sentence Permutation. 根据停止符，将一个document分成多个句子，然后打乱句子的顺序，</p><p>（5）Document Rotation. A token is chosen uniformly at random, and the document is rotated so that it begins with that token. This task trains the model to identify the start of the document.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and-Comprehension&quot;&gt;&lt;a href=&quot;#BART-Denoi
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
</feed>
