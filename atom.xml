<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShiYaya</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-04-02T09:00:57.098Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ShiYaya</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Cross-Batch Memory for Embedding Learning</title>
    <link href="http://yoursite.com/2021/04/02/Cross-Batch-Memory-for-Embedding-Learning/"/>
    <id>http://yoursite.com/2021/04/02/Cross-Batch-Memory-for-Embedding-Learning/</id>
    <published>2021-04-02T08:51:02.000Z</published>
    <updated>2021-04-02T09:00:57.098Z</updated>
    
    <content type="html"><![CDATA[<p>转载自：<a href="https://zhuanlan.zhihu.com/p/139187724" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/139187724</a></p><p>转载的原因：</p><ul><li>此篇论文与 MoCo 的思路非常的相似</li><li>其实度量学习的思想也是可以用于 跨模态检索任务中的，毕竟跨模态检索做的也是度量任务。<br>目前看到有两篇论文已经这样做了：<ul><li>HiT Hierarchical Transformer with Momentum Contrast for Video-Text Retrieval</li><li>Memory Enhanced Embedding Learning for Cross-Modal Video-Text Retrieval</li></ul></li></ul><p>码隆科技今年在 CVPR 多有斩获。在大会论文接受率仅有 22%、被称为“十年来最难的一届”的情况下，有两篇论文入选 CVPR 2020，其中本文 Cross-Batch Memory for Embedding Learning为口头报告论文（Oral）。口头报告论文是 CVPR 官方核定的含金量最高的论文，以 CVPR 2019 为例，投稿的 5160 篇论文中，仅有约 5.6% 获评口头报告论文。同时，作为以技术为先的人工智能企业，码隆科技和往年一样，是 CVPR 2020 的金牌赞助商。</p><p>本篇论文提出了 XBM 方法，能够用极小的代价，提供巨量的样本对，为 pair-based 的深度度量学习方法取得巨大的效果提升。这种提升难例挖掘效果的方式突破了过去两个传统思路：加权和聚类，并且效果也更加简单、直接，很好地解决了深度度量学习的痛点。XBM 在多个国际通用的图像搜索标准数据库上（比如 SOP、In-Shop 和 VehicleID 等)，取得了目前最好的结果。</p><p><strong>我们的算法研究员也在知乎上针对这篇论文撰写了一版更为通俗有趣的解读文章。感兴趣的话，可以点击下方直达。</strong></p><p><a href="https://zhuanlan.zhihu.com/p/136522363" target="_blank" rel="noopener">王珣：跨越时空的难样本挖掘zhuanlan.zhihu.com<img src="https://i.loli.net/2021/04/02/xsgfovFm5TVaCyq.jpg" alt="图标"></a></p><p><strong>论文</strong>：Cross-Batch Memory for Embedding Learning</p><p><strong>地址</strong>：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1912.06798" target="_blank" rel="noopener">https://arxiv.org/abs/1912.06798</a></p><h2 id="背景和动机"><a href="#背景和动机" class="headerlink" title="背景和动机"></a><strong>背景和动机</strong></h2><p>难例挖掘是深度度量学习领域中的核心问题，最近有颇多研究都通过改进采样或者加权方案来解决这一难题，目前主要两种思路：第一种思路是在 mini-batch 内下功夫，对于 mini-batch 内的样本对，从各种角度去衡量其难度，然后给予难样本对更高权重，比如 N-pairs、Lifted Struture Loss、MS Loss 使用的就是此种方案。第二种思路是在 mini-batch 的生成做文章，比如 HTL、Divide and Conquer，他们的做法虽然看上去各有不同，但是整体思路有异曲同工之处。大致思路都是对整个数据集进行聚类，每次生成 mini-batch 不是从整个数据集去采样，而是从一个子集，或者说一个聚类小簇中去采样。这样一来，由于采样范围本身更加集中，生成的 mini-batch 中难例的比例自然也会更大，某种程度上也能解决问题。然而，无论是第一种方法的额外注重难样本，还是第二种方法的小范围采样，他们的难例的挖掘能力其实依然有一个天花板——那就是 mini-batch 的大小。这个 mini-batch 的大小决定了在模型中单次迭代更新中，可以利用的样本对的总量。因此，即使是很精细的采样加权方法，在 mini-batch 大小有限的情况下，也很难有顶级的表现。我们在三个标准图像检索数据库上进行了实验，基于三种标准的 pair-based 方法，我们发现随着 mini-batch 变大，效果（Recall@1）大幅提升。实验结果如下图：</p><p><img src="https://i.loli.net/2021/04/02/XnEsAeKYZ78Sak3.png" alt="image-20210402165727364"></p><p>可以看出，随着 mini-batch 的增大，效果有显著提升。但是，在实际工业应用中 mini-batch 越大，训练所需要的 GPU 或 TPU 就越多，即使计算资源有充分保证，在多机多卡的训练过程中，如何在工程上保证通信的效率也是一个有挑战的问题。</p><h2 id="特征偏移"><a href="#特征偏移" class="headerlink" title="特征偏移"></a><strong>特征偏移</strong></h2><p>由此，我们希望另辟蹊径，得以在 mini-batch 有限的情况下，也能获得充足的难例样本对。首先，必须突破深度度量学习一直以来的一个思维局限——仅在对当前 mini-batch里的样本对两两比较，形成样本对。以此我们引入了 XBM（Cross-batch Memory）这一方法来突破局限，跨越时空进行难例挖掘，把过去的 mini-batch 的样本提取的特征也拿过来与当前 mini-batch 作比较，构建样本对。</p><p><img src="https://i.loli.net/2021/04/02/gzBpr4V6MLxbPCd.jpg" alt="img"></p><p>我们将样本特征随着模型训练的偏移量，称之为特征偏移（Feature Drift）。从上图我们发现，在训练的一开始，模型还没有稳定，特征剧烈变化，每过 100 次迭代，特征偏移大约 0.7 以上。但是，随着训练的进行，模型逐步稳定，特征的偏移也变小。我们称这个现象为慢偏移（Slow Drift），这是我们可以利用的一点。</p><h2 id="方法梗概"><a href="#方法梗概" class="headerlink" title="方法梗概"></a><strong>方法梗概</strong></h2><p>我们发现，虽然在训练的前 3K iterations，mini-batch 过去的提取特征与当前模型偏差很大，但是，随着训练时间的延长，过去的迭代里所提取过的特征，逐渐展示为当前模型的一个有效近似。我们要做的不过是把这些特征给存下来，每个特征不过是 128 个 float 的数组，即便我们存下了过去 100 个 mini-batch 的特征，不过是6400个（假设 batch size = 64）float 数组，所需要不过是几十 MB 的显存。而它带来的好处是显而易见的，我们能够组成的样本对的个数是仅仅利用当前 mini-batch 的一百倍。即便这些特征不能高精准地反映当前模型的信息，但是只要特征偏移在合理的范围内，这种数量上带来的好处，可以完全补足这种误差带来的副作用。具体来看，我们的 XBM 的方法架构大致如下：</p><p><img src="https://i.loli.net/2021/04/02/ixCLyrXDOHwck8u.jpg" alt="img"></p><p>伪代码如下：</p><p><img src="https://i.loli.net/2021/04/02/yWbPMmZtnerAHV3.jpg" alt="img"></p><p>我们的 XBM 从结构上非常简单清晰。我们先训练一个 epoch 左右，等待特征偏移变小。然后，我们使用 XBM：一个特征队列去记忆过去 mini-batch 的特征，每次迭代都会把当前 mini-batch 提取出来的新鲜特征加入队列，并把最旧的特征踢出队列，从而保证 XBM 里的特征尽量是最新的。每次去构建样本队列的时候，我们将当前 mini-batch 和 XBM 里的所有特征都进行配对比较，从而形成了巨量的样本对。如果说 XBM 存储了过去 100 个 mini-batch，那么其所产生的样本对就是基于 mini-batch 方法的 100 倍。不难发现，XBM 其实直接和过去基于样本对的方法结合，只需要把原来的 mini-batch 内的样本对换成当前 mini-batch 和 XBM 的特征构成的样本对就可以了。所以，我们通过 XBM 这种存储特征的机制，能够让不同时序的 mini-batch 的特征成功配对。</p><h2 id="消融实验一"><a href="#消融实验一" class="headerlink" title="消融实验一"></a><strong>消融实验一</strong></h2><p><img src="https://i.loli.net/2021/04/02/Be4GvZMLxiCbEyt.jpg" alt="img"></p><p>首先，我们在三个常用的检索数据集，和三个基于样本对的深度学习的方法上，使用 XBM 进行测试，同时控制其他的设置全部不变。我们发现，XBM 带来的效果很明显。尤其是在最基本的对比损失（Contrastive Loss）上，可以看到，本来这个方法只利用 mini-batch 内的样本时，其效果并不显著，但是 XBM 带来了显著的效果提升。在三个数据集， Recall@1 都至少提升 10 个点，尤其是 VehicleID 数据集的最大（Large）测试集，效果提升了 22 个点，从 70.0 到 92.5。</p><h2 id="消融实验二"><a href="#消融实验二" class="headerlink" title="消融实验二"></a><strong>消融实验二</strong></h2><p><img src="https://i.loli.net/2021/04/02/VD6UHdejXizlyTf.jpg" alt="img"></p><p>关于 mini-batch 的大小对结果的影响， 从上图可发现三点：</p><p><strong>1.</strong> 无论是否使用 XBM，mini-batch 越大，效果越好；</p><p><strong>2.</strong> XBM 方法即便是使用很小的 batch (16)， 也比没有 XBM 使用大的 batch (256) 效果好；</p><p><strong>3.</strong> 由于 XBM 本身可以提供正样本对，所以可以不一定要用 PK sampler 来生成 mini-batch，而是可以直接使用原始的 shuffle sampler，效果相似。</p><h2 id="计算资源消耗"><a href="#计算资源消耗" class="headerlink" title="计算资源消耗"></a><strong>计算资源消耗</strong></h2><p>下图我们展示了在 SOP 上训练 XBM 时的计算资源消耗，即便把整个训练集（50K+）的特征都加载到 XBM，不过需要 0.2GB 的显存；而如果是使用增大 batch 的方法，会额外需要 15GB 显存，是 XBM 的 80 倍，但是效果的提升比 XBM 差很多。毕竟 XBM 仅仅需要存特征，特征也是直过去的 mini-batch 的前向计算的结果，计算资源的需求很小。</p><p><img src="https://i.loli.net/2021/04/02/ATGEJLXR9UjWCN6.jpg" alt="img"></p><h2 id="对比-SOTA"><a href="#对比-SOTA" class="headerlink" title="对比 SOTA"></a><strong>对比 SOTA</strong></h2><p>与最近的深度度量学习方法对比，我们在四个检索数据库上效果均大幅提升，这里仅列出 VehicleID 的效果，其他数据集的效果见原论文。</p><p><img src="https://i.loli.net/2021/04/02/veWF2knBIuEogwj.jpg" alt="img"></p><p>简单来说，不同于部分文章中使用更好的网络结构，更大的输出维度，或者更大的 mini-batch 来提升效果，强行 SOTA。我们列出 XBM 在 64 的 mini-batch 在不同的主干网络下及各种维度下的效果，其效果一直是最好的。</p><p><strong>另外，我们的方法也将整理开源，地址在：</strong></p><p><strong><a href="https://link.zhihu.com/?target=https%3A//github.com/MalongTech/research-xbm" target="_blank" rel="noopener">https://github.com/MalongTech/research-xbm</a></strong></p><p><strong>论文地址在：</strong></p><p><strong><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1912.06798" target="_blank" rel="noopener">https://arxiv.org/abs/1912.06798</a></strong></p><p><strong>期待与更多研究者共同学习交流。</strong></p><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a><strong>可视化</strong></h2><p><img src="https://i.loli.net/2021/04/02/TIxiYh9LDWnUq4s.jpg" alt="img"></p><p>更多可视化见论文补充材料，有更多实例说明效果。</p><h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a><strong>总结与展望</strong></h2><p>第一，本文提出的 XBM 方法能够记住过去的特征，使得模型的训练不再仅仅依靠当前 mini-batch 的样本，而是可以跨越时空进行样本配对。从而用极小的代价，提供了巨量的样本对，为 pair-based 的深度度量学习方法取得了巨大的效果提升。这种提升难例挖掘效果的方式，也是突破了过去两个传统思路：加权和聚类，并且效果也更加简单、直接，很好地解决了深度度量学习的痛点。</p><p>第二，其实 Memory 机制并不是本文原创，但是用 Memory 来做难例挖掘是一个全新的尝试。同样在 CVPR 2020 获得 Oral，也是由 Kaiming He 作为一作的 MoCo 也是这种思路。<font color="red"><strong>本文的方法其实可以认为是 MoCo 在 ｍ=0 的特例</strong></font>，Kaiming 通过动量更新 key encoder，可以直接控制住特征偏移。作者认为，这种方法还会在很多任务带来提升，不局限于 Kaiming 的自监督表示学习，以及此前我们一直关注研究的度量学习（或者说监督表示学习）。</p><p>第三，在本文中，虽然 XBM 在所有的 pair-based 的方法都有提升，但是明显在对比损失（Contrastive Loss）上提升最大，具体原因待考。另外，我们也把在无监督表示上表现很好的 infoNCE 方法用到了深度度量学习，但效果并不显著。作者认为这两个问题的答案是同一个，且有值得深究的价值，希望在后续研究中进行进一步跟进探索。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载自：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/139187724&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/139187724&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;转载的
      
    
    </summary>
    
    
      <category term="metric learning" scheme="http://yoursite.com/tags/metric-learning/"/>
    
  </entry>
  
  <entry>
    <title>HiT Hierarchical Transformer with Momentum Contrast for Video-Text Retrieval</title>
    <link href="http://yoursite.com/2021/04/02/HiT-Hierarchical-Transformer-with-Momentum-Contrast-for-Video-Text-Retrieval/"/>
    <id>http://yoursite.com/2021/04/02/HiT-Hierarchical-Transformer-with-Momentum-Contrast-for-Video-Text-Retrieval/</id>
    <published>2021-04-02T06:42:38.000Z</published>
    <updated>2021-04-02T08:53:54.233Z</updated>
    
    <content type="html"><![CDATA[<h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><ul><li><p>当前基于 transformer architecture 的 跨模态检索，仅仅使用最后一层的embeddings。 不同的层有不同的特诊特性，却没有被利用。</p></li><li><p>端到端的训练机制限制了负样本的交互仅能在一个mini-batch 中被执行（负样本的数量受到限制）。</p></li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><ul><li>提出了一个新颖的hierarchical transformer 来做 video-text retrieval。在feature-level（第一层） 和 semantic-level（最后一层） 来实现分层跨模态对比匹配。</li><li>受到MOCO的启发，在跨模态学习中使用Momentum Cross-modal Contrast（<strong>MCC</strong>），以使能 大规模的负样本交互，这种方式对生成更加精确且有判别力的表达是有贡献的。</li></ul><p>本文的 momentum encoder 与 论文“Memory Enhanced Embedding Learning forCross-Modal Video-Text Retrieval”的思想很一致</p><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><p><img src="https://i.loli.net/2021/04/02/uTMwEnhAfIVgixm.png" alt="image-20210402161043011"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ul><li><p>在各个数据集上的实验效果</p><p><img src="https://i.loli.net/2021/04/02/PrfJab2l3kLcxmg.png" alt="image-20210402164510842" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/04/02/rps6KOJ31wBnitX.png" alt="image-20210402164527524" style="zoom:50%;"></p></li><li><p>使用不同维度下的MCC的效果对比</p><p><img src="https://i.loli.net/2021/04/02/25nEX687HBZSqVN.png" alt="image-20210402164404377" style="zoom:50%;"></p></li><li><p>InfoNCE loss vs Triplet Ranking loss</p><p><img src="https://i.loli.net/2021/04/02/4LPx5Nh1nIAgaGf.png" alt="image-20210402164121908" style="zoom:50%;"></p></li></ul><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><p>值得注意的是本文使用的是 dual encoding的方法，</p><p>对 video feature 和 sentence feature 分别使用各自的transformer获得特征。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;存在的问题&quot;&gt;&lt;a href=&quot;#存在的问题&quot; class=&quot;headerlink&quot; title=&quot;存在的问题&quot;&gt;&lt;/a&gt;存在的问题&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;当前基于 transformer architecture 的 跨模态检索，仅仅使用最后一层的em
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>Memory Enhanced Embedding Learning for Cross-Modal Video-Text Retrieval</title>
    <link href="http://yoursite.com/2021/04/02/Memory-Enhanced-Embedding-Learning-for-Cross-Modal-Video-Text-Retrieval/"/>
    <id>http://yoursite.com/2021/04/02/Memory-Enhanced-Embedding-Learning-for-Cross-Modal-Video-Text-Retrieval/</id>
    <published>2021-04-02T03:15:49.000Z</published>
    <updated>2021-04-02T08:54:10.664Z</updated>
    
    <content type="html"><![CDATA[<h2 id="本文研究的任务"><a href="#本文研究的任务" class="headerlink" title="本文研究的任务"></a>本文研究的任务</h2><ul><li>视频文本跨模态检索</li></ul><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p>本文提出的问题其实也是度量学习中的一个关键问题，对于一个anchor, 为了在一个mini-batch中看到更多的负样本，一个简单的方法是扩大batch size，计算量就提升了。</p><p>针对此问题，有人提出了 memory bank, 将整个训练集中的样本特征预先提取出来，存储起来，每个epoch更新一次。Memory Bank 虽然可以保留足够多的样本进行 negative采样，但是其中存储的图像特征都是过去的 encoder 编码的特征。由于 Memory Bank 容量很大，导致了采样的特征具有不一致性（是由不同的encoder产生的）。</p><h4 id="MOCO"><a href="#MOCO" class="headerlink" title="MOCO"></a>MOCO</h4><p>鉴于此，MoCo 使用一个队列来存储和采样 negative 样本，队列中存储多个<strong>近期</strong>用于训练的 batch 的特征向量。队列容量要远小于 Memory Bank，但可以远大于 batch 的容量。本文提出了将队列作为一个动态的，而非静态的 Memory Bank。队列在不断的进行更新，新的训练batch入队列后，最老的训练batch 出队列。这里入队列的并不是图像本身，而是图像特征。</p><p>训练时, Anchor 样本记为 $x^{q}$ (query), 经过 encoder 网络 $f_{q}$ 进行编码得到 $q=f_{q}\left(x^{q}\right)$<br>o 随后从队列中采样了 $K+1$ 个样本 $\left\{k^{0}, \ldots, k^{K}\right\}$ 作为key。这些 key 是用不同的队列 encoder 网络 $f_{k}$ 进行编码得到的。由于 $f_{k}$ 的变化非常缓慢, 因此虽然 $\left\{k^{0}, \ldots, k^{K}\right\}$ 是通 过不同 encoder 编码的，编码器导致的差异会非常小。具体的，本文使用一种 Momentum 更新 的方式来更新 $f_{k},$ 其参数是对 Query Encoder 的平滑拷贝：<br>$\theta_{k}=m \theta_{k}+(1-m) \theta_{q}$<br>其中 $m=0.999,$ 表示 $\theta_{k}$ 呈现一种缓慢的变化。这种更新方式类似于 Q-learning 中 target-Q冈络更新。</p><p>下图形式化的表示了三种结构, end-to-end, memory-bank和MoCo的区别。MoCo的特点 是: $\quad(1)$ 用于 negative 采样的队列是动态的 (2) 用于 negative 样本特征提取的编码器与用于 query提取的编码器不一致，是一种Momentum更新的关系。而在memory-bank中两个编码器 是一致的。 (3) 与Memory Bank类似, NCE 产生的损失并不影响 $f_{k},$ 只影响 $f_{q}$ 。原因是 $x^{k}$ 样本一般是 $x^{q}$ 样本的几倍, 更新 $f_{k}$ 的代价很大（例如在 end-to-end中 $),$ 因此 Memory-Bank 和 MoCo 都不更新 $f_{k}$ 。</p><p><strong>在实验中，大的动量（例如0.999）往往效果好于小的动量（例如0.9），意味着缓慢变化的key encoder是利用好队列的关键所在。</strong></p><p><img src="https://i.loli.net/2021/04/02/ymTscqK8Q5Fpkua.png" alt="image-20210402122243715"></p><h2 id="本文的方法"><a href="#本文的方法" class="headerlink" title="本文的方法"></a>本文的方法</h2><ul><li>基于此思想，本文提出了用于跨模态检索任务中的 mementum cross-modal memory bank。</li></ul><h2 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h2><p>相关领域的知识是可以互相借鉴的。</p><p>跨模态检索领域，是可以从<strong>对比学习的无监督视觉表达学习</strong>，<strong>度量学习</strong>，等任务中借鉴的。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>MOCO： <strong>Momentum</strong> contrast for unsupervised visual representation learning</p><p><strong>InfoNCE</strong>: Representation learning with <strong>contrastive</strong> predictive coding.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;本文研究的任务&quot;&gt;&lt;a href=&quot;#本文研究的任务&quot; class=&quot;headerlink&quot; title=&quot;本文研究的任务&quot;&gt;&lt;/a&gt;本文研究的任务&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;视频文本跨模态检索&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;存在的问题&quot;&gt;&lt;a href=
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers</title>
    <link href="http://yoursite.com/2021/04/01/Thinking-Fast-and-Slow-Efficient-Text-to-Visual-Retrieval-with-Transformers/"/>
    <id>http://yoursite.com/2021/04/01/Thinking-Fast-and-Slow-Efficient-Text-to-Visual-Retrieval-with-Transformers/</id>
    <published>2021-04-01T02:40:32.000Z</published>
    <updated>2021-04-01T13:12:29.324Z</updated>
    
    <content type="html"><![CDATA[<p>发表在CVPR 2021</p><h2 id="研究任务"><a href="#研究任务" class="headerlink" title="研究任务"></a>研究任务</h2><p>提高跨模态检索的推理速度</p><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p>目前做 cross-modal retrieval task 的主流模型可以分为两类：（1）dual encoding，速度快，但是准确率低；（2）cross-attention model, 准确率高，但是速度慢。</p><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>保证模型准确率的同时，提高推理速度</p><ul><li><p>提高模型准确率，采取了两个点：（1）使用CNN特征上采样来获取细粒度视觉特征；（2）使用captioning loss来代替 对比学习损失来做检索任务。如下图</p><p><img src="https://i.loli.net/2021/04/01/VtX5mQn8T2LcwMs.png" alt="image-20210401205905230"></p></li><li><p>在提高模型推理速度上，采取了两个点：(1) 使用 teacher-student 方法，以 slow cross-attention model 作为教师，以 fast dual-encoding model 作为学生；（2）先使用 fast model检索，选取top-k，然后使用slow model 进行 re-rank。如下图</p><p><img src="https://i.loli.net/2021/04/01/bh9IRmOBQWlZKCg.png" alt="image-20210401205835827" style="zoom:50%;"></p></li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>VirTex: Learning Visual Representations from Textual Annotations</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;发表在CVPR 2021&lt;/p&gt;
&lt;h2 id=&quot;研究任务&quot;&gt;&lt;a href=&quot;#研究任务&quot; class=&quot;headerlink&quot; title=&quot;研究任务&quot;&gt;&lt;/a&gt;研究任务&lt;/h2&gt;&lt;p&gt;提高跨模态检索的推理速度&lt;/p&gt;
&lt;h2 id=&quot;存在的问题&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="real-time" scheme="http://yoursite.com/categories/cross-modal/real-time/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
      <category term="real-time" scheme="http://yoursite.com/tags/real-time/"/>
    
  </entry>
  
  <entry>
    <title>Neural Baby Talk</title>
    <link href="http://yoursite.com/2021/03/26/Neural-Baby-Talk/"/>
    <id>http://yoursite.com/2021/03/26/Neural-Baby-Talk/</id>
    <published>2021-03-26T06:31:12.000Z</published>
    <updated>2021-03-26T06:39:41.198Z</updated>
    
    <content type="html"><![CDATA[<h2 id="当前存在的问题"><a href="#当前存在的问题" class="headerlink" title="当前存在的问题"></a>当前存在的问题</h2><p>当前的模型缺少 visual grounding，比如，不能将生成的named concept 与 image 中的pixels相关联起来。</p><p>模型的关注点与人类的关注点不同，并且倾向于从训练数据中copy captionings，即，得到 Hallucination Objects。 </p><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>本文提出了两步走策略来处理图像描述任务：（1）sentence template generation （2）slot filling with object detectors。</p><ul><li>First generatesa sentence ‘template’ with slot locations explicitly tied tospecific  image  regions.</li><li>These  slots  are  then  filled  in  by visual  concepts  identified  in  the  regions  by  object  detectors.</li></ul><p>整个结构是端到端可微分的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;当前存在的问题&quot;&gt;&lt;a href=&quot;#当前存在的问题&quot; class=&quot;headerlink&quot; title=&quot;当前存在的问题&quot;&gt;&lt;/a&gt;当前存在的问题&lt;/h2&gt;&lt;p&gt;当前的模型缺少 visual grounding，比如，不能将生成的named concept 与 
      
    
    </summary>
    
      <category term="image captioning" scheme="http://yoursite.com/categories/image-captioning/"/>
    
    
      <category term="image captioning" scheme="http://yoursite.com/tags/image-captioning/"/>
    
  </entry>
  
  <entry>
    <title>Non-Autoregressive Image Captioning with Counterfactuals-Critical Multi-Agent Learning</title>
    <link href="http://yoursite.com/2021/03/26/Non-Autoregressive-Image-Captioning-with-Counterfactuals-Critical-Multi-Agent-Learning/"/>
    <id>http://yoursite.com/2021/03/26/Non-Autoregressive-Image-Captioning-with-Counterfactuals-Critical-Multi-Agent-Learning/</id>
    <published>2021-03-26T03:17:47.000Z</published>
    <updated>2021-03-26T03:34:34.240Z</updated>
    
    <content type="html"><![CDATA[<h2 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h2><p>图像描述任务</p><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p>目前的图像描述任务采用自回归的方式来生成序列，如下图中上部分表示，下一个单词的生成需要依据先前生成的单词。</p><p>这样的方式会导致：high inference latency, which is sometimes unaffordable for real-time industrial applications</p><p><img src="https://i.loli.net/2021/03/26/9biELph3vmCSzY1.png" alt="image-20210326112017198" style="zoom:50%;"></p><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><ul><li>本文提出使用非自回归的方式来生成，如上图中的下部分。但是，直接采用最朴素的方式来自回归的生成句子，并用交叉熵损失来优化，会导致生成的句子中单词之间的交互较少，几乎没有。进而导致生成的句子一致性较差。</li><li>因此，本文提出将 非自回归图像描述任务作为一个多智能体强化学习系统来学习。target sequence 中的位置被看做是智能体，智能体之间联合起来，一起最大化整个句子的质量。</li><li>具体地<ul><li>以当前非自回归方式生成的序列与GT caption比较得到整个句子的得分，以sentence-level的得分来作为每个智能体的得分(reward)，</li><li>但是这种方式会导致每个智能体的奖惩力度相同，很难学习。因此本文提出了counterfactual baseline。即将该单词去掉，计算剩下句子的得分，作为该agent的reward。</li><li>由于每个agent的action空间是整个词汇表（非常大），因此，仅仅考虑最大概率的k个action。</li></ul></li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="https://i.loli.net/2021/03/26/9trOPQS1pekXFGW.png" alt="image-20210326112702445" style="zoom: 67%;"></p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><p>没有细读论文，但是大体上是使用，多智能体强化学习来做非自回归图像描述任务。</p><p>这种以非自回归的方式来做text generation task的出发点就是自回归的方式推理速度慢。</p><p>但是非自回归的方式又会导致句子的一致性较差，因此，需要考虑一些方法来弥补这一不足。考虑的方式即为论文的创新点。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;研究内容&quot;&gt;&lt;a href=&quot;#研究内容&quot; class=&quot;headerlink&quot; title=&quot;研究内容&quot;&gt;&lt;/a&gt;研究内容&lt;/h2&gt;&lt;p&gt;图像描述任务&lt;/p&gt;
&lt;h2 id=&quot;存在的问题&quot;&gt;&lt;a href=&quot;#存在的问题&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Human-like Controllable Image Captioning with Verb-specific Semantic Roles</title>
    <link href="http://yoursite.com/2021/03/25/Human-like-Controllable-Image-Captioning-with-Verb-specific-Semantic-Roles/"/>
    <id>http://yoursite.com/2021/03/25/Human-like-Controllable-Image-Captioning-with-Verb-specific-Semantic-Roles/</id>
    <published>2021-03-25T11:49:21.000Z</published>
    <updated>2021-03-25T11:52:41.895Z</updated>
    
    <content type="html"><![CDATA[<h2 id="本文研究的内容"><a href="#本文研究的内容" class="headerlink" title="本文研究的内容"></a>本文研究的内容</h2><p>Controllable Image Captioning</p><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p>目前存在的objective control signals忽视了理想控制信号的两个必不可少的特征：</p><ul><li>Event-compatible</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;本文研究的内容&quot;&gt;&lt;a href=&quot;#本文研究的内容&quot; class=&quot;headerlink&quot; title=&quot;本文研究的内容&quot;&gt;&lt;/a&gt;本文研究的内容&lt;/h2&gt;&lt;p&gt;Controllable Image Captioning&lt;/p&gt;
&lt;h2 id=&quot;存在的问题&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="image captioning" scheme="http://yoursite.com/categories/cross-modal/image-captioning/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
      <category term="image captioning" scheme="http://yoursite.com/tags/image-captioning/"/>
    
  </entry>
  
  <entry>
    <title>[调研] non-autoregressive neural machine translation</title>
    <link href="http://yoursite.com/2021/03/24/%E8%B0%83%E7%A0%94-non-autoregressive-neural-machine-translation/"/>
    <id>http://yoursite.com/2021/03/24/调研-non-autoregressive-neural-machine-translation/</id>
    <published>2021-03-24T04:17:43.000Z</published>
    <updated>2021-03-24T04:18:56.253Z</updated>
    
    <content type="html"><![CDATA[<p>本篇对基于非自回归的机器翻译任务进行调研</p><h2 id="First-paper"><a href="#First-paper" class="headerlink" title="First paper"></a><strong style="color:blue;">First paper</strong></h2><p>Gu, J.; Bradbury, J.; Xiong, C.; Li, V. O.; and Socher, R. 2018. <strong>Non-autoregressive neural machine translation.</strong> In ICLR.</p><h2 id="Enhancing-the-decoder-inputs"><a href="#Enhancing-the-decoder-inputs" class="headerlink" title="Enhancing the decoder inputs"></a><strong style="color:blue;">Enhancing the decoder inputs</strong></h2><p>Bingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang Lin, and Xu Sun. 2019. <strong>Imitation learning for nonautoregressive neural machine translation.</strong> In ACL.</p><p>Wang, Y.; Tian, F.; He, D.; Qin, T.; Zhai, C.; and Liu, T.-Y. 2019b. <strong>Non-autoregressive machine translation with auxiliary regularization.</strong> In AAAI.</p><p>Guo, J.; Tan, X.; He, D.; Qin, T.; Xu, L.; and Liu, T.-Y. 2019. <strong>Non-autoregressive neural machine translation with enhanced decoder input.</strong> In AAAI.</p><p>Lee, J.; Mansimov, E.; and Cho, K. 2018. <strong>Deterministic nonautoregressive neural sequence modeling by iterative refinement.</strong> In EMNLP.</p><h2 id="Modeling-the-dependencies-among-target-outputs"><a href="#Modeling-the-dependencies-among-target-outputs" class="headerlink" title="Modeling the dependencies among target outputs"></a><strong style="color:blue;">Modeling the dependencies among target outputs</strong></h2><p>Ghazvininejad, M.; Levy, O.; Liu, Y.; and Zettlemoyer, L. 2019. <strong>Mask-predict: Parallel decoding of conditional masked language models.</strong> In EMNLP-IJCNLP.</p><p>Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, and Lei Li. 2020. <strong>Glancing transformer for non-autoregressive neural machine translation.</strong> arXiv preprint arXiv:2008.07905.</p><p>Gu, J.;Wang, C.; and Zhao, J. 2019. <strong>Levenshtein transformer.</strong> In Advances in NIPS.</p><p>Mansimov, E.; Wang, A.; and Cho, K. 2019. <strong>A generalized framework of sequence generation with application to undirected sequence models.</strong> arXiv preprint arXiv:1905.12790 .</p><ul><li><p><strong style="color:blue;">model the target-side dependencies in the latent space</strong></p><p>Non-Autoregressive Translation by Learning Target Categorical Codes. NAACL-2021</p><p><a href="https://mp.weixin.qq.com/s/qDN1ROXjkVI6wdTHa18YlA" target="_blank" rel="noopener">NAACL2021论文：基于隐式类别建模的非自回归式翻译</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本篇对基于非自回归的机器翻译任务进行调研&lt;/p&gt;
&lt;h2 id=&quot;First-paper&quot;&gt;&lt;a href=&quot;#First-paper&quot; class=&quot;headerlink&quot; title=&quot;First paper&quot;&gt;&lt;/a&gt;&lt;strong style=&quot;color:blue
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>[Retrieve Fast, Rerank] Smart Cooperative and Joint Approaches for Improved Cross-Modal Retrieval</title>
    <link href="http://yoursite.com/2021/03/23/Retrieve-Fast-Rerank-Smart-Cooperative-and-Joint-Approaches-for-Improved-Cross-Modal-Retrieval/"/>
    <id>http://yoursite.com/2021/03/23/Retrieve-Fast-Rerank-Smart-Cooperative-and-Joint-Approaches-for-Improved-Cross-Modal-Retrieval/</id>
    <published>2021-03-23T09:53:45.000Z</published>
    <updated>2021-03-23T12:03:32.198Z</updated>
    
    <content type="html"><![CDATA[<h2 id="现存的问题"><a href="#现存的问题" class="headerlink" title="现存的问题"></a>现存的问题</h2><ul><li>本文主要研究基于预训练跨模态模型<ul><li>pretrained from scratch and less scalable</li><li>huge retrieval latency</li></ul></li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><ul><li>为了同时提高模型<strong>性能</strong>与<strong>效率</strong>。提出了一个新颖的微调框架，可以将任意预训练的 text-image 多模态模型转化为一个有效的检索模型。</li><li>该框架基于对检索和重新排序进行协作，该方法结合了：（1）双胞胎网络分别对语料库的所有项目进行编码，从而实现有效的初始检索； 2）交叉编码器组件，用于对检索到的小项目集进行更细微（即更智能）的排名。 </li><li>我们还建议通过共享权重共同微调这两个分量，从而产生一个参数更有效的模型。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;现存的问题&quot;&gt;&lt;a href=&quot;#现存的问题&quot; class=&quot;headerlink&quot; title=&quot;现存的问题&quot;&gt;&lt;/a&gt;现存的问题&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;本文主要研究基于预训练跨模态模型&lt;ul&gt;
&lt;li&gt;pretrained from scratch and
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Large-Scale Zero-Shot Image Classification from Rich and Diverse Textual Descriptions</title>
    <link href="http://yoursite.com/2021/03/22/Large-Scale-Zero-Shot-Image-Classification-from-Rich-and-Diverse-Textual-Descriptions/"/>
    <id>http://yoursite.com/2021/03/22/Large-Scale-Zero-Shot-Image-Classification-from-Rich-and-Diverse-Textual-Descriptions/</id>
    <published>2021-03-22T11:59:13.000Z</published>
    <updated>2021-03-22T13:18:01.808Z</updated>
    
    <content type="html"><![CDATA[<h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><ul><li><p>【目前常用的benchmark都比较小，且常常使用手工标注的类别属性作为补充数据。但是这种类型的补充数据，由于是手工标注的因此很难扩展到大规模数据集上（eg: ImageNet）。因此，目前在ImageNet 这样的大规模数据集上的零样本学习性能比较差】</p><p>ZSL benchmarks mostly cover either a very <strong>small</strong> or narrow set of classes, where <strong>human-made class attributes</strong> are often used as auxiliary data. Unfortunately, on ImageNet, where such attributes are not available, the performance is still very low.</p></li><li><p>【小规模数据集提供的类别有限，有可能训练集，测试集都是动物类别，无法评估模型在一个新颖类别上的性能，比如汽车。因此对于零样本学习任务，有必要发展一个大规模的数据集】</p><p>The large-scale setup enables us to study the main challenges of a more realistic and practical zero-shot image classification scenario and study the generalization of models to novel groups of classes (e.g., animal species in general), not only  individual classes (e.g., specific animal species).</p></li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><ul><li><p>不对算法进行改进，而是研究 补充数据类型（auxiliary data type）对性能的影响。为 ImageNet class 收集对应的 wikipedia article，作为text descriptions。本文是第一个在大规模数据集上使用文本描述的。</p></li><li><p>在 ImageNet mp500 测试集上，使用本文提供的wikipedia文本描述作为补充数据，性能上取得了很大的提高，比以往的方法都好。</p></li><li>以前的小数据集受到类别有限的限制，无法评估在新颖类别上的泛化性。在本文提出的数据集上，证明了当前的ZSL model 的泛化性是比较差的。</li><li></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;存在的问题&quot;&gt;&lt;a href=&quot;#存在的问题&quot; class=&quot;headerlink&quot; title=&quot;存在的问题&quot;&gt;&lt;/a&gt;存在的问题&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;【目前常用的benchmark都比较小，且常常使用手工标注的类别属性作为补充数据。但是这种类型的补
      
    
    </summary>
    
      <category term="Image Classification" scheme="http://yoursite.com/categories/Image-Classification/"/>
    
    
      <category term="Image Classification" scheme="http://yoursite.com/tags/Image-Classification/"/>
    
  </entry>
  
  <entry>
    <title>[MDMMT] Multidomain Multimodal Transformer for Video Retrieval</title>
    <link href="http://yoursite.com/2021/03/22/MDMMT-Multidomain-Multimodal-Transformer-for-Video-Retrieval/"/>
    <id>http://yoursite.com/2021/03/22/MDMMT-Multidomain-Multimodal-Transformer-for-Video-Retrieval/</id>
    <published>2021-03-22T06:52:47.000Z</published>
    <updated>2021-03-22T07:54:56.006Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>How Many Data Points is a PromptWorth?</title>
    <link href="http://yoursite.com/2021/03/20/How-Many-Data-Points-is-a-PromptWorth/"/>
    <id>http://yoursite.com/2021/03/20/How-Many-Data-Points-is-a-PromptWorth/</id>
    <published>2021-03-20T03:24:45.000Z</published>
    <updated>2021-03-21T08:12:09.161Z</updated>
    
    <content type="html"><![CDATA[<p>转载：<a href="https://mp.weixin.qq.com/s/vX6o4lJKP4ajVfGSw8-tqg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/vX6o4lJKP4ajVfGSw8-tqg</a></p><blockquote><p>今天介绍的是一篇 NAACL’21 新鲜出炉的工作！NAACL 上周四出的结果，这篇工作本周一上传 arxiv，周二被王苏小哥哥发现，周三拜读了一下，今天就来和大家分享啦！！</p></blockquote><p>给大家提个问题：如果训练样本只有几百条，这时候我们该怎么办呢？</p><p>传统的 RNN 在这个样本大小下很难被训练好，自然地，我们会想到使用预训练模型，在其基础上进行 finetune。具体来讲，就是将预训练模型作为模型的底层，在上面添加与当前任务特点相关的网络结构。这样就引入了预训练的知识，对当前任务能产生很大的帮助。</p><p><img src="https://i.loli.net/2021/03/21/QodMJ9GWgv6fcUy.png" alt="微信截图_20210321123106" style="zoom:33%;"></p><p>除了预训练的知识，是不是还有其他的信息我们没有用上呢？近年来，越来越多的人在使用另一种 finetune 方法，即<strong>结合具体场景，设计新的 finetune 任务形式，从而将与当前任务相关的提示信息（prompt）引入模型</strong>。我们大名鼎鼎的 GPT 系列就是这么干的。比如我们拿 GPT3 做 QA 的 finetune，直接喂给他一串“<em>Question：问题内容 Answer：</em>”，剩下的答案部分就让 GPT3 自己填完。</p><p><img src="https://i.loli.net/2021/03/21/s8DHwgmNJY7Ryvk.png" alt="image-20210321123155512" style="zoom: 33%;"></p><p>这类 finetune 技巧虽然陆续被使用，但并没有人论证：<strong>这种做法相比于传统的 finetune 方法，真的能带来提升吗</strong>？如果答案是肯定的，<strong>那么究竟能提升多少呢（能否量化这种提升）？</strong></p><p>今天这篇来自 Huggingface 的文章就填补了上述两个问题的答案。他们通过大量实验证明：<strong>引入提示信息和多标注几百条数据带来的性能提升是相当的</strong>！所以，下次老板只给少量样本，就要你 finetune 模型——不要慌！我们今天又多学了一个 trick！</p><p><strong>论文题目</strong>:<br><strong><em>How Many Data Points is a Prompt Worth?</em></strong></p><p><strong>论文链接</strong>:<br><em><a href="https://arxiv.org/abs/2103.08493" target="_blank" rel="noopener">https://arxiv.org/abs/2103.08493</a></em></p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>前文提到，这一类 finetune 是将任务对应的输入改写成新的完形填空格式，让模型预测 <mask> 部分的词，作为抽取任务的答案或者生成任务的结果。这种方法不需要改变模型结构、没有额外的参数，简直不要太方便！</mask></p><h3 id="引入描述集合"><a href="#引入描述集合" class="headerlink" title="引入描述集合"></a>引入描述集合</h3><p>本文对这类方法进行了进一步简化：不要求 <mask> 处生成任意的文本，而是只需要完成类似于有选项的完形填空任务。这里的选项是固定的几个词，我们称之为描述集合（verbalizer），不同任务会有不同的描述集合。</mask></p><p>比如，对于判断题的阅读理解任务，就可以将阅读文本、问题和 <mask> 拼接，让预训练模型直接预测 <mask> 属于描述集合 {yes, no} 中的哪一种描述：</mask></mask></p><blockquote><p>小明天天码代码码到天明 [SEP] <strong>小明有女朋友吗？</strong> <mask></mask></p></blockquote><p>其中前半部分是阅读文本，后面<strong>加粗</strong>的部分是问题。模型只需要判断 <mask> 属于描述集合 {yes, no} 中的哪一种。</mask></p><p>可能读到这里，大家会疑惑：直接拼起来搞一个 True / False 的二分类不就好了嘛，何必让模型填空呢？嘿嘿，这恰好是作者的用意：通过让模型填空，<strong>模型可以习得描述集合中标签文本的语义信息</strong>。</p><h3 id="引入提示信息"><a href="#引入提示信息" class="headerlink" title="引入提示信息"></a>引入提示信息</h3><p>直接拼接是最朴素的，但这能让模型知道自己在做什么任务嘛？为此，作者引入了<strong>提示信息</strong>（prompt）。</p><p>还是判断题的阅读理解任务，对文章 和问题 ，作者将他们与一些固定的词进行整合，以此输入模型，让模型预测 <mask> 。作者提出了三种整合方式：</mask></p><p><img src="https://i.loli.net/2021/03/21/QEPwMG7bfI2UzNH.png" alt="image-20210321123632246" style="zoom: 33%;"></p><p>没错，就是这么简单！这些固定的词作为提示信息，让模型了解当前在做的任务；同时，提示词文本的含义也对于模型的理解产生了一定的帮助。</p><p>除了单选阅读理解，这篇文章还关注了文本蕴含、多选阅读理解、指代销歧等共六个任务。对于不同的任务，有不同的提示信息与输入格式：</p><p>对于文本蕴含任务，可以将前提 (premise, ) 与假设 (hyphothesis, ) 通过提示信息整合，作者提出了两种整合方式：</p><p><img src="https://i.loli.net/2021/03/21/k1ul7icFX69IKnL.png" alt="image-20210321123241288" style="zoom: 33%;"></p><p>这样就只需要让模型预测 <mask> 属于描述集合 {yes, no, maybe} 中的哪一种，以此判断前提能否支撑假设。</mask></p><p>对于指代销歧任务，可以将句子 、带标记的介词 与名词 通过提示信息整合：</p><p><img src="https://i.loli.net/2021/03/21/sRZIGzn2Pg76mpd.png" alt="image-20210321123253166" style="zoom: 33%;"></p><p>这样就只需要让模型预测 <mask> ，以此判断介词是否指代名词。这里的描述集合是不受限制的，即让模型在 <mask> 处预测指代的名词 。</mask></mask></p><p>其他任务也采用类似的整合方式，感兴趣可以参考原文～</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者发现，这种使用提示信息的整合方式，在低资源的情况下对模型性能有非常大的提升！</p><p>比如在阅读理解任务的 BoolQ 数据集上，作者将使用提示信息整合的 finetune 方法与增加一层分类层的 finetune 方法进行了对比。下图是在使用不同数量的样本训练时，模型准确率的对比。</p><p><img src="https://i.loli.net/2021/03/21/31cqsF5Q7VoSeLX.png" alt="image-20210321123431996" style="zoom:50%;"></p><p>可以发现，在数据量比较小的时候，使用提示信息整合的 finetune 方法（黄色）比增加一层分类层的 finetune 方法（紫色）有更好的表现。</p><p>在某些任务上，这种表现的提升是惊人的：</p><p><img src="https://i.loli.net/2021/03/21/WCo9uUakm1rOH8N.png" alt="image-20210321123504144" style="zoom:50%;"></p><p>这是在指代销歧任务的 WSC 数据集上的实验结果。在水平方向看，<strong>仅使用 25 个样本，就达到传统 fintune 方法使用 300 个样本才能达到的效果！</strong></p><p>此外，作者还进行了一系列的消融实验，得到一些有意思的结论：</p><ol><li>模型通过预测 <mask> 属于描述集合中的哪种，以此完成任务。如果将这里改为不带语义的单纯的分类，性能也会有所下降。</mask></li><li>作者为每个任务都提供了多种整合提示信息的方式，但是发现，不同方式的区别对性能影响甚微。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇文章对基于提示信息的 finetune 方法在进行了大量实验，证明了这类方法在低资源的情况下性能大幅优于传统方法。这种 finetune 的思路应该是可以应用于各类 NLP 下游任务的。尤其是低资源场景下，应该会非常有帮助。如果老板真的只给几百条数据让训练模型，这样的方法说不定就有奇效！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载：&lt;a href=&quot;https://mp.weixin.qq.com/s/vX6o4lJKP4ajVfGSw8-tqg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://mp.weixin.qq.com/s/vX6o4lJKP4ajVfGS
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="few-shot" scheme="http://yoursite.com/categories/NLP/few-shot/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="few-shot" scheme="http://yoursite.com/tags/few-shot/"/>
    
  </entry>
  
  <entry>
    <title>[It’s Not Just Size That Matters] Small Language Models Are Also Few-Shot Learners</title>
    <link href="http://yoursite.com/2021/03/20/It%E2%80%99s-Not-Just-Size-That-Matters-Small-Language-Models-Are-Also-Few-Shot-Learners/"/>
    <id>http://yoursite.com/2021/03/20/It’s-Not-Just-Size-That-Matters-Small-Language-Models-Are-Also-Few-Shot-Learners/</id>
    <published>2021-03-20T02:51:52.000Z</published>
    <updated>2021-03-21T08:12:00.787Z</updated>
    
    <content type="html"><![CDATA[<p>转载：<a href="https://www.sohu.com/a/422484297_500659" target="_blank" rel="noopener">https://www.sohu.com/a/422484297_500659</a></p><p>显然，这标题对标的就是 GPT-3，于是笔者饶有兴趣地点进去看看是谁这么有勇气挑战 GPT-3，又是怎样的小模型能挑战 GPT-3？经过阅读，原来作者提出通过适当的构造， <strong>用 BERT 的 MLM 模型</strong>也可以做小样本学习，看完之后颇有一种“原来还可以这样做”的恍然大悟感。在此与大家分享一下。</p><h2 id="冉冉升起的MLM"><a href="#冉冉升起的MLM" class="headerlink" title="冉冉升起的MLM"></a><strong>冉冉升起的MLM</strong></h2><p>MLM，全称“Masked Language Model”，可以翻译为“掩码语言模型”，实际上就是一个完形填空任务，随机 Mask 掉文本中的某些字词，然后要模型去预测被 Mask 的字词，示意图如下：</p><p><img src="https://i.loli.net/2021/03/20/izGapk4S6ZRqgFI.png" alt="img" style="zoom:50%;"></p><p>▲ BERT的MLM模型简单示意图</p><p>其中被 Mask 掉的部分，可以是直接随机选择的 Token，也可以是随机选择连续的能组成一整个词的 Token，后者称为 WWM（Whole Word Masking）。</p><p>开始，MLM 仅被视为 BERT 的一个预训练任务，训练完了就可以扔掉的那种，因此有一些开源的模型干脆没保留 MLM 部分的权重，比如 brightmart 版 [3] 和 clue 版 [4] 的 RoBERTa，而哈工大开源的 RoBERTa-wwm-ext-large [5]则不知道出于什么原因随机初始化了 MLM 部分的权重，因此如果要复现本文后面的结果，这些版本是不可取的。</p><p>然而，随着研究的深入，研究人员发现不止 BERT 的 Encoder 很有用，预训练用的 MLM 本身也很有用。</p><p>比如论文 <strong>BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model</strong>[6]指出 MLM 可以作为一般的生成模型用，论文 <strong>Spelling Error Correction with Soft-Masked BERT</strong>[7] 则将 MLM 用于文本纠错。</p><p>笔者之前在 的实验也表明 MLM 的预训练权重也可以当作 UniLM 来用做 Seq2Seq 任务，还有一文将 MLM 的思想用于无监督分词和句法分析了。可以说 MLM 已经是大放异彩了。</p><h2 id="将任务转成完形填空"><a href="#将任务转成完形填空" class="headerlink" title="将任务转成完形填空"></a><strong>将任务转成完形填空</strong></h2><p>在本文里，我们再学习 MLM 的一个精彩应用：用于小样本学习或半监督学习，某些场景下甚至能做到零样本学习。</p><p>怎么将我们要做的任务跟 MLM 结合起来呢？很简单， <strong>给任务一个文本描述，然后转换为完形填空问题</strong>即可。举个例子，假如给定句子“这趟北京之旅我感觉很不错。”，那么我们补充个描述，构建如下的完形填空：</p><blockquote><p> <strong>__</strong>满意。这趟北京之旅我感觉很不错。</p></blockquote><p>进一步地，我们限制空位处只能填一个“很”或“不”，问题就很清晰了，就是要我们根据上下文一致性判断是否满意，如果“很”的概率大于“不”的概率，说明是正面情感倾向，否则就是负面的，这样我们就将<strong>情感分类问题</strong>转换为一个完形填空问题了，它可以用 MLM 模型给出预测结果，而 MLM 模型的训练可以不需要监督数据，因此理论上这能够实现零样本学习了。</p><p><strong style="color:blue;">多分类问题</strong>也可以做类似转换，比如<strong>新闻主题分类</strong>，输入句子为“八个月了，终于又能在赛场上看到女排姑娘们了。”，那么就可以构建：</p><blockquote><p> 下面播报一则<strong>__</strong>新闻。八个月了，终于又能在赛场上看到女排姑娘们了。</p></blockquote><p>这样我们就将新闻主题分类也转换为完形填空问题了，一个好的 MLM 模型应当能预测出“体育”二字来。</p><p>还有一些<strong style="color:blue;">简单的推理任务</strong>也可以做这样的转换，常见的是给定两个句子<strong>，判断这两个句子是否相容</strong>，比如“我去了北京”跟“我去了上海”就是矛盾的，“我去了北京”跟“我在天安门广场”是相容的，常见的做法就是将两个句子拼接起来输入到模型做，作为一个二分类任务。如果要转换为完形填空，那该怎么构造呢？一种比较自然的构建方式是：</p><blockquote><p>我去了北京？<strong>__</strong>，我去了上海。</p><p>我去了北京？<strong>__</strong>，我在天安门广场。</p><p>其中空位之处的候选词为 是 的 不 是 。</p></blockquote><h2 id="Pattern-Exploiting-Training"><a href="#Pattern-Exploiting-Training" class="headerlink" title="Pattern-Exploiting Training"></a><strong>Pattern-Exploiting Training</strong></h2><p>读到这里，读者应该不难发现其中的规律了，就是给输入的文本增加一个前缀或者后缀描述，并且 Mask 掉某些 Token，转换为完形填空问题，这样的转换在原论文中称为 <strong>Pattern</strong>，这个转换要尽可能与原来的句子组成一句自然的话，不能过于生硬，因为预训练的 MLM 模型就是在自然语言上进行的。</p><p>显然同一个问题可以有很多不同的 Pattern，比如情感分类的例子，描述可以放最后，变成“这趟北京之旅我感觉很不错。<strong><strong>满意。”；也可以多加几个字，比如“觉得如何？</strong></strong>满意。这趟北京之旅我感觉很不错。”。</p><p>然后，我们需要构建预测 Token 的候选空间，并且建立 Token 到实际类别的映射，这在原论文中称为 <strong>Verbalizer</strong>，比如情感分类的例子，我们的候选空间是 很 不 ，映射关系是 很 正 面 不 负 面 ，候选空间与实际类别之间不一定是一一映射，比如我们还可以加入“挺”、“太”、“难”字，并且认为 很 挺 太 正 面 以 及 不 难 负 面 ，等等。</p><p>不难理解，不少 NLP 任务都有可能进行这种转换，但显然这种转换一般只适用于 <strong>候选空间有限</strong>的任务，说白了就是只用来做 <strong>选择题</strong>，常见任务的就是 <strong>文本分类</strong>。</p><p>刚才说了，同一个任务可以有多种不同的 Pattern，原论文是这样处理的：</p><ol><li><p>对于每种 Pattern，单独用训练集 Finetune一个 MLM 模型出来；</p></li><li><p>然后将不同 Pattern对应的模型进行集成，得到融合模型；</p></li><li><p>用融合模型预测未标注数据的伪标签；</p></li><li><p>用伪标签数据 Finetune 一个常规的（非 MLM 的）模型。</p></li></ol><p>具体的集成方式大家自己看论文就行，这不是重点。这种训练模式被称为 <strong>Pattern-Exploiting Training（PET）</strong>，它首先出现在论文 <strong>Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</strong>。</p><p><strong style="color:blue;">yaya: 与 这篇论文思想很类似：How Many Data Points is a PromptWorth? （arXiv: 2103.08493v1）</strong></p><p>本文要介绍的这篇论文则进一步肯定和完善了 Pattern-Exploiting Training 的价值和结果，并整合了多任务学习，使得它在 SuperGLUE 榜单上的小样本学习效果超过了 GPT3。两篇论文的作者是相同的，是一脉相承的作品。</p><p><img src="https://i.loli.net/2021/03/20/dOMZNLsKykIDioQ.png" alt="img"></p><p>▲ PET在SuperGLUE上的小样本学习的结果</p><p>不过要吐槽一个点是，上图中 PET 的 223M 参数，所用的模型是 ALBERT-xxlarge-v2，事实上称 ALBERT 为“小模型”是一种很耍流氓的行为，因为它前向计算的速度并没有得到任何提升。ALBERT-xxlarge 共有 12 层，层与层之间参数是共享的，就前向计算而言，它应该等价于约 2700M（12 倍）参数的 GPT 才对。</p><h2 id="中文实践，检验效果"><a href="#中文实践，检验效果" class="headerlink" title="中文实践，检验效果"></a><strong>中文实践，检验效果</strong></h2><p>要真正确认一个方法或模型的价值，看论文的实验表格是不够的，论文给出的实验结果谁都不好说能否复现，其次就算英文上能复现也不代表中文上有价值，因此最实际的还是亲自动手做实验验证。下面是笔者的实验代码，供读者参考：</p><p>Github 地址：</p><p><a href="https://github.com/bojone/Pattern-Exploiting-Training" target="_blank" rel="noopener">https://github.com/bojone/Pattern-Exploiting-Training</a></p><p>我们将从以下几个角度来探讨 PET 的可行性：</p><p>\1. 直接利用现成的 MLM 模型效果如何？ <strong>（零样本学习1）</strong></p><p>\2. 用“大量无标签数据”微调现成的 MLM 模型效果如何？ <strong>（零样本学习2）</strong></p><p>\3. 用“小量标签数据”微调现成的 MLM 模型效果如何？ <strong>（小样本学习）</strong></p><p>\4. 用“小量标签数据+大量无标签数据”微调现成的MLM模型效果如何？ <strong>（半监督学习）</strong></p><p>下面主要给出 <strong>情感二分类</strong>的实验结果。另外还有一个新闻主题的多分类，代码也放到 Github 了，其结果是类似的，就不重复陈述了。</p><h3 id="4-1-零样本学习1"><a href="#4-1-零样本学习1" class="headerlink" title="4.1 零样本学习1"></a><strong>4.1 零样本学习1</strong></h3><p>这里主要探索的是给输入文本补上对应的 Pattern 后，直接基于现成的 MLM 模型进行预测，预测的准确率。由于构建模型的整个过程都不涉及到标签数据监督训练，因此这算是一种“零样本学习”。我们需要比较的是不同 Pattern、不同 MLM 模型上的效果：</p><p>下面是实验的几个 Pattern，其中空位处候选词语都为“很”和“不”：</p><p>P1：____满意。这趟北京之旅我感觉很不错。</p><p>P2：这趟北京之旅我感觉很不错。____满意。</p><p>P3：____好。这趟北京之旅我感觉很不错。</p><p>P4：____理想。这趟北京之旅我感觉很不错。</p><p>P5：感觉如何？____满意。这趟北京之旅我感觉很不错。</p><p>至于 MLM 模型，则是下面几个：</p><p>M1：Google 开源的中文版 BERT Base：</p><p><a href="https://github.com/google-research/bert" target="_blank" rel="noopener">https://github.com/google-research/bert</a></p><p>M2：哈工大开源的 RoBERTa-wwm-ext Base：</p><p><a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">https://github.com/ymcui/Chinese-BERT-wwm</a></p><p>M3：腾讯 UER 开源的 BERT Base：</p><p><a href="https://share.weiyun.com/5QOzPqq" target="_blank" rel="noopener">https://share.weiyun.com/5QOzPqq</a></p><p>M4：腾讯 UER 开源的 BERT Large：</p><p><a href="https://share.weiyun.com/5G90sMJ" target="_blank" rel="noopener">https://share.weiyun.com/5G90sMJ</a></p><p>实验结果如下表（验证集/测试集）：</p><p><img src="https://i.loli.net/2021/03/20/8Yg2qH5CXoFPkcx.png" alt="img"></p><p>可以观察到，不同的 Pattern、不同的预训练模型之间还是有一定的差异的，整体而言 Large 版本的效果要明显好于 Base 版本的模型，说明像 GPT 到 GPT2 再到 GPT3 一样，还是把模型做得更大会更好。</p><p>此外，这还有可能说明实际上 MLM 还没有被充分训练好，或许是因为 BERT 这种 Mask 掉一部分的训练方式过于低效了，可能用 修改 Transformer 结构，设计一个更快更好的 MLM 模型 一文提到的改进版 MLM 会更好。</p><h3 id="4-2-零样本学习2"><a href="#4-2-零样本学习2" class="headerlink" title="4.2 零样本学习2"></a><strong>4.2 零样本学习2</strong></h3><p>看完上述结果，读者可能会想到：如果我用领域内的数据继续预训练 MLM 模型，那么能不能提升效果呢？答案是：能！下面是我们的实验结果，算力有限，我们只在 RoBERTa-wwm-ext（上述的 M2，继续预训练后的模型我们称为 M2+ 无监督）的基础上做了比较：</p><p><img src="https://i.loli.net/2021/03/20/Y8CdHbW1cjqByrD.png" alt="img"></p><p>要注意的是，这里我们只是用领域内的数据继续做 MLM 训练，这个过程是无监督的，也不需要标注信号，因此也算是“零样本学习”。同时，从到目前为止的结果我们可以看出，给输入本文加入“前缀”的效果比“后缀”更有优势一些。</p><h3 id="4-3-小样本学习"><a href="#4-3-小样本学习" class="headerlink" title="4.3 小样本学习"></a><strong>4.3 小样本学习</strong></h3><p>刚才我们讨论了无标签数据继续预训练 MLM 的提升，如果回到 PET 的目标场景，直接用小量的标签数据配合特定的 Pattern 训练 MLM 又如何呢？</p><p>这也就是真正的“小样本学习”训练了，这里我们保留约 200 个标注样本，构造样本的时候，我们先给每个句子补上 Pattern，除了 Pattern 自带的 Mask 位置之外，我们还随机 Mask 其他一部分，以增强对模型的正则。最终实验结果如下：</p><p><img src="https://i.loli.net/2021/03/20/qVFgDbyLQrXpZUt.png" alt="img"></p><p>结论就是除了“后缀式”的 P2 之外，其它结果都差不多，这进一步说明了“前缀式”的 Pattern 会比“后缀式”更有竞争力一些。在效果上，直接用同样的数据用常规的方法去微调一个 BERT 模型，大概的结果是 88.93 左右，所以基于 “MLP+Pattern” 的小样本学习方法可能带来轻微的性能提升。</p><h3 id="4-4-半监督学习"><a href="#4-4-半监督学习" class="headerlink" title="4.4 半监督学习"></a><strong>4.4 半监督学习</strong></h3><p>无监督的零样本学习和有监督的小样本学习都说完了，自然就轮到把标注数据和非标注数据都结合起来的“半监督学习”了。还是同样的任务，标注数据和非标注数据的比例大约是 1:99，标注数据带 Pattern，非标注数据不带 Pattern，大家都 Mask 掉一部分 Token 进行 MLM 预训练，最终测出来的效果如下：</p><p><img src="https://i.loli.net/2021/03/20/4QKcUYDtgAmeGqM.png" alt="img"></p><p>还是同样的，“后缀”明显比“前缀”差，“前缀”的效果差不多。具体效果上，则是肯定了额外的无标注数据也是有作用的。</p><p>直觉上来看，“前缀”比“后缀”要好，大体上是因为“前缀”的 Mask 位置比较固定，微弱的监督信号得以叠加增强？但这也不能解释为什么零样本学习的情况下也是“前缀”更好，估计还跟模型的学习难度有关系，可能句子前面部分的规律更加明显，相对来说更加容易学一些，所以前面部分就学习得更加充分？这一切都还只是猜测。</p><h3 id="4-5-汇总与结论"><a href="#4-5-汇总与结论" class="headerlink" title="4.5 汇总与结论"></a><strong>4.5 汇总与结论</strong></h3><p>将上述结果汇总如下：</p><p><img src="https://i.loli.net/2021/03/20/cpGqLZ2twR5z4di.png" alt="img"></p><p>读者还可以对比我们之前在文章 泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练 中用虚拟对抗训练（VAT）做半监督学习的结果，可以看到不管是零样本学习、小样本学习还是半监督学习，基于 MLM 模型的方式都能媲美基于 VAT 的半监督学习的结果。</p><p>我们在做短新闻多分类实验时的结果也是相似的。因此，这说明了 MLM 模型确实也可以作为一个优秀的零样本/小样本/半监督学习器来使用。</p><p>当然，基于 MLM 模型的缺点还是有的，比如 MLM 所使用的独立假设限制了它对更长文本的预测能力（说白了空位处的文字不能太长），以及无法预测不定长的答案也约束了它的场景（所以当前只能用于做选择题）。我们期待有更强的 MLM 模型出现，那时候就有可能在所有任务上都能与 GPT3 一较高下了。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文介绍了 BERT 的 MLM 模型的一个新颖应用：配合特定的描述将任务转化为完形填空，利用 MLM 模型做零样本学习、小样本学习和半监督学习。</p><p>在原论文的 SuperGLUE 实验里边，它能达到媲美 GPT3 的效果，而笔者也在中文任务上做了一些实验，进一步肯定了该思路的有效性。整个思路颇为别致，给人一种“原来还可以这样做”的恍然大悟感，推荐大家学习一下。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] <a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">https://arxiv.org/abs/2005.14165</a></p><p>[2] <a href="https://arxiv.org/abs/2009.07118" target="_blank" rel="noopener">https://arxiv.org/abs/2009.07118</a></p><p>[3] <a href="https://github.com/brightmart/roberta_zh" target="_blank" rel="noopener">https://github.com/brightmart/roberta_zh</a></p><p>[4] <a href="https://github.com/CLUEbenchmark/CLUEPretrainedModels" target="_blank" rel="noopener">https://github.com/CLUEbenchmark/CLUEPretrainedModels</a></p><p>[5] <a href="https://github.com/ymcui/Chinese-BERT-wwm" target="_blank" rel="noopener">https://github.com/ymcui/Chinese-BERT-wwm</a></p><p>[6] <a href="https://arxiv.org/abs/1902.04094" target="_blank" rel="noopener">https://arxiv.org/abs/1902.04094</a></p><p>[7] <a href="https://kexue.fm/archives/7661" target="_blank" rel="noopener">https://kexue.fm/archives/7661</a></p><p>[8] <a href="https://arxiv.org/abs/2001.07676" target="_blank" rel="noopener">https://arxiv.org/abs/2001.07676</a></p><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><p>不懂的问题：</p><ul><li><p>mask 一个 Span, 多个空位然后逐词预测？？</p></li><li><p>在 [MASK] 位置 预测空间是多大？整个vocabulary ??</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载：&lt;a href=&quot;https://www.sohu.com/a/422484297_500659&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.sohu.com/a/422484297_500659&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;显然
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="few-shot" scheme="http://yoursite.com/categories/NLP/few-shot/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="few-shot" scheme="http://yoursite.com/tags/few-shot/"/>
    
  </entry>
  
  <entry>
    <title>[All NLP Tasks Are Generation Tasks] A General Pretraining Framework</title>
    <link href="http://yoursite.com/2021/03/20/All-NLP-Tasks-Are-Generation-Tasks-A-General-Pretraining-Framework/"/>
    <id>http://yoursite.com/2021/03/20/All-NLP-Tasks-Are-Generation-Tasks-A-General-Pretraining-Framework/</id>
    <published>2021-03-20T01:53:44.000Z</published>
    <updated>2021-03-21T07:13:29.600Z</updated>
    
    <content type="html"><![CDATA[<h2 id="yaya-总结"><a href="#yaya-总结" class="headerlink" title="yaya 总结"></a>yaya 总结</h2><ul><li><p>模型结构上</p><p><strong style="color:red;">GLM 采用的是Transformer Model 中decoder 结构，而BERT采用的是 encoder 结构</strong></p></li><li><p>预训练任务上：<br>提出了两个 multi short span recover (benefit for NLU) and  a single longer span recover (benefit for NLG) 的预训练任务。</p><p>与这篇做 text filling 论文很相似：Enabling Language Models to Fill in the Blanks</p></li><li><p>下游任务的微调：</p><p>受到[1] [2] 的启发， 将分类任务转化为文本生成的填空任务。</p></li></ul><h2 id="1-存在的问题"><a href="#1-存在的问题" class="headerlink" title="1. 存在的问题"></a>1. 存在的问题</h2><p>目前基于预训练 的语言模型大致分为三类：</p><ul><li>autoregressive models (e.g.,GPT) 擅长长文本生成</li><li>autoencoding models (e.g., BERT) 擅长理解型任务，分类任务</li><li>encoder-decoder models (e.g., T5) 擅长基于条件的文本生成任务，比如 text summarize</li></ul><p>但是目前还未存在一个预训练框架可以在这三种任务上同时表现出优异的性能。这给模型的开发和选择带来了不便。</p><p>下表总结了不同的预训练框架可以处理的任务：</p><p><img src="https://i.loli.net/2021/03/20/ldKAem71a86xBcq.png" alt="image-20210320100920079" style="zoom: 25%;"></p><p>先前的工作试图通过多任务学习将各自的 objective 结合起来，从而统一不同的框架。但是，自回归和自编码的 objective 在本质上是不同的，简单的结合不能够充分的揭示所有框架的优势。</p><h2 id="2-本文的点"><a href="#2-本文的点" class="headerlink" title="2. 本文的点"></a>2. 本文的点</h2><h3 id="2-1-新颖的预训练框架GLM"><a href="#2-1-新颖的预训练框架GLM" class="headerlink" title="2.1 新颖的预训练框架GLM"></a>2.1 新颖的预训练框架GLM</h3><p>本文提出了一个新颖的预训练框架GLM（General Language Model）来解决这个问题。如图1。</p><p><img src="https://i.loli.net/2021/03/20/VrzfCnOFQi27NRU.png" alt="image-20210320103954828" style="zoom: 33%;"></p><ul><li>本文的预训练模型GLM基于autoregressive blank-filling（预训练方案），<strong style="color:red;">遵循自动编码(auto-encoding)的思想，我们从输入文本中随机消除了令牌的连续跨度。并遵循自回归预训练(auto-regressive)的思想训练模型以重建跨度。</strong></li><li>为了在一个框架中同时学习双向和单向的注意力机制，本文将文本分成两部分，未掩码的部分可以互相关注。掩码的部分不可以关注后续的掩码的token。</li><li>本文还提出了一个 2D位置编码技术，来指示inter- and intra- span position information。</li><li></li></ul><p>因此，本文的框架 GLM在预训练过程中，可以同时学习上下文表达和自回归生成。</p><h3 id="2-2-多任务预训练方案"><a href="#2-2-多任务预训练方案" class="headerlink" title="2.2 多任务预训练方案"></a>2.2 多任务预训练方案</h3><p>为了使本文的预训练模型更加适合文本生成任务，本文也研究了一个多任务预训练的设置：（1）采样多个short spans, 目标是重构masked spans，该预训练任务对下游NLU任务有益处（2）采样单个 longer span，目标是回复该单个spans。该预训练任务对下游NLG任务有益处。</p><p>这种多任务预训练方案，在理解型任务，条件生成任务和具有共享参数的语言建模任务方面均有改善。</p><h3 id="2-3-pretrain-finetune-consistency"><a href="#2-3-pretrain-finetune-consistency" class="headerlink" title="2.3 pretrain-finetune consistency"></a>2.3 pretrain-finetune consistency</h3><p>在下游任务微调GLM时，受到以下两篇文章[1] [2] 的启发，构建为blank-filling generation的形式。每个任务都与一个人工制作的完形填空问题相关联，并且该模型可以预测完形填空的答案。例如，情感分类任务被重新构造为一个 <strong>“[SENTENCE]. It’s really __ ”.</strong> 这种格式的填空任务。对于”good” or “bad” 的预测暗示了情感是积极地还是消极地。</p><p>在这种格式下，GLM 在预训练和微调的一致中受益。因为<strong style="color:blue;">预训练和微调都涉及到以给定上下文来生成文本的方式来训练模型</strong>。因此，GLM相比于BERT-like models 更适合下游分类任务。<strong style="color:red;"><strong>yaya：这里的因此，好像不能推断出来</strong></strong></p><h2 id="3-贡献"><a href="#3-贡献" class="headerlink" title="3. 贡献"></a>3. 贡献</h2><p>本文的结构有三个主要的优势：</p><ul><li>在一个预训练模型上，可以在三种任务上都表现的很好。</li><li>由于 <strong style="color:blue;">pretrain-finetune consistency</strong>，在分类任务上，本文提出的模型相比 BERT-like models 性能更加优异。</li><li>可以自然的处理 <strong style="color:red;">variable-length blank filling</strong>，这对很多下游任务是很重要的。</li></ul><h2 id="4-Method"><a href="#4-Method" class="headerlink" title="4. Method"></a>4. Method</h2><h3 id="4-1-Model-Architecture"><a href="#4-1-Model-Architecture" class="headerlink" title="4.1 Model Architecture"></a>4.1 Model Architecture</h3><p>本文提出的结构 GLM 与 BERT很相似。Following Megatron-LM, 对BERT的结构做了两点改动。（1）rearrange the order of layer normalization and the residual connection。（2）replace the feed-forward network for token prediction with a linear layer。</p><h3 id="4-2-Autoregressive-Blank-Infilling"><a href="#4-2-Autoregressive-Blank-Infilling" class="headerlink" title="4.2 Autoregressive Blank Infilling"></a>4.2 Autoregressive Blank Infilling</h3><p>通过优化 autoregressive blank infilling 任务对GLM进行训练。</p><p>给定 an input text $\boldsymbol{x}=\left[x_{1}, \cdots, x_{n}\right]$，多个被采样 text spans {$s_{1},…,s_{m}$} ，每个span $s_{i}$ 是一系列连续的tokens $\left[s_{i, 1}, \cdots, s_{i, l_{i}}\right]$。text spans 的数量和长度取决于预训练目标（将会在下文中被介绍）。</p><p>该模型以自回归的方式从损坏的文本中预测 span 中丢失的 tokens，这意味着在预测 span 中丢失的 tokens，模型可以访问损坏的文本<em>和</em>先前预测的spans。为了充分捕捉不同span之间的相互依存关系，我们随机地排列span的顺序。yaya: 以下公式中 $\boldsymbol{s}_{\boldsymbol{z}_{&lt;i}}$ 被随机排列了，并不是按照其在句子中的顺序。</p><p>预训练目标为：$\max _{\theta} \mathbb{E}_{\boldsymbol{z} \sim Z_{m}}\left[\sum_{i=1}^{m} \log p_{\theta}\left(\boldsymbol{s}_{z_{i}} \mid \boldsymbol{x}_{\text {corrupt }}, \boldsymbol{s}_{\boldsymbol{z}_{&lt;i}}\right)\right]$</p><p>该任务与SpanBERT 的区别在于 <strong>模型不知道span中丢失tokens 的数量</strong>。</p><p>具体来说，我们使用以下技巧实现了自动回归的空白填充任务。输入令牌分为两部分。A部分包含损坏的文本$x_{corrupt}$ ,其中采样的text span 被替换为 [MASK] 令牌。B部分由masked span 中的tokens 组成。A部分中的 tokens 可以 attend to A中的所有tokens ，但不能 attend to B中的任何tokens 。B部分中的tokens 可以 attend to A中的tokens 及其在B中的先行词，但不能attend to B中的任何后续位置。</p><p><strong style="color:red;">与原始Transformer 模型中的decoder 相似</strong>，span中的tokens被补充两个特殊token [START]和 [END]，以这种方式，本文提出的模型可以同时学习（1）一个双向encoder（PART A）和（2）一个单向decoder (PART B)。如下图2所示。</p><p><img src="https://i.loli.net/2021/03/21/i4AEV3J2psDv1aT.png" alt="image-20210321135425429"></p><h4 id="4-2-1-2D-Positional-Encoding"><a href="#4-2-1-2D-Positional-Encoding" class="headerlink" title="4.2.1 2D Positional Encoding"></a>4.2.1 2D Positional Encoding</h4><p>每个 token 都使用两个 position ids 进行编码。</p><p>第一个 position id 代表corrupted text 中的位置。对于B中的token，它是对应的[MASK] token的position。</p><p>第二位置id表示intra-span position。对于A中的令牌，第二个位置ID为0。对于B中的令牌，范围为1到span的长度。</p><p>这两个 position ids  通过两个单独的 embedding table 投影到两个位置向量中，并添加到 input embeddings 中。</p><h3 id="4-3-Pre-Training-Objectives"><a href="#4-3-Pre-Training-Objectives" class="headerlink" title="4.3 Pre-Training Objectives"></a>4.3 Pre-Training Objectives</h3><p>采样：the masked spans make up 15% of the original tokens.</p><p>span leagth:  drawn from a Poisson distribution with $\lambda$= 3</p><p>与其他BERT样式的模型类似，GLM 对 short spans 进行掩码，适用于NLU任务。 但是，我们对单个预训练模型可以同时处理NLU和text generation 感兴趣。</p><p>我们进一步研究了<em>多任务预训练</em>设置，第二个目标：生成更长文本。并与GLM联合优化。具体来说，我们采样了a single span 覆盖原始文本长度的50％–100％。跨度长度是从均匀分布中采样的。以与原始目标相同的方式定义新目标。唯一的区别是只有一个跨度，但跨度更长。</p><h3 id="4-4-Finetuning-GLM"><a href="#4-4-Finetuning-GLM" class="headerlink" title="4.4 Finetuning GLM"></a>4.4 Finetuning GLM</h3><h4 id="NLU-task"><a href="#NLU-task" class="headerlink" title="NLU task"></a>NLU task</h4><p><strong>对于NLU 任务，以前的PLMs 存在预训练-微调目标不一致的问题</strong>， 具体解释如下：</p><p>先前的方法处理NLU任务，通常采用将预训练模型得到的representation送入一个线性分类层中来预测答案。对于token classification: 使用 token representation；对于 sentence classification：使用 [CLS] token representation。但是对于预训练任务采用的是cloze filling task 。这就导致了预训练-微调目标不一致的问题。</p><p>本文中，将NLU中的分类任务定义为 blank filling task。</p><p>给定一个标注样本（$x$, y），经由一个包含了 single mask token 的 pattern。</p><p>将输入文本 $x$ 映射成一个 cloze question $c(x)$ 。情感分类任务被重新构造为一个 <strong>“[SENTENCE]. It’s really [MASK] ”.</strong>  </p><p>标签 $y$ 也映射为填空问题的答案，称为 verbalizer $v(y)$ 。在情感分类任务中消极和积极被映射到单词“好”或“坏”。</p><p>Therefore, the conditional probability of $y$ given $\boldsymbol{x}$ is</p><script type="math/tex; mode=display">p(y \mid \boldsymbol{x})=\frac{p(v(y) \mid c(\boldsymbol{x}))}{\sum_{y^{\prime} \in \mathcal{Y}} p\left(v\left(y^{\prime}\right) \mid c(\boldsymbol{x})\right)}</script><p>where $\mathcal{Y}$ is the label set. Then we can finetune GLM with the cross entropy loss.</p><h4 id="NLG-task"><a href="#NLG-task" class="headerlink" title="NLG task"></a>NLG task</h4><p>对于文本生成任务，可以直接的将GLM作为一个自回归模型来使用。</p><p>给定的上下文构成了输入的A部分，其中有一个[MASK]结尾的令牌。然后，GLM自动在B部分中生成文本。我们可以将预训练的GLM直接应用于无条件生成，也可以在下游的有条件生成任务上微调GLM。</p><p><img src="https://i.loli.net/2021/03/21/k3E8uCchorg5LO9.png" alt="image-20210321151303074" style="zoom:80%;"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] [It’s Not Just Size That Matters] Small Language Models Are Also Few-Shot Learners</p><p>[2] Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;yaya-总结&quot;&gt;&lt;a href=&quot;#yaya-总结&quot; class=&quot;headerlink&quot; title=&quot;yaya 总结&quot;&gt;&lt;/a&gt;yaya 总结&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;模型结构上&lt;/p&gt;
&lt;p&gt;&lt;strong style=&quot;color:red;&quot;&gt;G
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Transformer</title>
    <link href="http://yoursite.com/2021/03/19/Transformer/"/>
    <id>http://yoursite.com/2021/03/19/Transformer/</id>
    <published>2021-03-19T10:58:13.000Z</published>
    <updated>2021-03-19T11:32:07.770Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/03/19/DrhRUENTcwXPo89.png" alt="image-20210319185913348" style="zoom:50%;"></p><p>Transformer模型中采用了 encoer-decoder 架构。论文中encoder层由6个encoder堆叠在一起，decoder层也一样。</p><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>对于encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。</p><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>decoder也包含encoder提到的两层网络，但是在这两层中间还有一层attention层，帮助当前节点获取到当前需要关注的重点内容。</p><p>Decoder 和 Encoder的结构差不多，但是多了一个attention的sub-layer，这里先明确一下decoder的输入输出和解码过程：</p><ul><li>输出：对应 $i$ 位置的输出词的概率分布</li><li>输入：encoder的输出 与 对应  $i-1$ 位置decoder的输出。所以中间的attention不是self-attention，它的<strong style="color:blue;">K，V来自encoder</strong>，<strong style="color:blue;">Q来自上一位置decoder的输出</strong></li><li><p>解码：这里要注意一下，训练和预测是不一样的。在训练时，解码是一次全部decode出来，用上一步的ground truth来预测（mask矩阵也会改动，让解码时看不到未来的token）；而预测时，因为没有ground truth了，需要一个个预测。</p><p>为了确保按照生成顺序：从左到右，使用sequence mask。</p><p>sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。</p><p>那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。</p></li></ul><p><img src="https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156846899939997439.gif" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/19/DrhRUENTcwXPo89.png&quot; alt=&quot;image-20210319185913348&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Transformer模型中采用了 en
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>On Semantic Similarity in Video Retrieval</title>
    <link href="http://yoursite.com/2021/03/19/On-Semantic-Similarity-in-Video-Retrieval/"/>
    <id>http://yoursite.com/2021/03/19/On-Semantic-Similarity-in-Video-Retrieval/</id>
    <published>2021-03-19T08:50:22.000Z</published>
    <updated>2021-03-22T06:49:08.565Z</updated>
    
    <content type="html"><![CDATA[<h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><ul><li>当前的检索任务是目标实例进行检索（ target instance-based retrieval）（IVR），即，给定一个query caption，仅一个 origami video 被认为是正确的检索结果。但，实际上，数据集中的许多视频can be similar to the point of being identical。检索此类视频的顺序不应影响方法的评估。</li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p>本文提出 Semantic Similarity Video Retrieval（SVR），相比于 normal IVR 的不同点是：</p><ul><li>对于1个video，允许有多个captions与其 的相似度为1</li><li>如果 $sim(x_i, y_j)$ = $sim(x_i, j_k)$，$x_i$ 为 video set 中的一个video。则表示这两个caption被认为是与该video有相等的相关度。可以以任意的顺序来检索，并且不能被evaluation metric 惩罚。</li></ul><h2 id="Proxy-Measures-for-Semantic-Similarity"><a href="#Proxy-Measures-for-Semantic-Similarity" class="headerlink" title="Proxy Measures for Semantic Similarity"></a>Proxy Measures for Semantic Similarity</h2><p>video $x_i$ 与 caption $y_{i}$ 是ground truth pair。</p><p>定义video 与 other captions 的语义相似度为：corresponding caption 与 other captions 之间的语义相似度。</p><p>$S_{S}\left(x_{i}, y_{j}\right)=\left\{\begin{array}{ll}1 &amp; i==j \\ S^{\prime}\left(y_{i}, y_{j}\right) &amp; \text { otherwise }\end{array}\right.$</p><p>关于 $S^{\prime}$ ，本文使用了四种方式来度量文本之间的语义相似度：bag of words, part-of- speech knowledge, synset similarity and the METEOR metric。</p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p><img src="https://i.loli.net/2021/03/22/oMS76vcp2HX5DgK.png" alt="image-20210322144739237" style="zoom:50%;"></p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>the log-ratio loss :</p><blockquote><p>Tao Qin, Tie-Yan Liu, and Hang Li. A general approximation framework for direct optimization of information retrieval measures. Information retrieval, 2010. </p></blockquote><p>nDCG loss</p><blockquote><p>Sungyeon Kim, Minkyo Seo, Ivan Laptev, Minsu Cho, and Suha Kwak. Deep metric learning beyond binary supervision. In CVPR, 2019. </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;存在的问题&quot;&gt;&lt;a href=&quot;#存在的问题&quot; class=&quot;headerlink&quot; title=&quot;存在的问题&quot;&gt;&lt;/a&gt;存在的问题&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;当前的检索任务是目标实例进行检索（ target instance-based retrieval）（I
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
  </entry>
  
  <entry>
    <title>Non-Autoregressive Coarse-to-Fine Video Captioning</title>
    <link href="http://yoursite.com/2021/03/19/Non-Autoregressive-Coarse-to-Fine-Video-Captioning/"/>
    <id>http://yoursite.com/2021/03/19/Non-Autoregressive-Coarse-to-Fine-Video-Captioning/</id>
    <published>2021-03-19T08:49:05.000Z</published>
    <updated>2021-03-22T09:17:04.558Z</updated>
    
    <content type="html"><![CDATA[<p>发表在AAAI 2021</p><h2 id="现在存在的问题"><a href="#现在存在的问题" class="headerlink" title="现在存在的问题"></a>现在存在的问题</h2><ul><li>由于自回归解码（autoregressive decoding）导致的 <strong>slow inference speed</strong> </li><li>由于对视觉单词的训练不充分，而使得模型更加<strong>偏向于生成泛化性句子</strong>，缺乏细节和多样性。</li><li>此外，具有误差累积倾向的模型产生令人满意的字幕是具有挑战性的。 因此，还需要一种<strong>支持单词修改的灵活解码范例</strong>（decoding paradigm）。</li></ul><h2 id="本文的点"><a href="#本文的点" class="headerlink" title="本文的点"></a>本文的点</h2><p><img src="https://i.loli.net/2021/03/22/Kyegni76dZ4kCrH.png" alt="image-20210322164100678" style="zoom:33%;"></p><ul><li>我们提出了一种具有粗略至精细 （coarse-to-fine）字幕过程的基于非自回归解码（nonautoregressive<br>decoding）的模型</li><li><p><strong>For achieving inference speedup</strong>：employ a bi-directional self-attention based network as our language model for achieving inference speedup</p></li><li><p><strong>For improving caption quality：</strong>propose an alternative paradigm to decompose the captioning procedure into two stages,</p></li></ul><h2 id="Other-non-autoregressive"><a href="#Other-non-autoregressive" class="headerlink" title="Other non-autoregressive"></a>Other non-autoregressive</h2><ul><li>Masked Non-Autoregressive Image Captioning</li><li>Non-Autoregressive Image Captioning with Counterfactuals-Critical Multi-Agent Learning</li></ul><h2 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h2><ul><li>看看这篇论文是如何获得 tags of video</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;发表在AAAI 2021&lt;/p&gt;
&lt;h2 id=&quot;现在存在的问题&quot;&gt;&lt;a href=&quot;#现在存在的问题&quot; class=&quot;headerlink&quot; title=&quot;现在存在的问题&quot;&gt;&lt;/a&gt;现在存在的问题&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;由于自回归解码（autoregressive d
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[Less is More] CLIPBERT for Video-and-Language Learning via Sparse Sampling</title>
    <link href="http://yoursite.com/2021/03/18/Less-is-More-CLIPBERT-for-Video-and-Language-Learning-via-Sparse-Sampling/"/>
    <id>http://yoursite.com/2021/03/18/Less-is-More-CLIPBERT-for-Video-and-Language-Learning-via-Sparse-Sampling/</id>
    <published>2021-03-18T07:33:09.000Z</published>
    <updated>2021-03-18T07:33:45.556Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="end-to-end" scheme="http://yoursite.com/categories/cross-modal/end-to-end/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
      <category term="end-to-end" scheme="http://yoursite.com/tags/end-to-end/"/>
    
  </entry>
  
  <entry>
    <title>[LightningDOT] Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval</title>
    <link href="http://yoursite.com/2021/03/18/LightningDOT-Pre-training-Visual-Semantic-Embeddings-for-Real-Time-Image-Text-Retrieval/"/>
    <id>http://yoursite.com/2021/03/18/LightningDOT-Pre-training-Visual-Semantic-Embeddings-for-Real-Time-Image-Text-Retrieval/</id>
    <published>2021-03-18T06:02:17.000Z</published>
    <updated>2021-03-18T12:03:51.495Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-任务"><a href="#1-任务" class="headerlink" title="1. 任务"></a>1. 任务</h2><p>本文发表在 NAACL 2021，本文要研究的内容是如何提高 <strong>Image-text retrieval 任务的计算效率。</strong></p><h2 id="2-存在的问题"><a href="#2-存在的问题" class="headerlink" title="2. 存在的问题"></a>2. 存在的问题</h2><p>基于预训练的跨模态模型取得了很好的进展，但是在测试阶段存在<strong>推理速度慢</strong>的问题。 主要是由于Transformer 结构中的cross-modal attention 造成的巨大的计算消耗。 这种延迟以及计算消耗使其很难在实际中应用。</p><p>下图可视化了近年来 ITR task 的研究进展，（a） 早期，使用CNN和RNN分别提取视觉和语言特征，然后使用dot-product 来计算similarity。 （b）后来有人提出使用faster-RCNN 和 RNN 分别提取两个模态的特征，使用使用cross-attention，最后再计算相似性。（c）随着BERT的发展，有人使用BERT扩展出 V+L BERT 模型。(d) 由于cross-modal attention 是耗时的，因此，本文中提出去掉cross-modal 这个模块。</p><p><img src="https://i.loli.net/2021/03/18/fdU3lxwpkZo8yuG.png" alt="image-20210318143100485" style="zoom:50%;"></p><h2 id="3-本文的点"><a href="#3-本文的点" class="headerlink" title="3. 本文的点"></a>3. 本文的点</h2><ul><li>本文希望可以重新回归到 <strong>dot-product</strong> 这个简单的操作。本文中使用dot product 来做多模态融合，而不是使用计算量的self-attention。同时，为了利用有效的多模态嵌入学习，本文在两个encoder上都使用 [CLS] token。</li><li><p>通过消除模态之间耗时的交叉注意力，该模型可以在推理过程中学习视觉语义嵌入而无需在每个图像-文本对之间进行广泛匹配。此外，通过消除对图像-文本对的实时计算的依赖，我们可以一次<strong>离线地独立地</strong>计算所有图像和文本嵌入，并将这些嵌入重新用作新查询的<strong>缓存索引</strong>。</p></li><li><p>LightningDOT通过预先训练<strong>三个新颖的学习目标</strong>：Visual-embedding fused MLM (namely VMLM), Semantic-embedding  fused MRM (namely SMRM) and a cross-modal retrieval objective (namely CMR).</p><p>前两个预训练任务（VMLM 和 SMRM）是为了确保跨模态信息可以被获取到。CMR是为了鼓励模型在预训练阶段获得多模态融合。</p></li><li><p>重新排名（re-ranking）机制</p></li></ul><p>Note: 本文不是从模型压缩的角度来解决问题。</p><h2 id="4-贡献"><a href="#4-贡献" class="headerlink" title="4. 贡献"></a>4. 贡献</h2><p>提出了一个简单有效的方法，在不牺牲accuracy 的情况下，LightningDOT 可以数千倍的加速推理时间。</p><p>我们的工作是在基于预训练视觉语义嵌入，实现低延迟的实时跨模式检索的第一个已知工作。</p><h2 id="5-Method"><a href="#5-Method" class="headerlink" title="5. Method"></a>5. Method</h2><p><img src="https://i.loli.net/2021/03/18/kASuJa8xc6hYFne.png" alt="image-20210318175408610" style="zoom: 67%;"></p><p>在本节中，我们介绍LightningDOT框架，该框架由两个深层的Transformer作为图像和语言编码器。我们首先介绍三个预训练任务，然后介绍从<strong>离线特征提取</strong>到<strong>在线即时检索</strong>的 <strong>推理流程（inference pipline）</strong>。</p><p>图像编码器得到region features: $f_{\theta_{V}}(\mathbf{v})=\mathbf{h}=\left\{\mathbf{h}_{0}, \ldots, \mathbf{h}_{N}\right\}\left(\mathbf{h}_{j} \in \mathbb{R}^{d}\right)$</p><p>语言编码器得到token representations: $f_{\theta_{L}}(\mathbf{w})=\mathbf{z}=\left\{\mathbf{z}_{0}, \ldots, \mathbf{z}_{T}\right\}\left(\mathbf{z}_{j} \in \mathbb{R}^{d}\right)$</p><p>regard the output [CLS] embedding <strong><strong style="color:red;">$h_0$</strong> as global image representation</strong>, and <strong><strong style="color:red;">$z_0$</strong>as global text representation</strong></p><h3 id="5-1-Model-Pre-training"><a href="#5-1-Model-Pre-training" class="headerlink" title="5.1 Model Pre-training"></a>5.1 Model Pre-training</h3><h4 id="Visual-embedding-Fused-Masked-Language-Modeling-VMLM"><a href="#Visual-embedding-Fused-Masked-Language-Modeling-VMLM" class="headerlink" title="Visual-embedding Fused Masked Language Modeling (VMLM)"></a>Visual-embedding Fused Masked Language Modeling (VMLM)</h4><p>设有M个 masked tokens</p><p>对于 sentence $t$ and image $i$ ， The loss function of VMLM can be formulated as:</p><p>$\mathcal{L}_{\mathrm{VMLM}}(t, i)=-\log P_{\theta}\left(\mathbf{w}_{\mathbf{m}} \mid \mathbf{w}_{\backslash \mathbf{m}}, i\right)$<br>$=-\frac{1}{M} \sum_{k=1}^{M} \log P_{\theta_{\mathrm{mlm}}}\left(\mathbf{w}_{\mathbf{m}_{k}} \mid \mathbf{z}_{\mathbf{m}_{k}}+\mathbf{h}_{0}\right)$</p><p>其中 $z$ 是 hidden state。</p><p>Note： 这里的 +$h_0$ 是显式的加和，而不是使用cross-modal attention.</p><h4 id="Semantic-embedding-Fused-Masked-Region-Modeling-SMRM"><a href="#Semantic-embedding-Fused-Masked-Region-Modeling-SMRM" class="headerlink" title="Semantic-embedding Fused Masked Region Modeling (SMRM)"></a>Semantic-embedding Fused Masked Region Modeling (SMRM)</h4><p>$\mathcal{L}_{\mathrm{SMRM}}(i, t)=\mathcal{D}_{\theta_{\mathrm{mrm}}}\left(\mathbf{v}_{\mathbf{m}}, f_{\theta_{V}}\left(\mathbf{v}_{\backslash \mathbf{m}}\right), t\right)$<br>$=\frac{1}{M} \sum_{k=1}^{M} \mathcal{D}_{\theta_{\mathrm{mrm}}}\left(\mathbf{v}_{\mathbf{m}_{k}}, \mathbf{h}_{\mathbf{m}_{k}}+\mathbf{z}_{0}\right)$</p><p>这里的 $\mathcal{D}_{\theta_{\mathrm{mrm}}}$ 代表两个损失，一个是使用L2 distance 的 掩码区域特征回归，另外一个是用KL散度的掩码区域分类。</p><h4 id="Cross-modal-Retrieval-Objective-CMR"><a href="#Cross-modal-Retrieval-Objective-CMR" class="headerlink" title="Cross-modal Retrieval Objective (CMR)"></a>Cross-modal Retrieval Objective (CMR)</h4><p>The similarity score between query t and image i is defined as:</p><p>$S(t, i)=\left\langle\mathbf{z}_{0}, \mathbf{h}_{0}\right\rangle$</p><p>损失函数：</p><p>$\mathcal{L}_{\mathrm{IR}}^{(t)}=-\log \frac{e^{S\left(t, i_{1}\right)}}{\sum_{k=1}^{n} e^{S\left(t, i_{k}\right)}}$</p><p>$\mathcal{L}_{\mathrm{TR}}^{(i)}=-\log \frac{e^{S\left(i, t_{1}\right)}}{\sum_{k=1}^{n} e^{S\left(i, t_{k}\right)}}$</p><p>$\mathcal{L}_{\mathrm{CMR}}(B)=\frac{1}{2 n} \sum_{k=1}^{n} \mathcal{L}_{\mathrm{TR}}^{\left(i_{k}\right)}+\mathcal{L}_{\mathrm{IR}}^{\left(t_{k}\right)}$</p><h3 id="5-2-Real-time-Inference"><a href="#5-2-Real-time-Inference" class="headerlink" title="5.2 Real-time Inference"></a>5.2 Real-time Inference</h3><p>以text-to-image retrieval 作为样例来介绍 real-time inference pipline：</p><p>（1）离线图片特征提取与编码；（2）text query 在线检索；（3）使用top-retrieval images 做在线重拍</p><h4 id="Offline-Feature-Extraction"><a href="#Offline-Feature-Extraction" class="headerlink" title="Offline Feature Extraction"></a>Offline Feature Extraction</h4><p>首先使用 image encoder 来处理数据集中的所有图片，并存储其 global image representation 进入索引的内存中供以后使用。</p><p>整个image-to-index 过程，包括 faster rcnn 提取特征 以及 image transformer encoder 都是离线处理的。</p><h4 id="Online-Retrieval"><a href="#Online-Retrieval" class="headerlink" title="Online Retrieval"></a>Online Retrieval</h4><p>对于 text query, 使用language encoder 提取特征，然后依次计算与每个图片的相似度。图片将会被排序。实际中，人们感兴趣的是前top-k 检索结果。</p><p>使用FAISS来优化检索。</p><p>类似地，对于文本检索，可以通过简单地为所有句子预先计算嵌入并使用图像作为查询来应用相同的体系结构</p><h4 id="Re-ranking"><a href="#Re-ranking" class="headerlink" title="Re-ranking"></a>Re-ranking</h4><p>为了进一步提高检索结果，本文通过采用可选的<strong>重新排名模型</strong>提出了一种两阶段方法。</p><p>第一阶段，使用LightingDOT来检索 top-M images(or texts)。</p><p>第二阶段，使用一个性能更好的检索模型（通常比较慢）来重新排序从第一阶段检索到的 top-M pairs.</p><p>实验证明，可以同时从性能和效率两方面受益。</p><h2 id="6-Experiments"><a href="#6-Experiments" class="headerlink" title="6. Experiments"></a>6. Experiments</h2><h3 id="6-1-Results-on-Flickr30K-and-COCO"><a href="#6-1-Results-on-Flickr30K-and-COCO" class="headerlink" title="6.1 Results on Flickr30K and COCO"></a>6.1 Results on Flickr30K and COCO</h3><p><img src="https://i.loli.net/2021/03/18/TpcoUY7Zjel39dM.png" alt="image-20210318175824380" style="zoom: 50%;"></p><ul><li><p>在仅使用一阶段排序的情况下：</p><ul><li>相比于不使用预训练的模型，性能上有显著提升 CAAN (SOTA method with cross-attention）</li><li>与使用预训练的模型相比，UNITER，性能上仅下降了一点，但是速度上有600/1900倍的提升(Flickr30K/COCO)</li></ul></li><li><p>使用两阶段排序：</p><ul><li>性能上相比于一阶段有提升，同时比单纯的UNITER模型有 46-95倍速度的提升，</li></ul></li></ul><h3 id="6-2-Speed-amp-Space-Improvement"><a href="#6-2-Speed-amp-Space-Improvement" class="headerlink" title="6.2 Speed &amp; Space Improvement"></a>6.2 Speed &amp; Space Improvement</h3><ul><li><p>检索图像，比较推理速度差异</p><p>以 UNITER_base 作为比较对象。</p><p>SCAN，是一个不使用预训练的模型，但是采用了cross-modal attention.</p><p><img src="https://i.loli.net/2021/03/18/umlFqkgLfyKQ6Cx.png" alt="image-20210318180604083" style="zoom: 33%;"></p></li><li><p>扩大搜索池，性能仍然很好</p><p><img src="https://i.loli.net/2021/03/18/jelLQRdcFwWpaEU.png" alt="image-20210318180918374"></p></li></ul><h3 id="6-3-Ablation-Studies"><a href="#6-3-Ablation-Studies" class="headerlink" title="6.3 Ablation Studies"></a>6.3 Ablation Studies</h3><ul><li><p>观察各个模块的作用</p><p>(1) 【R-CNN only】不使用 image encoder, 直接使用 faster rcnn 提取的特征</p><p>(2)【 “+Image Encoder”】</p><p>(3)【+PT】 MLM+MRM+CMR 上预训练， 注意本文采用的预训练方案是 VMLM+SMRM+CMR</p><p><img src="https://i.loli.net/2021/03/18/ewAYK8M6idSBvEb.png" alt="image-20210318181810013" style="zoom: 33%;"></p><p><strong style="color:blue;">yaya: 其实，本文提出的预训练任务带来的提升并不明显。</strong></p></li><li><p>观察各个预训练任务的作用</p><p><img src="https://i.loli.net/2021/03/18/OkTcr9IVbxYBPgu.png" alt="image-20210318182135676" style="zoom: 50%;"></p><p>预训练任务对于本文提出的模型是有提升的，但是，提升的显著性似乎没有那么大。</p><p><strong style="color:blue;">yaya: 奇怪，为什么 这个 PT(ALL) 与 上个表Table 4 中的LightingDOT结果 不一致呢都？都是在Flickr30k validation上的结果</strong></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-任务&quot;&gt;&lt;a href=&quot;#1-任务&quot; class=&quot;headerlink&quot; title=&quot;1. 任务&quot;&gt;&lt;/a&gt;1. 任务&lt;/h2&gt;&lt;p&gt;本文发表在 NAACL 2021，本文要研究的内容是如何提高 &lt;strong&gt;Image-text retrieval 
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="Image-Text Retrieval" scheme="http://yoursite.com/categories/cross-modal/Image-Text-Retrieval/"/>
    
      <category term="real time" scheme="http://yoursite.com/categories/cross-modal/Image-Text-Retrieval/real-time/"/>
    
    
      <category term="cross-modal" scheme="http://yoursite.com/tags/cross-modal/"/>
    
      <category term="Image-Text Retrieval" scheme="http://yoursite.com/tags/Image-Text-Retrieval/"/>
    
      <category term="real time" scheme="http://yoursite.com/tags/real-time/"/>
    
  </entry>
  
  <entry>
    <title>Improving Translation Robustness with Visual Cues and Error Correction</title>
    <link href="http://yoursite.com/2021/03/17/Improving-Translation-Robustness-with-Visual-Cues-and-Error-Correction/"/>
    <id>http://yoursite.com/2021/03/17/Improving-Translation-Robustness-with-Visual-Cues-and-Error-Correction/</id>
    <published>2021-03-17T07:55:13.000Z</published>
    <updated>2021-03-17T08:33:45.221Z</updated>
    
    <content type="html"><![CDATA[<h2 id="本文研究的任务"><a href="#本文研究的任务" class="headerlink" title="本文研究的任务"></a>本文研究的任务</h2><p>多模态机器翻译任务中对噪声样本的鲁棒性。</p><p>神经机器翻译模型对输入噪声很脆弱。当前的鲁棒性技术大多使模型<strong>适应</strong>现有的嘈杂文本，但是这些模型通常在<strong>遇到看不见的噪声</strong>时会失效，并且在clean  text 上的性能会下降（即相比于那些普通的模型，使用噪声样本来扩充数据的模型，其在clean text 上的性能会下降）。</p><h2 id="本文提出的点"><a href="#本文提出的点" class="headerlink" title="本文提出的点"></a>本文提出的点</h2><p>（1） 模型上：引入了<strong><em>视觉上下文</em></strong>的概念，以提高针对嘈杂文本的翻译鲁棒性。</p><p>（2）多任务：通过<strong>将纠错作为辅助任务</strong>来提出一种新的<strong><em>纠错训练</em>方案</strong>，以进一步提高鲁棒性。</p><p>实验证明，在 English-French and English-German 翻译任务上，（1）对于训练中遇到的噪声以及未遇到的噪声都有很好的鲁棒性。（2）同时保持了在 clean text 上的翻译质量。</p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>不是重点来做 MMT model 的，略过</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;本文研究的任务&quot;&gt;&lt;a href=&quot;#本文研究的任务&quot; class=&quot;headerlink&quot; title=&quot;本文研究的任务&quot;&gt;&lt;/a&gt;本文研究的任务&lt;/h2&gt;&lt;p&gt;多模态机器翻译任务中对噪声样本的鲁棒性。&lt;/p&gt;
&lt;p&gt;神经机器翻译模型对输入噪声很脆弱。当前的鲁棒
      
    
    </summary>
    
      <category term="cross-modal" scheme="http://yoursite.com/categories/cross-modal/"/>
    
      <category term="image-guided MT" scheme="http://yoursite.com/categories/cross-modal/image-guided-MT/"/>
    
    
      <category term="cross-modal,image-guided MT" scheme="http://yoursite.com/tags/cross-modal-image-guided-MT/"/>
    
  </entry>
  
</feed>
