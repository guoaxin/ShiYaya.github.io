<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShiYaya</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-12-24T04:09:01.208Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>ShiYaya</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>图像描述任务的实际应用</title>
    <link href="http://yoursite.com/2020/12/24/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/"/>
    <id>http://yoursite.com/2020/12/24/图像描述任务的实际应用/</id>
    <published>2020-12-24T04:07:45.000Z</published>
    <updated>2020-12-24T04:09:01.208Z</updated>
    
    <content type="html"><![CDATA[<h3 id="CaptionBot"><a href="#CaptionBot" class="headerlink" title="CaptionBot"></a>CaptionBot</h3><p>from: <a href="https://www.theguardian.com/technology/2016/apr/14/captionbot-microsoft-latest-ai-experiment-it-isnt-racist" target="_blank" rel="noopener">https://www.theguardian.com/technology/2016/apr/14/captionbot-microsoft-latest-ai-experiment-it-isnt-racist</a></p><p>简介：用户上传一张图片到CaptionBot Service，该系统针对该图片自动的生成一个caption。用户可以评分生成的caption 是否准确。</p><blockquote><p>The idea is that you upload a photo to the service, and it tries to automatically generate a caption that describes what the algorithm sees. You are then able to rate how accurately it has detected what was on display. It learns from the rating, and in theory, the captions get better.</p></blockquote><p>The bot, from <a href="http://go.theguardian.com/?id=114047X1572903&amp;url=https%3A%2F%2Fwww.microsoft.com%2Fcognitive-services&amp;sref=https://www.theguardian.com/technology/2016/apr/14/captionbot-microsoft-latest-ai-experiment-it-isnt-racist" target="_blank" rel="noopener">Microsoft’s Cognitive Services team</a>, is the result of <a href="http://go.theguardian.com/?id=114047X1572903&amp;url=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F264408%2FImageCaptionInWild.pdf&amp;sref=https://www.theguardian.com/technology/2016/apr/14/captionbot-microsoft-latest-ai-experiment-it-isnt-racist" target="_blank" rel="noopener">some hefty research</a> into how to model objects in photographs so that a computer can understand them. They claim that their system can recognise “a broad range of visual concepts” and also performs entity extraction so that it can recognise celebrities. </p><p><img src="https://i.loli.net/2020/12/24/kJ31yRYX5dKH64g.jpg" alt="Taylor Swift and Kanye West both identified in Microsoft’s CaptionBot app"></p><h3 id="Seeing-AI-项目"><a href="#Seeing-AI-项目" class="headerlink" title="Seeing AI 项目"></a><strong>Seeing AI</strong> 项目</h3><p>微软研究院的研究员们不仅在寻找识别图像的方法，还在为图像进行描述。这项研究结合了图像识别技术与自然语言处理技术，能帮助视障人士获得对图像的准确描述，还可能帮助那些需要图像信息却无法直接看到图像的人——比如正在开车的司机。</p><p>Seeing AI项目组中的Margaret Mitchell是一名专攻自然语言处理的研究员，也是图像描述领域顶尖的研究者之一。她说，她和同事们正在寻找方法，让计算机可以用更加人性化的方式来描述图像。例如，计算机可以将一个场景准确地描述为“一群人坐在一起”，但真人可能会将这一场景描述为“一群人坐在一起享受美好时光。”<strong>目前的挑战就是让这项技术懂得一张图像中哪些是对人们最重要、最值得描述的内容。</strong>“<strong>一张图像中有什么，和我们如何谈论一张图像可是完全不同的两回事</strong>，”Mitchell说。</p><p>微软的另一些研究员们正在努力让最新的图像识别工具提供更深入的图片解释。例如，与单纯地将图片描述为“一个男人和一个女人坐在一起”相比，对人们更有帮助的描述可能是：“奥巴马和希拉里·克林顿正在摆pose拍照”。今天人们在网上搜索图片时，绝大多数情况下搜索引擎会根据与图片相关的文字内容，从而得到美国名媛金·卡戴珊或“霉霉”泰勒·斯威夫特的照片，这些搜索结果主要依据文本内容。而微软的资深研究员张磊及郭彦东等研究员正在开发一套借助机器学习识别名人、政治家和公众人物的系统，这套系统会根据图像本身的元素，而非与图像相关的文字内容来进行图像识别。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;CaptionBot&quot;&gt;&lt;a href=&quot;#CaptionBot&quot; class=&quot;headerlink&quot; title=&quot;CaptionBot&quot;&gt;&lt;/a&gt;CaptionBot&lt;/h3&gt;&lt;p&gt;from: &lt;a href=&quot;https://www.theguardian
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>DirectQE Direct Pretraining for Machine Translation Quality Estimation</title>
    <link href="http://yoursite.com/2020/12/24/DirectQE-Direct-Pretraining-for-Machine-Translation-Quality-Estimation/"/>
    <id>http://yoursite.com/2020/12/24/DirectQE-Direct-Pretraining-for-Machine-Translation-Quality-Estimation/</id>
    <published>2020-12-24T04:06:01.000Z</published>
    <updated>2020-12-24T06:12:44.384Z</updated>
    
    <content type="html"><![CDATA[<p>AAAI2021论文：基于数据生成的机器翻译质量评估方法</p><p>来源：<a href="https://mp.weixin.qq.com/s?__biz=Mzg3ODA0NTA2OA==&amp;mid=2247484204&amp;idx=1&amp;sn=1b24bb494110fff68beedde3cb422066&amp;chksm=cf18f1cff86f78d9bdd2d883201af98c4296ec45fe8fae91509421b226ccee923ae7bd158451&amp;mpshare=1&amp;scene=1&amp;srcid=1224CwmzTjgig5X1t5fCbU0T&amp;sharer_sharetime=1608779809453&amp;sharer_shareid=ab44667880fa06ced8bfa560d1d64d36&amp;key=22efb95c8c04cdfbe43c8f83a69b76927ad01c4ab53016600f84450f07570ee6fb6b4747790c342eb1180e349f3fbc3fa8a1ab1219b80a6d725aacd79413c692d47bc3d763cd18ca87eaf3dd823648170186d2f2873bc5c2ef24ff36d3b519a54723506b60b7d4161912fd28659694c1100797e597da6bfe583dad4ef0c80802&amp;ascene=1&amp;uin=MjQwOTk2MzUwOA%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=A8kVBtMx05%2Bw41YTdHRse0I%3D&amp;pass_ticket=5UOCG2UJoSjTMMc7Gh82YYMCZ5iDMMdAZnDIpIDdHbP7%2FsA%2FS9I%2B%2FHaK1%2BTsQBXL&amp;wx_header=0" target="_blank" rel="noopener">南大NLP</a></p><h3 id="研究问题"><a href="#研究问题" class="headerlink" title="研究问题"></a>研究问题</h3><p>目前机器翻译在各个领域得到广泛应用，不同的机器翻译系统，对于同样的原文句子可能会给出不同的翻译结果（如表1所示），质量有好有坏。该如何自动评价译文的翻译质量呢？</p><div class="table-container"><table><thead><tr><th>原文</th><th>黄河之水天上来，奔流到海不复回</th></tr></thead><tbody><tr><td>翻译1</td><td>The water of the Yellow River comes from the sky, rushes to the sea and never returns</td></tr><tr><td>翻译2</td><td>The Yellow River never comes back to the sea</td></tr><tr><td>翻译3</td><td>The Yellow River comes from the sky, runs to the sea and never comes back</td></tr><tr><td>翻译4</td><td>The water of the Yellow River came from the sky and ran to the sea</td></tr><tr><td>翻译5</td><td>The water of the Yellow River comes from the sky, and the waves rush to the East China Sea and never look back</td></tr></tbody></table></div><p>表格 1：机器翻译的不同结果</p><h3 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h3><p>目前的评价指标主要有两类。一类需要依赖参考译文，比如BLEU、TER、Meteor等，主要依赖机器翻译译文和参考译文之间的匹配程度，<strong style="color:red;">但参考译文在现实应用场景下往往难以获取。</strong>第二类评价指标不需要依赖参考译文，如质量评估Quality Estimation (QE)，仅通过原文和机器译文对翻译质量进行估计。</p><p>QE的粒度有很多种，包括词级别、句子级别、短语级别、文档级别等，<strong style="color:red;">本文主要关注词级别与句子级别QE任务。</strong>词级别QE将译文中的每一个词标记为“Ok”或“Bad“；句子级别QE给每一个句子标注一个[0, 1]之间的打分（0表示很好，1表示很差），这些标记都由人工标记得到，或基于人工编辑结果得到。表2展示了一个英-中语向的QE数据示例。</p><div class="table-container"><table><thead><tr><th>原文（英）</th><th>this insubordination earned him a now famous reprimand from the King .</th></tr></thead><tbody><tr><td>机器译文（中）</td><td>这种 <strong style="color:blue;">不 服从命令</strong> 的 态度 使 他 <strong style="color:blue;">赢得 了 国王 现在 著名</strong> 的 <strong style="color:blue;">训斥</strong> .</td></tr><tr><td>词级别标记</td><td>O B B O O O O B B B B B O B B</td></tr><tr><td>句子级别标记</td><td>0.6429</td></tr></tbody></table></div><p>表格 2：QE数据示例（WMT20英-中）</p><p>早期QE任务依赖人工设计的特征，例如原文、译文中的单词数量，词频等，但是该方法的适应性较弱，效果较差。<strong style="color:red;">随后有研究者使用神经网络对QE数据进行端到端建模，取得了一定的成绩。</strong>神经网络需要大规模数据进行训练，但是<strong style="color:red;">QE数据由于需要人工进行标记，暂时规模较小</strong>（万句级别），这限制了神经网络的训练。<strong style="color:red;">目前最流行的QE模型利用知识迁移技术，从无QE标记但具有大规模（百万句级别）的平行语料中迁移QE任务所需要的知识。</strong></p><p>Predictor-Estimator是一种流行的基于知识迁移的QE框架，它是一种两阶段模型（如图一所示）。第一阶段，预测器（Predictor）将在平行语料上进行预训练，其预训练任务一般为“词预测”类型的任务。第二阶段，使用预测器提取QE句对的特征，通过评估器（Estimator）学习如何在这些特征上拟合QE标记。</p><p><img src="https://i.loli.net/2020/12/24/lxWL7TugMvqFUtO.png" alt="image-20201224112451818" style="zoom: 33%;"></p><p>我们可以使用神经机器翻译模型NMT（Kim et al. 2017, Fan et al. 2018, Zhou et al. 2019）或者预训练语言模型PLM（Kepler et al. 2019, Kim et al. 2019）来作为预测器，使用LSTM模型作为评估器。</p><p><strong style="color:red;">该框架的问题在于，其两阶段之间存在差异，包括数据的差异和训练目标的差异。</strong> 数据的差异是指，<strong style="color:blue;">预测器</strong>在大规模平行语料上训练，平行语料由原文和正确译文组成；<strong style="color:blue;">评估器</strong>在QE数据上训练，句对由原文和包含错误的机器翻译译文组成。训练目标的差异是指，预测器是在做“词预测”任务；评估器是在预测词和句子的质量。那么预测器的预训练过程与目标QE任务存在差异，会导致学习不到QE任务真正需要的知识，无法充分利用大规模双语平行数据。</p><h3 id="本文提出的方法"><a href="#本文提出的方法" class="headerlink" title="本文提出的方法"></a>本文提出的方法</h3><p>QE模型的现存问题主要是，1.大规模神经网络训练参数依赖大量数据；2.数据分布及训练目标的差异可能对两阶段训练带来不利影响。</p><p>为了解决这两个问题，我们采取的改进方向是：</p><ol><li><p>使用相同/相似的数据进行预训练；</p></li><li><p>使用相同/相似的预训练目标。</p></li></ol><p>QE数据中包含一些翻译噪音，QE的训练目标需要质量标签。那么如何基于平行语料，获得带有一定噪音的数据，并且可以获得噪音数据的质量标签？</p><p>我们的解决方法是，<strong style="color:green;">首先基于平行数据训练生成器（generator）进行词改写任务；接着对平行语料进行词改写，从而引入一定量的可控噪音并利用可控噪音自动生成质量标签。最终可以将这些生成数据提供给判别器（detector）直接为QE任务进行预训练。</strong> 接下来具体介绍生成器的训练与生成过程。</p><p>首先，我们以Masked Language Model (MLM)的方式训练生成器。给定平行句对，随机的隐藏（mask）译文中某个位置的词，然后让模型预测被隐藏的词（如图2所示）。</p><p><img src="https://i.loli.net/2020/12/24/GURzxAJyHeq9ZdW.png" alt="image-20201224113643585" style="zoom: 33%;"></p><p>生成器训练结束后，我们将使用生成器对平行语料进行转化，具体分为两个步骤。</p><ol><li>生成伪造机器翻译译文。给定平行语料并隐藏译文中某个位置的词，让生成器进行预测并输出概率分布，根据概率分布采样新的词替换被隐藏词（如图3所示），即完成了对被隐藏词的改写。</li></ol><p><img src="https://i.loli.net/2020/12/24/Bef8uHQGNvkiRK7.png" alt="image-20201224113823958" style="zoom: 33%;"></p><ol><li><p>生成对应标签。</p><p>根据译文中的词是否被改写来获得词级别标签（见公式1）。</p><p>根据译文中被改写词的比例获得句子级别标签q’（见公式2）。</p><p>公式1： $o_{j}^{\prime}=\left\{\begin{array}{ll}1, &amp; \text { if } y_{j}=y_{j}^{\prime} \\ 0, &amp; \text { otherwise }\end{array}\right.$</p><p>公式2： $q^{\prime}=1-\frac{\operatorname{sum}\left(\mathbf{O}^{\prime}\right)}{\operatorname{len}\left(\mathbf{O}^{\prime}\right)}$</p></li></ol><p>通过生成器，我们能够将大规模平行语料转化为更大规模的伪造QE数据。比起平行译文，伪造的机器翻译译文在数据分布上与QE中的译文更加接近。同时，伪造QE数据针对每一个词有表示“是否由机器生成“的标签，对整个句子有表示”句子改写程度“的标签，形式上与QE数据类似。<strong style="color:green;">基于大规模伪造QE数据以及真实QE数据，我们将使用同样的训练目标，对判别器（Detector）分别在伪造数据上进行预训练、在真实数据上微调参数。最终也只需要使用判别器来做QE分数预测。</strong></p><blockquote><p>即，生成的数据，仅仅是在estimator 上提供预训练数据</p><p>即本文，并没有对 predictor做改动，是对estimator 的训练策略上做了改动</p></blockquote><p>我们对比了Predictor-Estimator框架中的两种具体实现。一种是基于NMT的QE模型，具体实现仿照QE Brain模型(Fan et al. 2018)；一种是基于PLM的QE模型，具体使用的PLM模型来自于huggingface。在本文的实现中，我们所提出的<strong>DirectQE</strong>参数量是最小的。</p><p>实验结果如表3，4，5所示，可以发现我们的模型在绝大多数情况下都是具有优势的。</p><p>表格 3：单模型结果（英-德）</p><p><img src="https://i.loli.net/2020/12/24/XGcIHvBlyzrfZJR.png" alt="image-20201224114857966"></p><p>表格 4：集成模型结果（英-德）</p><p><img src="https://i.loli.net/2020/12/24/UkC5rKy9ZoWh8Ab.png" alt="image-20201224114950396"></p><p>表格 5：单模型结果（英-中、英-俄）</p><p><img src="https://i.loli.net/2020/12/24/87bizODk5VYjdxQ.png" alt="image-20201224115010524"></p><p>为了找出我们模型性能具体的增长点，我们按照错误词的比例划分了真实QE数据集，并评估了模型在数据每个部分的性能。如图4所示，在翻译质量存在问题时（错误词比例&gt;12.5%），DirectQE的性能更好。</p><p>图表 4：模型在不同错误比例数据上的性能对比</p><p><img src="https://i.loli.net/2020/12/24/aXfOdF6buyhRPl8.png" alt="image-20201224115158658"></p><p>为了研究预训练使用的数据分布对QE性能的影响，我们使用基于NMT的QE模型，并且将其中训练预测器用到的平行语料替换为生成器制造的伪造机器翻译译文，其余部分均保持不变。从表6中可以看出，使用伪造译文的模型性能有所上升，说明对于QE任务而言，使用伪造译文预训练比平行译文更好。</p><p>表格 6：使用伪造译文/平行译文训练基于NMT的QE系统</p><p><img src="https://i.loli.net/2020/12/24/r2zCSgNepfovnET.png" alt="image-20201224115311979"></p><p>为了研究预训练数据质量对QE性能的影响，我们测试了不同质量的数据下QE性能，这里数据质量具体指是译文质量，可以体现在替换词比例上（在相同替换策略下，替换词比例越大，译文质量越差）。从图5中可以看出，伪造译文质量太好或者太坏都不利于最终QE的性能。伪造译文质量太好（替换比例很低），句子将接近于平行语料本身，数据中几乎没有噪音；而伪造译文质量太差时，会破坏句子结构，与真实QE译文数据分布有较大差异。图5中红点表示使用随机噪音替换被隐藏词，此时的译文质量很差，可以看到QE性能也很低。</p><p>图表 5：不同伪造译文质量下的QE模型性能</p><p><img src="https://i.loli.net/2020/12/24/ZgprCA5Yux8iUF3.png" alt="image-20201224115404648" style="zoom:50%;"></p><p>在固定规模的平行语料上，生成器每一次采样会产生不同的伪造QE数据，最终用于训练判别器的数据规模是超百万级别的，且更多样化。为了研究多样性的价值，我们使用生成器产生了固定数量的伪造QE数据，对比了在固定生成的数据上以及持续生成的数据上预训练的模型性能。结果（图6）显示，<strong style="color:red;">伪造QE数据的多样性对提升模型性能来说很重要。</strong></p><p>词级别QE任务需要判别当前词的质量，那么模型在建模当前词时，包含更多当前词的信息是有必要的。为了体现模型隐层表示含有当前词信息的多与少，我们计算了隐层表示与当前词之间的互信息（模型指判别器/预测器）。在图7中可以看到，DirectQE学到的表示中包含有更多当前词的信息。</p><p>图表 7：模型隐层表示 v.s. 当前词信息</p><p><img src="https://i.loli.net/2020/12/24/itonHNu9jE1rbeA.png" alt="image-20201224115822577" style="zoom: 33%;"></p><p>假设当模型针对下游任务进行微调时，模型的隐层表示改变越小，则原始的表示更适合该下游任务。为了研究是否DirectQE可以学习到更加适合QE任务的表示，我们测试了在真实QE数据上微调前后，DirectQE和基于NMT的QE模型的隐层表示之间的相似度。表7显示，DirectQE隐层表示的相似度较高，说明DirectQE可以学到更加适合QE任务的表示。</p><p>表格 7：微调前后隐层表示变化</p><p><img src="https://i.loli.net/2020/12/24/DdNP7FSa1QGc5hB.png" alt="image-20201224120047777" style="zoom:33%;"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>大规模的QE模型需要大规模数据进行参数训练。现有的两阶段方法，由于数据和训练目标差异，无法最大化利用大规模平行语料知识。我们提出一种直接为QE模型进行预训练的框架（DirectQE）——使用生成器由平行语料得到伪QE数据，而后使用判别器，在伪QE数据上进行预训练，并且使用真实QE数据微调。我们模型的优势是，参数规模更小，模型性能更好，并且易于使用。</p><p>未来，我们将考虑使用更多样化的方式来构造伪QE数据，进一步缓解数据差异带来的影响，最大程度利用大规模语料，提升QE模型性能。</p><h3 id="可查看的参考文献"><a href="#可查看的参考文献" class="headerlink" title="可查看的参考文献"></a>可查看的参考文献</h3><ul><li><p>Predictor-Estimator: Neural Quality Estimation Based on Target Word Prediction for Machine Translation</p><p><a href="https://unbabel.github.io/OpenKiwi/cli/train_predictor_estimator.html" target="_blank" rel="noopener">https://unbabel.github.io/OpenKiwi/cli/train_predictor_estimator.html</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;AAAI2021论文：基于数据生成的机器翻译质量评估方法&lt;/p&gt;
&lt;p&gt;来源：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=Mzg3ODA0NTA2OA==&amp;amp;mid=2247484204&amp;amp;idx=1&amp;amp;sn=1b2
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>BERT-ATTACK Adversarial Attack Against BERT Using BERT</title>
    <link href="http://yoursite.com/2020/12/01/BERT-ATTACK-Adversarial-Attack-Against-BERT-Using-BERT/"/>
    <id>http://yoursite.com/2020/12/01/BERT-ATTACK-Adversarial-Attack-Against-BERT-Using-BERT/</id>
    <published>2020-12-01T07:47:01.000Z</published>
    <updated>2020-12-01T09:30:26.130Z</updated>
    
    <content type="html"><![CDATA[<h1 id="复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法"><a href="#复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法" class="headerlink" title="复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法"></a>复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法</h1><p>针对离散数据（例如文本）的对抗攻击比连续数据（例如图像）更具挑战性，因为很难使用基于梯度的方法生成对抗样本。当前成功的文本攻击方法通常在字符或单词级别上采用启发式替换策略，替换时难以保持语义一致性和语言流畅性。在本文中，作者提出了BERT-Attack，这是一种高质量且有效的方法，可以使用以BERT为例的MLM预训练语言模型来生成对抗性样本。作者使用BERT对抗其微调模型和其他预训练模型，以误导目标模型，使其预测错误。作者的方法在成功率和扰动百分比方面均优于最新的攻击策略，并且生成的对抗性样本很流利，并且在语义一致。而且作者的方法计算成本低，可以大规模生成。</p><p>本期AI TIME PhD直播间，我们有幸邀请到了复旦大学 NLP group2019级研究生李林阳分享他的观点。</p><p>李林阳：复旦大学 NLP group2019级研究生；导师为邱锡鹏教授；</p><hr><h2 id="一、针对文本任务的攻击"><a href="#一、针对文本任务的攻击" class="headerlink" title="一、针对文本任务的攻击"></a><strong>一、针对文本任务的攻击</strong></h2><p>尽管深度学习取得了成功，但最近的研究发现神经网络容易受到对抗样本的攻击，这些对抗样本是对原始输入进行细微扰动而制成的。尽管对抗性样本对于人而言几乎不可察觉，但是它们会误导神经网络进行错误的预测。针对对抗性攻击的学习可以提升神经网络的可靠性和健壮性，在计算机视觉领域，攻击策略及其防御措施都得到了很好的探索，但由于语言的离散性，对文本的对抗性攻击较为困难，难以保证语法流利且语义一致。</p><p><img src="https://i.loli.net/2020/12/01/S3DQcwvOEAt1uma.jpg"> </p><p> 表1 BERT-Attack方法生成样本的例子</p><p>当前对文本的成功攻击通常采用启发式规则来修改单词的字符，并用同义词替换单词。</p><p>之前的研究包括使用word embedding生成替换词；对原有句子的短语进行添加或删除；使用人工构建的规则进行词语替换。<strong>尽管上述方法取得了良好的效果，但在攻击成功率，语法正确性和语义一致性等方面，仍有很大的改进空间</strong>。此外，这些方法的替换策略通常很简单，受限于特定任务。</p><p>本文提出了一种有效且高质量的对抗样本生成方法：BERT-Attack，使用BERT作为生成器生成对抗样本。BERT-Attack的核心算法包括<strong>两个阶段</strong>：<strong>在给定输入序列中查找易受攻击的单词，然后用如BERT的生成器来生成易受攻击单词的替代词。</strong> BERT能够捕捉文本的上下文语义，因此生成的样本更为流畅且合理。作者将BERT这样的MLM语言模型用作生成器，并找到让BERT模型得到最大错误预测风险的扰动。另外，本文的方法只需要一次生成器前向，而且无需反复使用语言模型对对抗样本进行评分，速度有一定改进。表1展示了该攻击方法在几个数据集上的生成文本样例。</p><h2 id="二、BERT-ATTACK攻击方法"><a href="#二、BERT-ATTACK攻击方法" class="headerlink" title="二、BERT-ATTACK攻击方法"></a><strong>二、BERT-ATTACK攻击方法</strong></h2><p><img src="https://i.loli.net/2020/12/01/cOpDH2hWr81uzMS.png" alt="image-20201201154053279"></p><p>图1. BERT-ATTACK替换策略一步的样例</p><p>本文提出BERT-Attack，它使用原始BERT模型制作对抗性样本以对抗微调的BERT模型。对抗样本的生成包括两个步骤：（1）找出针对目标模型的易受攻击的单词，（2）用语义相似且语法正确的单词替换它们，直到成功攻击为止。具体而言：</p><p><strong>1.寻找易受攻击词(Vulnerable Words)</strong></p><p>作者给句子中的每一个词一个评分，得分与易受攻击程度呈正比，该评分按照去掉该词的句子在判别器上的输出结果的扰动程度给出。作者使用目标模型（微调的BERT或其他神经模型）的logit输出作为判别器。易受攻击词定义为序列中对最终输出logit有重要影响的单词。令表示输入语句，表示目标模型输出的正确标签y的logit，重要性得分定义为</p><p>$I_{w_{i}}=o_{y}(S)-o_{y}\left(S_{\backslash w_{i} j}\right.)$</p><p>其中，</p><p>$S_{\backslash w_{i}}=\left[w_{0} ; \ldots ; w_{i-1} ;[M A S K] ; w_{i+1} ; \ldots\right]$</p><p>就是将该词替换成“[MASK]”。然后，对降序排名，获取其中的前百分之的词组成可替换词表，记为L。</p><h2 id="2-BERT生成器的优点"><a href="#2-BERT生成器的优点" class="headerlink" title="2.BERT生成器的优点"></a><strong>2.BERT生成器的优点</strong></h2><p>找到易受攻击的单词后，将列表L中的单词一一替换，以寻找可能误导目标模型的干扰。以前的替换方法包括同义词词典，POS检查器，语义相似性检查器等。但是因为替换的时候只有词表，不考虑上下文，因此需要用传统语言模型给替换单词的句子打分。由于换一个词就得评价一次，时间成本比较高。</p><p>作者利用BERT进行单词替换，可确保所生成的句子相对流利且语法正确，还保留了大多数语义信息。此外，掩码语言模型的预测是上下文感知的，因此可以动态搜索扰动，而不是简单的同义词替换。而且针对一个词而言，仅通过一个前向即可产生候选文本，无需再用语言模型来对句子评分，提升了效率。</p><h2 id="3-替换策略"><a href="#3-替换策略" class="headerlink" title="3.替换策略"></a><strong>3.替换策略</strong></h2><p><img src="https://i.loli.net/2020/12/01/MIsY1JPptlRxGB5.png" alt="image-20201201153433807" style="zoom:50%;">图2 BERT-ATTACK替换算法</p><p>如图1所示，作者输入原句子给BERT，并根据BERT输出生成候选词。<strong style="color:red;">注意这里不用[MSAK]替换被攻击词语</strong>，其原因作者给出了如下解释：1. 有些词语替换后，和原句子几乎一样流畅但是语义可能变更。例如给定一个序列“I like the cat”，如果遮盖cat这个词，那么MLM模型很难预测原始单词cat，因为如“I like the dog”一样很流畅。2. MASK掉给定的单词后，每个候选词都需要运行一遍BERT前向，时间成本太高。</p><p>令M代表BERT模型，为原序列，是利用BERT的分词器分完词的序列，将H输入BERT中得到输出预测。使用top-K策略选择可能的替换词预测，其中K是超参数。作者遍历所有候选易攻击词表L生成替换词表。</p><p>由于BERT使用字节对编码（BPE）分词，候选词可能会被分开，因此还需要将所选单词与BERT中相应的子单词对齐。</p><p>针对未被分开的单个单词，作者使用相应的前K个预测候选逐一尝试替换，并使用NLTK过滤其中的停用词，另外对于情感分类任务候选词可能包括同义词和反义词，作者使用同义词词典过滤反义词。然后将替换完成的句子重新输入判别器，如果判别器给出与原label相反的判断那么输出该句子作为攻击句；否则，从筛选出的候选词中选择一个对logit影响最大的。</p><p>针对字词组（sub-word 应该不能翻译为字词组），由于无法直接获取其替代词，作者使用子词组合中所有词的预测中找到合适的词替代。作者首先使用MLM模型分析整个词组的易攻击程度，然后再选出词组的top-k组合。剩余过程与单个单词一致。</p><p><strong>三、实验结果</strong></p><p><strong>3.1 数据集和评价指标</strong></p><p>为了衡量所生成样本的质量，作者设计了几种评估指标：</p><p>●成功率（success rate）：攻击样本的判别器准确率。</p><p>●扰动百分比（perturbed percentage）更改文本的占比。</p><p>●每个样本的查询数量（query number per sample）一个样本生成对抗样本的需要访问判别器的次数。</p><p>●语义相似度（semantic similarity）使用通用句子编码器（Universal Sentence Encoder）评价的句子相似度。</p><p><img src="https://i.loli.net/2020/12/01/jgXqBTRfJYniZFe.png" alt="image-20201201153907478" style="zoom:50%;">表2 实验结果</p><p><strong>3.2 实验结果</strong></p><p>如表2所示，BERT-Attack方法成功欺骗了其下游的微调模型。在文本分类和自然语言推断任务中，经过微调的BERT均无法正确地对生成的对抗样本进行分类，攻击后的平均准确度低于10％。同时，扰动百分比小于10％，明显小于以前的工作，BERT-Attack方法更有效且更不易察觉。查询数量也要少得多。</p><p>另外可以观察到，由于扰动百分比非常低，因此通常更容易攻击评论分类任务。BERT-Attack仅替换少数几个单词就可能误导判别器。由于平均序列长度相对较长，因此判别器倾向于仅按序列中的几个词进行判断，这不是人类预测的自然方式。因此，这些关键字的干扰将导致目标模型的预测不正确，从而揭示了该模型的脆弱性。</p><p><strong>3.3人工验证</strong></p><p>为了进一步评估生成的对抗性样本，作者人工评估了流利性，语法以及语义保留方面生成的样本的质量。</p><p><img src="https://i.loli.net/2020/12/01/oDVS914hYZWJNje.png" alt="image-20201201153955115" style="zoom:50%;"></p><p>作者要求三名标注人员对生成的对抗性样本和原始序列的混合句子的语法正确性进行评分（1-5分），然后将原始文本和对抗文本混在一起进行人工预测。在IMDB和MNLI数据集中，作者分别选择100个原始样本和对抗样本验证。对于IMDB，将多数类作为人类预测标签，对于MNLI，则使用标注人员之间的平均分数。从表2中可以看出，对抗性样本的语义分数和语法分数接近原始样本。MNLI<strong>任务数据长且更加复杂（存在句子对（sentence pair）之间，重复出现的词汇较多，而基于替换的对抗样本则破坏了这种相同词汇的对应关系</strong>），使标注人员难以正确预测，因此其准确性要比简单的句子分类任务低。作者同样做了大量消融实验，实验结果表明该对抗方法生成的样本迁徙性强，生成速度快。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>在这项工作中，作者提出了一种高质量有效的BERT-Attack方法，以使用BERT<strong>掩蔽语言模型（masked-LM）生成对抗性样本。</strong>实验结果表明，该方法在保持最小扰动的同时，取得了较高的成功率。然而，从屏蔽语言模型生成的候选者有时可能是反义词或与原始单词无关，从而导致语义损失。因此，增强语言模型以生成更多与语义相关的扰动可能是将来完善BERT-Attack的一种可能解决方案。</p><hr><blockquote><p>整理：李键铨<br>排版：杨梦蒗<br>审稿：李林阳</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法&quot;&gt;&lt;a href=&quot;#复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法&quot; class=&quot;headerlink&quot; title=&quot;复旦大学李林阳：应用预训练模型实现对抗样本生成的高效方法&quot;&gt;&lt;/a&gt;复旦
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</title>
    <link href="http://yoursite.com/2020/10/28/Beyond-Accuracy-Behavioral-Testing-of-NLP-Models-with-CheckList/"/>
    <id>http://yoursite.com/2020/10/28/Beyond-Accuracy-Behavioral-Testing-of-NLP-Models-with-CheckList/</id>
    <published>2020-10-28T04:09:31.000Z</published>
    <updated>2020-10-28T04:11:33.512Z</updated>
    
    <content type="html"><![CDATA[<p>转载：<a href="https://dy.163.com/article/FH8NB44C0511DPVD.html" target="_blank" rel="noopener">https://dy.163.com/article/FH8NB44C0511DPVD.html</a></p><p>现在，ACL2020各个奖项都已悉数公布，对此AI科技评论做了详细报道。其中，最受人瞩目的当属最佳论文奖，今年该奖项由微软团队的 《Beyond Accuracy: Behavioral Testing of NLP Models with CheckList》一举拿下。</p><p>　　小编看到论文题目的第一眼就觉得哪些有些不对，于是赶紧通读了一下文章，嗯~确实不太对，这貌似和之前我们熟悉的NLP“大力出奇迹”的模型套路不太一样啊？</p><p>　　那么这篇论文到底讲了什么呢，又何以摘得桂冠呢？</p><p>　　论文解读以外，我们进一步对论文的第二作者吴彤霜进行了专访，以更深入地了解最佳论文团队背后的工作。</p><h3 id="论文方法一览"><a href="#论文方法一览" class="headerlink" title="论文方法一览"></a><strong>论文方法一览</strong></h3><p>　　我们从论文的题目入手来了解一下这篇论文在讲什么。</p><p>　　首先是”Beyond Accuracy”：这是在说要超越Accuracy，这里Accuracy说的是NLP模型在各大数据集和任务上跑出的准确率，也即是性能的一种度量。</p><p>　　那既然要超越它总要有一个理由:</p><p>　　1.评估所用的训练-验证-测试划分集来估计模型的准确性时保留的数据集往往不全面。</p><p>　　2.测试集中往往包含与训练数据相同的偏差，这种方式可能高估了模型在真实世界的性能</p><p>　　3.通过Accuracy一刀切的方式很难找出模型失败在哪里，以及如何修复它。</p><p>　　对此本文提出的Beyond 方式又是如何呢？</p><p>　　Behavioral Testing of NLP Models with CheckList！也即用CheckList对NLP模型做行为测试。</p><h4 id="1、We-should-test-NLP-models"><a href="#1、We-should-test-NLP-models" class="headerlink" title="1、We should test NLP models"></a><strong>1、We should test NLP models</strong></h4><p>　　训练NLP模型的主要目标之一是泛化，虽然Accuracy是评价泛化的主要方法，但它往往高估了NLP模型的性能，用于评估模型的替代方法要么侧重于单个任务，要么侧重于特定的行为，benchmark的准确性不足以评估NLP模型。</p><p>　　除此之外许多额外的评估方法已经被提出来了，例如评估对噪声或对抗性变化的鲁棒性、公平性、逻辑一致性、可解释、诊断数据集和交互式错误分析。然而，这些方法要么侧重于单个任务，如问答或自然语言推理，要么侧重于一些能力（如鲁棒性），因此没有提供关于如何评估模型的全面指导。</p><p>　　因此在这这篇论文中，作者提出了CheckList(检查表)，一种新的评估方法和配套工具，用于NLP模型的综合行为测试。</p><h4 id="2、Software-engineering-gt-NLP"><a href="#2、Software-engineering-gt-NLP" class="headerlink" title="2、Software engineering-&gt;NLP"></a><strong>2、Software engineering-&gt;NLP</strong></h4><p>　　软件工程研究提出了测试复杂软件系统的各种范式和工具。特别是“行为测试”（黑盒测试）是指在不了解内部结构的情况下，通过验证输入输出行为来测试系统的不同能力。虽然有明显的相似之处，但软件工程的许多见解还没有应用到NLP模型中。</p><p>　　作者借鉴软件工程中行为测试的原理提出了CheckList：一种和模型、任务都无关的测试方法，它使用三种不同的测试类型来测试模型的各个功能。</p><p>　　作者用三个任务的测试来说明检查表的效用，识别商业和SOTA模型中的关键错误。在一项用户研究中，一个负责商业情绪分析模型的团队在一个经过广泛测试的模型中发现了新的、可操作的bug。在另一个用户研究中，使用CheckList的NLP实践者创建了两倍多的测试，发现的bug几乎是没有检查表的用户的三倍。</p><p>　　<img src="https://i.loli.net/2020/10/28/qk64rwGIt1lBJWE.png" alt="img"></p><p>　　图1</p><h4 id="3、What-is-CheckList"><a href="#3、What-is-CheckList" class="headerlink" title="3、What is CheckList"></a><strong>3、What is CheckList</strong></h4><p>　　CheckList包括一个通用语言能力和测试类型的矩阵，有助于全面的测试构思，以及一个快速生成大量不同测试用例的软件工具。从概念上讲，用户通过填写矩阵中的单元格来“检查”模型（图1），每个单元格可能包含多个测试。CheckList应用了“测试与实现脱钩”的行为测试原则，即将模型视为一个黑盒，允许对不同数据上训练的不同模型进行比较，或者对不允许访问训练数据或模型结构的第三方模型进行比较。</p><h4 id="4、What-to-test：capabilities"><a href="#4、What-to-test：capabilities" class="headerlink" title="4、What to test：capabilities"></a><strong>4、What to test：capabilities</strong></h4><p>　　CheckList通过提供适用于大多数任务的语言能力列表，指导用户测试什么。CheckList引入了不同的测试类型，比如在某些干扰下的预测不变性，或者一组“健全性检查”的性能。</p><p>　　虽然测试单个组件是软件工程中的常见实践，但现代NLP模型很少一次只构建一个组件。相反，CheckList鼓励用户考虑如何在手头的任务上表现出不同的自然语言能力，并创建测试来评估这些能力的模型。例如，词汇+POS能力取决于一个模型是否具有必要的词汇，以及它是否能够恰当地处理具有不同词性的单词对任务的影响。对于情绪，我们可能需要检查模型是否能够识别带有积极、消极或中性情绪的单词，方法是验证它在“这是一次很好的飞行”等示例上的行为。</p><p>　　基于此，作者建议用户至少考虑以下性能（capabilities）：</p><p>　　词汇+POS（任务的重要单词或单词类型）</p><p>　　Taxonomy（同义词、反义词等）</p><p>　　健壮性（对拼写错误、无关更改等）</p><p>　　NER（正确理解命名实体）</p><p>　　公平性</p><p>　　时态（理解事件顺序）</p><p>　　否定</p><p>　　共指（Coreference），</p><p>　　语义角色标记（理解诸如agent、object等角色）</p><p>　　逻辑（处理对称性、一致性和连词的能力）。</p><p>　　通过以上，CheckList实现包括多个抽象，帮助用户轻松生成大量测试用例，例如模板、词典、通用扰动、可视化和上下文感知建议。然而此功能列表并非详尽无遗，而是用户的一个起点，用户还应提供特定于其任务或域的附加功能。</p><h4 id="5、How-to-test"><a href="#5、How-to-test" class="headerlink" title="5、How to test"></a><strong>5、How to test</strong></h4><p>　　作者提示用户使用三种不同的测试类型来评估每个功能：最小功能测试、不变性和定向期望测试（矩阵中的列）。</p><p>　　1）最小功能测试（MFT）:它是受软件工程中单元测试的启发的一组简单的示例（和标签）的集合，用于检查功能中的行为。MFT类似于创建小而集中的测试数据集，尤其适用于在模型使用快捷方式处理复杂输入而不实际掌握功能的情况下进行检测。</p><p>　　<img src="https://i.loli.net/2020/10/28/2bFIGQ17wgVB3su.png" alt="img"></p><p>　　2）不变性测试（INV）：当对输入应用保留标签的扰动并期望模型预测保持不变时。不同的功能需要不同的扰动函数，例如，更改NER情感功能的位置名称，或者引入输入错误来测试健壮性能力。</p><p>　　<img src="https://i.loli.net/2020/10/28/c4Wa9IZzusY1Kj7.png" alt="img"></p><p>　　3）定向期望测试（DIR）：与不变性测试类似，只是标签会以某种方式发生变化。例如，我们预计，如果我们在针对某家航空公司的推文末尾添加“You are lame.”（图1C），情绪不会变得更积极。</p><p>　　<img src="https://i.loli.net/2020/10/28/Y78FjaetnpdgOJb.png" alt="img"></p><h4 id="6、可视化效果"><a href="#6、可视化效果" class="headerlink" title="6、可视化效果"></a><strong>6、可视化效果</strong></h4><p>　　调用test.visual_summary()</p><p>　　<img src="https://dingyue.ws.126.net/2020/0711/f079207eg00qdafs60095d200hr005pg00it0061.gif" alt="img"></p><p>　　在代码中调用suite.summary()（与test.summary相同）或suite.visual_summary_table() 显示测试结果如下：</p><p>　　<img src="https://dingyue.ws.126.net/2020/0711/c3640090g00qdafs701u7d200hr00bcg00it00c0.gif" alt="img"></p><p>　　模型保存和加载：精简至极！</p><p>　　<img src="https://i.loli.net/2020/10/28/KQDGslUnygeLESV.png" alt="img"></p><h4 id="7、更方便的大规模生成测试用例"><a href="#7、更方便的大规模生成测试用例" class="headerlink" title="7、更方便的大规模生成测试用例"></a><strong>7、更方便的大规模生成测试用例</strong></h4><p>　　用户可以从头开始创建测试用例，也可以通过扰动现有的数据集来创建测试用例。从头开始可以更容易地为原始数据集中可能未充分表示或混淆的特定现象创建少量高质量测试用例。然而，从头开始编写需要大量的创造力和努力，这通常会导致测试覆盖率低或者生成成本高、耗时长。扰动函数很难编写，但同时生成许多测试用例。为了支持这两种情况，作者提供了各种抽象，从零开始扩展测试创建，并使扰动更容易处理。</p><h4 id="8、使用CheckList测试SOTA模型"><a href="#8、使用CheckList测试SOTA模型" class="headerlink" title="8、使用CheckList测试SOTA模型"></a><strong>8、使用CheckList测试SOTA模型</strong></h4><p>　　作者通过付费API 检查了以下商业情绪分析模型：微软的文本分析、谷歌云的自然语言和亚马逊的Constract。我们还检查了在SST-23（acc:92.7%和94.8%）和QQP数据集（acc:91.1%和91.3%）上微调的BERT base和RoBERTa base。对于MC，作者使用了一个经过预训练的大BERT 微调阵容，达到93.2 F1。</p><p>　　<img src="https://i.loli.net/2020/10/28/pn7OqdKUx4shRLD.png" alt="img"></p><h4 id="9、测试商业系统"><a href="#9、测试商业系统" class="headerlink" title="9、测试商业系统"></a><strong>9、测试商业系统</strong></h4><p>　　作者联系了负责微软服务销售的通用情绪分析模型的团队（表1中的q）。由于它是一个面向公众的系统，模型的评估过程比研究系统更全面，包括公开可用的基准数据集以及内部构建的重点基准（例如否定、emojis）。此外，由于该服务已经成熟，拥有广泛的客户群，因此它经历了许多错误发现（内部或通过客户）和后续修复的周期，之后在基准测试中添加了新的示例。</p><p>　　作者的目标是验证检查表是否会增加价值，即使在这样的情况下，模型已经用当前的实践进行了广泛的测试。</p><p>　　作者邀请小组参加了一个持续约5小时的检查表会议。该团队集思广益地进行了大约30个测试，涵盖了所有功能。</p><p>　　从质量上讲，该小组称检查表非常有用：</p><p>　　（1）他们测试了他们没有考虑过的能力；</p><p>　　（2）他们测试了他们考虑过但不在benchmark中的能力；</p><p>　　（3）甚至他们有基准的能力（例如否定）也用检查表进行了更彻底和系统的测试。</p><p>　　他们发现了许多以前未知的错误，他们计划在下一个模型迭代中修复这些错误。最后，他们表示，他们肯定会将检查表纳入他们的开发周期，并要求访问我们的实现。</p><h4 id="10、用户研究"><a href="#10、用户研究" class="headerlink" title="10、用户研究"></a><strong>10、用户研究</strong></h4><p>　　作者进行了一项用户研究，以在一个更可控的环境中进一步评估检查表的不同子集，并验证即使是没有任务经验的用户也能获得洞察并发现模型中的错误。</p><p>　　尽管用户在使用CheckList时不得不解析更多的指令和学习新的工具，但他们同时为模型创建了更多的测试。</p><p>　　在实验结束时，作者要求用户评估他们在每个特定测试中观察到的失败的严重程度，研究结果令人鼓舞：有了检查表的子集，没有经验的用户能够在2小时内发现SOTA模型中的重大缺陷。此外，当被要求对检查表的不同方面进行评分时（1-5分），用户表示，测试环节有助于他们进一步了解模型，功能帮助他们更彻底地测试模型，模板也是如此。</p><p>　　评估特定语言能力的一种方法是创建挑战性数据集。我们的目标不是让检查表取代挑战或基准数据集，而是对它们进行补充。CheckList保留了挑战集的许多优点，同时也减轻了它们的缺点：用模板从头开始编写示例提供了系统控制，而基于扰动的INV和DIR测试允许在未标记的自然发生的数据中测试行为。</p><p>　　最后用户研究表明，CheckList可以轻松有效地用于各种任务：用户在一天内创建了一个完整的测试套件进行情绪分析，两个小时内创建了的MFTs，这两个都揭示了之前未知的严重错误。</p><h3 id="专访吴彤霜：最佳论文何以花落此家"><a href="#专访吴彤霜：最佳论文何以花落此家" class="headerlink" title="专访吴彤霜：最佳论文何以花落此家"></a>专访吴彤霜：最佳论文何以花落此家</h3><p>　　到这里我们大概明白了这篇论文到底在讲什么，但是我们还是心存疑惑，何以它能获得最佳论文殊荣？</p><p>　　<strong>以下为专访实录：</strong></p><p>　　<strong>AI 科技评论：</strong>首先恭喜您和您的团队斩获ACL2020最佳论文！我们想先了解一下这项工作的完成大概花了多长时间，把软件测试带入NLP模型检测的想法是最近才有的吗还是说之前就有了最近才实现？</p><p>　　<strong>吴彤霜：</strong>这个项目最早是一作快博士毕业时我们开始合作的，中间因为各种原因搁置了一段时间，实际做大概一年吧。引用软件测试应该可以算是一个新的想法。以前有很多nlp模型分析的论文本质上也可以说是我们论文里提到的那种“行为测试” (behavioral testing)，比如各种NLI challenge set。只不过这些工作大部分是针对某一个任务在做某个特定种类的分析，每次都要从头开始。我们工作其中的一个主要的一个贡献就是把这些分析做归一化，提供一个测试框架+开源系统。</p><p>　　<strong>AI 科技评论：</strong>这项测试系统是不是可以理解为对抗扰动系统啊？或者相比有什么优势呢？</p><p>　　<strong>吴彤霜：</strong>不变性测试 (INVariant test) 可以相当于扰动，就是模型在预测一个句子s和经修改后的版本s’时结果类似。CheckList还支持别的测试类别 (test type)：定向测试 (DIRectional test) 是用来测试预测结果的特定变化，最小功能测试 (Min Func Test) 不需要有配对的例子，只要一个能生成单个测试例句的模板就可以了。</p><p>　　只和INV（不变性测试 ）相比而言，现在NLP的大部分对抗句经常是在改拼写或者会产生乱码，比较难保证句子的连贯性，而能保证句子连贯性的居多是改近义词 (it’s a good movie -&gt; it’s a great movie)。CheckList因为允许自定义改写函数 (perturbation function)，所以可以更多样地测试模型的性能，比如看情感分析模型是否能辨认虚拟语气 (it’s a bad movie -&gt; it would have been a good movie)。这种测试也更系统化，因为可以生成很多对改写方法类似的句子/测试用例 (test case)。</p><p>　　当然相应的checklist的自动化就比较差，需要人来定义这些测试 :)</p><p>　　<strong>AI 科技评论：</strong>请问你们团队成员之前有过软件测试的经验吗，在CheckList的设计环节有什么亮点以及代码实现过程中有什么难点？</p><p>　　<strong>吴彤霜：</strong>应该不算有经验，没有在工业界实战过，顶多就是在软工课写单元测试，所以最开始其实也认真学习了一下软工 :)</p><p>　　设计上我觉得最大的亮点是对于性能 (capability) 的定义 。我们遇到一个瓶颈就是试图给每个NLP task设计需要测试的不同方面，但这样就非常繁琐，也失去了可拓展性。直到后来我们和其他researcher聊的时候意识到其实大部分的模型需要测试的“capability”基本比较稳定，只是不同任务里对标签的影响会有区别，比如[改主语]对NLI影响会比对情感分析要大。这样一个稳定的capability集合就让整个框架干净了很多。</p><p>　　开源上，其实NLP部分还是比较整洁的，但是为了让大家能比较流畅地在notebook里浏览和回顾test集，我们下了很大功夫研究怎们做交互插件，是一个大坑，但是最终效果还挺好看的，可以到readme里看看preview感受一下，哈哈。</p><p>　　写作上，因为marco在微软，我们很幸运能近水楼台找微软情感分析的工程组来做用户研究，让我们真的看到了CheckList在已经被认为是大规模测试过的模型仍然很有用。</p><p>　　<strong>AI 科技评论：</strong>很开心你们把这项工作开源，我想这项工作只是一个开始对吗？（大家都可以在你们开源的代码上进行哪些尝试和改进呢，比如自定义测试模板之类）</p><p>　　<strong>吴彤霜：</strong>最重要的是希望能看到大家提出的测试实例！其实比起很多NLP模型，CheckList是一个比较依靠人的力量的项目，测试者仔细设计实例才能用它最大程度暴露模型可能存在的缺陷。我们考虑的一个想法是希望可以做一个类似模型排行榜的测试榜，大家可以上传分享自己的测试集，甚至是顶或者踩别人的测试，最终让每个任务都能有一个比较稳定的测试集，也方便模型间的比较。</p><p>　　其次，我们也很期待看到大家会不会有关于如何让CheckList更自动化的想法，实现一键测试这个终极目标 :)</p><p>　　以及更研究向的：</p><p>　　我个人对于设计更稳定的测试也很感兴趣。CheckList对具体实例比较敏感，不同人在测试同一个模型性能时，如果实例设计不同，最终测试结果可能会有一些偏差。有没有什么交互手段能帮助测试者尽量穷尽一个性能所有的相关改写？甚至还有没有什么办法能慢慢形成一些自动的测试拓展？这个可能也和上面提到的自动化有一些关系。</p><p>　　最后测试带来的一个恒久不变的问题：so what？一个模型有问题之后，应该用什么样的标准来决定一个模型是不是可以被公开部署 (比如可能公平性测试的容错率可能远低于拼写错误)？应该如何改进它？</p><p>　　<strong>AI 科技评论：</strong>请问软件测试的思想只适用于NLP领域吗 ，在CV领域可行吗，应该怎么去设计测试系统？</p><p>　　<strong>吴彤霜：</strong>我相信是可行的！抽象来讲，本文图1的这种框架似乎能直接套用在CV上。</p><p>　　比如说一个最简单的狗和狼的分类，这个模型首先得能辨认有动物出现 (MFT)，然后改变图片的背景应该不影响预测 (INV)，但改变动物的头的形状应该是要影响的 (DIR)。vision里的“改写”效果其实比NLP好很多，也许更好用也说不定 :)</p><p>　　对设计系统而言，我觉得比较重要的是抽取基本组件。在NLP版本的CheckList里有一个重要组件就是写生成template/模板；也许在vision里则是需要提供一些基础像素之类的。</p><p>　　当然也可以考虑除了行为和单元测试之外的测试思想，比如如果是pipeline模型，考虑如何设计集成测试也许也会很有用 :)</p><p>　　<strong>AI 科技评论：</strong>可以简单介绍一下你们的团队成员吗，以及你们的近期工作、未来研究方向？</p><p>　　<strong>吴彤霜：</strong>隆重介绍一下没有出镜的一作吧，marco也是华大的博士，2018年毕业以后就加入了微软研究院，主要在做模型可解释性和分析，之前很有名的LIME（一种解释机器学习模型的方法——Local Interpretable Model-Agnostic Explanations）就是出自他手。除了CheckList，他今年在CVPR上也有一篇合作论文，是分析vqa model的稳定性的。现在主要在做vision模型的错误分析以及模型比较。</p><p>　　我们现在也在合作一个新工作，这项工作更多是关于如何人去探索模型的可解释性。虽然现在主要做的都是人如何检查模型，但是我们对于模型如何能反过来规范人或者帮助人也很感兴趣 :) 三四作Carlos和Sameer都是marco的导师，分别是ML和NLP的大佬。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>　　虽然CheckList目前也有一些不足比如CheckList不能直接用于非行为问题，例如数据版本控制问题、标记错误、注释器偏差、最坏情况下的安全问题或缺乏可解释性。</p><p>　　但是不可否认的是，使用CheckList创建的测试可以应用于任何模型，这使得它很容易被纳入当前的基准测试或评估pipeline中。用户研究表明，CheckList很容易学习和使用，对已经对模型进行了长时间测试的专家用户以及在任务中缺乏经验的实践者都很有帮助。</p><p>　　另外对吴同学的专访，我们相信， 本篇论文工作确实开创地把软件测试系统引入NLP模型的测试之中并且提供了完善的测试工具。 这将会给社区和企业带来很大的商业价值，比如CheckList测试工具将会节省很大的人力成本。</p><p>　　最后，我们相信，这种系统引进软件测试的思想也将会在CV乃至整个AI领域大有作为。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载：&lt;a href=&quot;https://dy.163.com/article/FH8NB44C0511DPVD.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://dy.163.com/article/FH8NB44C0511DPVD.
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>N-Gram模型</title>
    <link href="http://yoursite.com/2020/10/27/N-Gram%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/10/27/N-Gram模型/</id>
    <published>2020-10-27T09:22:01.000Z</published>
    <updated>2020-10-31T13:49:29.760Z</updated>
    
    <content type="html"><![CDATA[<p>转载：<a href="https://zhuanlan.zhihu.com/p/32829048" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32829048</a></p><h2 id="一、什么是n-gram模型"><a href="#一、什么是n-gram模型" class="headerlink" title="一、什么是n-gram模型"></a><strong>一、什么是n-gram模型</strong></h2><p>N-Gram是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。</p><p>每一个字节片段称为gram，对所有gram的出现频度进行统计，并且按照事先设定好的阈值进行过滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量维度。</p><p>该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。</p><p>说完了n-gram模型的概念之后，下面讲解n-gram的一般应用。</p><h2 id="二、n-gram模型用于评估语句是否合理"><a href="#二、n-gram模型用于评估语句是否合理" class="headerlink" title="二、n-gram模型用于评估语句是否合理"></a><strong>二、n-gram模型用于评估语句是否合理</strong></h2><p>如果我们有一个由 m 个词组成的序列（或者说一个句子），我们希望算得概率 $P(w_1, w_2,…,w_m)$，根据链式规则，可得</p><p><img src="https://i.loli.net/2020/10/27/ye3X8vh6LcKTzjr.png" alt="image-20201027172437915"></p><p>这个概率显然并不好算，不妨利用马尔科夫链的假设，即当前这个词仅仅跟前面几个有限的词相关，<strong><em>因此也就不必追溯到最开始的那个词，这样便可以大幅缩减上述算式的长度\</em></strong>。即</p><p><img src="https://i.loli.net/2020/10/27/XjDPInlQY9GN7ZU.png" alt="image-20201027172458291" style="zoom: 67%;"></p><p><strong><em>这个马尔科夫链的假设为什么好用？我想可能是在现实情况中，大家通过真实情况将n=1，2，3，….这些值都试过之后，得到的真实\</em></strong>的效果和时间空间的开销权衡之后，发现能够使<strong><em>用。\</em></strong></p><p>下面给出一元模型，二元模型，三元模型的定义：</p><p>当 n=1, 一个一元模型（unigram model)即为 ：</p><p><img src="https://i.loli.net/2020/10/27/UMNPucLtdw3zxgo.png" alt="img"></p><p>当 n=2, 一个二元模型（bigram model)即为 ：</p><p><img src="https://i.loli.net/2020/10/27/UmZkyzIK4fws7Nj.png" alt="img"></p><p>当 n=3, 一个三元模型（trigram model)即为</p><p><img src="https://i.loli.net/2020/10/27/Vr1LZIG7snCvokK.png" alt="img"></p><p>然后下面的思路就很简单了，在给定的训练语料中，利用贝叶斯定理，将上述的条件概率值（<strong>因为一个句子出现的概率都转变为右边条件概率值相乘了</strong>）都统计计算出来即可。下面会给出具体例子讲解。这里先给出公式：</p><p><img src="https://i.loli.net/2020/10/27/locAQnW9bGpXa3K.png" alt="img"></p><p>对第一个进行解释，后面同理,如下：</p><p><img src="https://i.loli.net/2020/10/27/iGbljDacPTSWu92.png" alt="image-20201027172711422"></p><p>下面给出具体的例子。</p><h2 id="三、二元语言模型判断句子是否合理"><a href="#三、二元语言模型判断句子是否合理" class="headerlink" title="三、二元语言模型判断句子是否合理\"></a><strong><em>三、二元语言模型判断句子是否合理\</em></strong></h2><p><strong><em>下面例子来自于：\</em></strong><a href="https://link.zhihu.com/?target=http%3A//blog.csdn.net/baimafujinji/article/details/51281816" target="_blank" rel="noopener">自然语言处理中的N-Gram模型详解 - 白马负金羁 - CSDN博客</a>和《北京大学 常宝宝 以及 The University of Melbourne “Web Search and Text Analysis” 课程的幻灯片素材》</p><p>假设现在有一个语料库，我们统计了下面的一些词出现的数量</p><p><img src="https://i.loli.net/2020/10/27/ufWOseUyrAm9qV3.png" alt="img"></p><p>下面的这些概率值作为已知条件：</p><p><img src="https://i.loli.net/2020/10/27/G2Y6zKZNxBCitVo.png" alt="img"></p><p><img src="https://pic1.zhimg.com/80/v2-a7c0d77143e0c997abd45e1535eaeb8c_1440w.jpg" alt="img"></p><p>$p(want|<s>) = 0.25$</s></p><p>下面这个表给出的是基于Bigram模型进行计数之结果</p><p><img src="https://i.loli.net/2020/10/27/G2Y6zKZNxBCitVo.png" alt="img"></p><p><img src="https://i.loli.net/2020/10/27/az7ewSWFYZGUbIu.png" alt="img"></p><p>例如，其中第一行，第二列 表示给定前一个词是 “i” 时，当前词为“want”的情况一共出现了827次。据此，我们便可以算得相应的频率分布表如下。</p><p><img src="https://i.loli.net/2020/10/27/9WhUqaZtcC3xDn7.jpg" alt="img"></p><p>比如说，我们就以表中的$p(eat|i)=0.0036$这个概率值讲解，从表一得出“i”一共出现了2533次，而其后出现eat的次数一共有9次，$p(eat|i)=p(eat,i)/p(i)=count(i,eat)/count(i)=9/2533 = 0.0036$</p><p>下面我们通过基于这个语料库来判断$s1=“<s> i want english food</s>” $ 与 $s2 = “<s> want i english food</s>“$哪个句子更合理：通过例子来讲解是最人性化的，我在网上找了这么久发现这个例子最好：</p><p><strong>首先来判断$p(s1)$</strong></p><p>$P(s1)=P(i|<s>)P(want|i)P(english|want)P(food|english)P(</s>|food)$</p><p>=$0.25×0.33×0.0011×0.5×0.68=0.000031$</p><p><strong>再来求$p(s2)$</strong></p><p>$P(s2)=P(want|<s>)P(i|want)P(english|want)P(food|english)P(</s>|food)$</p><p>=$0.25×0.0022×0.0011×0.5×0.68 = 0.00000002057$</p><p><strong>通过比较我们可以明显发现0.00000002057&lt;0.000031,也就是说s1= “i want english food&lt;/s&gt;”更像人话。</strong></p><p><strong>再深层次的分析，我们可以看到这两个句子的概率的不同，主要是由于顺序i want还是want i的问题，根据我们的直觉和常用搭配语法，i want要比want i出现的几率要大很多。所以两者的差异，第一个概率大，第二个概率小，也就能说的通了。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;转载：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32829048&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/32829048&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;一
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>PPL-语句通顺</title>
    <link href="http://yoursite.com/2020/10/27/PPL-%E8%AF%AD%E5%8F%A5%E9%80%9A%E9%A1%BA/"/>
    <id>http://yoursite.com/2020/10/27/PPL-语句通顺/</id>
    <published>2020-10-27T03:37:20.000Z</published>
    <updated>2020-11-05T00:36:17.976Z</updated>
    
    <content type="html"><![CDATA[<h3 id="语句通顺-一些调研"><a href="#语句通顺-一些调研" class="headerlink" title="语句通顺 - 一些调研"></a>语句通顺 - 一些调研</h3><ul><li><p>BERT模型通过在大量语料的训练可以判断一句话是否通顺</p></li><li><p>理解 NNLM <a href="https://zhuanlan.zhihu.com/p/65446874" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/65446874</a></p><p>以上推理就是</p><ol><li>用词汇的联合概率表达一个语句是否通顺；</li><li>将计算联合概率转换为计算条件概率；</li><li>将条件概率由不定长度的且一般较大的t维降到一般较小的n-1维;</li></ol></li><li><p><a href="https://blog.csdn.net/blmoistawinde/article/details/104966127" target="_blank" rel="noopener">https://blog.csdn.net/blmoistawinde/article/details/104966127</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/76912493" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76912493</a></p></li></ul><h3 id="PPL-评价指标"><a href="#PPL-评价指标" class="headerlink" title="PPL-评价指标"></a>PPL-评价指标</h3><p>在得到不同的语言模型（一元语言模型、二元语言模型….）的时候，我们如何判断一个语言模型是否好还是坏，一般有两种方法：</p><p>1、一种方法将其应用到具体的问题当中，比如机器翻译、speech recognition、spelling corrector等。然后看这个语言模型在这些任务中的表现（extrinsic evaluation，or in-vivo evaluation）。但是，这种方法一方面难以操作，另一方面可能非常耗时，可能跑一个evaluation需要大量时间，费时难操作。</p><p>2、针对第一种方法的缺点，大家想是否可以根据与语言模型自身的一些特性，来设计一种简单易行，而又行之有效的评测指标。于是，人们就发明了perplexity这个指标。</p><p><img src="https://i.loli.net/2020/10/27/UcxVWKjtlCi5Tw7.png" alt="image-20201027113452950" style="zoom: 25%;"></p><p>困惑度（perplexity）的基本思想是：<strong>给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，</strong></p><p>由公式可知，<strong>句子概率越大，语言模型越好，迷惑度越小。</strong></p><p>$P(W_1, W_2, … , W_t)$</p><p>= $P(W_t | W_{t-1}, … , W_2, W_1) * P(W_{t-1}, … , W_2, W_1))$</p><p>= $P(W_t | W_{t-1}, … , W_2, W_1)$ <em> $P(W_{t-1} | W{t-2}, …, W_2, W_1) $ </em> $ P(W{t-2}, …, W_2, W_1)$</p><p>= $P(W_t | W_{t-1}, … , W_2, W_1)$ <em> $ P(W_{t-1} | W_{t-2}, …, W_2, W_1) $ </em> $ … $ <em> $ P(W_2 | W_1) </em> P(W_1)$</p><p>一些 ngram 模型经 训练文本后在测试集上的困惑度值：</p><p><img src="https://i.loli.net/2020/10/31/bYtXO3hgJs8TE6f.jpg" alt="img" style="zoom: 50%;"></p><ul><li>求通俗解释NLP里的perplexity是什么？ - 习翔宇的回答 - 知乎 <a href="https://www.zhihu.com/question/58482430/answer/412012509" target="_blank" rel="noopener">https://www.zhihu.com/question/58482430/answer/412012509</a></li><li>也可以用交叉熵损失函数来表示</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;语句通顺-一些调研&quot;&gt;&lt;a href=&quot;#语句通顺-一些调研&quot; class=&quot;headerlink&quot; title=&quot;语句通顺 - 一些调研&quot;&gt;&lt;/a&gt;语句通顺 - 一些调研&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;BERT模型通过在大量语料的训练可以判断一句话是否通顺&lt;/
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Visual Grounding in Video for Unsupervised Word Translation</title>
    <link href="http://yoursite.com/2020/10/18/Visual-Grounding-in-Video-for-Unsupervised-Word-Translation/"/>
    <id>http://yoursite.com/2020/10/18/Visual-Grounding-in-Video-for-Unsupervised-Word-Translation/</id>
    <published>2020-10-18T10:53:46.000Z</published>
    <updated>2020-10-18T10:54:21.489Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>我们的目标是使用视觉基础来改进语言之间的非监督词映射。其核心思想是通过学习母语教学视频中未配对的嵌入语，在两种语言之间建立一种共同的视觉表达。</p><p>本文的工作就是<strong>向机器提供不同的教学视频</strong>，这些视频的内容是人们用本国语言的教学视频。比如说，说中文和英文教别人榨橙汁的教学视频。这类视频有两个特点：视频网站上<strong>大量存在</strong>和<strong>内容相似度高</strong>，非常适合用于训练。但是这些视频也有一些弊端，会有很多无关废话（如“观众老爷们记得素质三连哦~”）。</p><p>即使如此，这种基于视觉的翻译提高了翻译的精度。</p><h3 id="Unsupervised-Multilingual-Learning"><a href="#Unsupervised-Multilingual-Learning" class="headerlink" title="Unsupervised Multilingual Learning"></a>Unsupervised Multilingual Learning</h3><p><img src="https://i.loli.net/2020/10/18/qzv2QN3bd8oInSx.png" alt="image-20201018172741937"></p><p>一个无监督的系统，该系统通过将语言嵌入视频中翻译单词。其中，不需要任何配对数据来学习翻译。</p><p><strong>Our method</strong> is unsupervised in that it learns the correspondences between two languages $X$ and $Y$ (e.g. English and French) without any parallel (paired) corpora.</p><p>given two distinct collections of instructional videos, i.e. n videos narrated with language $X$and another m different videos with language $Y$.</p><p><strong>Our goal</strong> is to learn to map languages $X$ and $Y$ by leveraging the shared visual modality $Z$ – the videos.</p><p><strong>Loss function</strong></p><p><img src="https://i.loli.net/2020/10/18/uYrymIKnJwvL2G6.png" alt="image-20201018173014409" style="zoom: 25%;"></p><h4 id="Multilingual-Visual-Embedding-Architecture"><a href="#Multilingual-Visual-Embedding-Architecture" class="headerlink" title="Multilingual Visual Embedding: Architecture"></a>Multilingual Visual Embedding: Architecture</h4><p><img src="https://i.loli.net/2020/10/18/XU2slICkuhyLBRT.png" alt="image-20201018172904212" style="zoom:33%;"></p><p><strong>yaya:</strong>  通过 视觉将两种语言做一种映射是存在困难的。文中列出了三点：<br>（1）learning video-text embeddings from instructional videos is difficult as the speech in these videos is only loosely related to the scene.</p><p>（2）in multilingual setting, such errors compound since both languages have this low video-text relevance;<br>（3）visually similar videos may not be semantically similar.</p><p>因此本文不同video 作为桥梁直接学习两种语言的映射，而是采取了间接的方式：we learn a joint (monolingual) video-text embedding space from instructional videos.</p><p>对于一种语言X, 学习视频及其字幕的映射，对于另一种语言，也学习一种映射，同时，在这种语言上加一个Adaptlayer, 使得 X和Y 能够映射到一个共同的空间。</p><p><strong>模型细节：</strong></p><p>其中X编码器 = WordEmbed + （Liner + ReLU MaxPool) + Linear</p><p>（WordEmbed层，度向量的转换；Linear层，建立与 Joint Embedding Space的映射）</p><p>而Y编码器则多了一个调整层（AdaptLayer），进行的是跨语言共享模型的权重分配，尽量让Y语言的词和X语言的词有相似的嵌入。</p><h4 id="MUVE-Improving-Unsupervised-Translation"><a href="#MUVE-Improving-Unsupervised-Translation" class="headerlink" title="MUVE: Improving Unsupervised Translation"></a>MUVE: Improving Unsupervised Translation</h4><p>略</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;p&gt;我们的目标是使用视觉基础来改进语言之间的非监督词映射。其核心思想是通过学习母语教学视频中未配
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[VIVO] Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training</title>
    <link href="http://yoursite.com/2020/10/18/VIVO-Surpassing-Human-Performance-in-Novel-Object-Captioning-with-Visual-Vocabulary-Pre-Training/"/>
    <id>http://yoursite.com/2020/10/18/VIVO-Surpassing-Human-Performance-in-Novel-Object-Captioning-with-Visual-Vocabulary-Pre-Training/</id>
    <published>2020-10-18T04:11:10.000Z</published>
    <updated>2020-10-31T13:49:42.322Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>VIVO可以在没有文本标签的数据上进行文本和图像的多模态预训练，摆脱了对配对图文数据的依赖，可以直接利用ImageNet等数据集的类别标签。借助VIVO，模型可以学习到物体的视觉外表和语义之间的关系，建立视觉词表。</p><p>这个视觉词表是啥呢？其实就是一个图像和文本的联合特征空间，在这个特征空间中，语义相近的词会聚类到一起，如金毛和牧羊犬，手风琴和乐器等。</p><p>预训练建好词表后，模型只需在有少量共同物体的配对图文的数据上进行微调，模型就能自动生成通用的模板语句，使用时，即使出现没见过的词，也能从容应对，相当于把图片和描述的各部分解耦了。</p><p>所以VIVO既能利用预训练强大的物体识别能力，也能够利用模板的通用性，从而应对新出现的物体。</p><p><img src="https://i.loli.net/2020/10/17/gakiQWDp3YAITCd.png" alt="image-20201017185401941" style="zoom:50%;"></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文要针对describe novel objects which are unseen in caption-labeled training data。This paper presents VIsual VOcabulary pretraining (VIVO) that performs pre-training in the absence of caption annotations。</p><p>By breaking the dependency of paired image-caption training data in VLP, VIVO can leverage large amounts of <strong>paired image-tag data</strong> to learn <strong>a visual vocabulary</strong>.<br>This is done by pre-training a <strong>multi-layer Transformer model</strong> that learns to align image-level tags with their corresponding image region features. Given that tags are not ordered, we employ <strong>the Hungarian matching loss</strong> for tag prediction optimization. </p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="https://i.loli.net/2020/10/17/JuOMYRtzW7LEgfo.png" alt="image-20201017193144259"></p><h4 id="VIVO-Pre-training"><a href="#VIVO-Pre-training" class="headerlink" title="VIVO Pre-training"></a>VIVO Pre-training</h4><p>We pre-train the Transformer model on a large-scale dataset with abundant tags, e.g., the Open Images training set with <strong>6.4K classes of image-level tags.</strong></p><p><strong>The training objective</strong> is to predict the missing (masked) tags given a bag of image-level tags and image regions. </p><p>We denote the training set: N images $I_i$ and their corresponding tags $G_i$. 一个image有多个tags.</p><p>use <strong>bi-directional attention mask</strong> in VIVO pre-training.</p><h4 id="Fine-tuning-and-Inference"><a href="#Fine-tuning-and-Inference" class="headerlink" title="Fine-tuning and Inference"></a>Fine-tuning and Inference</h4><p>After pre-training, the Transformer model is fine-tuned on a dataset where both captions and tags are available, e.g., the COCO set annotated with tags from 80 object classes and captions.</p><p>the input to the model during <strong>fine-tuning is a triplet of image region features $V$, a set of tags $T$ and a  caption $C$</strong>, where $V$ and $T$ are constructedin the same way as described in pre-training, and $C$ is a sequence of tokens. During fine-tuning, we <strong>randomly mask outsome of the tokens in a caption sentence</strong> for  prediction, and optimize the model parameters using the cross-entropy loss.</p><p>during fine-tuning we apply <strong>the uni-directional attention mask</strong> on a caption sequence to prevent the positions from attending to subsequent positions.</p><p>During inference, we first extract image region features and detect tags from a given image. Then the model is applied to <strong>generate a sequence, one token at a time,</strong> until it outputs the end of sentence token or reaches the maximum length.</p><p><strong>detect tags</strong> ：We use an object detector trained on the Open Images dataset （500 classes bboxes）to detect object tags for all datasets.</p><p><strong style="color:red;"><strong>yaya：</strong> tags detector的限制，仅能输出 500个类别tags, 因此，novel objects 的生成也是受到限制的</strong></p><p>以下这个表就可以说明问题，当不预训练时，是第一行的数据；当仅使用tags detector 的500个类时，是第二行的数据；当使用open-image 所有的 6.4k 个类时，是第三行的数据。因此，在inference阶段，使用 tag detector 来提供tags 是存在问题的。至少限制了模型的性能。</p><p><strong>改进</strong>：本文的model，pre-training, fine-tune，都是在一个fix model上。但是pre-training 的目的，仅仅是为了构建 image-tag vocabulary, 可以先构建，然后再离线使用！！！</p><p><img src="https://i.loli.net/2020/10/18/eyKJkcQPibhX3nS.png" alt="image-20201018115538016" style="zoom:33%;"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;VIVO可以在没有文本标签的数据上进行文本和图像的多模态预训练，摆脱了对配对图文数据的依赖，可以直接利用ImageNet等数据集的类别标签。
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
      <category term="region-word-embedding" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/region-word-embedding/"/>
    
    
      <category term="region-word-embedding" scheme="http://yoursite.com/tags/region-word-embedding/"/>
    
  </entry>
  
  <entry>
    <title>Vokenization Improving Language Understanding with Contextualized, Visual-Grounded Supervision</title>
    <link href="http://yoursite.com/2020/10/18/Vokenization-Improving-Language-Understanding-with-Contextualized-Visual-Grounded-Supervision/"/>
    <id>http://yoursite.com/2020/10/18/Vokenization-Improving-Language-Understanding-with-Contextualized-Visual-Grounded-Supervision/</id>
    <published>2020-10-18T04:10:43.000Z</published>
    <updated>2020-11-04T08:08:14.568Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><h4 id="现存问题"><a href="#现存问题" class="headerlink" title="现存问题"></a>现存问题</h4><p>人类学习语言都是结合多模态信息，但是当前的 language pre-training frameworks 仅通过自监督的方式，学习语言这一种模态。</p><p>虽然这种自监督的方式取得了很大的成功，但是它们没有利用grounding information from external visual word.</p><blockquote><p>Emily M Bender and Alexander Koller. 2020. <strong>Climbing towards nlu: On meaning, form, and understanding in the age of data.</strong> In ACL.</p><p>Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata,<br>Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. 2020. <strong>Experience grounds language.</strong> In EMNLP</p></blockquote><h4 id="本文的解决"><a href="#本文的解决" class="headerlink" title="本文的解决"></a>本文的解决</h4><p><img src="https://i.loli.net/2020/10/17/1cz3MHUdNXA9o5J.png" alt="image-20201017095623630" style="zoom:33%;"></p><p>本文：介绍了一个 <strong>视觉</strong>监督语言模型，如图1，该模型使用 language tokens 作为输入，使用token-related images 作为视觉监督。本文将这些images称作 vokens，which act as visualizations of the corresponding tokens.</p><p>假若a large aligned token-voken dataset 存在，那么模型可以通过voken-prediction task 从这些vokens中进行学习。但是不幸的是，不存在这种大型数据集，主要是有两个挑战：(1) 视觉性单词与 其他非视觉性单词之间，数量上存在很大的差异。如，在visually-grounded language datasets中仅有120M tokens, 但是在BERT的训练数据中有3300M tokens。grounded language 一般会更短，偏向于instructive descriptions, 因此在句子长度和有效词的数量上与其他语言类型的分布不同。(2) 自然语言中的大部分单词是 not visually grounded，因此对是否建立一个 visual supervision的数据集提出了质疑。粗略估计，英语维基百科中 grounded tokens 的比例仅为大约28％。 这种 low grounded ratio 导致以前方法中的视觉监控覆盖率低。</p><p><img src="https://i.loli.net/2020/10/17/27sWCBNOiqp13Vz.png" alt="image-20201017111702738"></p><p>为解决以上的两个挑战，本文提出了一个 <strong>vokenization method, that contextually maps the tokens to the visualized tokens (i.e., vokens) by retrieval.</strong>  而不是直接使用具有visually grounded的语言数据集来监督语言模型。</p><p>解决第一个挑战：(1) relative small datasets to train the <strong>vokenization processor</strong> (2) generate vokens for large language corpora.<br>our visually-supervised language model will take the input supervision from these large datasets, thus <strong>bridging the gap between different data sources,</strong> which solves the first challenge.</p><p>解决第一个挑战：low grounded ratio 的第二个挑战似乎是语言的固有特征。 但是，我们发现，考虑到它的上下文，可以将一些非可视化的tokens 有效地映射到相关图像。by our contextual token-image matching model (defined in Sec. 3.2) inside our vokenization processor, where we map tokens to images by viewing the sentence as the context.</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>Using our proposed vokenizer with a <strong>contextualized</strong> token-image matching model, we generate vokens for English Wikipedia. </p><p>Supervised by these generated vokens, we show consistent improvements upon a BERT model on several diverse NLP tasks such as GLUE (Wang et al., 2019), SQuAD (Rajpurkar et al., 2016), and SWAG (Zellers et al., 2018).  We also show the transferability of our vokens to other frameworks (i.e., RoBERTa).</p><h3 id="Vokenization"><a href="#Vokenization" class="headerlink" title="Vokenization"></a>Vokenization</h3><p><img src="https://i.loli.net/2020/10/17/fUWtQpIO8kZjAcY.png" alt="image-20201017120304025" style="zoom:50%;"></p><p>we <strong>retrieve an image for a token</strong> from a set of images $X$ = {$x_1; x_2; … ; x_n$} regarding a token-image-relevance scoring function $r_\theta(w_i; x; s)$. This scoring function $r_\theta(w_i; x; s)$, parameterized by $\theta$</p><h4 id="Contextual-Token-Image-Matching-Model"><a href="#Contextual-Token-Image-Matching-Model" class="headerlink" title="Contextual Token-Image Matching Model"></a>Contextual Token-Image Matching Model</h4><p>输入：The model takes a sentence $s$ and an image $x$ as input.</p><p>输出：The output $r_\theta(w_i; x; s)$ is the relevance score between the token $w_i \in s$ and the image $x$ while considering the whole sentence $s$ as a context.</p><p>Model: an inner product of the language feature representation $f_\theta(w_i; s)$ and the visual feature representation $g_\theta(x)$: $r_\theta(w_i; x; s)$ = $f_\theta(w_i; s)^T$ $g_\theta(x)$</p><p>token-image paris: 使用MS-COCO image caption pairs， 将caption中的所有tokens的vokens 都指定为该 image.</p><p>Training: 训练模型，maximizing the relevance score of these aligned token-image pairs over unaligned pairs. 使用 hinge loss.</p><h3 id="Visually-Supervised-Language-Models"><a href="#Visually-Supervised-Language-Models" class="headerlink" title="Visually-Supervised Language Models"></a>Visually-Supervised Language Models</h3><p>Based on these vokens, we propose a new pre-training task for language: voken classification.</p><h4 id="The-Voken-Classification-Task"><a href="#The-Voken-Classification-Task" class="headerlink" title="The Voken-Classification Task"></a>The Voken-Classification Task</h4><p><img src="https://i.loli.net/2020/11/04/j32ZMfNUpHWwez4.png" alt="image-20201104160728232"></p><p>BERT 的结果，会在每个token $w_i$的位置输出一个localized feature representation ${h_i}$，因此这将会很容易增加一个 token-level classification task, 而不需要修改模型的结构。Suppose the vokens come<br>from a finite set $X$, we convert the hidden output to ${h_i}$ a probability distribution ${p_i}$ with a linear layer and a softmax layer. </p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><ul><li>受到这篇文章对的影响，是否可以结合视频，设计一个这种模型，比如有一些动词，仅能在视频中体现出来。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;h4 id=&quot;现存问题&quot;&gt;&lt;a href=&quot;#现存问题&quot; cla
      
    
    </summary>
    
    
      <category term="region-word-embedding" scheme="http://yoursite.com/tags/region-word-embedding/"/>
    
  </entry>
  
  <entry>
    <title>Re-evaluating Evaluation in Text Summarization</title>
    <link href="http://yoursite.com/2020/10/16/Re-evaluating-Evaluation-in-Text-Summarization/"/>
    <id>http://yoursite.com/2020/10/16/Re-evaluating-Evaluation-in-Text-Summarization/</id>
    <published>2020-10-16T08:34:45.000Z</published>
    <updated>2020-10-16T08:36:14.592Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>ROUGE 在 text summarization 任务中被广泛使用，但是，关于ROUGE是否可能偏离human judgement 以及这种偏离可能改变有关baseline 和 proposed methods的相对优势得出的结论的讨论很少。</p></li><li><p>为了表征<strong>评估指标</strong>的相对优势，有必要执行meta-evaluation。</p><p>where a dataset annotated with human judgments is used to test the degree to which automatic metrics correlate there with.</p></li><li><p>现在存在关键问题：现有的人类判断数据集很少，尚不清楚<strong>现有指标</strong>在<em>当前得分最高的摘要系统</em>上的表现。</p></li><li><p>在本文中提出一个问题：摘要模型中模型开发的快速发展是否需要我们<strong>重新评估</strong>用于文本摘要的<strong>评估过程</strong>。因此，在本文中，收集了一个large benchmark 来用于 meta-evaluating summarization metrics。</p><ul><li><p>数据来源于：25 top-scoring extractive and abstractive summarization systems on the CNN/DailyMail dataset.</p></li><li><p>Automatic: traditional metrics (e.g. ROUGE) and modern semantic matching metrics (e.g.  BERTScore, MoverScore).</p></li><li><p>Manual evaluations: 使用轻量级金字塔方法（Shapira等，2019），我们将其用作summarization systems 和 automated metrics的黄金标准。（yaya: 收集的human judgements 既可以作为评判systems好坏的标准，也可以作为评判metrics好坏的标准）</p><blockquote><p>Ori Shapira, <strong>Crowdsourcing lightweight pyramids for manual summary evaluation.</strong>  NAACL 2019</p></blockquote></li></ul></li></ul><h3 id="标注过程"><a href="#标注过程" class="headerlink" title="标注过程"></a>标注过程</h3><ul><li>对于一个document，仅存在一个reference，对该reference, 提取SCUs, 如下表展示出来的所示。该步骤由作者本人完成。</li><li>对于该document 的 candidate summary, 查看 SCUs 是否出现在 candidate summary 中，并标注为 “present” 或者是 “not present”。该步骤的操作由4个workers共同完成。</li><li>对于 each documents，查验是否存在 noisy worker, 即对于一个SCU，大多数认为其”present”，但是他却认为”not present”，在该document的大多数SCUs中他的annotations都与众数不同，则定义为 noisy workers，并将其标注的结果去除掉。</li></ul><p><img src="https://i.loli.net/2020/10/16/m7Cxl6hnvOwPeSc.png" alt="image-20201016121420397"></p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>本文想要研究的核心问题：“does the rapid progress of model development in summarization models require us to re-evaluate the evaluation process used for text summarization?”</p><p>因此本文从四个方面 to meta-evaluate current metrics。(1) evaluate all systems; (2) evaluate top-k strongest systems; (3) compare two systems; (4) evaluate individual summaries.</p><h4 id="evaluate-all-systems"><a href="#evaluate-all-systems" class="headerlink" title="evaluate all systems"></a>evaluate all systems</h4><p><img src="https://i.loli.net/2020/10/16/Cy7RSaKrP6XOUnY.png" alt="image-20201016160058531" style="zoom:33%;"></p><p>通过对比不同的metrics 在 system level 的相关性发现，在不同的数据集上，metrics的相关性，性能并不一致。</p><blockquote><p>that metrics run the risk of overfitting to some datasets</p></blockquote><p>本文建议，在不同的数据集上，使用不同的metric来评估该数据集上不同的systems 的性能好坏</p><h4 id="evaluate-top-k-strongest-systems"><a href="#evaluate-top-k-strongest-systems" class="headerlink" title="evaluate top-k strongest systems"></a>evaluate top-k strongest systems</h4><p><img src="https://i.loli.net/2020/10/16/Zfn4Hh6g8uG5eXM.png" alt="image-20201016160240565"></p><p>结论：当 top-systems 数量较少时，或者说数量不稳定时，不同的metrics在同一个数据集上的效果也不稳定。</p><h4 id="compare-two-systems"><a href="#compare-two-systems" class="headerlink" title="compare two systems"></a>compare two systems</h4><p>we only have 100 annotated summaries to compare any two systems, sys1 and sys2, we use paired bootstrap resampling,</p><p>对于人类在sys1/2 上对所有的summaries 都有一个得分。</p><p>现，要比较两个system, 若有95%以上的confidence认为sys1 better than sys2, 则 ytrue=1, 否则 ytrue=2, 如果confidence&lt;95%, ytrue=0.</p><p>同理，对于所有的metrics, 通过同样的方式，也可以有此比较得分 ypred。</p><p>现有 J 个systems, 则可以得到 J<em>(J-1)/2 个compaired paris. 即，得到 长度为 J</em>(J-1)/2的mask.</p><p>计算 ytrue_mask 与 ypred_mask 的F1score.即可评估metrics在 compare two systems上的性能与human 的一致性。</p><p><img src="https://i.loli.net/2020/10/16/ys241wqHUpGmltO.png" alt="image-20201016161340716" style="zoom:33%;"></p><p>结论：Different metrics are better suited for different datasets. For example, on the CNNDM datasets, we recommend using R-2 while, on the TAC datasets, we recommend using JS-2.</p><h4 id="evaluate-individual-summaries"><a href="#evaluate-individual-summaries" class="headerlink" title="evaluate individual summaries"></a>evaluate individual summaries</h4><p><img src="https://i.loli.net/2020/10/16/iWLrRBSITJGpMoN.png" alt="image-20201016162009349" style="zoom: 50%;"></p><ul><li>以上三个实验都是system-level , 此实验是 summary-level</li></ul><p>分析：在不同的数据集上，同一metrics的性能不一致。如，R_1在TAC上与human相关性较低，但是在CNNDM上相关性较高。</p><p>另，前有文章表明，automatic metrics趋向于在system level 与 human的相关性较好，但是在 instance-level (summary-level) 相关性较差。我们进行实验发现，这种现象仅在TAC-2009 上表现明显。</p><p>结论：autometrics 在 summary-level 与 system-level 上的性能是一样的。</p><blockquote><p>Even though some metrics might be good at comparing summaries, they may point in the wrong direction when comparing systems</p></blockquote><p>另外，<strong>some metric在不同的数据集上的性能是不同的，因此是有必要在不同的数据集上测试各个评价指标的有效性</strong></p><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><ul><li>（1）metric的选择不仅取决于不同的任务（例如，摘要，翻译），而且还取决于不同的数据集（例如，TAC，CNNDM）和应用程序场景（例如，系统级别，摘要级别）。未来的meta-evaluating 工作应调查效果这些设置对指标性能的影响。</li><li>（2）指标很容易在有限的数据集上过拟合。多数据集meta-evaluate 可以帮助我们更好地了解每个指标的特殊性，从而在各种情况下获得更好的指标选择。</li><li>（3）我们收集的人工判断可以用作监督，以实例化最近提出的预训练-然后-微调框架（最初用于机器翻译）（Sellam等，2020），学习一个强大的文本摘要指标。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ROUGE 在 text summariz
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="评价指标" scheme="http://yoursite.com/categories/NLP/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>shiyaya-Instruction-Final-bilibili</title>
    <link href="http://yoursite.com/2020/09/17/shiyaya-Instruction-Final-bilibili/"/>
    <id>http://yoursite.com/2020/09/17/shiyaya-Instruction-Final-bilibili/</id>
    <published>2020-09-17T02:18:37.000Z</published>
    <updated>2020-09-17T13:01:01.178Z</updated>
    
    <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning</title>
    <link href="http://yoursite.com/2020/09/15/Learning-to-Compose-Topic-Aware-Mixture-of-Experts-for-Zero-Shot-Video-Captioning/"/>
    <id>http://yoursite.com/2020/09/15/Learning-to-Compose-Topic-Aware-Mixture-of-Experts-for-Zero-Shot-Video-Captioning/</id>
    <published>2020-09-15T02:19:42.000Z</published>
    <updated>2020-09-15T02:21:37.215Z</updated>
    
    <content type="html"><![CDATA[<script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Just can be seen by yaya.
    
    </summary>
    
      <category term="视频描述" scheme="http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="视频描述" scheme="http://yoursite.com/tags/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>A Survey of Evaluation Metrics Used for NLG Systems</title>
    <link href="http://yoursite.com/2020/08/28/A-Survey-of-Evaluation-Metrics-Used-for-NLG-Systems/"/>
    <id>http://yoursite.com/2020/08/28/A-Survey-of-Evaluation-Metrics-Used-for-NLG-Systems/</id>
    <published>2020-08-28T06:20:13.000Z</published>
    <updated>2020-10-16T08:36:38.860Z</updated>
    
    <content type="html"><![CDATA[<h3 id><a href="#" class="headerlink" title=" "></a> </h3><h3 id="Recommendations-Possible-future-research-directions"><a href="#Recommendations-Possible-future-research-directions" class="headerlink" title="Recommendations (Possible future research directions)"></a>Recommendations (Possible future research directions)</h3><ul><li><p><strong style="color:blue;">为所有的评估指标构建一个通用的工具包</strong></p></li><li><p><strong style="color:blue;">构建一个包含 human judgments 的数据集。</strong></p><p>（1）human 会从不同的角度进行评估</p><p>（2）根据收集的数据，可以训练出来一个评价指标</p></li><li><p><strong style="color:blue;">提出 task-specific and context-dependent metrics</strong></p><p>类似于 dialog 这个任务，reference response 与 right prediction response 之间的word overlap 很小，因此，only reference dependent 的评价指标是有缺陷的，还需要结合 context 来设计评价指标</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh954nipuej319z0k4q9z.jpg"></p></li><li><p><strong style="color:blue;">提出具有可解释性的评价指标</strong></p><p>当前大部分的评价指标， 对预测仅仅给出 a single score, 没有任何具体的指向。但是对于 human evaluation, 会从具体的层面，eg: fluency, adequency. coherence 来进行评价。因此 a single score 不够具有可解释性。</p><p>应该设计不同的评价指标，每一个评价指标，从特定的层面进行评价。</p></li><li><p><strong style="color:red;">Creating robust benchmarks for evaluating evaluation metrics </strong></p><p>early metrics, 例如，BLEU，METEOR 等，已经在各种各样的任务上进行了验证。</p><p>但是，最近新提出的评价指标还没有被 examined critically，为了实施这一研究，需要收集一个 <strong>对抗 evaluation benchmarks</strong>， <strong>这个 benchmarks可以测试这些metrics 的鲁棒性。</strong></p><p>举个例子：对于dialog，给定一个context， 可以收集一些 adversarially crafted responses（与 passage 有较高的 word overlap, 但实际上是不相关的，或者是不正确的）  。查看evaluation metrics 是否会对这种手工创造的对抗例子给<strong>低分</strong>，已验证其鲁棒性。</p><p><strong>除了这种 adversarial evaluations， 还需要研究 提出的evaluation metrics 是否有specific biases.</strong> 比如，GAN based evaluators 会在一些systems上进行训练，则其更容易对这些 systems 给高分。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot; &quot;&gt;&lt;/a&gt; &lt;/h3&gt;&lt;h3 id=&quot;Recommendations-Possible-future-research-directions&quot;&gt;&lt;a href=&quot;#Recommendat
      
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
      <category term="评价指标" scheme="http://yoursite.com/categories/NLP/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>correlation coefficient</title>
    <link href="http://yoursite.com/2020/08/26/correlation-coefficient/"/>
    <id>http://yoursite.com/2020/08/26/correlation-coefficient/</id>
    <published>2020-08-26T06:41:03.000Z</published>
    <updated>2020-08-27T02:58:28.571Z</updated>
    
    <content type="html"><![CDATA[<h1 id="correlation-coefficient"><a href="#correlation-coefficient" class="headerlink" title="correlation coefficient"></a>correlation coefficient</h1><ul><li>spearman 和 kendall 计算的都是对排序 之间的计算</li><li>pearson 计算的是直接的数值，协方差，标准差之间的计算</li></ul><h3 id="pearson"><a href="#pearson" class="headerlink" title="pearson"></a>pearson</h3><ul><li><p>代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下两种方式都可以</span></span><br><span class="line">scipy.stats.pearsonr(array_1, array_2)</span><br><span class="line">np.corrcoef(array_1, array_2)</span><br></pre></td></tr></table></figure></li><li><p>计算公式</p><p><img src="https://i.loli.net/2020/08/26/lXA4uMz9UkG2vFx.png" alt="image-20200826144001717" style="zoom: 33%;"></p></li><li><p>适用范围</p><p>当两个变量的标准差都不为零时，相关系数才有定义，皮尔逊相关系数适用于：</p><p>(1)、两个变量之间是线性关系，都是连续数据。</p><p>(2)、两个变量的总体是正态分布，或接近正态的单峰分布。</p><p>(3)、两个变量的观测值是成对的，每对观测值之间相互独立。</p></li><li><p>注意</p><p>公式的分母是变量的标准差，这就意味着计算pearson时，变量的标准差不能为0（分母不能为0），也就是说你的两个变量中任何一个的值不能都是相同的。如果没有变化，用pearson是没办法算出这个变量与另一个变量之间是不是有相关性的。</p><p>就好比我们想研究人跑步的速度与心脏跳动的相关性，如果你无论跑多快，心跳都不变（即心跳这个变量的标准差为0），或者你心跳忽快忽慢的，却一直保持一个速度在跑（即跑步速度这个变量的标准差为0），那我们都无法通过pearson的计算来判断心跳与跑步速度到底相不相关。</p></li><li><p>使用Pearson线性相关系数有2个局限：</p><ol><li>必须假设数据是成对地从正态分布中取得的。</li><li>数据至少在逻辑范围内是等距的。</li></ol></li></ul><h3 id="spearman"><a href="#spearman" class="headerlink" title="spearman"></a>spearman</h3><p><a href="https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php" target="_blank" rel="noopener">https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php</a></p><ul><li><p>代码实现</p><ul><li><p>对于一般情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result, _ = scipy.stats.spearmanr(array_1, array_2)</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>对于离散整数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spearmanr</span><span class="params">(set_1, set_2)</span>:</span></span><br><span class="line"></span><br><span class="line">    ar = np.apply_along_axis(scipy.stats.rankdata, <span class="number">0</span>, set_1)</span><br><span class="line">    br = np.apply_along_axis(scipy.stats.rankdata, <span class="number">0</span>, set_2)</span><br><span class="line">    d = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(ar)):</span><br><span class="line">        d.append(ar[i] - br[i])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    d_sq = [i ** <span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> d]</span><br><span class="line">    sum_d_sq = sum(d_sq)</span><br><span class="line">    n_cu_min_n = len(set_1) ** <span class="number">3</span> - len(set_1)</span><br><span class="line">    r = <span class="number">1</span> - ((<span class="number">6.0</span> * sum_d_sq) / n_cu_min_n)</span><br><span class="line">    <span class="keyword">return</span> r</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>计算公式</p><ul><li><p>存在 并列排序时：</p><p>先排序，对排序值 pair 计算 pearson 系数</p></li><li><p>不存在并列排序时，</p><p>先排序，计算d<sub>i</sub> 再按照下面第一个公式进行计算</p></li></ul></li></ul><p><img src="https://i.loli.net/2020/08/26/ON5iLc2kl6EAM1p.png" alt="微信截图_20200826151113"></p><ul><li><p>另外一种说法</p><ul><li><p>一般情况：</p><p>先排序，对排序值 pair 计算 pearson 系数</p></li><li><p>对于数值为离散的整数时，</p><p>先排序，计算d<sub>i</sub> 再按照吐下的公式进行计算</p></li></ul><p><img src="https://i.loli.net/2020/08/26/z3iJWqUxRcBSegV.png" alt="微信截图_20200826151324"></p></li><li><p>适用范围</p><p>spearman 对数据条件的要求没有皮尔逊相关系数严格，只要两个变量的观测值是成对的等级评定资料，或者是由连续变量观测资料转化得到的等级资料，不论两个变量的总体分布形态、样本容量的大小如何，都可以用spearman 来进行研究</p></li></ul><h3 id="kendall"><a href="#kendall" class="headerlink" title="kendall"></a>kendall</h3><ul><li><p>代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scipy.stats.kendalltau(array_1, array_2)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Kendallta</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    Lens = len(a)</span><br><span class="line"></span><br><span class="line">    ties_onlyin_x = <span class="number">0</span></span><br><span class="line">    ties_onlyin_y = <span class="number">0</span></span><br><span class="line">    con_pair = <span class="number">0</span></span><br><span class="line">    dis_pair = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Lens - <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, Lens):</span><br><span class="line">            test_tying_x = np.sign(a[i] - a[j])</span><br><span class="line">            test_tying_y = np.sign(b[i] - b[j])</span><br><span class="line">            panduan = test_tying_x * test_tying_y</span><br><span class="line">            <span class="keyword">if</span> panduan == <span class="number">1</span>:</span><br><span class="line">                con_pair += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> panduan == <span class="number">-1</span>:</span><br><span class="line">                dis_pair += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> test_tying_y == <span class="number">0</span> <span class="keyword">and</span> test_tying_x != <span class="number">0</span>:</span><br><span class="line">                ties_onlyin_y += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> test_tying_x == <span class="number">0</span> <span class="keyword">and</span> test_tying_y != <span class="number">0</span>:</span><br><span class="line">                ties_onlyin_x += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    result = (con_pair - dis_pair) / np.sqrt(</span><br><span class="line">        (con_pair + dis_pair + ties_onlyin_x) * (dis_pair + con_pair + ties_onlyin_y))</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure></li><li><p>计算公式</p><p><a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient" target="_blank" rel="noopener">Kendall_rank_correlation_coefficient</a></p><p>有好几个计算公式</p></li><li><p>适用范围</p><p>kendall与spearman 对数据条件的要求相同，可参见<a href="http://blog.csdn.net/wsywl/archive/2010/09/02/5859751.aspx" target="_blank" rel="noopener">统计相关系数(2)—Spearman Rank(斯皮尔曼等级)相关系数及MATLAB实现</a>中介绍的spearman 对数据条件的要求。</p></li></ul><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><ul><li>这三种 相关系数，计算 array_1 与 array_2 之间的相关性，若array_1 或者 array_2 中的元素都相同（eg: array_1 = np.array([5,5,5,5,5])） 则会使得输出为NaN.</li></ul><p>三种方法的适用场合</p><h4 id="主要参数methods介绍"><a href="#主要参数methods介绍" class="headerlink" title="主要参数methods介绍:"></a>主要参数methods介绍:</h4><ol><li>pearson correlation coefficient（皮尔逊相关性系数）。<br> 常用的相关系数求法，采用协方差cov(X,Y)/标准差的乘积(σX, σY)。<br> 数据要求： 线性数据、连续且符合正态分布；数据间差异不能太大；变量准差不能为0，即两变量中任何一个值不能都是相同。</li><li>spearman correlation coefficient（斯皮尔曼秩相关性系数）。<br> 根据原始数据的排序位置进行计算。<br> 数据要求：用于解决称名数据和顺序数据相关的问题，适用于两列变量，而且具有等级变量性质具有线性关系的数据，能够很好处理序列中相同值和异常值。</li><li>kendall correlation coefficient（肯德尔相关性系数）。<br> 等级相关系数，适用于两个变量均为有序分类的情况<br> 数据要求：肯德尔相关性系数，它也是一种秩相关系数，不过它所计算的对象是分类变量。</li></ol><p>所以针对【连续、正态分布、线性】数据，采用pearson相关系数；针对【非线性的、非正态】数据，采用spearman相关系数；针对【分类变量、无序】数据，采用Kendall相关系数。一般来讲，线性数据采用pearson，否则选择spearman，如果是分类的则用kendall。</p><p>作者：王叽叽的小心情<br>链接：<a href="https://www.jianshu.com/p/f9304da68d98" target="_blank" rel="noopener">https://www.jianshu.com/p/f9304da68d98</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><h3 id="相关系数和P-value-值"><a href="#相关系数和P-value-值" class="headerlink" title="相关系数和P-value 值"></a>相关系数和P-value 值</h3><p>看两者是否算相关要看两方面</p><p>显著水平以及相关系数</p><p>（1）显著水平,就是P值,这是首要的,因为如果不显著,相关系数再高也没用,可能只是因为偶然因素引起的,那么多少才算显著,一般p值小于0.05就是显著了；如果小于0.01就更显著；例如p值=0.001,就是很高的显著水平了,只要显著,就可以下结论说：拒绝原假设无关,两组数据显著相关也说两者间确实有明显关系.通常需要p值小于0.1,最好小于0.05甚至0.01,才可得出结论：两组数据有明显关系,如果p=0.5,远大于0.1,只能说明相关程度不明显甚至不相关.起码不是线性相关.</p><p>（2）相关系数,也就是pearson spearman等,通常也称为R值,在确认上面指标显著情况下,再来看这个指标,一般相关系数越高表明两者间关系越密切.R&gt;0 代表连个变量正相关,即一个变大另一个随之变大</p><h3 id="需要的数据量"><a href="#需要的数据量" class="headerlink" title="需要的数据量"></a>需要的数据量</h3><p><a href="https://bbs.pinggu.org/thread-3240378-1-1.html" target="_blank" rel="noopener">https://bbs.pinggu.org/thread-3240378-1-1.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;correlation-coefficient&quot;&gt;&lt;a href=&quot;#correlation-coefficient&quot; class=&quot;headerlink&quot; title=&quot;correlation coefficient&quot;&gt;&lt;/a&gt;correlation coeff
      
    
    </summary>
    
    
      <category term="评价指标" scheme="http://yoursite.com/tags/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    
  </entry>
  
  <entry>
    <title>评价者之间的一致性-Kappas</title>
    <link href="http://yoursite.com/2020/08/06/%E8%AF%84%E4%BB%B7%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7-Kappas/"/>
    <id>http://yoursite.com/2020/08/06/评价者之间的一致性-Kappas/</id>
    <published>2020-08-06T09:06:38.000Z</published>
    <updated>2020-08-06T09:14:45.311Z</updated>
    
    <content type="html"><![CDATA[<h3 id="评价者之间的一致性—Kappas-Inter-rater-agreement-Kappas"><a href="#评价者之间的一致性—Kappas-Inter-rater-agreement-Kappas" class="headerlink" title="评价者之间的一致性—Kappas Inter-rater agreement Kappas"></a>评价者之间的一致性—Kappas Inter-rater agreement Kappas</h3><p>inter-rater reliability == inter-rater agreement == concordance</p><p>评价者之间的一致性的Kappa分数代表着在打分判断中，他们有多少共识，有多一致。</p><p>Kappa分数处于0-1之间，具体地：</p><div class="table-container"><table><thead><tr><th style="text-align:center">K</th><th style="text-align:center">Interpretation</th></tr></thead><tbody><tr><td style="text-align:center">&lt;0</td><td style="text-align:center">Poor agreement 不一致</td></tr><tr><td style="text-align:center">0.0-0.20</td><td style="text-align:center">Slight agreement</td></tr><tr><td style="text-align:center">0.21-0.40</td><td style="text-align:center">Fair agreement</td></tr><tr><td style="text-align:center">0.41-0.60</td><td style="text-align:center">Moderate agreement</td></tr><tr><td style="text-align:center">0.61-0.80</td><td style="text-align:center">Substantial agreement</td></tr><tr><td style="text-align:center">0.81-1.0</td><td style="text-align:center">Almost perfect agreement</td></tr></tbody></table></div><h3 id="Cohen’s-Kappa"><a href="#Cohen’s-Kappa" class="headerlink" title="Cohen’s Kappa"></a>Cohen’s Kappa</h3><p>Cohen’s Kappa 计算了评分者之间的一致性。当评分者对同一项任务给出了相同的判断或分数，那么他们的一致性得到了体现。</p><p>Cohen’s Kappa 只能在以下的条件下使用：</p><ul><li>两个评价者分别对每个样本进行评分</li><li>一个评价者对每个样本进行两次评分</li></ul><p><strong>Cohen’s Kappa 计算</strong></p><p>要注意的是，一般情况下，Cohen’s Kappa 的计算背景是：有<strong>两个</strong>评分者对每个样本进行<strong>二分类</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">postive (rater A)</th><th style="text-align:center">negative (rater A)</th><th style="text-align:center">Total</th></tr></thead><tbody><tr><td style="text-align:center"><strong>postive (rater B)</strong></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B11%7D" alt="n_{11}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B12%7D" alt="n_{12}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B1.%7D" alt="n_{1.}"></td></tr><tr><td style="text-align:center"><strong>negative (rater B)</strong></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B21%7D" alt="n_{21}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B22%7D" alt="n_{22}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B2.%7D" alt="n_{2.}"></td></tr><tr><td style="text-align:center"><strong>Total</strong></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B.1%7D" alt="n_{.1}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B.2%7D" alt="n_{.2}"></td><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D" alt="n_{11}+n_{12}+n_{21}+n_{22}"></td></tr></tbody></table></div><p>计算公式为：<br> <img src="https://math.jianshu.com/math?formula=k%20%3D%20%5Cfrac%7Bp_o-p_e%7D%7B1-p_e%7D%20%3D%201-%5Cfrac%7B1-p_o%7D%7B1-p_e%7D" alt="k = \frac{p_o-p_e}{1-p_e} = 1-\frac{1-p_o}{1-p_e}"><br> 其中，<img src="https://math.jianshu.com/math?formula=p_o" alt="p_o"> 代表评价者之间的相对观察一致性（the relative <strong>observed agreement</strong> among raters）<br> <img src="https://math.jianshu.com/math?formula=p_o%3D%5Cfrac%7Bn_%7B11%7D%2Bn_%7B22%7D%7D%7Bn_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D%7D" alt="p_o=\frac{n_{11}+n_{22}}{n_{11}+n_{12}+n_{21}+n_{22}}"><br> <img src="https://math.jianshu.com/math?formula=p_e" alt="p_e"> 代表偶然一致性的假设概率（the hypothetical probability of <strong>chance agreemnet</strong>）<br> <img src="https://math.jianshu.com/math?formula=p_e%3D%5Cfrac%7Bn_%7B.1%7D*n_%7B1.%7D%7D%7B(n_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D" alt="p_e=\frac{n_{.1}*n_{1.}}{(n_{11}+n_{12}+n_{21}+n_{22})^2}+\frac{n_{.2}*n_{2.}}{(n_{11}+n_{12}+n_{21}+n_{22})^2}=\frac{n_{.1}*n_{1.}+n_{.2}*n_{2.}}{(n_{11}+n_{12}+n_{21}+n_{22})^2}">%5E2%7D%2B%5Cfrac%7Bn_%7B.2%7D<em>n_%7B2.%7D%7D%7B(n_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D)%5E2%7D%3D%5Cfrac%7Bn_%7B.1%7D</em>n_%7B1.%7D%2Bn_%7B.2%7D<em>n_%7B2.%7D%7D%7B(n_%7B11%7D%2Bn_%7B12%7D%2Bn_%7B21%7D%2Bn_%7B22%7D)%5E2%7D)<br> <em>*例子</em></em></p><p>rater A和rater B对50张图片进行分类，正类和负类。结果为：</p><ul><li>20张图片两个评价者都认为是正类</li><li>15张图片两个评价者都认为是负类</li><li>rater A认为25张图片是正类，25张图片是负类</li><li>rater B 认为30张图片是正类，20张图片是负类</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">postive (rater A)</th><th style="text-align:center">negative (rater A)</th><th style="text-align:center">Total</th></tr></thead><tbody><tr><td style="text-align:center"><strong>postive (rater B)</strong></td><td style="text-align:center">20</td><td style="text-align:center">10</td><td style="text-align:center">30</td></tr><tr><td style="text-align:center"><strong>negative (rater B)</strong></td><td style="text-align:center">5</td><td style="text-align:center">15</td><td style="text-align:center">20</td></tr><tr><td style="text-align:center"><strong>Total</strong></td><td style="text-align:center">25</td><td style="text-align:center">25</td><td style="text-align:center">50</td></tr></tbody></table></div><p><strong>Step1</strong> ：计算<img src="https://math.jianshu.com/math?formula=p_o" alt="p_o"><br> <img src="https://math.jianshu.com/math?formula=p_o%3Dnumber%5C%20in%5C%20agreement%2F%5C%20total%3D(20%2B15" alt="p_o=number\ in\ agreement/\ total=(20+15)/50=0.70">%2F50%3D0.70)</p><p><strong>Step2</strong> ：计算<img src="https://math.jianshu.com/math?formula=p_e" alt="p_e"><br> <img src="https://math.jianshu.com/math?formula=p_e%3DThe%5C%20total%5C%20probability%5C%20the%5C%20raters%5C%20both%5C%20saying%5C%20postive%20%5C%5Cand%5C%20negative%20%5C%20randomly%20%3D(25%2F50" alt="p_e=The\ total\ probability\ the\ raters\ both\ saying\ postive \\and\ negative \ randomly =(25/50)*(30/50)+(25/50)*(20/50)=0.50"><em>(30%2F50)%2B(25%2F50)</em>(20%2F50)%3D0.50)<br> <strong>Step3</strong> ：计算<img src="https://math.jianshu.com/math?formula=k" alt="k"><br> <img src="https://math.jianshu.com/math?formula=k%3D%5Cfrac%7Bp_o-p_e%7D%7B1-p_e%7D%3D%5Cfrac%7B0.70-0.50%7D%7B1-0.50%7D%3D0.40" alt="k=\frac{p_o-p_e}{1-p_e}=\frac{0.70-0.50}{1-0.50}=0.40"><br> <img src="https://math.jianshu.com/math?formula=k%3D0.40" alt="k=0.40"> 代表<strong>fair agreement</strong></p><h3 id="Fleiss’s-Kappa"><a href="#Fleiss’s-Kappa" class="headerlink" title="Fleiss’s Kappa"></a>Fleiss’s Kappa</h3><p>Fleiss’s Kappa 是对 Cohen‘s Kappa 的扩展：</p><ul><li>衡量<strong>三个或更多</strong>评分者的一致性</li><li>不同的评价者可以对不同的项目进行评分，而不用像Cohen’s 两个评价者需要对相同的项目进行评分</li><li>Cohen’s Kappa 的评价者是精心选择和固定的，而Fleiss’s Kappa 的评价者是从较大的人群中随机选择的</li></ul><p>举一个例子对 Fleiss’s Kappa 的计算进行说明：14个评价者对10个项目进行1-5的评分，<img src="https://math.jianshu.com/math?formula=N%3D10%2Cn%3D14%2Ck%3D5" alt="N=10,n=14,k=5"></p><blockquote><p>关于这  “10个项目” 的理解：比如在 NLI 数据标注中，需要为很多 promise-hypotheses pair 进行打分，这每一个pair就是一个pair.</p></blockquote><div class="table-container"><table><thead><tr><th style="text-align:center"><img src="https://math.jianshu.com/math?formula=n_%7Bij%7D" alt="n_{ij}"></th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th><th style="text-align:center">5</th><th style="text-align:center"><img src="https://math.jianshu.com/math?formula=P_i" alt="P_i"></th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">14</td><td style="text-align:center">1.000</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">6</td><td style="text-align:center">4</td><td style="text-align:center">2</td><td style="text-align:center">0.253</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">3</td><td style="text-align:center">5</td><td style="text-align:center">6</td><td style="text-align:center">0.308</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">0</td><td style="text-align:center">3</td><td style="text-align:center">9</td><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">0.440</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">8</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0.330</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">7</td><td style="text-align:center">7</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0.462</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">3</td><td style="text-align:center">2</td><td style="text-align:center">6</td><td style="text-align:center">3</td><td style="text-align:center">0</td><td style="text-align:center">0.242</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">2</td><td style="text-align:center">5</td><td style="text-align:center">3</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">0.176</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">6</td><td style="text-align:center">5</td><td style="text-align:center">2</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0.286</td></tr><tr><td style="text-align:center">10</td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">3</td><td style="text-align:center">7</td><td style="text-align:center">0.286</td></tr><tr><td style="text-align:center">Total</td><td style="text-align:center">20</td><td style="text-align:center">28</td><td style="text-align:center">39</td><td style="text-align:center">21</td><td style="text-align:center">32</td><td style="text-align:center">140</td></tr><tr><td style="text-align:center"><img src="https://math.jianshu.com/math?formula=p_j" alt="p_j"></td><td style="text-align:center">0.143</td><td style="text-align:center">0.200</td><td style="text-align:center">0.279</td><td style="text-align:center">0.150</td><td style="text-align:center">0.229</td></tr></tbody></table></div><p><strong>Step1</strong> ：计算<img src="https://math.jianshu.com/math?formula=p_j" alt="p_j"> ，以<img src="https://math.jianshu.com/math?formula=p_1" alt="p_1">为例，评价者随机打1分的概率<br> <img src="https://math.jianshu.com/math?formula=p_1%3Dthe%5C%20total%5C%20number%5C%20of%5C%20the%5C%20column%2F%5C%20the%5C%20total%5C%20number%5C%20of%20%5C%20tasks%20%3D%2020%2F14*10%3D0.143" alt="p_1=the\ total\ number\ of\ the\ column/\ the\ total\ number\ of \ tasks = 20/14*10=0.143"><br> <strong>Step2</strong> ：计算<img src="https://math.jianshu.com/math?formula=P_i" alt="P_i"> ，以<img src="https://math.jianshu.com/math?formula=P_2" alt="P_2">为例,14个评价者对第2个任务达成共识的程度<br> <img src="https://math.jianshu.com/math?formula=P_2%3D%5Cfrac%7Bthe%5C%20sum%5C%20of%5C%20suqare%20%5C%20of%5C%20the%5C%20row%7D%7Bn*(n-1" alt="P_2=\frac{the\ sum\ of\ suqare \ of\ the\ row}{n*(n-1)}=\frac{0^2+2^2+6^2+4^2-14}{14*(14-1)}=0.253">%7D%3D%5Cfrac%7B0%5E2%2B2%5E2%2B6%5E2%2B4%5E2-14%7D%7B14<em>(14-1)%7D%3D0.253)<br> <strong>Step3</strong> ：计算<img src="https://math.jianshu.com/math?formula=P_e%2CP_o" alt="P_e,P_o"><br> ![P_o=\frac{1}{N}\sum_{i=1}^{N}P_i=\frac{1}{10}</em>3.78=0.378](<a href="https://math.jianshu.com/math?formula=P_o%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DP_i%3D%5Cfrac%7B1%7D%7B10%7D*3.78%3D0.378" target="_blank" rel="noopener">https://math.jianshu.com/math?formula=P_o%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DP_i%3D%5Cfrac%7B1%7D%7B10%7D*3.78%3D0.378</a>)</p><p><img src="https://math.jianshu.com/math?formula=P_e%3D%5Csum_%7Bj%3D1%7D%5E%7Bk%7Dp_j%5E2%3D0.143%5E2%2B0.200%5E2%2B0.279%5E2%2B0.150%5E2%2B0.229%5E2%3D0.213" alt="P_e=\sum_{j=1}^{k}p_j^2=0.143^2+0.200^2+0.279^2+0.150^2+0.229^2=0.213"></p><p><img src="https://math.jianshu.com/math?formula=k%3D%5Cfrac%7BP_o-P_e%7D%7B1-P_e%7D%3D%5Cfrac%7B0.378-0.213%7D%7B1-0.213%7D%3D0.210" alt="k=\frac{P_o-P_e}{1-P_e}=\frac{0.378-0.213}{1-0.213}=0.210"></p><p><img src="https://math.jianshu.com/math?formula=k%3D0.210" alt="k=0.210"> 代表<strong>fair agreement</strong></p><blockquote><p>[1] Landis JR, Koch GG. The measurement of observer agreement for categorical data. Biometrics.     1977;33(1):159–74</p><p>[2] <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.pmean.com%2Fdefinitions%2Fkappa.htm" target="_blank" rel="noopener">http://www.pmean.com/definitions/kappa.htm</a></p><p>[3] <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.statisticshowto.datasciencecentral.com%2Fcohens-kappa-statistic%2F" target="_blank" rel="noopener">https://www.statisticshowto.datasciencecentral.com/cohens-kappa-statistic/</a></p><p>[4] <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.statisticshowto.datasciencecentral.com%2Ffleiss-kappa%2F" target="_blank" rel="noopener">https://www.statisticshowto.datasciencecentral.com/fleiss-kappa/</a></p><p>[5]  <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Famirziai%2Flearning%2Fblob%2Fmaster%2Fstatistics%2FInter-rater%20agreement%20kappas.ipynb%5D(https%3A%2F%2Fgithub.com%2Famirziai%2Flearning%2Fblob%2Fmaster%2Fstatistics%2FInter-rater" target="_blank" rel="noopener">[https://github.com/amirziai/learning/blob/master/statistics/Inter-rater%20agreement%20kappas.ipynb](https://github.com/amirziai/learning/blob/master/statistics/Inter-rater</a> agreement kappas.ipynb)</p><p>[6] <a href="https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fqq_31113079%2Farticle%2Fdetails%2F76216611" target="_blank" rel="noopener">https://blog.csdn.net/qq_31113079/article/details/76216611</a></p></blockquote><p>作者：Luuuuuua<br>链接：<a href="https://www.jianshu.com/p/f9c383b39859" target="_blank" rel="noopener">https://www.jianshu.com/p/f9c383b39859</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;评价者之间的一致性—Kappas-Inter-rater-agreement-Kappas&quot;&gt;&lt;a href=&quot;#评价者之间的一致性—Kappas-Inter-rater-agreement-Kappas&quot; class=&quot;headerlink&quot; title=&quot;评价
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Bridging by Word Image Grounded Vocabulary Construction for Visual Captioning</title>
    <link href="http://yoursite.com/2020/08/01/Bridging-by-Word-Image-Grounded-Vocabulary-Construction-for-Visual-Captioning/"/>
    <id>http://yoursite.com/2020/08/01/Bridging-by-Word-Image-Grounded-Vocabulary-Construction-for-Visual-Captioning/</id>
    <published>2020-08-01T08:34:04.000Z</published>
    <updated>2020-08-01T08:35:13.103Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>当前 普遍使用的 CNN-RNN 策略，基于整个 training dataset构建 vocabulary，但是，这会<strong>导致生成的句子中的 N-grams 也是在训练集中常见的</strong>，但是语义上却与given image 无关。</p><p>为了解决这个问题，本文提出了构建一个 image-grounded vocabulary。具体地，提出了一个 two-step approach，通过结合 visual information 和 relationships among words来构建 新的vocabulary。</p><p>并提出了两个策略在 text generation过程中<strong>利用</strong>构建的vocabulary。（1）generator 从image-grounded vocabulary中挑选words （2）soft-attention聚合 vocabulary information 到RNN cell 中来生成下一个单词。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>当前生成单词的方式，是从整个vocab 中select，但是当描述一个 particular image时，the possible words 应该是从一小部分单词集中挑选出来。因此，可以想一个方案，在image caption generation 过程中，有效的约束 word selection space。这将会解决 生成的句子中常常是 irrelavant n-gram problem. </p><p>本文，提出构建一个 image-grounded vocabulary，</p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>整个结构包括两个阶段：(1) image-grounded vocabulary construction, (2) text generation with vocabulary constraints.</p><p><strong>The image-grounded vocabulary</strong> constructor builds a  vocabulary related to a given image.</p><p><strong>The text generator</strong> with vocabulary constraints generates captions using the constructed vocabulary in two different ways. (1) words generated are strictly limited to those in the image-grounded vocabulary. (2) words in the image-grounded vocabulary are re-weighted within the RNN cell such that they are more likely to be generated.</p><h4 id="Image-Grounded-Vocabulary-Construction"><a href="#Image-Grounded-Vocabulary-Construction" class="headerlink" title="Image-Grounded Vocabulary Construction"></a>Image-Grounded Vocabulary Construction</h4><p>caption 中的单词，一般可以分类两类，一类是直接与image content 相关的单词（entities or objects depicted in the image），另一类是function words or words which 没有和image content 有直接的对应关系。</p><p>本文假设， directly-related words 可以由视觉信息来决定，而第二类单词，可以由第一类单词之间的relationship 来决定。因此提出了两步策略来构建 image-grounded vocabulary。</p><ul><li><p>第一步</p><p>使用 <code>From captions to visual concepts and back</code> 中提到的方法，获取 textual concept as H.</p><p>H中words 与 image 的相关性分布：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbctjwumaj30cz02ga9z.jpg" style="zoom:33%;"></p></li><li><p>第二步</p><p>计算  full vocabulary <code>V</code> 中单词的 相关性分数，</p><p>The probability distribution of words in <code>V</code>:</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbd1t40doj30if03b3yt.jpg" style="zoom:33%;"></p></li></ul><p>本文挑选 top k words 来构成 <strong style="color:red;">the image-grounded vocabulary ($W_i$) </strong>for given image.</p><h4 id="Text-Generation-with-Vocabulary-Constraints"><a href="#Text-Generation-with-Vocabulary-Constraints" class="headerlink" title="Text Generation with Vocabulary Constraints"></a>Text Generation with Vocabulary Constraints</h4><p>提出了两个不同的策略来利用 the image-grounded vocab-ulary $W_i$ 和 word relevance distribution  $S_i^{(V)}$ : 一种，使用$W_i$ 作为 hard constraint; 另一种，聚合每个单词的相关性到 RNN cell 来生成 caption.</p><p><strong>Generator with Hard Constraint</strong></p><p>正常的方法，是生成 full vocab 的 概率分布，然后取 argmax。</p><p>但是在 hard constraint 下，生成 $W_i$ 的概率分布，再取 argmax。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbdj6x0ruj30m402haa0.jpg" style="zoom: 50%;"></p><p>对于在 $W_i$ 中没有出现的单词，打掩码：a mask operation $m_i$ is introduced  to replace the $j_{th}$ value in the vector with 1 if $w_j$ is not found in $W_i$ 。</p><p><strong>Generator with Soft Constraint </strong></p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbdt8khggj31cg0mw4i2.jpg" style="zoom:50%;"></p><p>图中展示的 image-grounded vocabulary 其实是 $S_i^{V}$</p><p>结合到RNN cell: </p><p>这个新的 RNN cell 结合了image-grounded vocabulary，因此，会更加容易生成该vocab中的单词。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1ghbe0h10qcj30pt0avmy9.jpg" style="zoom:33%;"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;p&gt;当前 普遍使用的 CNN-RNN 策略，基于整个 training dataset构建 vo
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations</title>
    <link href="http://yoursite.com/2020/07/31/Aligning-Visual-Regions-and-Textual-Concepts-for-Semantic-Grounded-Image-Representations/"/>
    <id>http://yoursite.com/2020/07/31/Aligning-Visual-Regions-and-Textual-Concepts-for-Semantic-Grounded-Image-Representations/</id>
    <published>2020-07-31T09:08:23.000Z</published>
    <updated>2020-07-31T09:09:05.184Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Understanding the image, which necessitates the acquisition of grounded image representations. </p><p>以下，提供了几种方式来 表达image content.</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha8v7kk80j316b0ciqmw.jpg"></p><p><strong style="color:blue;">[1]</strong> 基于 R-CNN 的方法可以获得 regions，但是却没有与 actual words关联起来，这将会造成两个域之间的语义不一致，并且需要由 downstream systems 自己学习 alignments。</p><p><strong style="color:blue;">[2]</strong> 此外，这些representations 仅包含局部特征，缺少全局结构信息。 这些问题 使system 难以有效地理解图像。</p><p>因此，本文提出一个 Mutual Iterative Attention (MIA) 模块，在编码阶段，从<strong style="color:red;">视觉域和语言域</strong>（解决[1]） 构建<strong style="color:red;">聚合的 image representations</strong>(解决[2])。</p><p>we perform mutual attention <strong style="color:blue;">iteratively </strong> between the two domains to realize the procedure <strong>without annotated alignment data.</strong> </p><p>The visual receptive fields gradually concentrate on salient visual regions, and the original word-level concepts are gradually merged to recapitulate corresponding visual regions. </p><p>In addition, the aligned visual features and textual concepts provide a more clear definition of the image aspects they represent. </p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha9crz1pbj30de0s4jxg.jpg" style="zoom:50%;"></p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha9gjk5vbj31h50j3jvs.jpg"></p><h3 id="textual-concepts"><a href="#textual-concepts" class="headerlink" title="textual concepts"></a>textual concepts</h3><p>从下面这篇论文中提取 text concepts</p><blockquote><p> <strong>From captions to visual concepts and back.</strong> In CVPR, 2015</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h3&gt;&lt;p&gt;Understanding the image, which necessi
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Improving Image Captioning with Conditional Generative Adversarial Nets</title>
    <link href="http://yoursite.com/2020/07/31/Improving-Image-Captioning-with-Conditional-Generative-Adversarial-Nets/"/>
    <id>http://yoursite.com/2020/07/31/Improving-Image-Captioning-with-Conditional-Generative-Adversarial-Nets/</id>
    <published>2020-07-31T07:56:54.000Z</published>
    <updated>2020-07-31T07:58:07.993Z</updated>
    
    <content type="html"><![CDATA[<p>From:  <a href="https://zhuanlan.zhihu.com/p/39890390" target="_blank" rel="noopener">GAN in Image Captioning</a></p><h3 id="Improving-Image-Captioning-with-Conditional-Generative-Adversarial-Nets"><a href="#Improving-Image-Captioning-with-Conditional-Generative-Adversarial-Nets" class="headerlink" title="Improving Image Captioning with Conditional Generative Adversarial Nets"></a><strong>Improving Image Captioning with Conditional Generative Adversarial Nets</strong></h3><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>本文提出了一种新的基于条件生成对抗网络的图像字幕框架，作为传统的基于增强学习(RL)的编解码结构的扩展。为了应对不同的目标语言的指标之间不一致的评价问题，，论文设计了两种鉴别器网络来自动地、逐步地确定生成的描述是人工描述的还是机器生成的。</p><p>生成器是采用传统图像描述的模型，<strong>在强化学习自我批判算法（SCST）下进行优化</strong>。</p><p>由于基于CNN和RNN的结构各有其优点，因此引入了两种鉴别器结构。</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha02tza7dj31im0hvtk0.jpg"></p><h4 id="CNN-discriminator："><a href="#CNN-discriminator：" class="headerlink" title="CNN discriminator："></a><strong>CNN discriminator：</strong></h4><p>（1）首先创建了一个feature map,编码了图像与句子特征。</p><p>（2）接着采用了m组有不同窗大小，核数目的卷积核来获取不同的特征，</p><p>（3）然后把所有特征作max pooling操作再联结在一起，并用一个highway架构提升性能。</p><p>（4）最后激活特征通过全连接层与sigmoid 转换来获得决策器的输出。输出在[0,1]之间。</p><h4 id="RNN-discriminator："><a href="#RNN-discriminator：" class="headerlink" title="RNN discriminator："></a><strong>RNN discriminator：</strong></h4><p>基于RNN的决策器采用了一种标准的LSTM架构，把图像特征输入到第一个LSTM，接下来输入的LSTM是输入是单词编码信息。最后通过全连接层与softmax层获得RNN决策器的输出。</p><h4 id="固定G，更新D："><a href="#固定G，更新D：" class="headerlink" title="固定G，更新D："></a>固定G，更新D：</h4><p>Discriminator的目标函数为：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha04mt3u7j31m4070tal.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha02tyrdqj31j50kxdv1.jpg"></p><h4 id="固定D，更新G"><a href="#固定D，更新G" class="headerlink" title="固定D，更新G"></a>固定D，更新G</h4><p>设计reward，以强化学习来更新generator。</p><p>在强化学习的设定下，本文采用GAN与RL结合的reward来权衡<strong style="color:red;">图像描述的保真度</strong>（在评价标准下获得高得分）与<strong style="color:red;">自然性</strong>（生成描述符合人类的风格）。</p><blockquote><p><strong style="color:blue;">这也是本文主要的创新点，结合 保真度 的评价</strong></p></blockquote><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha06jgm6tj31e409cmz3.jpg" style="zoom: 50%;"></p><h4 id="整个算法的伪代码如下："><a href="#整个算法的伪代码如下：" class="headerlink" title="整个算法的伪代码如下："></a>整个算法的伪代码如下：</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gha09ceqgkj30ns0wg13f.jpg" style="zoom: 50%;"></p><h3 id="GAN-image-captioning-task"><a href="#GAN-image-captioning-task" class="headerlink" title="GAN + image captioning task"></a>GAN + image captioning task</h3><p><strong>[1703.06029] Towards Diverse and Natural Image Descriptions via a Conditional GAN</strong></p><p><strong>[1703.10476] Speaking the Same Language Matching Machine to Human Captions by Adversarial Training</strong></p><p><strong>[1705.00930] Show, Adapt and Tell Adversarial Training of Cross-domain Image Captioner</strong></p><p><strong>[1805.00063] Improved Image Captioning with Adversarial Semantic Alignment</strong></p><p><strong>[1804.00861] Generating Diverse and Accurate Visual Captions by Comparative Adversarial Learning</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;From:  &lt;a href=&quot;https://zhuanlan.zhihu.com/p/39890390&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GAN in Image Captioning&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Improving-I
      
    
    </summary>
    
      <category term="图像描述" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
      <category term="GAN" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/GAN/"/>
    
    
      <category term="图像描述" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>GAN简介</title>
    <link href="http://yoursite.com/2020/07/30/GAN%E7%AE%80%E4%BB%8B/"/>
    <id>http://yoursite.com/2020/07/30/GAN简介/</id>
    <published>2020-07-30T13:47:25.000Z</published>
    <updated>2020-07-31T03:33:37.708Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>From: <a href="https://blog.csdn.net/shanlepu6038/article/details/84335117?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param" target="_blank" rel="noopener">GAN（一）:基本框架</a></p></blockquote><p>一个GAN包含两部分，一个generator,一个discriminator（互相对抗）<br>generator和discriminator就像是猎食者和猎物之间的关系，一个产生图片，一个辨别图片的真假，互相促进，使得最终产生的图片接近realistic</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ol><li><p>随机初始化 generator 和 discriminator</p></li><li><p>In each training iteration：</p><ul><li><p>固定generator，更新discriminator</p><p>Discriminator learns to assign high scores to real objects and low scores to generated objects.</p></li><li><p>固定discriminator， 更新generator</p></li></ul></li></ol><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh9bgltng3j30hu0djage.jpg"></p><h3 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h3><p><strong>G和D是互相促进的：</strong>G的目的是产生的图像让D感到模糊不知道该分成realistic（看起来像是现实的）还是fake（看起来是假的），D的目的是将realistic和fake的图像准确分辨。所以G产生的图像会越来越真，D的辨别能力会越来越强，最终达到一个平衡。</p><p>P<sub>data</sub> 表示真实数据的分布，P<sub>g</sub> 表示generator产生的分布，最终的目的就是让P<sub>g</sub> 的分布尽可能的和P<sub>data</sub> 相同。<br>我们用D(x)表示真实图像经过discriminator后的分数，G(z)表示随机变量z经过generator后产生的图像，那么有：D(G(z)) 表示generator产生的图像经过discriminator后的分数</p><p>第一阶段，固定 generator，更新discriminator，最大化下面对的这个式子：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh9bxggo1rj30gv01lwf1.jpg"></p><p>第二阶段，固定discriminator，更新generator，最大化下面的这个式子：</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh9bxgfqjnj30ad01s3yt.jpg"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;From: &lt;a href=&quot;https://blog.csdn.net/shanlepu6038/article/details/84335117?utm_medium=distribute.pc_relevant.none-task-blog-
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment</title>
    <link href="http://yoursite.com/2020/07/30/Align2Ground-Weakly-Supervised-Phrase-Grounding-Guided-by-Image-Caption-Alignment/"/>
    <id>http://yoursite.com/2020/07/30/Align2Ground-Weakly-Supervised-Phrase-Grounding-Guided-by-Image-Caption-Alignment/</id>
    <published>2020-07-30T08:44:12.000Z</published>
    <updated>2020-07-30T08:57:17.807Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文提出使用caption-to-image retrieval 作为下游任务，来引导 phrase localization。</p><p>第一步，学习 RoIs 与 phrases 之间的隐式对应，并利用这些匹配的RoIs来生成具有判别性的image representation。</p><p>第二步，learnedd representaion 与caption 对齐。</p><p>本文的贡献是，构建了“caption-conditioned” image encodinng，这件所有的任务都耦合在一起，并使得弱监督可以有效的引导 visual grounding。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li>监督学习的方法：依赖 region-phrase correspondence 数据。</li><li>弱监督：grounding free-form textual phrase，从image-caption pairs 这种weak correspondence中进行学习。</li></ul><p><strong>weakly supervised paradigm 的一个关键是，紧密耦合监督学习任务（image-caption matching）和无法获得显示标签的任务（region-phrase matching）。联合推理确保前者的监督损失可以有效的引导后者的学习。</strong></p><blockquote><p>[1] Andrej Karpathy and Li Fei-Fei. <strong>Deep visual-semantic alignments for generating image descriptions.</strong> In CVPR, 2015</p><p>[2] AndrejKarpathy,ArmandJoulin,andLiFFei-Fei. <strong>Deep fragment embeddings for bidirectional image sentence mapping.</strong> In NIPS,  2014</p></blockquote><p>[ 1 ] [ 2 ] 采用了 such paradigm，一般地，这种模型存在两个阶段：（1）local matching mudule: 得到 region-phrase 的隐式对应，进而生成local matching information.（2）gobal matching module: 使用（1）中得到的information来得到 image-caption matching.</p><p>需要注意的是，这种方案的设计，primary objective 是 image-caption matching 而不是 phrase matching。这种训练方式，将会放大selective regions 和 phrases 之间的相关性。举例说明：如果 第一阶段中，a small subset of phrases 存在很强的 match, 那么将会传递到第二阶段，<strong>via average pooling of the RoI–phrase matching scores</strong>, 使得image 和 caption 之间存在 high matching score。</p><p>这将会<strong>使得模型不去学习 准确的ground <strong style="color:red;">所有的</strong> phrases</strong>。分析可得，将visual grounding 作为 primary aim 不是一个有效的解决办法。<strong>这种“作弊”倾向，使模型学会了在下游任务上做得很好而不必在中间任务上做得更好。</strong></p><p>本文将这种现象称之为：“selective amplification” behavior</p><p>本文解决这个问题：我们通过提出一种novel mechanism 来解决这一问题，该机制以使两个阶段之间更紧密耦合的方式 to relay this information about the latent, inferred correspondences</p><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh8vyf8slej30wt0q914q.jpg" style="zoom: 50%;"></p><p>Our novelty lies in designing this effective transfer of  information between the supervised and unsupervised parts  of the model such that the quality of image representations for the supervised matching task is a direct consequence of  <strong>the correct localization of all phrases.</strong> </p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>our proposed model uses a novel technique that builds a discriminative image representation from the matched RoIs and uses this representation for the image-caption matching. </p><p>Specifically, the image representation that is used to match an image with a caption is conditioned only on <strong style="color:red;">the subset of image regions</strong> that align semantically with <strong style="color:red;">all the phrases</strong> in that caption. </p><p>本文认为，与标准的 pooling-based method 相比，这种结构的设计使图像字幕对的监督，成为 visual grounding 的更强学习信号。</p><h4 id="The-Local-Matching-module"><a href="#The-Local-Matching-module" class="headerlink" title="The Local Matching module"></a>The Local Matching module</h4><p>将 region 和 phrase 映射到相同的空间，然后计算 cosine similarity。</p><p>infer the matched RoI for a phrase最直接的方法是 选择top scoring box，但是，这种方案容易<strong>过拟合</strong>，因为模型经常持续选择 相同的错误region。</p><p>改进：使用attened region vector 作为matched RoI，虽然这种方法在其他的多模态任务中是有效的，但在这个任务中不是一个有效的方法。这是因为在训练过程中，多个匹配的RoI的加权平均似乎会损害匹配的RoI的辨别力（discriminativeness）。</p><p>再次改进：选择 top-k (k=3) scoring RoI candidates，然后随机的选择其中的一个作为 query phrase 的 匹配 RoI。这种策略通过在巡林过程中探索多样性的选择，进而可以增加鲁棒性。</p><h4 id="The-Local-Aggregator-module"><a href="#The-Local-Aggregator-module" class="headerlink" title="The Local Aggregator module"></a>The Local Aggregator module</h4><p>这个模块的设计比较玄学。说是为了generate a caption-conditioned representation of the image.</p><p>设计的模块：a two-layer Multilayer Perceptron (MLP) with a mean operation. </p><p>这个模块的输入，是从上一步中得到的 matched RoIs for correspondance phrases.</p><h4 id="The-Global-Matching-module"><a href="#The-Global-Matching-module" class="headerlink" title="The Global Matching module"></a>The Global Matching module</h4><p>将 caption 映射到与 上一步中得到  representation of the image 相同的空间，然后利用 cosine similarity</p><h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p><img src="http://ww1.sinaimg.cn/large/006uWRWVly1gh939pfhcgj30tq040wen.jpg" style="zoom:33%;"></p><h3 id="yaya"><a href="#yaya" class="headerlink" title="yaya"></a>yaya</h3><p>motivation 挺好的 ，但是local aggregator 的设计比较朴素。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;p&gt;本文提出使用caption-to-image retrieval 作为下游任务，来引导 ph
      
    
    </summary>
    
      <category term="Visual Grounding" scheme="http://yoursite.com/categories/Visual-Grounding/"/>
    
    
      <category term="Visual Grounding" scheme="http://yoursite.com/tags/Visual-Grounding/"/>
    
  </entry>
  
</feed>
